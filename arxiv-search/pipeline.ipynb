{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/1zuu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/1zuu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from data_pipe import *\n",
    "from configure_llm import *\n",
    "from arxiv_scraper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arxiv Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'new innovations regarding local llms oolama'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refine_query(\"New innovations regarding Local LLMs and OOLAMA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:arxiv:Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=new+innovations+regarding+local+llms+oolama&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=100\n",
      "Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=new+innovations+regarding+local+llms+oolama&id_list=&sortBy=relevance&sortOrder=descending&start=0&max_results=100\n",
      "INFO:arxiv:Got first page: 100 of 680331 total results\n",
      "Got first page: 100 of 680331 total results\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'url': 'http://arxiv.org/pdf/2310.16218v3',\n",
       "  'title': 'Knowledge Editing for Large Language Models: A Survey',\n",
       "  'text': 'Knowledge Editing for Large Language Models: A Survey\\nSONG WANG, University of Virginia, USA\\nYAOCHEN ZHU, University of Virginia, USA\\nHAOCHEN LIU, University of Virginia, USA\\nZAIYI ZHENG, University of Virginia, USA\\nCHEN CHEN, University of Virginia, USA\\nJUNDONG LI, University of Virginia, USA\\nLarge Language Models (LLMs) have recently transformed both the academic and industrial landscapes due\\nto their remarkable capacity to understand, analyze, and generate texts based on their vast knowledge and\\nreasoning ability. Nevertheless, one major drawback of LLMs is their substantial computational cost for\\npre-training due to their unprecedented amounts of parameters. The disadvantage is exacerbated when new\\nknowledge frequently needs to be introduced into the pre-trained model. Therefore, it is imperative to develop\\neffective and efficient techniques to update pre-trained LLMs. Traditional methods encode new knowledge\\nin pre-trained LLMs through direct fine-tuning. However, naively re-training LLMs can be computationally\\nintensive and risks degenerating valuable pre-trained knowledge irrelevant to the update in the model.\\nRecently, Knowledge-based Model Editing (KME), also known as Knowledge Editing (KE), has attracted\\nincreasing attention, which aims to precisely modify the LLMs to incorporate specific knowledge, without\\nnegatively influencing other irrelevant knowledge. In this survey, we aim to provide a comprehensive and\\nin-depth overview of recent advances in the field of KME. We first introduce a general formulation of KME to\\nencompass different KME strategies. Afterward, we provide an innovative taxonomy of KME techniques based\\non how the new knowledge is introduced into pre-trained LLMs, and investigate existing KME strategies while\\nanalyzing key insights, advantages, and limitations of methods from each category. Moreover, representative\\nmetrics, datasets, and applications of KME are introduced accordingly. Finally, we provide an in-depth analysis\\nregarding the practicality and remaining challenges of KME and suggest promising research directions for\\nfurther advancement in this field.\\nCCS Concepts: •Computing methodologies →Natural language processing .\\nKeywords : Model Editing, Knowledge Update, Fine-tuning, Large Language Models\\nACM Reference Format:\\nSong Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong Li. 2023. Knowledge Editing for\\nLarge Language Models: A Survey. 1, 1 (December 2023), 33 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\\n1 INTRODUCTION\\nRecently, large language models (LLMs) have become a heated topic that revolutionizes both\\nacademia and industry [ 11,97,129,149]. With the substantial factual knowledge and reasoning\\nability gained from pre-training on large corpora, LLMs have exhibited an unprecedented un-\\nderstanding of textual information, which are able to analyze and generate texts akin to human\\nexperts [ 75,77,121,123,152]. Nevertheless, one main drawback of LLMs is the extremely high\\ncomputational overhead of the training process due to the large amounts of parameters [ 54,58,155].\\nThis is exacerbated by the continuous evolvement of the world where the requirement of updating\\npre-trained LLMs to rectify obsolete information or incorporate new knowledge to maintain their\\nrelevancy is constantly emerging [ 76,81,116,120]. For example, as in Fig. 1, an outdated LLM\\nAuthors’ addresses: Song Wang, University of Virginia, Charlottesville, Virginia, USA, sw3wv@virginia.edu; Yaochen\\nZhu, University of Virginia, Charlottesville, Virginia, USA, uqp4qh@virginia.edu; Haochen Liu, University of Virginia,\\nCharlottesville, Virginia, USA, sat2pv@virginia.edu; Zaiyi Zheng, University of Virginia, Charlottesville, Virginia, USA,\\nsjc4fq@virginia.edu; Chen Chen, University of Virginia, Charlottesville, Virginia, USA, zrh6du@virginia.edu; Jundong Li,\\nUniversity of Virginia, Charlottesville, Virginia, USA, jundong@virginia.edu.arXiv:2310.16218v3  [cs.CL]  14 Dec 2023 2 Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong Li\\nFig. 1. An intuitive example of KME for efficient knowledge update of pre-trained LLMs.\\ncannot precisely describe the latest achievements of Lionel Messi, which requires explicit injection\\nof new knowledge to generate the correct answers.\\nOne feasible while straightforward strategy for updating pre-trained LLMs is through naive fine-\\ntuning [ 21,31,126,141], where parameters of pre-trained LLMs are directly optimized to encode new\\nknowledge from new data [ 6,88,99,149]. For example, various instruction-tuning-based methods\\nare proposed to fine-tune the pre-trained LLMs on the newly collected corpus in a supervised\\nlearning manner [ 89,100,137,139]. Although such fine-tuning techniques are widely used and\\ncapable of injecting new knowledge into LLMs, they are known for the following disadvantages: (1)\\nEven with several parameter-efficient strategies proposed to improve the efficiency [ 79,138,147],\\nfine-tuning LLMs may still require intensive computational resources [ 86,91,150]. (2) The fine-\\ntuned model could overfit the new data, especially when the dataset utilized for fine-tuning is small\\nin scale [ 25,87,90]. (3) More importantly, fine-tuning LLMs alters the pre-trained weights with no\\nconstraints, which risks losing valuable existing knowledge in LLMs [ 29,59,84]. These challenges\\nlimit the practicality of fine-tuning techniques in updating LLMs with new knowledge.\\nTo address the drawbacks of naive fine-tuning for updating LLMs, more attention has been\\ndevoted to Knowledge-based Model Editing1(KME). In general, KME aims to precisely modify\\nthe behavior of pre-trained LLMs to update specific knowledge without negatively influencing\\nother pre-trained knowledge irrelevant to the updates [ 104,135,145]. In KME, the update of a\\nspecific piece of knowledge in LLMs is typically formulated as an edit, such as rectifying the answer\\nto “Who is the president of the USA? ” from “ Trump ” to “ Biden ”. Regarding a specific edit, KME\\nstrategies typically modify the model output by either introducing an auxiliary network (or set of\\nparameters) into the pre-trained model [ 47,72,151] or updating the (partial) parameters to store\\nthe new knowledge [ 22,45,46,74]. Through these strategies, the KME techniques can store new\\nknowledge in memories or locate it in model parameters for updating, thereby precisely injecting\\nthe knowledge into the model. In addition, certain methods also introduce explicit loss to contain\\nthe updating process, such that the edited model maintains consistent behaviors on unmodified\\nknowledge. With these advantages, KME techniques can provide an efficient and effective way to\\nconstantly update LLMs with novel knowledge without explicit model re-training.\\nAlthough KME shares certain similarities with fine-tuning strategies, it poses unique advantages\\nin updating LLMs, which are worthy of deeper investigations. Particularly, both KME and model\\nfine-tuning seek to update pre-trained LLMs by injecting new knowledge. However, aside from\\nthis shared objective, KME focuses more on two crucial properties that cannot be easily addressed\\nby fine-tuning. (1) Locality requires that the edited model does not unintentionally influence the\\noutput of other irrelevant inputs with distinct semantics. For example, when the edit regarding the\\npresident of the USA is updated, the edited model should not alter its knowledge about the prime\\nminister of the UK. The practicality of KME methods largely relies on their ability to maintain the\\n1The concept is also termed as Knowledge Editing or Model Editing. For simplicity, we refer to it as KME in this paper. Knowledge Editing for Large Language Models: A Survey 3\\noutputs for unrelated inputs, which serves as a major difference between KME and fine-tuning [ 105].\\n(2)Generality represents whether the edited model can generalize to a broader range of relevant\\ninputs regarding the edited knowledge. Specifically, it indicates the model’s capability to present\\nconsistent behavior on inputs that share semantic similarities. For example, when the model is\\nedited regarding the president, the answer to the query of the president’s spouse should also change\\naccordingly. In practice, it is important for KME methods to ensure that the edited model can adapt\\nwell to these related input texts. In summary, due to these two unique objectives, KME remains a\\nchallenging task that requires specific strategies for satisfactory effectiveness.\\nDifferences between this survey and existing ones. Several surveys have been conducted to\\nexamine various aspects of (large) language models [ 13,34,63,65,127,149]. Nevertheless, there is\\nstill a dearth of thorough investigations that comprehensively encompass the existing literature\\nand the continuous progress in the field of LLM editing. For example, recent works [ 89,139] have\\ndiscussed the fine-tuning strategies to incorporate new knowledge in pre-trained LLMs with more\\ndata samples. However, the distinctiveness of KME, i.e., locality and generality, is not adequately\\ndiscussed, which will be thoroughly analyzed in this survey. Two additional surveys [ 35,57]\\nreview knowledge-enhanced language models. However, their main focus is on leveraging external\\nknowledge to enhance the performance of the pre-trained LLMs, without addressing the editing\\ntask based on specific knowledge. To the best of our knowledge, the most related paper to our\\nsurvey is [ 145], which provides a brief overview of KME and concisely discusses the advantages of\\nKME methods and their challenges. Nevertheless, the investigation lacks a thorough examination\\nof more details of KME, e.g., categorizations, datasets, and applications. Another recent work [ 135]\\nproposes a framework for KME that unifies several representative methods. This work focuses on the\\nimplementation of KME techniques, with less emphasis on the technical details of different strategies.\\nMore recently, a work [ 104] discusses the limitations of KME methods regarding the faithfulness of\\nedited models, while it is relatively short and lacks a more comprehensive introduction to all existing\\nmethods. Considering the rapid advancement of KME techniques, we believe it is imperative to\\nreview details of all representative KME methods, summarizing the commonalities while discussing\\nthe uniqueness of each method, and discuss open challenges and prospective directions in the\\ndomain of KME, which will facilitate the further advancement of the field.\\nContributions of this survey. This survey provides a comprehensive and in-depth analysis of\\ntechniques, challenges, and opportunities associated with the editing of pre-trained LLMs. We first\\nprovide an overview of KME tasks along with an innovative formulation. Particularly, we formulate\\nthe general KME task as a constrained optimization problem, which simultaneously incorporates\\nthe goals of accuracy ,locality , and generality . We then classify the existing KME strategies into\\nthree main categories, i.e., external memorization ,global optimzation , and local modification .\\nMore importantly, we demonstrate that methods in each category can be formulated as a specialized\\nconstrained optimization problem, where the characteristics are theoretically summarized based\\non the general formulation. In addition, we provide valuable insights into the effectiveness and\\nfeasibility of methods in each category, which can assist practitioners in selecting the most suitable\\nKME method tailored to a specific task. Our analysis regarding the strengths and weaknesses of\\nKME methods also serves as a catalyst for ongoing progress within the KME research community.\\nIn concrete, our key contributions can be summarized into three folds as follows:\\n•Novel Categorization. We introduce a comprehensive and structured categorization framework\\nto systematically summarize the existing works for LLM editing. Specifically, based on how\\nthe new knowledge is introduced into pre-trained LLMs, our categorization encompasses three\\ndistinct categories: external memorization, global optimization, and local modification, where\\ntheir commonality and differences are thoroughly discussed in this survey. 4 Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong Li\\n•In-Depth Analysis. We formulate the task of LLM editing as a constrained optimization prob-\\nlem, where methods from each category can be viewed as a special case with refined constraints.\\nFurthermore, we emphasize the primary insights, advantages, and limitations of each cate-\\ngory. Within this context, we delve deep into representative methods from each category and\\nsystematically analyze their interconnections.\\n•Future Directions. We analyze the practicality of existing KME techniques regarding a variety\\nof datasets and applications. We also comprehensively discuss the challenges of the existing\\nKME techniques and suggest promising research directions for future exploration.\\nThe remainder of this paper is organized as follows. Section 2 introduces the background\\nknowledge for LLM editing. Section 3 provides a general formulation of the KME task, which can\\nfit into various application scenarios. Section 4 provides a comprehensive summary of evaluation\\nmetrics for KME strategies, which is crucial for a fair comparison of various methods. Before\\ndelving into the specific methods, we provide a comprehensive categorization of existing methods\\ninto three classes in Section 5.1, where their relationship and differences are thoroughly discussed.\\nThen we introduce the methods from the three categories in detail, where the advantages and\\nlimitations of each category are summarized. Section 6 introduces the prevalently used public\\ndatasets. Section 7 provides a thorough introduction to various realistic tasks that can benefit from\\nKME techniques. Section 8 discusses the potential challenges of KME that have not been addressed\\nby existing techniques. This section also provides several potential directions that can inspire future\\nresearch. Lastly, we conclude this survey in Section 9.\\n2 BACKGROUND\\nIn this section, we provide an overview of the editing strategies for machine learning models and the\\nbasics of large language models (LLMs) as background knowledge to facilitate the understanding of\\ntechnical details in KME. In this survey, we use bold uppercase letters (e.g., KandV) to represent\\nmatrices, use lowercase bold letters (e.g., kandv) to represent vectors, and use calligraphic uppercase\\nletters (e.g.,XandY) to represent sets. We summarize the primary notations used in this survey\\nin Table 1 for the convenience of understanding.\\n2.1 Editing of Machine Learning Models\\nMachine learning models [ 39,49,66] pre-trained on large datasets frequently serve as foundation\\nmodels for various tasks in the real-world [ 26,114]. In practical scenarios, there is often a need to\\nmodify these pre-trained models to enhance the performance for specific downstream tasks [ 19,21,\\n92,143,154], reduce biases or undesirable behaviors [ 38,93,101,111], tailor models to align more\\nclosely with human preferences [42, 64, 78], or incorporate novel information [90, 145, 153].\\nModel Editing is a special type of model modification strategy where the modification should\\nbe as precise as possible. Nevertheless, it should accurately modify the pre-trained model to en-\\ncode specific knowledge while maximally preserving the existing knowledge, without affecting\\ntheir behavior on unrelated inputs [ 61]. First explored in the computer vision field, Bau et al . [8]\\ninvestigate the potential of editing generative adversarial networks (GAN) [ 43] by viewing an\\nintermediate layer as a linear memory, which can be manipulated to incorporate novel content.\\nAfterward, Editable Training [119] is proposed to encourage fast editing of the trained model\\nin a model-agnostic manner. The goal is to change the model predictions on a subset of inputs\\ncorresponding to misclassified objects, without altering the results for other inputs. In [ 113], the\\nauthors propose a method that allows for the modification of a classifier’s behavior by editing its\\ndecision rules, which can be used to correct errors or reduce biases in model predictions. In the\\nfield of natural language processing, several works [ 22,91] have been proposed to perform editing Knowledge Editing for Large Language Models: A Survey 5\\nTable 1. Important notations used in this survey.\\nNotations Detailed Descriptions\\n𝑥 Input (prompt) to LLMs\\n𝑦 Output of LLMs\\n(𝑥,𝑦) Input-output pair\\n𝑡=(𝑠,𝑟,𝑜) Original knowledge triple (before editing)\\n𝑠 Subject in a knowledge triple\\n𝑟 Relation in a knowledge triple\\n𝑜 Object in a knowledge triple\\n𝑡∗=(𝑠,𝑟,𝑜∗) Target knowledge triple (after editing)\\n𝑒=(𝑠,𝑟,𝑜→𝑜∗)Edit descriptor\\nX𝑒 In-scope input space\\nY𝑒 Original output space (before editing)\\nY∗\\n𝑒 Target output space (after editing)\\nE={𝑒𝑖} Set of edits\\nO𝑒 Out-scope input space\\nq(𝑙)\\n𝑖Query vector for the 𝑖-th attention head of the 𝑙-th attention module in Transformer\\nk(𝑙)\\n𝑖Key vector for the 𝑖-th attention head of the 𝑙-th attention module in Transformer\\nv(𝑙)\\n𝑖Value vector for the 𝑖-th attention head of the 𝑙-th attention module in Transformer\\nW(𝑙)\\n1,W(𝑙)\\n2Weights of the fully connected layers of the 𝑙-th attention module in Transformer\\nh(𝑙)Output from the 𝑙-th self-attention module in Transformer\\n∥ Vector concatenation\\nregarding textual information. Specifically, Zhu et al . [153] propose a constrained fine-tuning loss\\nto explicitly modify specific factual knowledge in transformer-based models [ 131]. More recent\\nworks [ 40,41] discover that the MLP layers in transformers actually act as key–value memories,\\nthereby enabling the editing of specific knowledge within the corresponding layers.\\n2.2 Large Language Models\\n2.2.1 Transformers. Transformers lie in the core of large language models (LLMs) [ 27,109,131].\\nThe fully-fledged transformer possesses an encoder-decoder architecture initially designed for the\\nneural machine translation (NMT) task [ 122]. Nowadays, transformers have found wide applications\\nin most fields of the NLP community, beyond their original purpose. Generally, a transformer\\nnetwork is constructed from multiple stacks of the self-attention module with residual connections,\\nwhich is pivotal for capturing contextual information from textual sequences. The self-attention\\nmodule is composed of a self-attention layer (SelfAtt) and a point-wise feed-forward neural\\nnetwork layer (FFN) formulated as follows:\\nh𝐴,(𝑙−1)\\n𝑖=SelfAtt𝑖\\x10\\nh(𝑙−1)\\n𝑖\\x11\\n=Softmax\\x12\\nq(𝑙)\\n𝑖\\x10\\nk(𝑙)\\n𝑖\\x11⊤\\x13\\nv(𝑙)\\n𝑖,\\nh𝐹,(𝑙−1)=FFN\\x10\\nh(𝑙−1)\\x11\\n=GELU\\x10\\nh(𝑙−1)W(𝑙)\\n1\\x11\\nW(𝑙)\\n2,h(0)=x,\\nh(𝑙)=h𝐴,(𝑙−1)+h𝐹,(𝑙−1)=\\r\\r\\n𝑖SelfAtt𝑖\\x10\\nh(𝑙−1)\\n𝑖\\x11\\n+FFN\\x10\\nh(𝑙−1)\\x11\\n,(1) 6 Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong Li\\nwhere q(𝑙)\\n𝑖,k(𝑙)\\n𝑖, and v(𝑙)\\n𝑖represent the sequences of query, key, and value vectors for the 𝑖-th\\nattention head of the 𝑙-th attention module, respectively. GELU is an activation function. They are\\ncalculated from h(𝑙−1)\\n𝑖, the𝑖-th slice of the outputs from the (𝑙−1)-th self-attention module (i.e.,\\nh(𝑙−1)), and xdenotes the input sequence of token embeddings. ∥represents vector concatenation.\\nNormalizing factors in the self-attention layer are omitted for simplicity.\\nGenerally, multi-head self-attention directs the model to attend to different parts of the sequence\\nto predict the next token. Specifically, the prediction is based on different types of relationships\\nand dependencies within the textual data, where the output h𝐴,(𝑙−1)\\n𝑖is a weighted sum of the value\\nvector of other tokens. In contrast, FFN adds new information h𝐹,(𝑙−1)\\n𝑖to the weighted sum of the\\nembeddings of the attended tokens based on the information stored in the weights of the fully\\nconnected layers, i.e., W(𝑙)\\n1andW(𝑙)\\n2. The final layer outputs of the transformer, i.e., h(𝐿), can be\\nused in various downstream NLP tasks. For token-level tasks (e.g., part-of-speech tagging [ 20]), the\\nentire hidden representation sequence h(𝐿)can be utilized to predict the target sequence. For the\\nsequence-level tasks (e.g., sentiment analysis [ 140]), the hidden representation of the last token,\\ni.e.,h(𝐿)\\n−1, can be considered as a summary of the sequence and thus used for the predictions.\\n2.2.2 Large Language Models (LLMs). Transformers with billions of parameters trained on\\nlarge corpora have demonstrated emergent ability , showcasing an unprecedented understanding\\nof factual and commonsense knowledge [ 149]. Consequently, these models are referred to as large\\nlanguage models (LLMs) to indicate their drastic distinction from traditional small-scale language\\nmodels [ 34,127]. Generally, based on the specific parts of the transformer utilized for language\\nmodeling, existing LLMs can be categorized into three classes: encoder-only LLMs, such as BERT\\n(Bidirectional Encoder Representations from Transformers) [ 66], encoder-decoder-based LLMs such\\nasT5(Text-To-Text Transfer Transformer) [ 107], and decoder-only models (also the most common\\nstructures in LLMs) such as different versions of GPT (Generative Pre-trained Transformer) [ 106]\\nandLLaMA (Large Language Model Meta AI) [129].\\n2.2.3 Training Strategies. Typically, LLMs with encoder-only structures are trained based on\\nmasked language modeling (MLM) [ 27], which randomly masks out tokens in the corpus and\\npredicts the missing tokens by attending to tokens from both sides. Therefore, token embeddings\\ncan be better learned by considering context from a broader scope. On the other hand, LLMs\\nbased on encoder-decoder and decoder-only structures utilize the causal language modeling (CLM)\\nstrategy for pre-training, which autoregressively predicts the next token by only attending to\\nprevious tokens. Therefore, they are more powerful at generating new content based on pretexts.\\nThe training of generative LLMs is mainly based on two stages, i.e., the pre-training stage and the\\nfine-tuning stage. In the first stage, LLMs are trained by language modeling (i.e., next/masked token\\nprediction) on large corpus such as Wikipedia [ 133] with commonsense and factual knowledge,\\nArXiv papers [ 10] with scientific/math knowledge, codes from GitHub [ 14] with coding knowledge,\\netc. Then during the fine-tuning stage, exemplar prompt-answer pairs (e.g., question and answers,\\ndemands and codes, description, and formula) are provided to the LLMs such that they are able\\nto answer questions based on the prompt instead of simply completing the sentence. In addition,\\nRLHF (reinforcement learning from human feedback) [ 69] is usually introduced to further fine-tune\\nthe model by learning from human ranks of different generated answers without the explicit\\nrequirement of ground truth pairs. After training, the LLM can be viewed as a database, where\\nprompts formed as natural language can be used as queries to obtain knowledge encoded in the\\nweights of transformers [13, 65]. Knowledge Editing for Large Language Models: A Survey 7\\n3 PROBLEM FORMULATION\\nIn this section, we provide a formal definition for the knowledge-based model editing (KME) task\\nfor pre-trained LLMs, where a general formulation of the KME objective is formulated to encompass\\nspecific KME strategies. The task of KME for LLMs can be broadly defined as the process of precisely\\nmodifying the behavior of pre-trained LLMs, such that new knowledge can be incorporated to\\nmaintain the currentness and relevancy of LLMs can be maintained, without negatively influencing\\nother pre-trained knowledge irrelevant to the edits. To provide a clear formulation, we present the\\ndefinitions of different terms used in KME, where the overall process is illustrated in Fig. 2.\\nEditing Target. In this survey, we represent the knowledge encoded or required to be injected into\\nthe pre-trained LLMs as a knowledge triple 𝑡=(𝑠,𝑟,𝑜), where𝑠is the subject (e.g., president of the\\nUSA),𝑟is the relation (e.g., is), and𝑜is the object (e.g., Biden ). From the perspective of knowledge\\ntriple, the objective of KME for LLMs is to modify the original knowledge triple 𝑡=(𝑠,𝑟,𝑜)\\nencoded in the pre-trained weights of the model into the target knowledge triple 𝑡∗=(𝑠,𝑟,𝑜∗),\\nwhere𝑜∗is the target object different from 𝑜. In this manner, we can define an edit as a tuple\\n𝑒=(𝑡,𝑡∗)=(𝑠,𝑟,𝑜→𝑜∗), which denotes the update of the obsolete old knowledge 𝑡into the new\\nknowledge𝑡∗.\\nFig. 2. The formulation of the KME objective.Input and Output Space. Given a\\npair of subject 𝑠and relation 𝑟, in or-\\nder to query LLMs to obtain the ob-\\nject𝑜,(𝑠,𝑟)needs to be transformed\\ninto natural language, which we de-\\nnoted as𝑥.𝑥is also referred to as\\nthe prompt in this survey. The LLM\\noutput𝑦is also textual and can be\\nconverted back to an object 𝑜as the\\nquery result. In this manner, (𝑥,𝑦)\\ncan be considered as the natural language input-output pair associated with the knowledge triple\\n𝑡=(𝑠,𝑟,𝑜). For example, the prompt 𝑥transformed from 𝑠and𝑟can be “The president of the USA is” ,\\nand𝑦is the model output “Joe Biden” . Note that due to the diversity of natural language, multiple\\n(𝑥,𝑦)pairs can be associated with the same knowledge triple 𝑡. We denote the set of textual inputs\\nthat can be associated with subject 𝑠and relation 𝑟in an edit𝑒asX𝑒=𝐼(𝑠,𝑟), referred to as\\nin-scope input space . Similarly, we define the set of textual outputs that can be associated with\\nthe object𝑜in the same edit 𝑒asY∗\\n𝑒=𝑂∗(𝑠,𝑟,𝑜∗)(i.e.,target output space ), and the original\\ntextual output space as Y𝑒=𝑂(𝑠,𝑟,𝑜)(i.e.,original output space ). Given an edit 𝑒, the aim of\\nKME is to modify the behavior of language models from Y𝑒toY∗\\n𝑒, regarding the input in X𝑒. To\\naccommodate the scenarios where multiple edits are performed, we can define the union of X𝑒\\nover a set of editsE={𝑒1,𝑒2,...}asXE=Ð\\n𝑒∈EX𝑒. Similarly, we can define YE=Ð\\n𝑒∈EY𝑒and\\nY∗\\nE=Ð\\n𝑒∈EY∗\\n𝑒.\\nFormulation. We denote the pre-trained LLM with parameter 𝜙as𝑓:X→Y and the edited\\nmodel with updated parameter 𝜙∗as𝑓∗:X→Y∗. The objective of knowledge-based model\\nediting is to precisely update the pre-trained LLM 𝑓into𝑓∗according to edits in the edit set E\\nsuch that for each edit 𝑒and for each 𝑦∈Y𝑒, the changes to the input-output pairs irrelevant to\\nthe edits is minimized. The problem of KME can be formulated as follows: 8 Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong Li\\nDefinition 1. The objective for KME on a series of edits Eis represented as follows:\\nminE𝑒∈EE𝑥,𝑦∗∈X𝑒,Y∗𝑒L(𝑓∗(𝑥),𝑦∗),where𝑓∗=𝑀(𝑓;E),\\ns.t.𝑓∗(𝑥)=𝑓(𝑥),∀𝑥∈X\\\\XE,(2)\\nwhereLis a specific loss function that measures the discrepancy between the model output 𝑓∗(𝑥)\\nand𝑦∗from the desirable response set Y∗\\n𝑒.𝑀(𝑓;E)denotes the modification applied to 𝑓based on the\\ndesirable editsE.\\nFrom the above definition, we can summarize two crucial perspectives regarding the objective of\\nKME: (1) Generality , which requires that the correct answers in the target output space Y∗\\n𝑒can be\\nachieved, provided prompts in the in-scope input space, i.e., X𝑒, where the target knowledge triple\\n𝑡∗∈𝑒can be updated into the pre-trained model; (2) Locality , which requires the consistency of\\nmodel output regarding unrelated input, i.e., X\\\\XE, where valuable pre-trained knowledge can\\nbe maximally preserved after the editing. Here, we note that locality is especially important for\\nediting LLMs, as the knowledge that needs to be updated often occupies only a small fraction of all\\nknowledge encompassed by the pre-trained model. In other words, the output of an edited model\\nregarding most input prompts should remain consistent with the output before editing.\\n4 EVALUATION METRICS\\nBefore introducing the taxonomy of KME and the exemplar methods in detail, in this section,\\nwe first discuss various metrics commonly used to evaluate the effectiveness of different KME\\nstrategies from varied perspectives. We summarize the metrics to facilitate the understanding in\\nterms of the properties and advantages of different methods.\\n4.1 Accuracy\\nAccuracy is the most straightforward measure for the effectiveness of KME techniques, which is\\ndefined as the success rate of editing in terms of a specific set of pre-defined input-output pairs\\nE={𝑥𝑖,𝑦∗\\n𝑖}associated with all the edited knowledge. Accuracy can be easily defined to evaluate the\\nperformance of KME on classification tasks, e.g., fact checking [ 91,102], where the answers 𝑦are\\ncategorical. If we define the prompt and ground truth related to an edit 𝑒as𝑥𝑒and𝑦∗\\n𝑒, respectively,\\nthe metric of the accuracy of an edited model 𝑓∗can be formulated as follows:\\nAcc(𝑓∗;E)=E𝑒∈E 1{𝑓∗(𝑥𝑒)=𝑦∗\\n𝑒}. (3)\\nSince accuracy is defined on a deterministic set of prompts-answer pairs E={𝑥𝑖,𝑦∗\\n𝑖}, it provides a\\nfair comparison between KME methods. Nevertheless, it is challenging to evaluate the practicality\\nof KME methods with accuracy, as there is no consensus on how to design the E, especially when\\nthe task needs to output a long sequence such as question answering or text generation [ 29,86,87].\\n4.2 Locality\\nOne crucial property for the KME strategies is locality , which measures the capability of the edited\\nmodel𝑓∗to preserve the pre-trained knowledge in 𝑓irrelevant to the edits in E. We note that in\\nmost cases of KME, the number of required edits makes for an extremely small fraction of the entire\\nknowledge learned and preserved in the pre-trained LLMs. Consequently, the locality measurement\\nis of great importance in assessing the capability of edited models to preserve unrelated knowledge.\\nGiven an edit 𝑒, an edited model 𝑓∗, and the original pre-trained model 𝑓, the locality of 𝑓∗can be\\ndefined as the expectation of unmatched agreement between the edited model and unedited model\\nfor out-scope inputs, which can be defined as follows:\\nLoc(𝑓∗,𝑓;𝑒)=E𝑥∉X𝑒 1{𝑓∗(𝑥)≠𝑓(𝑥)}. (4) Knowledge Editing for Large Language Models: A Survey 9\\nWe can also consider the locality regarding the entire edit set E, which can be defined as follows:\\nLoc(𝑓∗,𝑓;E)=E𝑥∉XE 1{𝑓∗(𝑥)≠𝑓(𝑥)},whereXE=Ø\\n𝑒∈EX𝑒. (5)\\nAlthough the above metric measures the overall locality of 𝑓∗based on all inputs that are not in\\nXE, it is difficult to compute in realistic scenarios, as the entire input space can be excessively\\nlarge or even infinite. Therefore, existing methods generally resort to alternative solutions that\\npre-define the specific range of out-scope inputs to calculate the expected difference in Eqs. (4) and\\n(5). For example, in SERAC [91], the authors generate hard out-scope examples from the dataset\\nzsRE [71] by selectively sampling from training inputs with high semantic similarity with the\\nedit input, based on embeddings obtained from a pre-trained semantic embedding model. If we\\ndenote the out-scope input space related to the input X𝑒asO𝑒, we can similarly define the feasible\\nout-scope input space for multiple edits as OE=Ð\\n𝑒∈EO𝑒. In this manner, we define a specific\\nmetric of locality of 𝑓∗regardingEas follows:\\nLoc(𝑓∗,𝑓;O𝑒)=E𝑥∈O𝑒 1{𝑓∗(𝑥)≠𝑓(𝑥)}, (6)\\nLoc(𝑓∗,𝑓;OE)=E𝑥∈OE 1{𝑓∗(𝑥)≠𝑓(𝑥)},whereOE=Ø\\n𝑒∈EO𝑒. (7)\\n4.3 Generality\\nAside from locality, another crucial property to measure the effectiveness of KME is generality ,\\nwhich indicates the capability of the edited model 𝑓∗to correctly respond to similar prompts. This\\nrequires the generalization of the updated knowledge to other in-scope inputs that do not appear\\nin the training set while conveying similar or related meanings. Ensuring the generality of edited\\nmodels prevents the edited model from overfitting to a particular input. Specifically, in the scenarios\\nof knowledge-based model editing, the inherent diversity of natural language determines that\\nvarious in-scope inputs 𝑥can correspond to a specific knowledge triple 𝑡[135]. These semantically\\nequivalent inputs can involve differences in aspects such as syntax, morphology, genre, or even\\nlanguage. Existing works mostly pre-define a specific in-scope input space of each edit via different\\nstrategies. For example, in the CounterFact dataset proposed in ROME [86], the authors utilize\\nprompts that involve distinct yet semantically related subjects as the in-scope input. In general, the\\ngenerality of an edited model 𝑓∗is defined as the expectation of exact-match agreement between\\nthe output of the edited model and true labels for in-scope inputs, which can be defined on either\\nan edit𝑒or the edit setEas follows:\\nGen(𝑓∗;𝑒)=E𝑥∈X𝑒 1{𝑓∗(𝑥)∈Y∗\\n𝑒}, (8)\\nGen(𝑓∗;E)=E𝑥∈XE 1{𝑓∗(𝑥)∈Y∗\\n𝑒},whereXE=Ø\\n𝑒∈EX𝑒. (9)\\n4.4 Retainability\\nRetainability characterizes the ability of KME techniques to preserve the desired properties of\\nedited models after multiple consecutive edits . In the presence of ever-evolving information,\\npractitioners may need to frequently update a conversational model (i.e., sequential editing). Such\\na KME setting requires that the model not forget previous edits after each new modification. It is\\nessential to distinguish retainability from scalability , which evaluates the model’s ability to handle a\\nvast number of edits. In contrast, retainability assesses the consistent performance of the model after\\neach individual edit, presenting a more challenging objective to achieve. Recently, T-Patcher [59]\\nfirst explores the sequential setting of KME and observes that many existing approaches significantly\\nfall short in terms of retainability. In SLAG [48], the authors also discover a significant drop in 10 Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong Li\\nFig. 3. The detailed categorization of knowledge-based model editing (KME) techniques for LLMs.\\nediting performance when multiple beliefs are updated continuously. To thoroughly assess the\\nretainability of an edited language model 𝑓∗, we define it as follows:\\nRet(𝑀;E)=1\\n|E|− 1|E|− 1∑︁\\n𝑖=1Acc(𝑀(𝑓;{𝑒1,𝑒2,...,𝑒𝑖+1}))− Acc(𝑀(𝑓;{𝑒1,𝑒2,...,𝑒𝑖})) (10)\\nwhere Accis the accuracy measurement, |E|is the number of edits in the edit set, and 𝑀denotes\\nthe editing strategy that modifies the pre-trained model 𝑓into𝑓∗with𝑖/𝑖+1consecutive edits\\n{𝑒1,𝑒2,...,𝑒𝑖,(𝑒𝑖+1)}. The retainability metric aims to quantify the effect of applying consecutive\\nedits to a model and measures how the performance will change the editing strategy 𝑀, where a\\nhigher retainability means that after each edit, the less the change in the overall performance of\\nthe edited model 𝑓∗is required.\\n4.5 Scalability\\nThescalability of an editing strategy refers to its capability to incorporate a large number of edits\\nsimultaneously. Recently, several works have emerged that can inject multiple new knowledge into\\nspecific parameters of pre-trained LLMs. For instance, SERAC [91] can perform a maximum of 75\\nedits. In addition, MEMIT [87] is proposed to enable thousands of edits without significant influence\\non editing accuracy. When there is a need to edit a model with a vast number of edits concurrently,\\nsimply employing the current knowledge-based model editing techniques in a sequential manner\\nis proven ineffective in achieving such scalability [ 145]. To effectively evaluate the scalability of\\nedited language models, we define the scalability of an edited model as follows:\\nSca(𝑀;E)=E𝑒∈EAcc(𝑀(𝑓;𝑒))−Acc(𝑀(𝑓;E)), (11)\\nwhere Acc(𝑀(𝑓;E))denotes the accuracy of the edited model after conducting all edits in E,\\nwhereas Acc(𝑀(𝑓;𝑒))is the accuracy of only performing the edit 𝑒.Scademonstrates the model\\nperformance and practicality in the presence of multiple edits. Nevertheless, we note that baseline\\nvalue Acc(𝑀(𝑓;{𝑒}))is also important in evaluating the scalability of various models. This is\\nbecause, with higher accuracy for each 𝑒, the retainment of such performance after multiple edits\\nis more difficult. Therefore, we further define the relative version of Eq. (11) as follows:\\nSca𝑟𝑒𝑙(𝑀;E)=(E𝑒∈EAcc(𝑀(𝑓;{𝑒}))− Acc(𝑀(𝑓;E)))/E𝑒∈EAcc(𝑀(𝑓;{𝑒})). (12)\\nThe introduced scalability measurement further considers the magnitude of the original accuracy\\nto provide a fairer evaluation. Knowledge Editing for Large Language Models: A Survey 11\\nTable 2. Comparisons among different categories of methods across the four metrics. The properties that\\neach type of strategy focuses on are highlighted with check marks.\\nCategory Strategy Locality Generality Retainability Scalability References\\nExternal MemorizationMemory-based ✔ ✔ [72, 84, 91, 93, 150, 151]\\nExtension-based ✔ ✔ ✔ [29, 47, 59]\\nGlobal OptimizationConstrained Fine-tuning ✔ ✔ [15, 70, 94, 119, 153],\\nIntermediate Fine-tuning ✔ ✔ ✔ [18, 25, 46, 48, 90]\\nLocal ModificationGradient-based ✔ ✔ ✔ [22, 74, 86, 87, 144, 146]\\nPrompt-based ✔ ✔ [45, 82, 86]\\n5 METHODOLOGIES\\nIn this section, we introduce existing knowledge-based model editing (KME) strategies in detail.\\nWe first provide an innovative taxonomy of existing KME strategies based on how and where the\\nnew knowledge is injected into the pre-trained LLMs, where the advantages and drawbacks are\\nthoroughly discussed. We then introduce various methods from each category, with an emphasis\\non analyzing the technical details, insights, shortcomings, and their relationships.\\n5.1 Categorization of KME Methods\\nFaced with the rapid deprecation of old information and the emergence of new knowledge, various\\nKME methodologies have been proposed to update the pre-trained LLMs to maintain their updat-\\nedness and relevancy. KME ensures that new knowledge can be efficiently incorporated into the\\npre-trained LLMs without negatively influencing the pre-trained knowledge irrelevant to the edit.\\nIn this survey, we categorize existing KME methods into three main classes as follows:\\n•External Memorization -based methods leverage an external memory to store the new\\nknowledge for editing without modifying the pre-trained weights, where the pre-trained knowl-\\nedge can be fully preserved in the LLM weights. By storing new knowledge with external\\nparameters, the memory-based strategies enable precise representation of new knowledge with\\ngood scalability, as the memory is easily extensible to incorporate new knowledge.\\n•Global Optimization -based methods seek to achieve generalizable incorporation of the\\nnew knowledge into pre-trained LLMs via optimization with the guidance of new knowledge,\\nwhere tailored strategies are introduced to limit the influence of other pre-trained knowledge,\\ndistinguishing it from naive fine-tuning. Nevertheless, these methods may fall short in editing\\nefficiency when applied to LLMs due to the large number of parameters to be optimized.\\n•Local Modification -based methods aim to locate the related parameters of specific knowledge\\nin LLMs and update it accordingly to incorporate the new knowledge relevant to the edit. The\\nmain advantage of local modification is the possibility of only updating a small fraction of model\\nparameters, thereby providing considerable memory efficiency compared to memorization-based\\nmethods and computational efficiency compared to global optimization.\\nThe above categorization is achieved based on where (e.g., external parameters or internal\\nweights) and how (e.g., via optimization or direct incorporation) new information is introduced\\ninto the LLM during editing. Specifically, methods from each category exhibit different strengths\\nand weaknesses regarding the four crucial evaluation metrics introduced in Sec. 4. For example,\\nexternal memorization prevails in scenarios that require massive editing while the computational\\nresources are limited, as the size of the memory is controllable to fit into different requirements. On\\nthe other hand, global optimization is advantageous when practitioners focus more on the generality\\nof edited knowledge, as the optimization can promote the learning of relevant knowledge [ 2]. The 12 Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong Li\\nFig. 4. The illustration of three categories of KME methods: External Memorization ,Global Optimization ,\\nand Local Modification .\\ntaxonomy is visually illustrated in Fig. 3, and a more detailed demonstration of each category is\\npresented in Fig. 4. The specific characteristics of all methods are summarized in Table 2.\\n5.2 External Memorization\\n5.2.1 Overview .The editing approaches via external memorization aim to modify the current\\nmodel𝑓𝜙(with parameter 𝜙) via introducing external memory represented by additional trainable\\nparameters𝜔that encodes the new knowledge, resulting in an edited LLM model 𝑓∗\\n𝜙,𝜔. The rationale\\nbehind the external memorization strategy is that storing new knowledge in additional parameters\\nis intuitive and straightforward to edit the pre-trained LLMs with good scalability, as the parameter\\nsize can be expanded to store more knowledge. In addition, the influence on the pre-trained\\nknowledge can be minimized as this strategy does not alter the original parameters 𝜙. Based on the\\ngeneral formulation of KME in Eq. (2), the objective of external memorization approaches can be\\nformulated as follows:\\nminE𝑒∈EE𝑥,𝑦∗∈X𝑒,Y∗𝑒L(𝑓∗\\n𝜙,𝜔(𝑥),𝑦∗),where𝑓∗\\n𝜙,𝜔=𝑀(𝑓𝜙,𝜔;E),\\ns.t.𝑓∗\\n𝜙,𝜔(𝑥)=𝑓𝜙(𝑥),∀𝑥∈X\\\\XE,(13)\\nwhere𝑓𝜙denotes the LLM before editing with the pre-trained parameter 𝜙, and𝑓∗\\n𝜙,𝜔denotes the\\nedited LLM with 𝜙and additional parameter 𝜔as the external memorization. Moreover, based\\non whether the introduced parameters are directly incorporated into the model process or not,\\nexternal memorization strategies can be divided into two categories, i.e., memory-based methods\\nandextension-based methods .\\n5.2.2 Memory-based Strategies .In memory-based strategies, the external memory, outside the\\nintrinsic architecture of the pre-trained LLM, functions as a repository to store edited knowledge.\\nThe LLM can access and update this memory as required during inference.\\nOne exemplar work is SERAC [91], which stores the edited samples 𝑥,𝑦∗∈X𝑒,Y∗\\n𝑒in a cache\\nwithout performing modifications on the original model. When presented with a new prompt\\n𝑥′, SERAC uses a scope classifier to determine whether the prompt falls within the scope of any\\ncached instances. If yes, the desirable output 𝑦′associated with the new prompt 𝑥′is predicted via\\na counterfactual model 𝑓𝑐which utilizes the most relevant edit example as follows:\\n𝑓∗\\n𝜙,𝜔(𝑥)=\\x1a𝑓𝜙(𝑥), if𝑥is not in scope of any edit ,\\n𝑓𝑐(𝑥,E),otherwise.(14)\\nSERAC is a gradient-free approach to KME without relying on gradients of the target label 𝑦∗\\nw.r.t. the pre-trained model parameters. In addition to using memory as an external repository, the\\ndesirable edits can also be stored in the form of human feedback. For example, Language Patch [93] Knowledge Editing for Large Language Models: A Survey 13\\nperforms editing by integrating patches in natural language, and MemPrompt [84] involves\\nhuman feedback prompts to address the issue of lacking commonsense knowledge regarding a\\nparticular task. An integral feature of the Language Patch [ 93] framework is its ability to empower\\npractitioners with the capability to create, edit, or remove patches without necessitating frequent\\nmodel re-training. This trait not only streamlines the development process but also enhances the\\nadaptability and versatility of the edited model. To enable the automatic correction in memory,\\nMemPrompt [ 84] equips the language model with a memory bank containing corrective feedback to\\nrectify misunderstandings. Specifically, MemPrompt leverages question-specific historical feedback\\nto refine responses on novel and unencountered instances through prompt adjustments.\\nInKAFT [72], controllability is achieved through the utilization of counterfactual data augmen-\\ntations. In this approach, the entity representing the answer within the context is substituted with\\nan alternative but still plausible entity. This substitution is intentionally designed to introduce a\\nconflict with the genuine ground truth, thereby enhancing the controllability and robustness of\\nLLMs with respect to their working memory. The aim is to ensure that LLMs remain responsive to\\npertinent contextual information while filtering out noisy or irrelevant data.\\nIn addition to relying on parameter-based memory, recent works also leverage prompting\\ntechniques of LLMs, e.g., in-context learning [ 30] and chain-of-thought prompting [ 142], to promote\\nediting performance of external memorization. Specifically, IKE [150] introduces novel factual\\ninformation into a pre-trained LLM via in-context learning, where a set of 𝑘demonstrations, i.e.,\\n𝜔={𝑥𝑖,𝑦∗\\n𝑖}𝑘\\n𝑖=1, is selected as the reference points. These demonstrations will alter the prediction\\nof a target factual detail when the input is influenced by an edit. Particularly, IKE guarantees a\\nbalance between generality and locality via storing factual knowledge as prompts. The process can\\nbe formulated as follows:\\n𝑓∗\\n𝜙,𝜔(𝑥)=𝑓𝜙(𝜔∥𝑥),where𝜔={𝑥𝑖,𝑦∗\\n𝑖}𝑘\\n𝑖=1. (15)\\nHere∥denotes the concatenation of the reference points in 𝜔and the input 𝑥, which follows an\\nin-context learning manner. Note that in this process, the framework first transforms all new facts\\ninto natural language to input them into LLMs. Although IKE effectively edits factual knowledge\\nvia in-context learning, it cannot solve more complex questions that involve multiple relations. To\\ndeal with this, MQuAKE [151] first explores the evaluation of the editing effectiveness in language\\nmodels regarding multi-hop knowledge. For example, when editing knowledge about the president\\nof the USA, the query regarding the president’s children should change accordingly. Specifically,\\nMQuAKE aims to assess whether edited models can correctly answer questions that represent\\nsuch chains of facts. The authors propose to enable editing based on multi-hop knowledge by\\nbreaking down each question into iterative subquestions, such that the base model generates a\\nprovisional answer. Subsequently, each subquestion is used to retrieve the most pertinent fact from\\nthe edited fact memory. The language model then assesses whether the retrieved fact contradicts\\nthe provisional answer and adjusts the prediction accordingly.\\n5.2.3 Extension-based Strategies .Extension-based strategies utilize supplementary param-\\neters to assimilate modified or additional information into the original language model. These\\nsupplementary parameters are designed to represent the newly introduced knowledge or necessary\\nadjustments tailored for specific tasks or domains. Different from memory-based methods, by\\nincorporating new parameters into the language model, extension-based approaches can effectively\\nleverage and expand the model’s functionality.\\nExtension-based methods can be implemented through various means, and one representative\\nway is to modify the Feed-forward Neural Network (FFN) output. For example, CALINET [29] uses\\nthe output from sub-models fine-tuned specifically on factual texts to refine the original FFN output 14 Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong Li\\nproduced by the base model. Another technique T-Patcher [59] introduces a limited number of\\ntrainable neurons, referred to as “patches,” in the final FFN layer to alter the model’s behavior while\\nretaining all original parameters to avoid reducing the model’s overall performance. Generally,\\nthese methods that refine the structure of FFN can be formulated as follows:\\nFFN(h)=GELU(hW1)W2+GELU\\x00h·k𝑝+𝑏𝑝\\x01·v𝑝, (16)\\nwhere k𝑝is the patch key, v𝑝is the patch value, and 𝑏𝑝is the patch bias scalar. The introduced\\npatches are flexible in size and can be accurately activated to edit specific knowledge without\\naffecting other model parameters.\\nAlternatively, a different technique involves integrating an adapter into a specific layer of a\\npre-trained model. This adapter consists of a discrete dictionary comprising keys and values, where\\neach key represents a cached activation generated by the preceding layer and each corresponding\\nvalue decodes into the desired model output. This dictionary is systematically updated over time.\\nIn line with this concept, GRACE [47] introduces an adapter that enables judicious decisions\\nregarding the utilization of the dictionary for a given input, accomplished via the implementation\\nof a deferral mechanism. It is crucial to achieve a balance between the advantages of preserving\\nthe original model’s integrity and the practical considerations associated with storage space when\\nimplementing this approach.\\n5.2.4 Summary .The eternal memorization methodology operates by preserving the parameters\\nwithin the original model while modifying specific output results through external interventions\\nvia memory or additional model parameters. One notable advantage of this approach is its minimal\\nperturbation of the original model, thereby ensuring the consistency of unedited knowledge. It\\nallows for precise adjustments without necessitating a complete overhaul of the model’s architecture.\\nHowever, it is imperative to acknowledge a trade-off inherent in this methodology. Its efficacy is\\ncontingent upon the storage and invocation of the edited knowledge, a factor that leads to concerns\\nregarding storage capacity. Depending on the scale of knowledge to be edited, this approach may\\nentail substantial storage requisites. Therefore, cautiously seeking a balance between the advantages\\nof preserving the original model’s integrity and the practical considerations of storage capacity\\nbecomes a pivotal concern when employing this particular approach.\\n5.3 Global Optimization\\n5.3.1 Overview .Different from external memorization-based methods that introduce new pa-\\nrameters to assist the editing of pre-trained LLMs, there also exist branches of works that do not\\nrely on external parameters or memory. Concretely, global optimization strategies aim to inject\\nnew knowledge into LLMs by updating all parameters, i.e., 𝜙in Eq. (13). Through fine-tuning\\nmodel parameters with specific designs to ensure the preservation of knowledge irrelevant to the\\ntarget knowledge 𝑡∗, the LLMs are endowed with the ability to absorb new information without\\naltering unedited knowledge. Generally, the goal of global optimization methods can be formulated\\nas follows:\\nminE𝑒∈EE𝑥,𝑦∗∈X𝑒,Y∗𝑒L(𝑓𝜙∗(𝑥),𝑦∗),where𝑓𝜙∗=𝑀(𝑓𝜙;E),\\ns.t.𝑓𝜙∗(𝑥)=𝑓𝜙(𝑥),∀𝑥∈X\\\\XE,(17)\\nwhere𝑓𝜙denotes the LLM before editing with the pre-trained parameter 𝜙, and𝑓𝜙∗denotes the\\nedited LLM with updated parameter 𝜙∗. Generally, these methods focus more on the precision\\nand generality of desirable knowledge, as the fine-tuning process ensures that the LLMs achieve\\nsatisfactory results regarding the edits and relevant knowledge. Nevertheless, as fine-tuning affects\\nall parameters, they cannot easily preserve the locality of edited models, i.e., maintaining consistent\\noutput for unedited knowledge [ 145]. In practice, directly applying fine-tuning strategies typically Knowledge Editing for Large Language Models: A Survey 15\\nexhibits suboptimal performance on KME due to overfitting concerns [ 87,135]. Furthermore,\\nfine-tuning large language models is also time-consuming and lacks scalability for multiple edits.\\nTherefore, recently, motivated by these two challenges in fine-tuning, several global optimization\\nworks have been proposed and can be categorized as constrained fine-tuning methods and\\nintermediate fine-tuning methods .\\n5.3.2 Constrained Fine-tuning .Constrained fine-tuning strategies generally apply specific\\nconstraints to prevent updating on non-target knowledge in {X\\\\XE,Y\\\\YE}. In this manner, the\\nobjective in Eq. (17) is transformed into a constrained optimization problem:\\nminE𝑒∈EE𝑥,𝑦∗∈X𝑒,Y∗𝑒L(𝑓𝜙∗(𝑥),𝑦∗),where𝑓𝜙∗=𝑀(𝑓𝜙;E),\\ns.t.∥L(𝑓𝜙∗(𝑥),𝑦)−L(𝑓𝜙(𝑥),𝑦)∥≤𝛿,∀𝑥,𝑦∈X\\\\XE,Y\\\\YE,(18)\\nwhere𝜙,𝜙∗are the parameters before and after updating, respectively. 𝛿is a scalar hyper-parameter\\nto restrict the difference between losses of 𝑓𝜙∗and𝑓𝜙. The constraint in Eq. (18) restricts the change\\nof the edited model on unmodified knowledge. Zhu et al. [ 153] first propose an approximate\\noptimization constraint that is easier for implementation and computation:\\nminE𝑒∈EE𝑥,𝑦∗∈X𝑒,Y∗𝑒L(𝑓𝜙∗(𝑥),𝑦∗),where𝑓𝜙∗=𝑀(𝑓𝜙;E),\\ns.t.∥𝜙∗−𝜙∥≤𝛿.(19)\\nThe updates are regularized by restricting the norm of parameters before and after updating.\\nAlthough restricting the norm is helpful in preventing the forgetting of original knowledge, the\\nfine-tuning process can be less effective. To deal with this, RecAdam [15], in addition to the norm\\nconstraint, applies an annealing technique to control the ratio between the parameter norm and\\nthe fine-tuning loss as follows:\\nL𝑡𝑜𝑡𝑎𝑙=𝜆(𝑡)L𝐹𝑇+(1−𝜆(𝑡))∥𝜙∗−𝜙∥,where𝜆(𝑡)=1\\n1+exp(−𝑘·(𝑡−𝑡0)). (20)\\nHere𝑘and𝑡0are hyper-parameters. 𝑡is the number of fine-tuning steps. Such a design enables a\\ngradual fine-tuning process that prevents massive parameter updates at the beginning. Motivated\\nby the intuition of regularization to preserve original knowledge, PPA [70] employs LoRA [ 56]\\nin the feed-forward (FFN) layers of the transformer decoder. LoRA is proposed to train the ex-\\npansion/reduction matrix, instead of the model parameter 𝜙, to improve training speed by only\\nupdating parameters with a low intrinsic rank via dimensionality reduction. PPA leverages plug-in\\nmodules trained with constraints via LoRA to keep original knowledge intact. Moreover, the au-\\nthors assess whether the content of the inputs falls within the scope of XEusing the K-adapter\\nmodule [ 136], and redirect such inputs to the new plug-in modules. This information is then used\\nto determine whether to employ LoRA within the FFN layers. To preserve original knowledge,\\nF-Learning (Forgetting before Learning) [ 94] proposes to learn knowledge parameters Δ𝜙that\\nindicates old knowledge to be forgotten, defined as follows:\\n𝜙∗=𝜙−𝜆Δ𝜙,where Δ𝜙=FT(𝜙;K𝑜𝑙𝑑)−𝜙. (21)\\nHereK𝑜𝑙𝑑denotes the dataset composed of old knowledge that we desire to forget, and FT(𝜙;K𝑜𝑙𝑑)\\nis the supervised fine-tuning process of parameters 𝜙on datasetK𝑜𝑙𝑑.𝜆is a hyper-parameter used\\nto control the rate of forgetting. Based on the assumption that subtracting the parameters Δ𝜙from\\n𝜙can help the model forget this part of old knowledge [ 61], F-Learning defines the forgetting\\nprocess as a subtraction operation to obtain the updated model parameter 𝜙∗.\\nOn the other hand, other works also resort to meta-learning [ 36,130] to apply more flexible\\nconstraints. Meta-learning addresses the issue of overfitting by training a model that can quickly\\nadapt to new tasks [ 55]. By exposing the model to a variety of tasks during training, meta-learning 16 Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong Li\\nimproves the model’s ability to generalize from limited data and reduces the risk of overfitting\\nindividual tasks [ 60]. In the scenario of KME, the optimal model parameters 𝜙∗should minimize\\nthe expected loss over a variety of meta-tasks [108]:\\n𝜙∗=argmin\\n𝜙E𝐷∼D[L𝜙(𝐷)], (22)\\nwhereDcorresponds to the sample set for each meta-task 𝐷. Moreover, each meta task 𝐷contains\\nmultiple(𝑥∗,𝑦∗)pairs for editing. In practice, such methods often introduce additional objective\\nfunctions or networks to regulate parameter updates. As a typical meta-learning method for KME,\\nEditable Training [119] focuses on effectively rectifying errors within models while preserving\\ntheir performance on other irrelevant data instances. Following a model-agnostic training manner,\\nthe authors introduce additional constraints to restrict parameter updates in a different way. Specif-\\nically, the loss function is separated into L𝑏𝑎𝑠𝑒(task-specific objective function), L𝑒𝑑𝑖𝑡(computed\\non the edit setXE), andL𝑙𝑜𝑐𝑎𝑙 (computed on samples in X\\\\XE). Moreover, the models are updated\\nin a meta-learning manner, where 𝑘steps of gradient descent would be applied for parameters\\nbefore computing the objective function.\\n5.3.3 Intermediate Fine-tuning Strategies .While constrained fine-tuning techniques have\\ndemonstrated remarkable efficacy in a variety of NLP tasks [ 7,143,155], they still exhibit instability\\nand high computational cost when applied to KME, primarily due to the necessity of altering\\nall parameters [ 145]. A potential solution to address this challenge is to utilize an intermediate\\nmodel to obtain the updated parameters in an efficient manner. Such an intermediate model is\\nrequired to maintain significantly fewer parameters to ensure efficiency [ 18]. In general, recent\\nworks have widely adopted the Hyper-Network [46] as the intermediate model. Specifically, the\\nHyper-Network is a small network that generates the weights for a larger network, referred to as\\nthe main network. Specifically, the Hyper-Network takes inputs that contain information about\\nthe structure of the weights and generates the weights for layers in the main network. With the\\ngenerated weights, the main network is updated to map input data to desired output targets. The\\nupdating process for the main network, denoted as 𝜙, can be defined as follows:\\n𝜙∗=𝜙+Δ𝜙,\\nΔ𝜙=H(∇𝜙L(𝑓𝜙(𝑥),𝑦∗)),where𝑥,𝑦∗∈XE,Y∗\\nE,(23)\\nwhere H(·)denotes the hyper-network. Δ𝜙is the weight deviation calculated by the hyper-network.\\nAccording to a recent study [ 132], task-specific Hyper-Networks (i.e., networks that generate target\\nmodel weights based on task attributes) are effective in mitigating catastrophic forgetting issues.\\nTherefore, such methods are suitable for the setting of KME, which requires the preservation of\\nunedited knowledge.\\nRecently, researchers have proposed to adopt hyper-networks in various ways for parameter\\nupdates in KME. As a classic example, KE[25] first proposes to edit knowledge and rectify erroneous\\nor unexpected predictions without expensive fine-tuning. Specifically, it trains a hyper-network\\nvia constrained optimization to modify facts without affecting pre-trained knowledge irrelevant to\\nthe edit. The trained hypernetwork is then used to predict the weight update at the inference time.\\nBased on KE, SLAG [48] further appends metrics for two types of input texts: (1) Inputs that are\\nnot in the desired edit set XEbut logically related to E; (2) Inputs that share a formal resemblance\\nto edited knowledge, but do not lead to changes in the prediction outcomes.\\nHowever, hyper-networks are generally not capable of updating large language models due to\\nthe massive parameter size. To tackle this challenge, MEND [90] adopts a mechanism referred to as\\ngradient decomposition. In particular, it leverages small auxiliary editing networks to transform the\\ngradients obtained by standard fine-tuning into edits of weights in a pre-trained model. As gradients Knowledge Editing for Large Language Models: A Survey 17\\nare generally high-dimensional objects, a low-rank decomposition of the gradients is utilized to\\nachieve the transformation. Particularly, MEND parameterizes the gradient mapping functions as\\nMLPs with a single hidden layer, such that a significantly small number of parameters are required,\\ncompared with the edited models. In this manner, MEND enables fast model editing that can operate\\non considerably large pre-trained language models. Moreover, KGEditor [18] proposes to combine\\nthe benefits of memory-based methods and hyper-networks to ensure flexibility and further reduce\\ncomputation costs. Particularly, KGEditor introduces an additional layer with the same architecture\\nof FFN layers for storing knowledge. Then it constructs a hyper-network based on a bi-directional\\nLSTM [ 53] that encodes embeddings of triples. In this manner, KGEditor becomes an efficient way\\nto edit knowledge graph embeddings.\\n5.3.4 Summary .KME methods based on global optimization typically apply specific fine-tuning\\nrestrictions to regularize parameter updates, namely constrained fine-tuning strategies. This is to\\nprevent overfitting and ensure the model’s performance on the unedited knowledge. One crucial\\nadvantage of such strategies is its generality regarding the relevant knowledge, i.e., in-scope inputs\\nX𝑒of edit𝑒. As the global optimization affects all parameters in a language model, the relevant\\nknowledge in it will also be edited, thereby generalizing to such knowledge. On the other hand,\\nthe high computation costs of fine-tuning all parameters also motivate researchers to propose\\nintermediate fine-tuning strategies that leverage hyper-networks. Furthermore, global optimization\\nmethods are mostly model-agnostic, which means they can be applied to other editing methods.\\nNevertheless, such possibilities are less explored in the context of KME. In terms of the drawbacks,\\nglobal optimization methods are suboptimal in maintaining the locality of edited models, as the\\noptimization can easily influence unedited knowledge. Hence, it is crucial to achieve a balance\\nbetween generality and locality when optimizing language models with specific constraints or\\nintermediate designs.\\n5.4 Local Modification\\n5.4.1 Overview .To tackle the challenge of fine-tuning methods with respect to locality, extensive\\nresearch has been conducted on the local modification strategy for KME tasks [ 91,145]. These\\ntechniques originate from the concept of identifying and modifying specific relevant weights\\nin a pre-trained model to achieve desirable outputs. The primary objective is to first locate the\\nweights𝜙𝑘that store the knowledge in a pre-trained model 𝑓𝜙regarding the input 𝑥. Afterward, by\\nadjusting these weights, it becomes possible to generate the correct output 𝑦∗from the same input\\n𝑥without re-training or fine-tuning the whole model. Recently, researchers have generalized the\\nlocal modification strategy to LLMs, where the efficiency of information updates for pre-trained\\nLLMs can be substantially improved. Generally, the goal of the local modification strategy of KME\\ncan be formulated as a constrained optimization problem with refined constraints as follows:\\nmin\\n𝜙∗\\n𝑘E𝑒∈EE𝑥,𝑦∗∈X𝑒,Y∗𝑒L(𝑓∗\\n𝜙𝑘,𝜙∗\\n𝑘(𝑥),𝑦∗),\\ns.t.𝑓∗\\n𝜙𝑘,𝜙∗\\n𝑘(𝑥)=𝑓(𝑥),∀𝑥∈X\\\\XE,\\nwhere𝜙𝑘=𝐿(𝑓𝜙,E),𝜙𝑘=𝜙\\\\𝜙𝑘, 𝑓∗\\n𝜙𝑘,𝜙∗\\n𝑘=𝑀(𝑓𝜙,E).(24)\\nHere𝜙∗denotes the edited weights related to the new knowledge, and 𝜙𝑘denotes the unedited\\nweights. In Eq. (24), the local modification strategy for KME is naturally broken down into two\\nsteps: (1) The locating step , denoted by function 𝐿, locates the relevant weights 𝜙𝑘in pre-trained\\nmodel𝑓𝜙that store the obsolete information regarding the query 𝑥. (2) The editing step , denoted\\nby function 𝑀, edits the located weights 𝜙𝑘into new weights 𝜙∗\\n𝑘such that the correct answer 𝑦∗ 18 Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong Li\\ngiven the query 𝑥can be generated by the model with 𝜙∗\\n𝑘. By only updating a small fraction of\\nmodel weights, the editing step avoids negatively influencing other irrelevant information, (i.e.,\\n𝑥∈X\\\\XE).\\nIn the following subsections, we first introduce the concept of knowledge neuron in LLMs,\\nwhich are specific neurons that store factual knowledge and can be activated to generate the\\ndesirable answer based on a certain query 𝑥. Then we discuss two local modification strategies\\nfor KME: (1) the gradient-based strategies, which identify and edit knowledge neurons based\\non gradients calculated from the model outputs; (2) the prompt-based strategies, which locate\\nknowledge neurons based on the input prompts.\\nKnowledge Neurons. LLMs pre-trained on large corpora can be viewed as databases that store\\nfactual and common-sense knowledge in the pre-trained model weights [ 45]. To update such\\nknowledge by locally modifying the weights in the pre-trained LLMs, it is imperative to identify\\nwhich weights store such information, i.e., locating the knowledge neurons. This can be challenging\\ndue to the complex transformer architecture of LLMs [7].\\nAs described in Section 2.2.1, the transformer structure of LLMs consists of two primary types\\nof layers, i.e., (1) the self-attention layer and (2) the point-wise feed-forward (FFN) layer, which\\nis implemented as a two-layer multi-layer perceptron (MLP). Particularly, given a prompt 𝑥, the\\nself-attention layers of the LLMs use the query vector of the last token and the key vectors of the\\nprevious tokens to calculate a weighted sum of their value vectors. Therefore, given the input 𝑥,\\nthese layers provide information about which previous tokens we should consider when generating\\nthe answer. Here we provide a simplified example for illustration. To answer the question “Who is\\nthe current president of the USA?” , the self-attention layer indicates that the model should attend\\nto words “president” and “USA” , i.e., v𝑝𝑟𝑒𝑠𝑖𝑑𝑒𝑛𝑡 ,v𝑈𝑆𝐴, to determine the answer. This provides us\\nwith a start-up embedding h𝑠𝑡𝑎𝑟𝑡to generate the answer token, which is the weighted sum of the\\nvalues of the two attended words, i.e., 𝑤1v𝑝𝑟𝑒𝑠𝑖𝑑𝑒𝑛𝑡+𝑤2v𝑈𝑆𝐴. However, the information regarding\\nthe current president of the USA is not provided. In contrast, recent works [ 40,41,86,87] claim\\nthat the residual added to h𝑠𝑡𝑎𝑟𝑡by the outputs of FNN layers, i.e., h𝑛𝑒𝑥𝑡=h𝑠𝑡𝑎𝑟𝑡+FFN(h𝑠𝑡𝑎𝑟𝑡),\\ninjects the information “Biden” toh𝑠𝑡𝑎𝑟𝑡and leads to the generation of correct answers. Therefore,\\nneurons in the FFN can be viewed as the knowledge neurons that store the factual knowledge.\\nThe role of FFN in storing knowledge can be theoretically analyzed by revisiting their formulation\\nin Eq. (1), which we rewrite as follows:\\nSelfAtt𝑖(x)=Softmax\\x00q𝑖k⊤\\n𝑖\\x01v𝑖,\\nFFN(h)=GELU(hW 1)W2.(25)\\nSpecifically, comparing the above two equations, we observe that the input hto the FFN acts\\nsimilarly to the query qto the SelfAtt. Moreover, the weights of the first layer W1can be viewed as\\nthe key v, where GELU(hW 1)can be viewed as calculating an unnormalized attention score over\\nthe row vectors of W2. Finally, the weights of the second layer W2can be viewed as the value (or\\nthe memory) that stores the knowledge, which can be retrieved according to the unnormalized\\nweights calculated by the first layer.\\n5.4.2 Gradient-based Strategies .Based on the knowledge neuron view of the FFN layer weights\\nin pre-trained LLMs, various gradient-based methods are proposed to locate and edit the pre-\\ntrained LLMs. Generally, they perform editing in a top-down manner, utilizing the supervision\\nsignal provided by the correct answer 𝑦∗. As an exemplar work, KD[22] proposes to change each\\nweight𝑤(𝑙)\\n𝑖(i.e., the𝑖-th weight in the 𝑙-th layer of FFN) from 0 to the pre-trained value ˆ𝑤(𝑙)\\n𝑖. Then it\\ncalculates the cumulative change in the probability of predicting the output 𝑦∗with input𝑥, where\\nthe weights with a high cumulative probability are considered relevant for knowledge regarding 𝑦∗. Knowledge Editing for Large Language Models: A Survey 19\\nIn contrast to locating and editing an individual weight 𝑤(𝑙)\\n𝑖in the FFN layer, ROME [86] proposes\\nto update a whole FFN layer to encode the new knowledge 𝑦∗. Specifically, they view the second\\nlayer weights W2in the FFN layer in Eq. (25) as a linear associative memory [ 3,67] in the form\\nofKW 2=V, where the keys Kand values Vassociated with W2can be directly calculated with\\nMoore-Penrose pseudoinverse. With such a view of W2in the FFN layer, the optimization objective\\nof updating it into ˆW2to encode new knowledge in the edit 𝑒=(𝑠,𝑟,𝑜→𝑜∗)can be formulated as\\nfollows:\\nmin∥KˆW2−V∥s.t.ˆWk∗=h∗. (26)\\nHere k∗, which should encode the information of the subject 𝑠, is calculated by sampling multiple\\n𝑥∼X𝑒and taking the average of the output of the first dense layer of the FFN. The target activation\\nh∗is calculated via optimizing the probability of outputting the correct answers 𝑦∗∈Y𝑒of the\\npre-trained LLM via the subsequent layers. Then, an efficient rank-one update is conducted on\\nthe weights W2according to Eq. (26), such that after the update, the edited FFN layer can output\\nthe correct hidden representation h∗conducive to the generation of the right answer 𝑦∗from k∗.\\nRecently, MEMIT [87] proposes to further generalize the above editing strategy of the FFN layers\\nof pre-trained LLMs to the mass editing of different knowledge. Particularly, with 𝑢new edits\\n{𝑒1,𝑒2,...,𝑒𝑢}that are required to be updated in the weights W2, the mass editing problem can be\\nformulated as the following optimization problem:\\nmin 𝑛∑︁\\n𝑖=1\\r\\rk𝑖ˆW2−v𝑖\\r\\r2+𝑛+𝑢∑︁\\n𝑖=𝑛+1\\r\\rk∗\\n𝑖ˆW2−v∗\\n𝑖\\r\\r2!\\n, (27)\\nwhere k𝑖,v𝑖are the original key, value pairs associated with the weights W2(i.e., row vectors in\\nmatrices K,Vin Eq. (26)), whereas k∗\\n𝑖,v∗\\n𝑖are the updated key, value pairs calculated from the 𝑖-th\\nedit𝑒𝑖as with Eq. (26). In addition, since multiple edits are required, the update is shared among\\ndifferent MLP layers, which is conducted in a top-down manner to prevent the potential issue of\\nediting layers that could affect the ones that have already been edited. The residual for each edit\\nis spread evenly over the range of the critical FFN layer. The strategy of residual attribution has\\nrecently been improved by PMET [74], which adopts a square root strategy to spread residuals to\\nbottom FFN layers such that more precise information can be conveyed to critical layers.\\n5.4.3 Prompt-based Strategies .Tailored to characteristics of LLMs that provide answer 𝑦∗based\\non the prompt 𝑥, the operation of locating and editing knowledge neurons can also be conducted in\\na bottom-up manner, which aims to change the prompt to detect neurons to be edited. Specifically,\\nby masking out the key information and observing the difference of activations in the intermediate\\nlayers of the LLM, the weights that store the information regarding the query 𝑥can be located\\nand updated to store the new information 𝑦∗. For example, ROME [86] proposes a corruption-and-\\nrestore based strategy to identify relevant layers (or their hidden output variables h) that store the\\ninformation based on the prompt 𝑥. It first randomly masks the hidden representations of the key\\nvectors k(as described in Eq. (1)) of the tokens in the prompts from a certain intermediate layer of\\nthe pre-trained LLM. Then it calculates the reduced probability of predicting 𝑦(i.e., the obsolete\\noutputs) as the causal mediation effects of 𝑥on𝑦mediated by h. Consequently, the weights in\\nlayers with large mediated effects are viewed as knowledge neurons that store the information of 𝑦.\\nMEMIT CSK[45] extends the above corruption-based strategy to editing common sense knowledge.\\nThe authors argue that, different from the factual knowledge that can be directly retrieved by the\\nsubject𝑠, the object𝑜and relation 𝑟also matter for commonsense knowledge. Therefore, three types\\nof corruption and edit locations, i.e., subject, verb, and object, are thoroughly analyzed, where the\\nperformance of editing commonsense knowledge can be improved. Moreover, BIRD [82] studies\\nthe novel problem of bidirectional KME, which requires the edited model to possess reversibility. 20 Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong Li\\nFor example, if the phrase “The capital of France is” is edited to a counterfactual “London” within a\\nmodel, it should logically be able to retrieve the inverse fact. That is, when presented with “London\\nis the capital of,” the model should respond with “France” rather than “England”. Based on the\\nstrategy of ROME, BIRD introduces a novel objective that involves the bidirectional relationships\\nbetween subject and object in an edit. In this manner, the updated model weights can preserve\\nreversibility by learning such information.\\n5.4.4 Summary .In this part, we introduce the local modification strategy for pre-trained LLMs for\\nefficient updates of new information without adding new weights or optimizing the whole network.\\nWe start by analyzing the pivotal role of the point-wise feedforward layers, i.e., the FFNs, to store\\nthe factual information in pre-trained LLMs, with the knowledge neurons associated with the FFN\\nlayer thoroughly analyzed. We then discuss the gradient-based and prompt-based strategies that\\nconduct local modification of LLMs, which achieve the modification in a top-down manner based\\non the output 𝑦and in a bottom-up manner with the input prompt 𝑥, respectively. Nevertheless,\\nthe scalability and retainability of local modification methods lack further improvements, as the\\nperformance might deteriorate with more edits performed [87].\\n6 DATASETS\\nRecently, multiple datasets have been established to facilitate the evaluation of KME methods. In\\nthis section, we summarize the commonly-used datasets in Table 3 to benefit future KME research.\\nSpecifically, these datasets can be divided into two groups: textual-output datasets (i.e., generation\\ntasks) and categorical-output datasets (i.e., classification tasks). The datasets are obtained from a\\nvariety of sources, including knowledge graphs, Wikipedia pages, crowd-sourced responses, etc.,\\nwhich are adapted by researchers to fit into the KME setting.\\n6.1 Generation Datasets\\nFor generation datasets, the target is in the form of textual content that is required to be generated\\nby LLMs. Serving as pivotal resources to evaluate KME methods, most generation datasets are\\nbased on relational knowledge and used for assessing the ability of editing techniques to inject\\nnew factual knowledge. This is because relational datasets preserve more definitive answers for\\neach input and thus are more convenient and precise for evaluation. Specifically, these datasets\\nare generally curated from the corresponding relational datasets to encompass diverse relational\\ncontexts, ranging from question-answer pairs to intricate multi-hop queries. Therefore, the most\\nprevalent output format is an object to be predicted.\\nIn this subsection, we present the most representative generation datasets, shedding light on\\ntheir unique attributes, the nature of their content, and the specific challenges they present for\\nevaluating KME methods on factual knowledge as follows:\\n•zsRE [71]: zsRE is one of the most prevalent Question Answering (QA) datasets extended and\\nadopted by [ 25,90] for KME evaluation. zsRE is suitable for evaluating KME due to its annotations\\nof human-generated question paraphrases, which allow researchers to assess the model resilience\\nto semantically equivalent inputs. In zsRE, each relation is associated with a set of crowd-sourced\\ntemplate questions, such as \"What is Albert Einstein’s alma mater?\". Each data entry cites a\\nWikipedia sentence that serves as the factual basis or provenance. The dataset also contains\\nnegative examples, which are generated by pairing a valid question with a random sentence.\\n•WikiGen [90]: Firstly proposed in MEND [ 90], WikiGen consists of approximately 68k question-\\nanswer pairs, with a similar size to zsRE. Here, each question corresponds to a sentence randomly\\nsampled from Wikitext-103, and each answer is a 10-token sample obtained from a pre-trained Knowledge Editing for Large Language Models: A Survey 21\\nTable 3. Statistics of prevalent KME datasets, including generation and classification datasets.\\nDataset Type # Train # Test Input Output Used in\\nzsRE Relational 244,173 244,173 Factual Statement Object [25, 47, 59, 70, 86, 87, 90, 91, 94]\\nWikiGen Generation N/A 68k Wiki Passage Continuation [90]\\nT-REx-100/-1000 Relational N/A 100/1,000 Factual Statement Object [29, 72]\\nCounterFact Relational N/A 21,919 Factual Question Object [86, 87, 94, 150]\\nParaRel Relational N/A 253,448 Factual Question Object [22]\\nNQ-SituatedQA QA N/A 67.3k User Query Answer [23, 70]\\nMQuAKE-CF/-T Relational N/A 9,218/1,825 Multi-hop Question Object [151]\\nHallucination Hallucination N/A 1,392 (Fake) Biography Biography [47]\\nMMEdit-E-VQA Multimodal 6,346 2,093 Image & Question Answer [17]\\nMMEdit-E-IC Multimodal 2,849 1,000 Image Description [17]\\nECBD Relational N/A 1000 Reference to Entity Completion [96]\\nFEVER Fact Checking 104,966 10,444 Fact Description Binary Label [25, 59, 90]\\nConvSent Sentimental 287,802 15,989 Topic Opinion Sentiment [91]\\nBias in Bio Biographical 5,000 5,000 Biographical Sentence Occupation [52]\\nVitaminC-FC Fact Checking 370,653 55,197 Fact Description Binary Label [91]\\nSCOTUS Categorization 7,400 931 Court Documents Dispute Topic [47]\\ndistilGPT-2 model [ 83]. It is noteworthy that greedy 10-token prediction of the base model only\\naligns with edit targets for less than 1% of samples.\\n•T-REx-100 & T-REx-1000 [33]: First used in CALINET [ 29], the authors adopt the classic\\nrelational dataset T-REx [ 33] for evaluating model editors by extracting factual triplets of varying\\nsizes (100 and 1,000). Particularly, for each triplet, the authors insert the head and tail entities\\ninto the template in LAMA [ 103] based on the relation they share, which results in two datasets\\nwith 100 and 1,000 facts, respectively, for the purpose of false knowledge detection. It should be\\nnoted that each fact in these datasets is represented by several paraphrased sentences.\\n•CounterFact [86]: CounterFact is established to distinguish superficial alterations in the word\\nselections and significant, generalized modifications in its foundational factual knowledge.\\nProposed in ROME [ 86], each entry in CounterFact originates from a related record in ParaRel [ 32],\\ncontaining a knowledge triple and meticulously crafted prompt templates. It is important to note\\nthat all subjects, relations, and objects in this tuple are recognized entities in Wikidata [133].\\n•ParaRel [32]: ParaRel is an expert-curated dataset that comprises diverse prompt templates for\\n38 relations, sourced from the T-REx dataset [ 33]. Firstly used in KN [ 22], the authors insert the\\nhead entity into each relational fact and set the tail entity as a blank for prediction. To ensure a\\nrich variety in templates, relations with less than four prompt templates are excluded, resulting in\\n34 relations in total. Each of these relations, on average, preserves 8.63 distinct prompt templates,\\nleading to a total of 253,448 knowledge-revealing prompts for 27,738 relational facts.\\n•NQ-SituatedQA [68]: NQ (Natural Questions) is a comprehensive question-answering dataset\\noriginating from user searches. In PPA [ 70], the authors utilize NQ as the source knowledge\\nwhile excluding any outdated information as identified by SituatedQA [ 148] to create a novel\\ndataset NQ-SituatedQA. SituatedQA is a dataset containing questions within a subset of NQ that\\nare dependent on specific time and location. The authors then incorporate the time-dependent\\nQA pairs from this subset, annotated using the 2021 Wikipedia [133] dump.\\n•MQuAKE [151]: MQuAKE is constructed from Wikidata [133] for evaluating the effectiveness\\nof KME methods on multi-hop questions. In particular, it is designed to assess whether the edited\\nmodels can correctly answer questions generated by chains of facts in plain text. MQuAKE 22 Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong Li\\nconsists of two datasets. (1) MQuAKE-CF is a diagnostic dataset, specifically crafted to evaluate\\nKME methods in the context of counterfactual edits. (2) MQuAKE-T focuses on temporal-based\\nknowledge updates and is aimed at assessing the effectiveness of KME techniques in updating\\noutdated information with contemporary factual data.\\n•Hallucination [47]: Firstly processed in GRACE [ 47], Hallucination is created from the dataset\\nreleased in SelfCheckGPT [ 85], where the authors prompt GPT-3 to generate biographies based\\non concepts extracted from WikiBio. The sentences are annotated regarding the factual accuracy,\\nand hallucinations in them are identified. Then in GRACE, the authors process this dataset by\\nfurther extracting Wikipedia summaries from WikiBio and thereby acquire the correct entry of\\neach sentence. In this manner, every edit consists of a potentially false biography generated by\\nGPT-3 as prompt, and a ground truth output, which is the correct next sentence extracted from\\nWikipedia. There exist 1,392 potential edits for test.\\n•MMEdit [17]: This dataset is the first to explore the possibility of editing multimodal LLMs.\\nSpecifically, MMEdit consists of two prevalent multimodal tasks: Visual Question Answering\\n(VQA) [ 4] and Image Captioning [ 51]. VQA involves developing algorithms that can analyze an\\nimage’s visual content, comprehend questions asked in natural language about the image, and\\naccurately respond to those questions. Image Captioning aims to understand an image and then\\ngenerate a detailed and coherent natural language description of that image. To create dataset\\nMMEdit, the authors utilize BLIP-2 OPT [ 73] and extract edit data from the evaluation datasets\\nVQAv2 [44] and COCO Caption [16], specifically focusing on their suboptimal entries.\\n•ECBD [96]: Based on the original dataset ECBD (Entity Cloze By Date) [ 95], the authors process\\nthis dataset for a novel task, namely Entity Knowledge Propagation (EKP). The task aimed at\\nupdating model parameters to incorporate knowledge about newly emerged entities that are\\nnot present in the pre-training data of the language models. For instance, BERT [ 27], trained in\\n2018, does not recognize “COVID-19” as it is a more recent entity. The processed dataset aims\\nto provide evaluation for such a task with the help of definition sentences as input to update\\nknowledge about new entities. The entities are taken from date between 2020/01 and 2021/09\\nto ensure that they are not in training data. Each edit consists of a new entity, a description\\nsentence, a probe sentence, and a ground truth completion\\n6.2 Classification Datasets\\nClassification datasets are also widely adopted to evaluate the effectiveness of KME. These datasets\\nconsist of prompt-target pairs, where the target is a discrete label instead of a textual sentence. In\\nthe context of KME, these labels help ascertain the alignment of model performance with desired\\nedits, whether they are sentiment-based, fact-checking, or entaiKMEnt-related. The advantages of\\nclassification datasets also involve their preciseness in evaluation without the need to define the\\nspecific output space. In this section, we summarize notable classification datasets that have been\\ntailored and leveraged for assessing KME techniques as follows:\\n•FEVER [128]: FEVER is a fact-checking dataset originally processed in KILT [ 102] for verifying\\nfactual knowledge in the form of binary classification. It necessitates the retrieval of sentence-\\nlevel evidence to determine whether a claim is supported or refuted, and is widely used for\\nevaluating the performance of KME. Specifically, FEVER excludes claims labeled as lacking\\nsufficient information, as they typically do not provide any evidence to evaluate the claim.\\n•ConvSent [91]: Firstly processed in SERAC [ 91], ConvSent is used to evaluate the capability of\\nan editor to modify a dialog agent’s sentiment about a particular topic without influencing its\\nresponses to other topics. ConvSent is obtained from a list of 15,000 non-numeric entities from\\nzsRE [ 25,71], combined with 989 noun phrases from GPT-3 [ 11] for 15,989 topics. Particularly, Knowledge Editing for Large Language Models: A Survey 23\\nTable 4. Examples of different downstream applications of KME: Question Answering (QA), Fact Checking\\n(FC), and Natural Language Generation (NLG).\\nTask Edit Descriptor 𝑒 In-scope Input 𝑥∼X𝑒 Original Output 𝑦∼Y𝑒Target Output 𝑦∼Y∗\\n𝑒\\nQA (Kazakhstan, Captital, What is the capital of Astana Nur-Sultan\\nAstana→Nur-Sultan) Kazakhstan?\\nFC (Marathon, Record, Kipchoge holds the men’s True False\\nKipchoge→Kiptum) marathon world record.\\nNLG(Jordan Poole, Play In, Provide a short introduction Jordan Poole entered In 2023, Jordan Poole transitioned\\nWarriors→Wizards) to Jordan Poole, describing the Warriors’ rotation from the Warriors to the Wizards,\\nhis current position. recently. remarking a significant change.\\nfor each entity, there are ten positive and ten negative sentiment completions, which can be\\nnoisy, from the BlenderBot model with 3B parameters [ 112]. The refined sentiment labels are\\nachieved by a sentiment classifier [50] pre-trained on RoBERTa [80].\\n•Bias in Bios [24]: Bias in Bios is a dataset originally proposed for fairness-related machine\\nlearning, containing approximately 397k short professional biographies of online individuals,\\nwhich are not relatively famous. Each biographical sentence is assigned an associated occupation\\nlabel for the described person. To adopt this dataset for evaluating the performance of KME\\nmethods, the authors of REMEDI [ 52] extract a single sentence, modify it to display only the\\nperson’s first name, and then query the language model with the prompt that follows the structure:\\n\"Person has the occupation of...\". Then they evaluate the relative probabilities of the language\\nmodel assigned to 28 potential occupations, where the language model is considered to be correct\\nif the ground-truth occupation is ranked top-1.\\n•VitaminC-FC [115]: Firstly processed in SERAC [ 91], VitaminC-FC is constructed based on\\na fact-checking dataset, VitaminC [ 115]. Particularly, VitaminC consists of more than 400,000\\nevidence-claim pairs, each of which is assigned a binary label to indicate whether the evidence\\nentails the claim. The dataset was gathered from over 100,000 Wikipedia revisions that modify an\\nunderlying fact, along with additional synthetic ones. In SERAC, the authors convert VitaminC\\ninto a KME dataset by using the evidence as the edit descriptor and using claims from the same\\nWiki pages accordingly as in-scope samples.\\n•SCOTUS [47]: Firstly proposed in GRACE [ 47], SCOTUS is processed with label shift based on\\nthe dataset with the same name from Fairlex [ 12]. This classification task is to categorize U.S.\\nSupreme Court documents from various decades into one of 11 topics. The topics are clustered\\nbased on the specific matter of dispute, such as Criminal Procedure, Civil Rights, and First\\nAmendment. Due to the evolution of categorization rules over time, the label distributions in\\nthis dataset also shift. Specifically, 7.4k cases from 1946-1982 are used for training, and 931 cases\\nfrom the 1991-2009 period are for test.\\n7 APPLICATIONS\\nKME can benefit multiple downstream applications with the ability to precisely and efficiently\\ninject knowledge into pre-trained LLMs. In the following, we introduce several key applications of\\nKME techniques in realistic scenarios, where intuitive examples are provided in Table 4.\\n7.1 Question Answering\\n7.1.1 Background .Question Answering (QA) is a core NLP task that aims to comprehend queries\\nposed by users in natural language and provide answers based on the encoded knowledge in the\\npre-trained language model [ 118]. Traditional models for QA are generally fixed in their knowledge, 24 Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong Li\\ncapturing only the information available at the training time of [ 62,103]. However, in our dynamic\\nworld, new information is generated incessantly, which necessitates the constant update of QA\\nmodels [ 124]. Fortunately, KME methods enable the modification of QA models to cater to specific\\nquestions without disrupting responses to other unrelated inputs. Therefore, with KME strategies,\\nthe QA model can be efficiently updated on the run, where the currentness of the model can be\\nguaranteed. Consequently, language model editing techniques have found broad applications across\\na myriad of QA contexts with potentially distinct requirements [70].\\n7.1.2 Existing Works .The QA task encompasses various aspects, such as conversational QA,\\ndefinition-based QA, and notably, relation-based QA [ 98]. Relation-based QA is primarily adopted\\nas an evaluation benchmark as it necessitates the retrieval of precise real-world facts in response\\nto queries. This particular emphasis on specific information retrieval renders relation-based QA\\nespecially conducive to the benefits of KME techniques. For example, PPA [70] introduces an\\ninnovative task of CuQA (Continuously-updated QA), which intentionally emphasizes recurrent,\\nsubstantial edits for language models to constantly update them with new information. An important\\naspect of the CuQA task is to ensure that the existing pre-trained knowledge remains unaltered with\\nthe integration of new knowledge. Therefore, this property is one important evaluation to assess\\nmodel editing in CuQA tasks. In MQuAKE [151], the authors innovatively propose a multi-hop QA\\ntask that involves answering questions generated by chains of facts in plain text. Specifically, the\\ntask requires edited models to infer implicit relations that can be several hops away from the objects\\nin the edit. For example, when a language model is modified regarding the president of the USA, an\\nideal model should also authentically alter answers to “Who is the son of the president of the USA”,\\nwhich is a two-hop relation. Such a task is significantly more challenging as it necessitates the model\\nto alter its reasoning results in addition to the original edit. Nevertheless, the proposed method\\nMeLLo in MQuAKE still exhibits outstanding performance on this difficult task, demonstrating the\\npotential of KME in generalizing edited knowledge to multi-hop relations.\\n7.2 Fact Checking\\n7.2.1 Background .Fact-checking (FC) is a pivotal task in journalism, information verification,\\nand combating misinformation that aims to scrutinize and affirm the authenticity of claims, state-\\nments, or information in news articles, social media, and other media content [ 37,115]. In a world\\noverwhelmed with ever-emerging information, fact-checking facilitates the trustworthiness in the\\nsharing of distributed information, promotes information transparency, and aids individuals in\\nmaking well-informed decisions [ 128]. However, it is crucial to constantly update fact-checking\\nmodels. For instance, during the COVID-19 pandemic, initial understandings and guidelines about\\nthe virus evolved as researchers gathered more data [ 117]. A fact-checking model that cannot adapt\\nto these rapidly changing facts would quickly become outdated and potentially spread misinforma-\\ntion, thereby requiring the application of language model editing. By integrating KME techniques\\ninto fact-checking models to consistently update them with the latest information and facts, it\\nbecomes possible to ensure the currentness, trustworthiness, and accuracy of the model despite the\\npersistent evolution of information.\\n7.2.2 Existing Works .Recently, several works have proposed to apply KME techniques in fact-\\nchecking models. In [ 153], the authors first explore the potential of modifying specific factual\\nknowledge within the transformer backbone of the fact-checking model while ensuring that overall\\nmodel performance remains intact on facts irrelevant to the editing purpose. Particularly, they\\nidentify the critical components within the transformer backbones conducive to effective knowledge\\nmodifications. In SERAC [91], the authors propose to use evidence gathered from Wikipedia as Knowledge Editing for Large Language Models: A Survey 25\\nedit descriptors to update potentially outdated knowledge in the model. The proposed method\\nexhibits significant performance improvements over baselines and can be generalized to other\\nin-scope inputs collected from the same Wikipedia page.\\n7.3 Natural Language Generation\\n7.3.1 Background .KME techniques are also promising to ensure the relevancy of the Natural\\nLanguage Generation (NLG) task, which aims to generate coherent and contextually relevant content\\nbased on provided instructions [ 110]. Considering the rapid evolution of the global information\\nlandscape, it is essential for NLG models to remain up-to-date and ensure the accuracy of generated\\ntext while avoiding potentially false statements that may mislead the users.\\n7.3.2 Existing Works .In practice, several works have been proposed to apply KME methods\\nto promote model performance in natural language generation tasks. For instance, FRUIT [5]\\nproposes to update outdated Wikipedia articles according to the collection of new information about\\nthe article’s subject. Based on the T5 model [ 107], the authors utilize a compressed output format\\nto eliminate the necessity of generating the entire update from scratch and promote thoughtful\\ncontent structuring, which effectively handles the challenge of incoherence. In MEND [90], the\\nauthors apply their proposed method in the Wikitext generation task, where the edited model is\\nrequired to produce credible 10-token extensions based on a provided Wikitext prefix [ 83]. With\\nmodification on multi-layer token-wise activations and gradients, the edited model presents higher\\ncoherence on the NLG task, which demonstrates the effectiveness of KME in generating target\\ntexts with richer information than QA or FC.\\n8 DISCUSSION\\nDespite the continual progress of works on KME, several critical aspects have been inadequately\\naddressed by existing studies. Delving deeper into these challenges could offer researchers fresh\\ninsights and pave the way for the further advancement of the field. Consequently, in this section,\\nwe outline the pressing challenges that await solutions in KME.\\n8.1 Challenges\\n8.1.1 Trade-off between Locality and Generality. In KME, it is crucial to balance two objectives,\\nlocality andgenerality (as defined in Sec. 4), such that a higher edit success rate can be achieved with\\nminimal negative influence on other knowledge irrelevant to the edits. When editing a language\\nmodel, a potential trade-off might emerge between these two desirable properties. As demonstrated\\nin [145], local modification methods, such as MEMIT [ 87] and ROME [ 86] generally preserve a\\nhigher level of locality, as they locate precise locations of target knowledge to conduct the edition,\\nwhich does not largely affect the unrelated weights. In addition, T-Patcher [ 59] points out that\\nincreasing the size of memory increases locality while decreasing the generality. These observations\\nunderscore the intricate balance between locality and generality. However, it remains challenging\\nto tackle the trade-off problem and achieve a balance between these two desirable properties of\\nKME methods.\\n8.1.2 Theoretical Analysis. While many current KME studies focus on developing effective\\nmethods to enhance the editing performance regarding various desirable properties, there exists a\\nnotable gap between the practical application and the comparatively less discovered theoretical\\nanalysis. Recently, in [ 125], the authors provide theoretical support for the justification of identifying\\nharmful training examples and editing the model by erasing the information from a Bayesian view.\\nLEACE [ 9] introduces an analytical framework that offers a theoretical perspective for the task of\\nerasing target concept information from every layer in language models. In general, the benefits of 26 Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong Li\\nincorporating theoretical analysis are multi-faceted. First, theoretical analysis provides a deeper\\nunderstanding of the mechanics underlying KME, allowing for more principled approaches to\\nediting. Second, a strong theoretical basis sets a solid foundation for future research, encouraging\\nmore rigorous and systematic exploration in the field of KME. However, to the best of our knowledge,\\nthere still does not exist any comprehensive theoretical analysis regarding the KME problem that\\ninvolves novel knowledge. We hope that future research will enrich the theoretical discourse that\\ncan deliver profound insights into the substantial foundations of KME methods.\\n8.1.3 Editing at Scale. Another crucial property that hinders the practical application of KME is\\nscalability — the ability of editing strategy to effectively perform a large number of edits simul-\\ntaneously [ 90]. For example, conversational systems [ 150] are expected to be constantly updated\\nto incorporate an enormous number of global events and the information originating from them.\\nHowever, as the number of applied edits increases, the coherence of language models is severely\\njeopardized, as multiple edits might contradict a broader spectrum of pre-existing knowledge in\\nthe models [ 135]. This can lead to decreased editing performance in both locality and generality\\nmetrics [ 91]. Although external memorization methods can alleviate such problems with a larger\\nsize of memories of additional parameters, they are still vulnerable if thousands of edits are re-\\nquired [ 86]. Moreover, simply adapting single-edit techniques for a multi-edit environment by\\nmerely applying them sequentially has been demonstrated to be proven suboptimal [ 87]. Therefore,\\nthe unique and intricate challenge of coherence renders editing at scale a formidable task.\\n8.2 Future Directions\\nDespite the recent achievements in the development of KME strategies for effective and efficient\\nupdating of new knowledge into LLMs, KME research is still in its emerging stage. Several promising\\ndirections could be pursued to further advance this field. Accordingly, we identify five inspiring\\nand important open problems worthy of exploration in the future as follows:\\nOptimization-Free Editing. Recently, prompt engineering has become a prevalent solution\\nfor modifying the behaviors of pre-trained LLMs in a human-preferable manner without the\\nrequirement of parameter update [ 30]. For example, in-context learning provides task descriptions\\nand/or demonstrations in the form of plain text to promote the model performance [ 11], which\\nmakes it a potentially more efficient and practical strategy for language models. We note that\\nIKE [ 150] proposes a novel framework that relies on demonstration contexts for KME without\\nparameter updating, which explicitly formats the demonstrations that can guide the language\\nmodel to copy, update, and retain the prediction of different prompts. However, such a strategy\\nis difficult to scale and usually has unsatisfactory retention. Therefore, it remains a crucial while\\nchallenging task to develop optimization-free KME methods.\\nAuto-Discovery of Editing Targets. Current KME methods mainly rely on human expertise to\\nidentify and incorporate desirable knowledge into pre-trained LLMs. This approach is inherently\\nlabor-intensive and can incur significant costs, especially considering the vast and rapidly expanding\\nnew information needed to be integrated into language models. A promising future direction lies\\nin the automation of the edits, which aims to identify, evaluate, and prioritize new knowledge that\\nneeds to be integrated from raw resources such as websites and social media. Through this strategy,\\nthe application of KME can be streamlined, rendering it more practical and adaptable in real-world\\nscenarios. A straightforward solution would be crawling new knowledge and transforming it\\ninto a knowledge base, querying LLMs for each knowledge triple, and editing the wrong answer.\\nHowever, such a strategy still lacks efficiency. Therefore, it remains a crucial task to discover editing\\nknowledge from various resources without human effort. Knowledge Editing for Large Language Models: A Survey 27\\nContinual Editing. Current methodologies on LLM editing primarily consider one-step offline\\nediting [ 5,25]; however, such an approach is not aligned with real-world applications where models\\nmight continually encounter novel knowledge to be injected. For example, an online question-\\nanswering (QA) model may continually encounter reports of incorrect answers from end users,\\nwhere the editing needs to be conducted on the run [ 59]. Therefore, an optimal KME technique\\nshould be capable of instantaneously and continuously rectifying emergent issues. We note that\\ncontinual editing of pre-trained LLMs presents a unique challenge: preventing the edited models\\nfrom forgetting or contradicting previous edits. Despite the inherent complexities, the persistent\\ndemand for continual editing in practice underscores the importance of solving this challenge.\\nRobust Editing. An important direction for the advancement of KME lies in enhancing its ro-\\nbustness. In an era where misinformation spreads rapidly, it is urgent that edited models not only\\nretain their accuracy but also resist adversarial attacks and misinformation [ 38]. Here, we should\\nnote that the concept of robustness extends beyond just maintaining factual accuracy; it involves\\nfortifying the model against potentially adversarial external perturbations [ 101]. For example, if\\nKME is maliciously applied to inject harmful knowledge into language models, the edited models\\ncan be easily transformed into tools for misinformation [ 126]. Therefore, to prevent such cases, it is\\ncrucial for KME techniques to develop capabilities that can identify and counteract such unwanted\\ninputs, thereby enhancing their resilience against adversarial actions. In practice, as the trend leans\\ntowards open-sourcing large language models, it becomes ever more crucial to safeguard against\\npotential manipulations that can turn these models harmful.\\nEditable Fairness. With the wide application of large language models (LLMs) to support decisions,\\nthe emphasis on fairness has grown significantly [ 134], which requires LLMs to fairly treat people\\nwith diverse background [ 1]. However, LLMs trained on large datasets inevitably incorporate\\ncertain biases during this pre-training phase [ 28]. Fortunately, the precision and efficiency of KME\\ntechniques offer a promising solution to mitigate such biases and promote fairness in pre-trained\\nLLMs. For instance, in a model designed to classify biographical sentences with occupation [ 24],\\nKME can be used to inject nuanced knowledge about a particular profession, guiding the model\\ntowards a more equitable understanding of individuals associated with that profession [ 52]. How-\\never, this remains a complex challenge, as fairness often entails considering disparate groups of\\nindividuals rather than specific people. This broader focus makes knowledge injection via KME\\na non-trivial task. Despite these difficulties, the enhancement of fairness in language models is\\nparamount, and KME techniques present a promising avenue to achieve this goal.\\n9 CONCLUSIONS\\nIn this survey, we present a comprehensive and in-depth review of knowledge-based model editing\\n(KME) techniques for precise and efficient updating of new knowledge in pre-trained LLMs. We\\nfirst formulate the KME problem as a constrained optimization objective that simultaneously\\nensures the accuracy and retention of editing, which is general to encompass different KME\\nstrategies. We then provide an overview of the evaluation metrics for KME, which sheds light\\non the desirable attributes of edited models. Subsequently, we propose a structured taxonomy\\nframework to systematically categorize existing KME techniques. Within each category, we outline\\nthe central challenges, elaborate on the representative methods, and discuss their strengths and\\nweaknesses. Furthermore, we summarize the datasets widely utilized to assess KME techniques,\\nhighlighting that certain techniques demand specific dataset structures for training or evaluation.\\nTo inspire researchers to devise more practical implementations, we also spotlight the real-world\\napplications of KME techniques. Finally, we identify several potential challenges for future research\\nand provide insightful directions that are conducive to further advancement of the field. 28 Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong Li\\nACKNOWLEDGMENTS\\nThis work is supported by the National Science Foundation under grants (IIS-2006844, IIS-2144209,\\nIIS-2223769, CNS2154962, and BCS-2228534), the Commonwealth Cyber Initiative awards (VV-\\n1Q23-007, HV-2Q23-003, and VV-1Q24-011), the JP Morgan Chase Faculty Research Award, the\\nCisco Faculty Research Award, the Jefferson Lab subcontract, and the UVA 4-VA collaborative\\nresearch grant.\\nREFERENCES\\n[1]Abubakar Abid, Maheen Farooqi, and James Zou. 2021. Persistent anti-muslim bias in large language models. In\\nProceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society . 298–306.\\n[2]Armen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer. 2021. Intrinsic Dimensionality Explains the Effectiveness of\\nLanguage Model Fine-Tuning. In ACL.\\n[3]James A Anderson. 1972. A simple neural network generating an interactive memory. Mathematical biosciences 14,\\n3-4 (1972), 197–220.\\n[4]Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi\\nParikh. 2015. Vqa: Visual question answering. In ICCV .\\n[5]Robert L. Logan IV au2, Alexandre Passos, Sameer Singh, and Ming-Wei Chang. 2022. FRUIT: Faithfully Reflecting\\nUpdated Information in Text. arXiv:2112.08634 [cs.CL]\\n[6]Razvan Azamfirei, Sapna R Kudchadkar, and James Fackler. 2023. Large language models and the perils of their\\nhallucinations. Critical Care 27, 1 (2023), 1–2.\\n[7]Michiel Bakker, Martin Chadwick, Hannah Sheahan, Michael Tessler, Lucy Campbell-Gillingham, Jan Balaguer, Nat\\nMcAleese, Amelia Glaese, John Aslanides, Matt Botvinick, et al .2022. Fine-tuning language models to find agreement\\namong humans with diverse preferences. In NeurIPS .\\n[8]David Bau, Steven Liu, Tongzhou Wang, Jun-Yan Zhu, and Antonio Torralba. 2020. Rewriting a deep generative\\nmodel. In ECCV .\\n[9]Nora Belrose, David Schneider-Joseph, Shauli Ravfogel, Ryan Cotterell, Edward Raff, and Stella Biderman. 2023.\\nLEACE: Perfect linear concept erasure in closed form. arXiv preprint arXiv:2306.03819 (2023).\\n[10] Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A Pretrained Language Model for Scientific Text. In EMNLP-\\nIJCNLP .\\n[11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\\nPranav Shyam, Girish Sastry, Amanda Askell, et al .2020. Language models are few-shot learners. In NeurIPS , Vol. 33.\\n1877–1901.\\n[12] Ilias Chalkidis, Tommaso Pasini, Sheng Zhang, Letizia Tomada, Sebastian Schwemer, and Anders Søgaard. 2022.\\nFairLex: A Multilingual Benchmark for Evaluating Fairness in Legal Text Processing. In ACL.\\n[13] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang,\\nYidong Wang, et al .2023. A survey on evaluation of large language models. arXiv preprint arXiv:2307.03109 (2023).\\n[14] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards,\\nYuri Burda, Nicholas Joseph, Greg Brockman, et al .2021. Evaluating large language models trained on code. arXiv\\npreprint arXiv:2107.03374 (2021).\\n[15] Sanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che, Ting Liu, and Xiangzhan Yu. 2020. Recall and Learn: Fine-tuning\\nDeep Pretrained Language Models with Less Forgetting. In EMNLP .\\n[16] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and C Lawrence Zitnick.\\n2015. Microsoft coco captions: Data collection and evaluation server. arXiv 2015. arXiv preprint arXiv:1504.00325\\n(2015).\\n[17] Siyuan Cheng, Bozhong Tian, Qingbin Liu, Xi Chen, Yongheng Wang, Huajun Chen, and Ningyu Zhang. 2023. Can\\nWe Edit Multimodal Large Language Models?. In EMNLP .\\n[18] Siyuan Cheng, Ningyu Zhang, Bozhong Tian, Zelin Dai, Feiyu Xiong, Wei Guo, and Huajun Chen. 2023. Editing\\nLanguage Model-based Knowledge Graph Embeddings. arXiv preprint arXiv:2301.10405 (2023).\\n[19] Cheng-Han Chiang and Hung-yi Lee. 2023. Can Large Language Models Be an Alternative to Human Evaluations?\\narXiv preprint arXiv:2305.01937 (2023).\\n[20] Alebachew Chiche and Betselot Yitagesu. 2022. Part of speech tagging: a systematic review of deep learning and\\nmachine learning approaches. Journal of Big Data 9, 1 (2022), 1–25.\\n[21] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa De-\\nhghani, Siddhartha Brahma, et al .2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416\\n(2022). Knowledge Editing for Large Language Models: A Survey 29\\n[22] Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. 2022. Knowledge Neurons in Pretrained\\nTransformers. In ACL.\\n[23] Damai Dai, Wenbin Jiang, Qingxiu Dong, Yajuan Lyu, and Zhifang Sui. 2023. Neural knowledge bank for pretrained\\ntransformers. In CCF International Conference on Natural Language Processing and Chinese Computing .\\n[24] Maria De-Arteaga, Alexey Romanov, Hanna Wallach, Jennifer Chayes, Christian Borgs, Alexandra Chouldechova,\\nSahin Geyik, Krishnaram Kenthapadi, and Adam Tauman Kalai. 2019. Bias in bios: A case study of semantic\\nrepresentation bias in a high-stakes setting. In FAccT .\\n[25] Nicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Editing Factual Knowledge in Language Models. In EMNLP .\\n[26] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image\\ndatabase. In CVPR . Ieee, 248–255.\\n[27] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional\\ntransformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).\\n[28] Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen\\nHon. 2019. Unified language model pre-training for natural language understanding and generation. NeurIPS (2019).\\n[29] Qingxiu Dong, Damai Dai, Yifan Song, Jingjing Xu, Zhifang Sui, and Lei Li. 2022. Calibrating Factual Knowledge in\\nPretrained Language Models. In EMNLP .\\n[30] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022.\\nA survey for in-context learning. arXiv preprint arXiv:2301.00234 (2022).\\n[31] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and\\nTatsunori B Hashimoto. 2023. Alpacafarm: A simulation framework for methods that learn from human feedback.\\narXiv preprint arXiv:2305.14387 (2023).\\n[32] Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich Schütze, and Yoav Goldberg.\\n2021. Measuring and improving consistency in pretrained language models. Transactions of the Association for\\nComputational Linguistics (2021).\\n[33] Hady Elsahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon Hare, Frederique Laforest, and Elena\\nSimperl. 2018. T-rex: A large scale alignment of natural language with knowledge base triples. In LREC .\\n[34] Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Jiliang Tang, and Qing Li. 2023. Recom-\\nmender systems in the era of large language models (llms). arXiv preprint arXiv:2307.02046 (2023).\\n[35] Hao Fei, Yafeng Ren, Yue Zhang, Donghong Ji, and Xiaohui Liang. 2021. Enriching contextualized language model\\nfrom knowledge graph for biomedical information extraction. Briefings in Bioinformatics (2021).\\n[36] Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-agnostic meta-learning for fast adaptation of deep\\nnetworks. In ICML .\\n[37] Boris A Galitsky. 2023. Truth-O-Meter: Collaborating with LLM in Fighting its Hallucinations. (2023).\\n[38] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez,\\nNicholas Schiefer, Kamal Ndousse, et al .2022. Red teaming language models to reduce harms: Methods, scaling\\nbehaviors, and lessons learned. arXiv preprint arXiv:2209.07858 (2022).\\n[39] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal,\\nand Marvin Ritter. 2017. Audio set: An ontology and human-labeled dataset for audio events. In ICASSP . IEEE,\\n776–780.\\n[40] Mor Geva, Avi Caciularu, Kevin Wang, and Yoav Goldberg. 2022. Transformer Feed-Forward Layers Build Predictions\\nby Promoting Concepts in the Vocabulary Space. In EMNLP .\\n[41] Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. 2021. Transformer Feed-Forward Layers Are Key-Value\\nMemories. In EMNLP . 5484–5495.\\n[42] Amelia Glaese, Nat McAleese, Maja Trębacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura\\nWeidinger, Martin Chadwick, Phoebe Thacker, et al .2022. Improving alignment of dialogue agents via targeted\\nhuman judgements. arXiv preprint arXiv:2209.14375 (2022).\\n[43] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\\nYoshua Bengio. 2020. Generative adversarial networks. Commun. ACM (2020).\\n[44] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the v in vqa matter:\\nElevating the role of image understanding in visual question answering. In CVPR .\\n[45] Anshita Gupta, Debanjan Mondal, Akshay Krishna Sheshadri, Wenlong Zhao, Xiang Lorraine Li, Sarah Wiegreffe,\\nand Niket Tandon. 2023. Editing Commonsense Knowledge in GPT. arXiv preprint arXiv:2305.14956 (2023).\\n[46] David Ha, Andrew Dai, and Quoc V. Le. 2016. HyperNetworks. arXiv:1609.09106 [cs.LG]\\n[47] Thomas Hartvigsen, Swami Sankaranarayanan, Hamid Palangi, Yoon Kim, and Marzyeh Ghassemi. 2022. Aging with\\nGRACE: Lifelong Model Editing with Discrete Key-Value Adaptors. In NeurIPS Workshop on Robustness in Sequence\\nModeling . 30 Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong Li\\n[48] Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and Srinivasan\\nIyer. 2023. Methods for measuring, updating, and visualizing factual beliefs in language models. In EACL .\\n[49] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In\\nCVPR . 770–778.\\n[50] Mark Heitmann. 2020. More than a feeling: Benchmarks for sentiment analysis accuracy. In More than a Feeling:\\nBenchmarks for Sentiment Analysis Accuracy: Heitmann, Mark .\\n[51] Simao Herdade, Armin Kappeler, Kofi Boakye, and Joao Soares. 2019. Image captioning: Transforming objects into\\nwords. NeurIPS (2019).\\n[52] Evan Hernandez, Belinda Z Li, and Jacob Andreas. 2023. Measuring and manipulating knowledge representations in\\nlanguage models. arXiv preprint arXiv:2304.00740 (2023).\\n[53] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural Computation 9, 8 (1997), 1735–1780.\\n[54] Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. 2022. Unnatural Instructions: Tuning Language Models\\nwith (Almost) No Human Labor. arXiv preprint arXiv:2212.09689 (2022).\\n[55] Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. 2021. Meta-learning in neural networks: A\\nsurvey. IEEE TPAMI (2021).\\n[56] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\\n2021. LoRA: Low-Rank Adaptation of Large Language Models. arXiv:2106.09685 [cs.CL]\\n[57] Linmei Hu, Zeyi Liu, Ziwang Zhao, Lei Hou, Liqiang Nie, and Juanzi Li. 2023. A Survey of Knowledge Enhanced\\nPre-Trained Language Models. IEEE TKDE (2023).\\n[58] Zhiqiang Hu, Yihuai Lan, Lei Wang, Wanyu Xu, Ee-Peng Lim, Roy Ka-Wei Lee, Lidong Bing, and Soujanya Poria. 2023.\\nLLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models. arXiv preprint\\narXiv:2304.01933 (2023).\\n[59] Zeyu Huang, Yikang Shen, Xiaofeng Zhang, Jie Zhou, Wenge Rong, and Zhang Xiong. 2023. Transformer-Patcher:\\nOne Mistake Worth One Neuron. In ICLR .\\n[60] Mike Huisman, Jan N Van Rijn, and Aske Plaat. 2021. A survey of deep meta-learning. Artificial Intelligence Review\\n(2021).\\n[61] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi.\\n2023. Editing models with task arithmetic. In ICLR .\\n[62] Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know?\\nTransactions of the Association for Computational Linguistics (2020).\\n[63] Katikapalli Subramanyam Kalyan, Ajit Rajasekharan, and Sivanesan Sangeetha. 2022. AMMU: a survey of transformer-\\nbased biomedical pretrained language models. Journal of Biomedical Informatics 126 (2022), 103982.\\n[64] Atoosa Kasirzadeh and Iason Gabriel. 2023. In conversation with Artificial Intelligence: aligning language models\\nwith human values. Philosophy & Technology (2023).\\n[65] Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser,\\nGeorg Groh, Stephan Günnemann, Eyke Hüllermeier, et al .2023. ChatGPT for good? On opportunities and challenges\\nof large language models for education. Learning and Individual Differences 103 (2023), 102274.\\n[66] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional\\nTransformers for Language Understanding. In NAACL .\\n[67] Teuvo Kohonen. 1972. Correlation matrix memories. IEEE Trans. Comput. (1972).\\n[68] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle\\nEpstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al .2019. Natural questions: a benchmark for question answering\\nresearch. Transactions of the Association for Computational Linguistics (2019).\\n[69] Nathan Lambert, Louis Castricato, Leandro von Werra, and Alex Havrilla. 2022. Illustrating Reinforcement Learning\\nfrom Human Feedback (RLHF). Hugging Face Blog (2022). https://huggingface.co/blog/rlhf.\\n[70] Kyungjae Lee, Wookje Han, Seung won Hwang, Hwaran Lee, Joonsuk Park, and Sang-Woo Lee. 2022. Plug-and-Play\\nAdaptation for Continuously-updated QA. arXiv:2204.12785\\n[71] Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. 2017. Zero-Shot Relation Extraction via Reading\\nComprehension. In CoNLL 2017 .\\n[72] Daliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix X. Yu, and Sanjiv\\nKumar. 2023. Large Language Models with Controllable Working Memory. In ACL.\\n[73] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pre-training with\\nfrozen image encoders and large language models. arXiv preprint arXiv:2301.12597 (2023).\\n[74] Xiaopeng Li, Shasha Li, Shezheng Song, Jing Yang, Jun Ma, and Jie Yu. 2023. PMET: Precise Model Editing in a\\nTransformer. arXiv preprint arXiv:2308.08742 (2023).\\n[75] Xiaonan Li and Xipeng Qiu. 2023. Finding supporting examples for in-context learning. arXiv preprint arXiv:2302.13539\\n(2023). Knowledge Editing for Large Language Models: A Survey 31\\n[76] Yuchao Li, Fuli Luo, Chuanqi Tan, Mengdi Wang, Songfang Huang, Shen Li, and Junjie Bai. 2022. Parameter-efficient\\nsparsity for large language models fine-tuning. arXiv preprint arXiv:2205.11005 (2022).\\n[77] Q Vera Liao and Jennifer Wortman Vaughan. 2023. AI Transparency in the Age of LLMs: A Human-Centered Research\\nRoadmap. arXiv preprint arXiv:2306.01941 (2023).\\n[78] Hao Liu, Carmelo Sferrazza, and Pieter Abbeel. 2023. Chain of hindsight aligns language models with feedback. arXiv\\npreprint arXiv:2302.02676 3 (2023).\\n[79] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel. 2022.\\nFew-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. In NeurIPS .\\n[80] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer,\\nand Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692\\n(2019).\\n[81] Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. 2023. An empirical study of catastrophic\\nforgetting in large language models during continual fine-tuning. arXiv preprint arXiv:2308.08747 (2023).\\n[82] Jun-Yu Ma, Jia-Chen Gu, Zhen-Hua Ling, Quan Liu, and Cong Liu. 2023. Untying the Reversal Curse via Bidirectional\\nLanguage Model Editing. arXiv preprint arXiv:2310.10322 (2023).\\n[83] Yuxuan Ma. 2021. distilgpt2-finetuned-wikitext2. https://huggingface.co/MYX4567/distilgpt2-finetuned-wikitext2\\n[84] Aman Madaan, Niket Tandon, Peter Clark, and Yiming Yang. 2022. Memory-assisted prompt editing to improve\\nGPT-3 after deployment. CoRR (2022).\\n[85] Potsawee Manakul, Adian Liusie, and Mark JF Gales. 2023. Selfcheckgpt: Zero-resource black-box hallucination\\ndetection for generative large language models. arXiv preprint arXiv:2303.08896 (2023).\\n[86] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating and editing factual associations in\\nGPT. In NeurIPS , Vol. 35. 17359–17372.\\n[87] Kevin Meng, Arnab Sen Sharma, Alex J Andonian, Yonatan Belinkov, and David Bau. 2023. Mass-Editing Memory in\\na Transformer. In The Eleventh International Conference on Learning Representations .\\n[88] Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah\\nYoung, Lucy Campbell-Gillingham, Geoffrey Irving, et al .2022. Teaching language models to support answers with\\nverified quotes. arXiv preprint arXiv:2203.11147 (2022).\\n[89] Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana\\nHeintz, and Dan Roth. 2023. Recent advances in natural language processing via large pre-trained language models:\\nA survey. Comput. Surveys 56, 2 (2023), 1–40.\\n[90] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D Manning. 2022. Fast Model Editing at\\nScale. In ICLR .\\n[91] Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D. Manning, and Chelsea Finn. 2022. Memory-Based Model\\nEditing at Scale. In ICML .\\n[92] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful\\nBari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al .2022. Crosslingual generalization through multitask\\nfinetuning. arXiv preprint arXiv:2211.01786 (2022).\\n[93] Shikhar Murty, Christopher D. Manning, Scott M. Lundberg, and Marco Túlio Ribeiro. 2022. Fixing Model Bugs with\\nNatural Language Patches. In EMNLP .\\n[94] Shiwen Ni, Dingwei Chen, Chengming Li, Xiping Hu, Ruifeng Xu, and Min Yang. 2023. Forgetting before Learning:\\nUtilizing Parametric Arithmetic for Knowledge Updating in Large Language Models. arXiv preprint arXiv:2311.08011\\n(2023).\\n[95] Yasumasa Onoe, Michael Zhang, Eunsol Choi, and Greg Durrett. 2022. Entity Cloze By Date: What LMs Know About\\nUnseen Entities. In Findings of NAACL .\\n[96] Yasumasa Onoe, Michael J. Q. Zhang, Shankar Padmanabhan, Greg Durrett, and Eunsol Choi. 2023. Can LMs Learn\\nNew Entities from Descriptions? Challenges in Propagating Injected Knowledge. In ACL.\\n[97] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]\\n[98] Hariom A. Pandya and Brijesh S. Bhatt. 2021. Question Answering Survey: Directions, Challenges, Datasets, Evaluation\\nMatrices. arXiv:2112.03572 [cs.CL]\\n[99] Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu,\\nWeizhu Chen, et al .2023. Check your facts and try again: Improving large language models with external knowledge\\nand automated feedback. arXiv preprint arXiv:2302.12813 (2023).\\n[100] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with GPT-4.\\narXiv preprint arXiv:2304.03277 (2023).\\n[101] Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese,\\nand Geoffrey Irving. 2022. Red Teaming Language Models with Language Models. In EMNLP . 32 Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong Li\\n[102] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine\\nJernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. 2021. KILT: a\\nBenchmark for Knowledge Intensive Language Tasks. In ACL.\\n[103] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller.\\n2019. Language Models as Knowledge Bases?. In EMNLP-IJCNLP .\\n[104] Yuval Pinter and Michael Elhadad. 2023. Emptying the Ocean with a Spoon: Should We Edit Models?. In EMNLP .\\n[105] Yujia Qin, Xiaozhi Wang, Yusheng Su, Yankai Lin, Ning Ding, Jing Yi, Weize Chen, Zhiyuan Liu, Juanzi Li, Lei\\nHou, Peng Li, Maosong Sun, and Jie Zhou. 2022. Exploring Universal Intrinsic Task Subspace via Prompt Tuning.\\narXiv:2110.07867 [cs.CL]\\n[106] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al .2018. Improving language understanding by\\ngenerative pre-training. (2018).\\n[107] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\\nPeter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of\\nMachine Learning Research (2020).\\n[108] Sachin Ravi and Hugo Larochelle. 2016. Optimization as a model for few-shot learning. In International conference on\\nlearning representations .\\n[109] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In\\nEMNLP-IJCNLP .\\n[110] EHUD REITER and ROBERT DALE. 1997. Building applied natural language generation systems. Natural Language\\nEngineering (1997).\\n[111] Marco Tulio Ribeiro and Scott Lundberg. 2022. Adaptive testing and debugging of NLP models. In ACL.\\n[112] Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Eric Michael\\nSmith, Y-Lan Boureau, et al. 2021. Recipes for Building an Open-Domain Chatbot. In EACL .\\n[113] Shibani Santurkar, Dimitris Tsipras, Mahalaxmi Elango, David Bau, Antonio Torralba, and Aleksander Madry. 2021.\\nEditing a classifier by rewriting its prediction rules. In NeurIPS .\\n[114] Christoph Schuhmann, Robert Kaczmarczyk, Aran Komatsuzaki, Aarush Katta, Richard Vencu, Romain Beaumont,\\nJenia Jitsev, Theo Coombes, and Clayton Mullis. 2021. LAION-400M: Open Dataset of CLIP-Filtered 400 Million\\nImage-Text Pairs. In NeurIPS Workshop Datacentric AI .\\n[115] Tal Schuster, Adam Fisch, and Regina Barzilay. 2021. Get Your Vitamin C! Robust Fact Verification with Contrastive\\nEvidence. In NAACL .\\n[116] Thomas Scialom, Tuhin Chakrabarty, and Smaranda Muresan. 2022. Fine-tuned language models are continual\\nlearners. In EMNLP .\\n[117] Gautam Kishore Shahi, Anne Dirkson, and Tim A Majchrzak. 2021. An exploratory study of COVID-19 misinformation\\non Twitter. Online social networks and media 22 (2021), 100104.\\n[118] Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. 2020. AutoPrompt: Eliciting\\nKnowledge from Language Models with Automatically Generated Prompts. In EMNLP .\\n[119] Anton Sinitsin, Vsevolod Plokhotnyuk, Dmitry Pyrkin, Sergei Popov, and Artem Babenko. 2020. Editable Neural\\nNetworks. In ICLR .\\n[120] Chenyang Song, Xu Han, Zheni Zeng, Kuai Li, Chen Chen, Zhiyuan Liu, Maosong Sun, and Tao Yang. 2023. ConPET:\\nContinual Parameter-Efficient Tuning for Large Language Models. arXiv preprint arXiv:2309.14763 (2023).\\n[121] Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang. 2023. Preference\\nRanking Optimization for Human Alignment. arXiv preprint arXiv:2306.17492 (2023).\\n[122] Felix Stahlberg. 2020. Neural machine translation: A review. Journal of Artificial Intelligence Research (2020).\\n[123] Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke\\nZettlemoyer, Noah A Smith, et al .2022. Selective annotation makes language models better few-shot learners. arXiv\\npreprint arXiv:2209.01975 (2022).\\n[124] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2018. Commonsenseqa: A question answering\\nchallenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937 (2018).\\n[125] Ryutaro Tanno, Melanie F Pradier, Aditya Nori, and Yingzhen Li. 2022. Repairing Neural Networks by Leaving the\\nRight Past Behind. In NeurIPS .\\n[126] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B.\\nHashimoto. 2023. Stanford Alpaca: An Instruction-following LLaMA model. https://github.com/tatsu-lab/stanford_\\nalpaca.\\n[127] Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel\\nShu Wei Ting. 2023. Large language models in medicine. Nature medicine 29, 8 (2023), 1930–1940.\\n[128] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a Large-scale Dataset\\nfor Fact Extraction and VERification. In ACL. Knowledge Editing for Large Language Models: A Survey 33\\n[129] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste\\nRozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al .2023. Llama: Open and efficient foundation language models.\\narXiv preprint arXiv:2302.13971 (2023).\\n[130] Joaquin Vanschoren. 2018. Meta-learning: A survey. arXiv preprint arXiv:1810.03548 (2018).\\n[131] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\\nPolosukhin. 2017. Attention is all you need. NeurIPS 30 (2017).\\n[132] Johannes von Oswald, Christian Henning, Benjamin F. Grewe, and João Sacramento. 2022. Continual learning with\\nhypernetworks. arXiv:1906.00695 [cs.LG]\\n[133] Denny Vrandečić and Markus Krötzsch. 2014. Wikidata: a free collaborative knowledgebase. Commun. ACM (2014).\\n[134] Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023.\\nLarge language models are not fair evaluators. arXiv preprint arXiv:2305.17926 (2023).\\n[135] Peng Wang, Ningyu Zhang, Xin Xie, Yunzhi Yao, Bozhong Tian, Mengru Wang, Zekun Xi, Siyuan Cheng, Kangwei\\nLiu, Guozhou Zheng, et al .2023. EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models.\\narXiv preprint arXiv:2308.07269 (2023).\\n[136] Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Guihong Cao, Daxin Jiang, Ming Zhou, et al .\\n2020. K-adapter: Infusing knowledge into pre-trained models with adapters. arXiv preprint arXiv:2002.01808 (2020).\\n[137] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi.\\n2022. Self-Instruct: Aligning Language Model with Self Generated Instructions. arXiv preprint arXiv:2212.10560\\n(2022).\\n[138] Yaqing Wang, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan Awadallah, and Jianfeng Gao. 2022.\\nAdamix: Mixture-of-adapter for parameter-efficient tuning of large language models. arXiv preprint arXiv:2205.12410\\n(2022).\\n[139] Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and\\nQun Liu. 2023. Aligning large language models with human: A survey. arXiv preprint arXiv:2307.12966 (2023).\\n[140] Mayur Wankhade, Annavarapu Chandra Sekhara Rao, and Chaitanya Kulkarni. 2022. A survey on sentiment analysis\\nmethods, applications, and challenges. Artificial Intelligence Review (2022).\\n[141] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and\\nQuoc V Le. 2021. Finetuned Language Models are Zero-Shot Learners.\\n[142] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al .2022.\\nChain-of-thought prompting elicits reasoning in large language models. In NeurIPS , Vol. 35. 24824–24837.\\n[143] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo\\nLopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al .2022. Robust fine-tuning of zero-shot models.\\nInCVPR .\\n[144] Xinwei Wu, Junzhuo Li, Minghui Xu, Weilong Dong, Shuangzhi Wu, Chao Bian, and Deyi Xiong. 2023. DEPN:\\nDetecting and Editing Privacy Neurons in Pretrained Language Models. In EMNLP .\\n[145] Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu Zhang.\\n2023. Editing Large Language Models: Problems, Methods, and Opportunities. arXiv preprint arXiv:2305.13172 (2023).\\n[146] Charles Yu, Sullam Jeoung, Anish Kasi, Pengfei Yu, and Heng Ji. 2023. Unlearning bias in language models by\\npartitioning gradients. In ACL Findings .\\n[147] Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. 2022. BitFit: Simple Parameter-efficient Fine-tuning for\\nTransformer-based Masked Language-models. In ACL.\\n[148] Michael Zhang and Eunsol Choi. 2021. SituatedQA: Incorporating Extra-Linguistic Contexts into QA. In EMNLP .\\n[149] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie\\nZhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223 (2023).\\n[150] Ce Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, Zhiyong Wu, Jingjing Xu, and Baobao Chang. 2023. Can We Edit\\nFactual Knowledge by In-Context Learning? arXiv:2305.12740 [cs.CL]\\n[151] Zexuan Zhong, Zhengxuan Wu, Christopher D. Manning, Christopher Potts, and Danqi Chen. 2023. MQuAKE: Assess-\\ning Knowledge Editing in Language Models via Multi-Hop Questions. CoRR abs/2305.14795 (2023). arXiv:2305.14795\\n[152] Jiawei Zhou, Yixuan Zhang, Qianni Luo, Andrea G Parker, and Munmun De Choudhury. 2023. Synthetic lies:\\nUnderstanding ai-generated misinformation and evaluating algorithmic and human solutions. In CHI. 1–20.\\n[153] Chen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix Yu, and Sanjiv Kumar. 2020.\\nModifying Memories in Transformer Models. arXiv:2012.00363 [cs.CL]\\n[154] Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. 2020. A\\ncomprehensive survey on transfer learning. Proc. IEEE (2020).\\n[155] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and\\nGeoffrey Irving. 2019. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593 (2019). '},\n",
       " {'url': 'http://arxiv.org/pdf/2310.05797v2',\n",
       "  'title': 'Are Large Language Models Post Hoc Explainers?',\n",
       "  'text': 'ARELARGE LANGUAGE MODELS POST HOC\\nEXPLAINERS ?\\nNicholas Kroeger1∗, Dan Ley2∗, Satyapriya Krishna2, Chirag Agarwal2, Himabindu Lakkaraju2\\n1University of Florida\\n2Harvard University\\nABSTRACT\\nLarge Language Models (LLMs) are increasingly used as powerful tools for a\\nplethora of natural language processing (NLP) applications. A recent innovation,\\nin-context learning (ICL), enables LLMs to learn new tasks by supplying a few\\nexamples in the prompt during inference time, thereby eliminating the need\\nfor model fine-tuning. While LLMs have been utilized in several applications,\\ntheir applicability in explaining the behavior of other models remains relatively\\nunexplored. Despite the growing number of new explanation techniques, many\\nrequire white-box access to the model and/or are computationally expensive,\\nhighlighting a need for next-generation post hoc explainers. In this work, we\\npresent the first framework to study the effectiveness of LLMs in explaining other\\npredictive models. More specifically, we propose a novel framework encompass-\\ning multiple prompting strategies: i) Perturbation-based ICL, ii) Prediction-based\\nICL, iii) Instruction-based ICL, and iv) Explanation-based ICL, with varying\\nlevels of information about the underlying ML model and the local neighborhood\\nof the test sample. We conduct extensive experiments with real-world benchmark\\ndatasets to demonstrate that LLM-generated explanations perform on par with\\nstate-of-the-art post hoc explainers using their ability to leverage ICL examples\\nand their internal knowledge in generating model explanations. On average,\\nacross four datasets and two ML models, we observe that LLMs identify the most\\nimportant feature with 72.19% accuracy, opening up new frontiers in explainable\\nartificial intelligence (XAI) to explore LLM-based explanation frameworks.\\n1 I NTRODUCTION\\nOver the past decade, machine learning (ML) models have become ubiquitous across various indus-\\ntries and applications. With their increasing use in critical applications ( e.g., healthcare, financial\\nsystems, and crime forecasting), it becomes essential to ensure that ML developers and practitioners\\nunderstand and trust their decisions. To this end, several approaches (Ribeiro et al., 2016; 2018;\\nSmilkov et al., 2017; Sundararajan et al., 2017; Lundberg & Lee, 2017; Shrikumar et al., 2017) have\\nbeen proposed in explainable artificial intelligence (XAI) literature to generate explanations for un-\\nderstanding model predictions. However, these explanation methods are highly sensitive to changes\\nin their hyperparameters (Yeh et al., 2019; Bansal et al., 2020), require access to the underlying\\nblack-box ML model (Lundberg & Lee, 2017; Ribeiro et al., 2016), and/or are often computationally\\nexpensive (Situ et al., 2021), thus impeding reproducibility and the trust of relevant stakeholders.\\nMore recently, generative models such as Large Language Models (LLMs) (Radford et al., 2017)\\nhave steered ML research into new directions and shown exceptional capabilities, allowing them\\nto surpass state-of-the-art models at complex tasks like machine learning translation (Hendy et al.,\\n2023), language understanding (Brown et al., 2020), commonsense reasoning (Wei et al., 2022b;\\nKrishna et al., 2023), and coding tasks (Bubeck et al., 2023). However, there is very little work\\non systematically analyzing the reliability of LLMs as explanation methods. While recent research\\nhas used LLMs to explain what patterns in a text cause a neuron to activate, they simply explain\\ncorrelations between the network input and specific neurons and do not explain what causes model\\n∗Equal contribution. Corresponding author: nkroeger@ufl.edu\\n1arXiv:2310.05797v2  [cs.CL]  10 Oct 2023 LLM API CallModelDatasetPrompt GeneratorReply ParserExplanationMetrics\\nPrompt Text“Context: We are analyzing a fixed set of perturbations around a specific input…{In-Context Learning Examples}…provide your answer as the top five most important features ranked …”\\nTop-K Features Reply“To determine the most important features, we need to…Answer: D, C, F, G, A”Figure 1: Overview of our framework. Given a dataset and model to explain, we provide 1) different prompt-\\ning strategies to generate explanations using LLMs, 2) functions to parse LLM-based explanations, 3) utility\\nfunctions to support new LLMs, and 4) diverse performance metrics to evaluate the faithfulness of explanations.\\nbehavior at a mechanistic level (Bills et al., 2023). Thus, the ability of LLMs to act as reliable\\nexplainers and improve the understanding of ML models lacks sufficient exploration.\\nPresent work. In this work, we present the first framework to study the effectiveness of LLMs\\nin explaining other predictive models (see Fig. 1). More specifically, we introduce four broad\\nprompting strategies — Perturbation-based ICL, Prediction-based ICL, Instruction-based ICL, and\\nExplanation-based ICL — for generating post hoc explanations using LLMs. Our first three strate-\\ngies entail providing local neighborhood samples and labels of a given instance whose prediction we\\nwant to explain, before asking an LLM to identify features that are key drivers in the model’s predic-\\ntions. In our last approach, we leverage the in-context learning (ICL) (Liu et al., 2023b) behavior of\\nLLMs by providing a small set of instances and their corresponding explanations (output by state-of-\\nthe-art post hoc explanation methods) as input to an LLM and ask it to generate feature importance-\\nbased explanations for new samples. We also explore different prompting and design choices, such\\nas increasing the level of information in each, to generate more faithful explanations using LLMs.\\nWe conduct extensive experimentation with four benchmark datasets, two black-box models, and\\ntwo GPT models to analyze the efficacy of our proposed framework. Our empirical studies reveal\\nthe following key findings. 1) LLMs, on average, accurately identify the most important feature (top-\\n𝑘=1) with 72.19% accuracy across different datasets, with performance drop for larger values of top-\\n𝑘features. 2) LLMs can mimic the behavior of six state-of-the-art post hoc explanation methods us-\\ning the proposed Explanation-based ICL prompting strategy and only four ICL samples. On average,\\nLLMs behave as post hoc explainers by providing explanations that are on par with existing methods,\\nsuch as LIME and gradient-based methods, in terms of their faithfulness. 3) LLMs struggle to re-\\ntrieve relevant information from longer prompts, resulting in a decrease in the faithfulness of the ex-\\nplanations generated using a large set of ICL samples. 4) Our proposed framework paves the way for\\na new paradigm in XAI research, where LLMs can aid in explaining black-box model predictions.\\n2 R ELATED WORKS\\nOur work lies at the intersection of post hoc explanations, large language models, and in-context\\nlearning, which we discuss below.\\nPost Hoc Explanations. The task of understanding model predictions has become increasingly in-\\ntricate with the growing popularity of complex ML models (Doshi-Velez & Kim, 2017) due to their\\ninherent black box nature, which makes it difficult to interpret their internal reasoning. To this end, a\\nplethora of feature attribution methods (commonly referred to as post hoc explanation methods) have\\nbeen proposed to provide explanations for these models’ predictions. These explanations are pre-\\ndominantly presented in the form of feature attributions, which highlight the importance of each in-\\nput feature on the model’s prediction. Broadly, post hoc explainers can be divided into perturbation-\\nbased and gradient-based methods. While perturbation-based methods (Ribeiro et al., 2016; Lund-\\nberg & Lee, 2017; Zeiler & Fergus, 2014) leverage perturbations of the given instance to construct\\nan interpretable approximation of the black-box model behavior, gradient-based methods (Smilkov\\net al., 2017; Sundararajan et al., 2017) leverage gradients w.r.t. the given instance to explain model\\npredictions. In this work, we primarily focus on state-of-the-art local post hoc explainers, i.e.,meth-\\nods explaining individual feature importance for model predictions of individual instances.\\nLarge Language Models. LLMs have seen exponential growth in recent years, both in terms of\\ntheir size and the complexity of tasks they can perform (Radford et al., 2017). Recent advances in\\nLLMs like GPT-4 (OpenAI), Bard (Google), Claude-2 (Anthropic) and Llama-2 (Meta) are chang-\\n2 ing the paradigm of NLP research and have led to their widespread use across applications spanning\\nmachine translation (Vaswani et al., 2017), question-answering (Brown et al., 2020), text genera-\\ntion (Radford et al., 2017), and medical data records (Lee et al., 2020; Alsentzer et al., 2019). In this\\nwork, we, for the first time, explore the use of LLMs in explaining other predictive models.\\nIn-context Learning. While the high performance and generalization capabilities have led to highly\\neffective language models for numerous tasks (Wei et al., 2022a), they have also increased the mod-\\nels’ parameter sizes and the computational costs for additional fine-tuning on new downstream tasks.\\nTo alleviate this, recent works have introduced in-context learning (ICL), which allows an LLM to\\nperform well on new tasks by simply using a few task samples in the prompt (Liu et al., 2023b). De-\\nspite their effectiveness in enhancing the performance of LLMs, these methods have not been thor-\\noughly explored for their potential to generate post-hoc explanations. In this work, we investigate the\\nutility of LLMs in generating post hoc explanations by leveraging their in-context learning abilities.\\n3 O URFRAMEWORK\\nNext, we describe our framework that aims to generate explanations using LLMs. To achieve this\\ngoal, we outline four distinct prompting strategies — Perturbation-based ICL (Sec. 3.1), Prediction-\\nbased ICL (Sec. 3.2), Instruction-based ICL (Sec. 3.3), and Explanation-based ICL (Sec. 3.4).\\nNotation. Let𝑓:R𝑑→ [ 0,1]denote a black-box ML model that takes an input x∈R𝑑and\\nreturns the probability of xbelonging to a class 𝑐∈Cand the predicted label 𝑦. Following previous\\nXAI works (Ribeiro et al., 2016; Smilkov et al., 2017), we randomly sample points from the local\\nneighborhood N𝑥of the given input xto generate explanations, where N𝑥=N(x, 𝜎2)denotes the\\nneighborhood of perturbations around xusing a Normal distribution with mean 0 and variance 𝜎2.\\n3.1 P ERTURBATION -BASED ICL\\nIn the Perturbation-based ICL prompting strategy, we use an LLM to explain 𝑓, trained on tabular\\ndata, by querying the LLM to identify the top- 𝑘most important features in determining the output of\\n𝑓in a rank-ordered manner. To tackle this, we sample input-output pairs from the neighborhood N𝑥\\nofxand generate their respective strings following a serialization template; for instance, a perturbed\\nsample’s feature vector x′=[0.058,0.632,−0.015,1.012,−0.022,−0.108], belonging to class 0 in\\nthe COMPAS dataset, is converted into a natural-language string as:\\n#Serialization template\\nInput: A = 0.058, B = 0.632, C = -0.015, D = 1.012, E = -0.022, F = -0.108\\nOutput: 0\\nWhile previous post hoc explainers suggest using a large number of neighborhood samples (Ribeiro\\net al., 2016; Smilkov et al., 2017), it is impractical to provide all samples from N𝑥in the prompt for\\nan LLM due to their constraint on the maximum context length and performance loss when given\\nmore information (Liu et al., 2023a). Consequently, we select 𝑛ICLsamples from N𝑥to use in the\\nLLM’s prompt. In the interest of maintaining a neutral and fundamental approach, we employ two\\nprimary sampling strategies, both selecting balanced class representation within the neighborhoods\\ndefined by N𝑥. The first strategy selects samples randomly, while the second chooses those with the\\nhighest confidence levels, aiding the LLM in generating explanations centered on model certainty.\\nGiven 𝑛ICLinput-output pairs from N𝑥and the test sample xto be explained, we add context with\\nrespect to the predictive model, dataset, and task description in our prompt to aid the LLM in be-\\nhaving like a post hoc explanation method. Motivated by the local neighborhood approximation\\nworks in XAI, the Perturbation-based ICL prompting strategy presumes that the local behavior of\\n𝑓is a simple linear decision boundary, contrasting with the often globally exhibited complex non-\\nlinear decision boundary. Hence, assuming a sufficient number of perturbations in N𝑥, the LLM\\nis expected to accurately approximate the black box model’s behavior and utilize this information\\nto identify the top- 𝑘most important features. The final prompt structure is given below, where the\\n“Context” provides the LLM with the background of the underlying ML model, the number of fea-\\ntures in the dataset, and model predictions, “Dataset” denotes the 𝑛ICLinstances sampled from the\\n3 neighborhood N𝑥ofx,“Question” is the task we want our LLM to perform, and “Instructions” are\\nthe guidelines we want the LLM to follow while generating the output explanations.\\n# Perturbation-based ICL Prompt Template\\nContext : “We have a two-class machine learning model that predicts based on 6 features: [‘A’, ‘B’, ‘C’,\\n‘D’, ‘E’, ‘F’]. The model has been trained on a dataset and has made the following predictions. ”\\nDataset:\\nInput: A = -0.158, B = 0.293, C = 0.248, D = 1.130, E = 0.013, F = -0.038\\nOutput: 0\\n. . .\\nInput: A = 0.427, B = 0.016, C = -0.128, D = 0.949, E = 0.035, F = -0.045\\nOutput: 1\\nQuestion: “Based on the model’s predictions and the given dataset, what appears to be the top five most\\nimportant features in determining the model’s prediction?”\\nInstructions: “Think about the question. After explaining your reasoning, provide your answer as the top\\nfive most important features ranked from most important to least important, in descending order. Only\\nprovide the feature names on the last line. Do not provide any further details on the last line. ”\\n3.2 P REDICTION -BASED ICL\\nHere, we devise Prediction-based ICL, a strategy closer to the traditional ICL prompting style, where\\nthe primary objective remains the same — understanding the workings of the black-box model 𝑓by\\nidentifying the top- 𝑘most important features. This strategy positions the LLM to first emulate the\\nrole of the black-box model by making predictions, staging it to extract important features that influ-\\nenced its decision. We follow the perturbation strategy of Sec. 3.1 and construct the Prediction-based\\nICL prompt using 𝑛ICLinput-output pairs from N𝑥. The main difference in the Prediction-based\\nICL prompting strategy lies in the structuring of the prompt, which is described below:\\n#Prediction-based ICL Prompt Template\\nContext: “We have a two-class machine learning model that predicts based on 6 features: [‘A’, ‘B’, ‘C’,\\n‘D’, ‘E’, ‘F’]. The model has been trained on a dataset and has made the following predictions. ”\\nDataset:\\nInput: A = 0.192, B = 0.240, C = 0.118, D = 1.007, E = 0.091, F = 0.025\\nOutput: 0\\n. . .\\nInput: A = 0.709, B = -0.102, C = -0.177, D = 1.056, E = -0.056, F = 0.015\\nOutput: 1\\nInput: A = 0.565, B = -0.184, C = -0.386, D = 1.003, E = -0.123, F = -0.068\\nOutput:\\nQuestion: “Based on the model’s predictions and the given dataset, estimate the output for the final input.\\nWhat appears to be the top five most important features in determining the model’s prediction?”\\nInstructions: “Think about the question. After explaining your reasoning, provide your answer as the top\\nfive most important features ranked from most important to least important, in descending order. Only\\nprovide the feature names on the last line. Do not provide any further details on the last line. ”\\nHere, we construct the prompt using the task description followed by the 𝑛ICLICL samples and then\\nask the LLM to provide the predicted label for the test sample xand explain how it generated that\\nlabel. The primary motivation behind the Prediction-based ICL prompting strategy is to investigate\\nwhether the LLM can learn the classification task using the ICL set and, if successful, identify\\nthe important features in the process. This approach aligns more closely with the traditional ICL\\nprompting style, offering a different perspective on the problem.\\n3.3 I NSTRUCTION -BASED ICL\\nThe Instruction-based prompting transitions from specifying task objectives to providing detailed\\nguidance on the strategy for task execution. Rather than solely instructing the LLM on what the task\\nentails, this strategy delineates how to conduct the given task. The objective remains to understand\\nthe workings of the black-box model and identify the top- 𝑘most important features. However, in\\n4 using step-by-step directives, we aim to induce a more structured and consistent analytical process\\nwithin the LLM to target more faithful explanations. The final prompt structure is as follows:\\n#Instruction-based ICL Prompt Template\\nContext: “We are analyzing a fixed set of perturbations around a specific input to understand the influ-\\nence of each feature on the model’s output. The dataset below contains the change in features ‘A’ through\\n‘F’ (with negative values denoting a decrease in a feature’s value) and the corresponding outputs. ”\\nDataset:\\nChange in Input: A: -0.217, B: 0.240, C: 0.114, D: 0.007, E: 0.091, F: 0.025\\nChange in Output: -1\\n. . .\\nChange in Input: A: 0.185, B: -0.185, C: -0.232, D: -0.130, E: -0.020, F: 0.015\\nChange in Output: 0\\nInstructions: “For each feature, starting with ‘A’ and continuing to ‘F’:\\n1. Analyze the feature in question:\\na. Compare instances where its changes are positive to where its changes are negative and explain how\\nthis difference correlates with the change in output.\\nb. Rate the importance of the feature in determining the output on a scale of 0-100, considering both pos-\\nitive and negative correlations. Ensure to give equal emphasis to both positive and negative correlations\\nand avoid focusing only on absolute values.\\n2. After analyzing the feature, position it in a running rank compared to the features already analyzed.\\nFor instance, after analyzing feature ‘B’, determine its relative importance compared to ‘A’ and position it\\naccordingly in the rank (e.g., BA or AB). Continue this process until all features from ‘A’ to ‘F’ are ranked.\\nUpon completion of all analyses, provide the final rank of features from ‘A’ to ‘F’ on the last line.\\nAvoid providing general methodologies or suggesting tools. Justify your findings as you go. ”\\nHere, we provide some general instructions to the LLM for understanding the notion of important\\nfeatures and how to interpret them through the lens of correlation analysis. To achieve this, we\\ninstruct LLMs to analyze each feature sequentially and ensure that both positive and negative corre-\\nlations are equally emphasized. The LLM assigns an importance score for each feature in the given\\ndataset and then positions it in a running rank. This rank is necessary to differentiate features and\\navoid ties in the LLM’s evaluations. The final line ensures that the LLM’s responses are strictly an-\\nalytical, minimizing non-responsiveness or digressions into tool or methodology recommendations.\\n3.4 E XPLANATION -BASED ICL\\nRecent studies show that LLMs can learn new tasks through ICL, enabling them to excel in\\nnew downstream tasks by merely observing a few instances of the task in the prompt. In the\\nExplanation-based ICL prompting strategy, we leverage the ICL capability of LLMs to alleviate\\nthe computation complexity of some post hoc explanation methods. In particular, we investigate\\nwhether an LLM can mimic the behavior of a post hoc explainer by looking at a few input, output,\\nand explanation examples. We generate explanations for a given test sample xusing LLMs by\\nutilizing the ICL framework and supplying 𝑛ICLinput, output, and explanation examples to the\\nLLM, where the explanations in the ICL can be generated using any post hoc explanation method.\\nFor constructing the ICL set, we randomly select 𝑛ICLinput instances XICLfrom the ICL split of\\nthe dataset and generate their predicted labels yICLusing model 𝑓. Next, we generate explanations\\nEICLfor samples ( XICL,yICL) using any post hoc explainer. Using the above input, output, and\\nexplanation samples, we construct a prompt by concatenating each pair as follows:\\n#Explanation-based ICL Prompt Template\\nInput: A = 0.172, B = 0.000, C = 0.000, D = 1.000, E = 0.000, F = 0.000\\nOutput: 1\\nExplanation: A,C,B,F ,D,E\\n. . .\\nInput: A = 0.052, B = 0.053, C = 0.073, D = 0.000, E = 0.000, F = 1.000\\nOutput: 0\\nExplanation: A,B,C,E,F ,D\\nInput: A = 0.180, B = 0.222, C = 0.002, D = 0.000, E = 0.000, F = 1.000\\nOutput: 0\\nExplanation:\\n5 Using the Explanation-based ICL prompting strategy, we aim to investigate the learning capability\\nof LLMs such that they can generate faithful explanations by examining the 𝑛ICLdemonstration\\npairs of inputs, outputs, and explanations generated by state-of-the-art post hoc explainer.\\n4 E XPERIMENTS\\nNext, we evaluate the effectiveness of LLMs as post hoc explainers. More specifically, our ex-\\nperimental analysis focuses on the following questions: Q1) Can LLMs generate faithful post hoc\\nexplanations? Q2) Do LLM-Augmented post hoc explainers achieve similar faithfulness vs. their\\nvanilla counterpart? Q3) Are LLMs better than state-of-the-art post hoc explainers at identifying the\\nmost important feature? Q4) Is G PT-4 a better explainer than G PT-3.5? Q5) Are changes to the\\nLLM’s prompting strategy necessary for generating faithful explanations?\\n4.1 D ATASETS AND EXPERIMENTAL SETUP\\nWe first describe the datasets and models used to study the reliability of LLMs as post hoc explainers\\nand then outline the experimental setup.\\nDatasets. Following previous LLM works (Hegselmann et al., 2023), we performed analysis on four\\nreal-world tabular datasets: Blood (Yeh et al., 2009) having four features, Recidivism (ProPublica)\\nhaving six features, Adult (Kaggle) having 13 features, and Default Credit (UCI) having 10 fea-\\ntures. The datasets come with a random train-test split, and we further subdivide the train set, allocat-\\ning 80% for training and the remaining 20% for ICL sample selection, as detailed in Sec. 3.4. We use\\na random set of 100 samples from the test split to generate explanations for all of our experiments.\\nPredictive Models. We consider two ML models with varying complexity in our experiments: i) Lo-\\ngistic Regression (LR) and ii) Artificial Neural Networks (ANN). We use PyTorch (Paszke et al.,\\n2019) to implement the ANNs with the following combination of hidden layers: one layer of size 16\\nfor the LR model; and three layers of size 64, 32, and 16 for the ANN, using R ELU for the hidden\\nlayers and S OFTMAX for the output (see Table 1 for predictive performances of these models).\\nLarge Language Model. We consider G PT-3.5 and G PT-4 as language models for all experiments.\\nBaseline Explanation Methods. We use six post hoc explainers as baselines to investigate the ef-\\nfectiveness of explanations generated using LLMs: LIME (Ribeiro et al., 2016), SHAP (Lundberg\\n& Lee, 2017), Vanilla Gradients (Zeiler & Fergus, 2014), SmoothGrad (Smilkov et al., 2017), Inte-\\ngrated Gradients (Sundararajan et al., 2017), and Gradient x Input (ITG) (Shrikumar et al., 2017).\\nPerformance Metrics. We employ four distinct metrics to measure the faithfulness of an explana-\\ntion. To quantify the faithfulness of an explanation where there exists a ground-truth top- 𝑘explana-\\ntion for each test input ( i.e.,LR model coefficients), we use the Feature Agreement (FA) and Rank\\nAgreement (RA) metrics introduced in Krishna et al. (2022), which compares the LLM’s top- 𝑘di-\\nrectly with the model’s ground-truth. The FA and RA metrics range from [0,1], where 0 means no\\nagreement and 1 means full agreement. However, in the absence of a top- 𝑘ground-truth explanation\\n(as is the case with ANNs), we use the Prediction Gap on Important feature perturbation (PGI) and\\nthe Prediction Gap on Unimportant feature perturbation (PGU) metrics from OpenXAI (Agarwal\\net al., 2022). While PGI measures the change in prediction probability that results from perturbing\\nthe features deemed as influential, PGU examines the impact of perturbing unimportant features.\\nHere, the perturbations are generated using Gaussian noise N(0, 𝜎2).\\nImplementation Details. To generate perturbations for each ICL prompt, we use a neighborhood\\nsize of 𝜎=0.1and generate local perturbation neighborhoods N𝑥for each test sample x. We sample\\nn𝑥=10,000points sampled for each neighborhood, where the values for 𝜎andn𝑥were chosen to\\ngive an equal number of samples for each class, whenever possible. We present perturbations in\\ntwo main formats: as the raw perturbed inputs alongside their corresponding outputs (shown in the\\nSec. 3.1 and 3.2 templates); or as the change between each perturbed input and the test sample, and\\nthe corresponding change in output (shown in Sec. 3.3). The second approach significantly aids\\nthe LLM in discerning the most important features, providing only the changes relative to the test\\nsample, and bypassing the LLM’s need to internally compute these differences. As a result, the\\nconsistent value of the original test point becomes irrelevant, and this clearer, relational view allows\\n6 the LLM to focus directly on variations in input and output. Note that both of these formats are\\nabsent from Sec. 3.4, which uses test samples directly and does not compute perturbations.\\nFor the LLMs, we use OpenAI’s text generation API with a temperature of 𝜏=0for our main\\nexperiments. To evaluate the LLM explanations, we extract and process its answers to identify the\\ntop-𝑘most important features. We first save each LLM query’s reply to a text file and use a script to\\nextract the features. We added explicit instructions like “ . . . provide your answer as a feature name\\non the last line. Do not provide any further details on the last line. ” to ensure reliable parsing of\\nLLM outputs. In rare cases, the LLM won’t follow our requested response format or it replies with\\n“I don’t have enough information to determine the most important features. ” See Appendix 6.1 for\\nfurther details. Codes for our framework are available here1.\\n4.2 R ESULTS\\nNext, we discuss experimental results that answer key questions highlighted at the beginning of this\\nsection about LLMs as post hoc explainers (Q1-Q5).\\n1) LLMs can generate faithful explanations. We compare our proposed prompting-based LLM\\nexplanation strategies to existing post hoc explainers on the task of identifying important features for\\nunderstanding ANN (Fig. 2) and LR (Fig. 3) model predictions across four real-world datasets (see\\nTable 2). For the ANN model, the LLM-based explanations perform on par with the gradient-based\\nmethods (despite having white-box access to the underlying black-box model) and LIME (that ap-\\nproximates model behavior using a surrogate linear model). In particular, our proposed prompting\\nstrategies perform better than ITG, SHAP, a Random baseline, and a 16-sample version of LIME,\\nnamely LIME 16, which is analogous to the number of ICL samples used in the LLM prompts. We\\nobserve that LLM explanations, on average, achieve 51.74% lower PGU and 163.40% higher PGI\\nthan ITG, SHAP, and Random baseline for larger datasets (more number of features) like Adult and\\nCredit compared to 25.56% lower PGU and 22.86% higher PGI for Blood and Recidivism datasets.\\nWhile our prompting strategies achieve competitive PGU and PGI scores among themselves across\\ndifferent datasets for ANN models, the Instruction-based ICL strategy, on average across datasets,\\nachieves higher FA and RA scores for the LR model (Fig. 3). We find that gradient-based methods\\nand LIME achieve almost perfect scores on FA and RA metrics as they are able to get accurate model\\ngradients and approximate the model behavior with high precision. Interestingly, the LLM-based ex-\\nplanations perform better than ITG, SHAP, and Random baseline methods, even for a linear model.\\nP3: Instruction-based ICL   P2: Prediction-based ICL   P1: Perturbation-based ICL  \\nFigure 2: PGU and PGI scores of explanations generated using post hoc methods and LLMs\\n(Instruction-based, Prediction-based, and Perturbation-based ICL prompting strategies) for an ANN\\nmodel. On average, across four datasets, we find that LLM-based explanations perform on par with\\ngradient-based and LIME methods and outperform LIME 16, ITG, and SHAP methods.\\n2) LLM-augmented explainers achieve similar faithfulness to their vanilla counterparts. We\\nevaluate the faithfulness of the explanations generated using the Explanation-based ICL prompting\\nstrategy. Our results show that LLMs generate explanations that achieve faithfulness performance\\non par with those generated using state-of-the-art post hoc explanation methods for LR and large\\nANN predictive models across all four datasets (Fig. 4; see Table 3 for complete results) and four\\nevaluation metrics. We demonstrate that very few in-context examples (here, 𝑛ICL=4) are sufficient\\nto make the LLM mimic the behavior of any post hoc explainer and generate faithful explanations,\\nsuggesting the effectiveness of LLMs as an explanation method. Interestingly, for low-performing\\nexplanation methods like ITG and SHAP, we find that explanations generated using their LLM\\n1GitHub link: https://github.com/AI4LIFE-GROUP/LLM Explainer\\n7 P3: Instruction-based ICL   P2: Prediction-based ICL   P1: Perturbation-based ICL  Figure 3: FA and RA scores of explanations generated using post hoc methods and LLMs\\n(Instruction-based, Prediction-based, and Perturbation-based ICL prompting strategies) for an\\nLR model. On average, across four datasets, we find that gradient-based methods and the LIME\\nmethod (with 1000 samples) outperform all other methods and Instruction-based ICL explanations\\noutperform other two prompting strategies across all datasets.\\ncounterparts achieve higher feature and rank agreement (Fig. 4) scores in the case of LR models,\\nhinting that LLMs can use their internal knowledge to improve the faithfulness of explanations.\\nLOGISTIC REGRESSION NEURAL NETWORK\\nGrad SG IG ITGSHAP LIME\\nExplanation Methods0.000.250.500.751.00FA ()\\nGrad SG IG ITGSHAP LIME\\nExplanation Methods0.000.250.500.751.00RA ()\\nGrad SG IG ITGSHAP LIME\\nExplanation Methods0.000.020.040.060.08PGU ()\\nGrad SG IG ITGSHAP LIME\\nExplanation Methods0.000.050.100.15PGI ()\\nLLM-Augmented ExplainerBase Post Hoc Explainer\\nFigure 4: Faithfulness metrics on the Recidivism dataset for six post hoc explainers and their\\nLLM-augmented counterparts for a given LR (left) and ANN (right) model. LLM-augmented\\nexplanations achieve on-par performance w.r.t. post hoc methods across all four metrics (see Table 3\\nfor complete results on all other datasets).\\n3) LLMs accurately identify the most important feature. To demonstrate the LLM’s capability\\nin identifying the most important feature, we show the faithfulness performance of generated\\nexplanations across four datasets. Our results in Fig. 5 demonstrate the impact of different top- 𝑘\\nfeature values on the faithfulness of explanations generated using our prompting strategies. We\\nobserve a steady decrease in RA scores (0.722 for top- 𝑘=1vs. 0.446 for top- 𝑘=2vs. 0.376\\nfor top- 𝑘=4) across three datasets (Blood, Credit, and Adult) as the top- 𝑘value increases.\\nInterestingly, the RA value for top- 𝑘=1for the Recidivism dataset is almost zero, though this can\\nbe attributed to the LLM’s handling of the two primary features, whose LR coefficients have nearly\\nidentical magnitudes; the LLM generally places them both within the top two but, due to their\\nsimilar importance, defaults to alphabetical order. However, when employing our Instruction-based\\nICL running-rank strategy, we find that the RA value rises from 0 to 0.5, highlighting the influence\\nof nuanced prompts on the LLM’s ranking mechanism. Further, we observe that LLMs, on average\\nacross four datasets and three prompting strategies, faithfully identify top- 𝑘=1features with\\n72.19% FA score (see Fig. 12), and their faithfulness performance takes a hit for higher top- 𝑘values.\\n4) G PT-3.5 vs. G PT-4.An interesting question is how the reasoning capability of an LLM affects\\nthe faithfulness of the generated explanations. Hence, we compare the output explanations from\\nGPT-3.5 and G PT-4 models to understand black-box model predictions. Results in Fig. 6-8 show\\nthat explanations generated using G PT-4, on average across four datasets, achieve higher faithful-\\nness scores than explanations generated using the G PT-3.5 model. Across four prompting strategies,\\nGPT-4, on average, obtains 4.53% higher FA and 48.01% higher RA scores than G PT-3.5 on\\nexplanations generated for the Adult dataset. We attribute this increase in performance of G PT-4 to\\nits superior reasoning capabilities compared to the G PT-3.5 model (OpenAI, 2023). In Figure 6, we\\nfind that Instruction-based ICL, on average across four datasets, outperforms the Perturbation-based\\nICL and Prediction-based ICL strategies on the RA metric. Further, our results in Fig. 8 show that\\nthe faithfulness performance of G PT-4 and G PT-3.5 are on par with each other when evaluated using\\nour Explanation-based ICL strategy, which highlights that both models are capable of emulating the\\nbehavior of a post hoc explainer by looking at a few input, output, and explanation examples.\\n8 PERTURBATION -BASED ICL P REDICTION -BASED ICL I NSTRUCTION -BASED ICL\\n1 2 3 4 5\\nT opK Feature Importance0.00.20.40.60.81.0RA ()\\n1 2 3 4 5\\nT opK Feature Importance0.00.20.40.60.81.0RA ()\\n1 2 3 4 5\\nT opK Feature Importance0.00.20.40.60.81.0RA ()\\nBlood\\nRecidivism\\nCredit\\nAdult\\nFigure 5: Effects of top- 𝑘value on the RA metric using Perturbation-based, Prediction-based, and Instruction-\\nbased ICL prompting strategies. Shown are the results for three prompting strategies and four datasets using\\nthe LR model. On average, LLMs successfully achieve high scores in identifying the most important feature\\n(top- 𝑘=1) and the performance decreases as we increase the top- 𝑘value (see Fig. 12 for results on FA).\\nPERTURBATION -BASED ICL P REDICTION -BASED ICL I NSTRUCTION -BASED ICL\\nBlood Recidivism Credit Adult0.00.20.40.6RA ()\\nBlood Recidivism Credit Adult0.00.20.40.6RA ()\\nFigure 6: RA faithfulness metric of explanations generated using Perturbation-based ICL, Prediction-based\\nICL, and Instruction-based ICL prompting strategies on four real-world datasets. Explanations from G PT-4, on\\naverage, achieve higher RA scores than their G PT-3.5 counterparts (see Figures 8-9 for similar plots on Feature\\nAgreement metric and Explanation-based ICL strategy).\\n5) Ablation Study. We conduct ablations on several components of the prompting strategies,\\nnamely the number of ICL samples, perturbation format, and temperature values. Results show that\\nour choice of hyperparameter values is important for the prompting techniques to generate faithful\\npost hoc explanations (Figs. 7,10). Our ablation on the number of ICL samples (Fig. 7) shows that\\nfewer and larger numbers of ICL samples are not beneficial for LLMs to generate post hoc expla-\\nnations. While fewer ICL samples provide insufficient information to the LLM to approximate the\\npredictive behavior of the underlying ML model, a large number of ICL samples increases the input\\ncontext, where the LLM struggles to retrieve relevant information from longer prompts, resulting\\nin a decrease in the faithfulness of the explanations generated by LLMs. In contrast to LIME,\\nthe faithfulness of LLM explanations deteriorates upon increasing the number of ICL samples\\n(analogous to the neighborhood of a given test sample). Across all four prompting strategies, we\\nobserve a drop in FA, RA, and PGI scores as we increase the number of ICL samples to 64. Further,\\nour ablation on the temperature parameter of the LLMs shows that the faithfulness performance\\nof the explanations does not change much across different values of temperature (see Appendix\\nFig. 10). Finally, results in Fig. 11 show that our prompting strategies achieve higher faithfulness\\nwhen using the difference between the perturbed and test sample as input in the ICL sample.\\n4 8 12 16 64\\nnshot in ICL samples0.20.40.60.8FA ()\\n4 8 12 16 64\\nnshot in ICL samples0.00.20.40.60.8RA ()\\n4 8 12 16 64\\nnshot in ICL samples0.050.100.150.20PGI ()\\nFigure 7: FA, RA, and PGI Performance of LIME and all four proposed prompting strategies as we increase\\nthe number of ICL samples (analogous to neighborhood samples in LIME) for the LR model trained on the\\nAdult dataset. In contrast to LIME, the faithfulness of LLM explanations across different metrics decreases for\\na higher number of ICL samples, likely due to the limited capabilities of LLM for longer prompt length.\\n9 5 C ONCLUSION\\nWe introduce and explore the potential of using state-of-the-art LLMs as post hoc explainers. To\\nthis end, we propose four prompting strategies — Perturbation-based ICL, Prediction-based ICL,\\nInstruction-based ICL, and Explanation-based ICL— with varying levels of information about the\\nlocal neighborhood of a test sample to generate explanations using LLMs for black-box model pre-\\ndictions. We conducted several experiments to evaluate LLM-generated explanations using four\\nbenchmark datasets. Our results across different prompting strategies highlight that LLMs can gen-\\nerate faithful explanations and consistently outperform methods like ITG and SHAP. Our work paves\\nthe way for several exciting future directions in explainable artificial intelligence (XAI) to explore\\nLLM-based explanation frameworks.\\nREFERENCES\\nChirag Agarwal, Satyapriya Krishna, Eshika Saxena, Martin Pawelczyk, Nari Johnson, Isha Puri,\\nMarinka Zitnik, and Himabindu Lakkaraju. Openxai: Towards a transparent evaluation of model\\nexplanations. NeurIPS , 2022.\\nEmily Alsentzer, John R Murphy, Willie Boag, Wei-Hung Weng, Di Jin, Tristan Naumann, and\\nMatthew McDermott. Publicly available clinical bert embeddings. arXiv , 2019.\\nAnthropic. Anthropic \\\\claude 2. https://www.anthropic.com/index/claude-2 . (Ac-\\ncessed on 07/17/2023).\\nNaman Bansal, Chirag Agarwal, and Anh Nguyen. Sam: The sensitivity of attribution methods to\\nhyperparameters. In CVPR , 2020.\\nSteven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya\\nSutskever, Jan Leike, Jeff Wu, and William Saunders. Language models can explain\\nneurons in language models. https://openaipublic.blob.core.windows.net/\\nneuron-explainer/paper/index.html , 2023.\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\\nfew-shot learners. NeurIPS , 2020.\\nS´ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Ka-\\nmar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general\\nintelligence: Early experiments with gpt-4. arXiv , 2023.\\nFinale Doshi-Velez and Been Kim. Towards a rigorous science of interpretable machine learning.\\narXiv , 2017.\\nGoogle. Try bard, an ai experiment by google. https://bard.google.com/ . (Accessed on\\n07/17/2023).\\nStefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, and David\\nSontag. Tabllm: Few-shot classification of tabular data with large language models. In AISTATS .\\nPMLR, 2023.\\nAmr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Mat-\\nsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. How good are gpt models\\nat machine translation? a comprehensive evaluation. arXiv , 2023.\\nKaggle. Adult income dataset. https://www.kaggle.com/wenruliu/\\nadult-income-dataset . Accessed: 2020-01-01.\\nSatyapriya Krishna, Tessa Han, Alex Gu, Javin Pombra, Shahin Jabbari, Steven Wu, and Himabindu\\nLakkaraju. The disagreement problem in explainable machine learning: A practitioner’s perspec-\\ntive. arXiv , 2022.\\nSatyapriya Krishna, Jiaqi Ma, Dylan Slack, Asma Ghandeharioun, Sameer Singh, and Himabindu\\nLakkaraju. Post hoc explanations of language models can improve language models. arXiv , 2023.\\n10 Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jae-\\nwoo Kang. Biobert: a pre-trained biomedical language representation model for biomedical text\\nmining. Bioinformatics , 2020.\\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and\\nPercy Liang. Lost in the middle: How language models use long contexts. arXiv , 2023a.\\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-\\ntrain, prompt, and predict: A systematic survey of prompting methods in natural language pro-\\ncessing. ACM Computing Surveys , 2023b.\\nScott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In NeurIPS ,\\n2017.\\nMeta. Llama 2 - meta ai. https://ai.meta.com/llama/ . (Accessed on 09/15/2023).\\nOpenAI. Introducing chatgpt. https://openai.com/blog/chatgpt . (Accessed on\\n07/17/2023).\\nOpenAI. Gpt-4 technical report, 2023.\\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K ¨opf, Ed-\\nward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,\\nLu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep\\nlearning library. In NeurIPS , 2019.\\nProPublica. How we analyzed the compas recidivism algorithm. https://www.propublica.\\norg/article/how-we-analyzed-the-compas-recidivism-algorithm . Ac-\\ncessed: 2021-01-20.\\nAlec Radford, Rafal Jozefowicz, and Ilya Sutskever. Learning to generate reviews and discovering\\nsentiment, 2017.\\nMarco Ribeiro, Sameer Singh, and Carlos Guestrin. Anchors: High-precision model-agnostic ex-\\nplanations. In AAAI , 2018.\\nMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. “Why should I trust you?” Explaining the\\npredictions of any classifier. In KDD , 2016.\\nAvanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through\\npropagating activation differences. In ICML , 2017.\\nXuelin Situ, Ingrid Zukerman, Cecile Paris, Sameen Maruf, and Gholamreza Haffari. Learning to\\nexplain: Generating stable explanations fast. In IJNLP , 2021.\\nDaniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Vi ´egas, and Martin Wattenberg. Smoothgrad:\\nRemoving noise by adding noise. arXiv , 2017.\\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In\\nICML , 2017.\\nUCI. Default of credit card clients data set. https://archive.ics.uci.edu/ml/\\ndatasets/default+of+credit+card+clients . Accessed: 2020-01-01.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS , 2017.\\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yo-\\ngatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language\\nmodels. arXiv , 2022a.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou.\\nChain of thought prompting elicits reasoning in large language models. arXiv , 2022b.\\n11 Chih-Kuan Yeh, Cheng-Yu Hsieh, Arun Suggala, David I Inouye, and Pradeep K Ravikumar. On\\nthe (in) fidelity and sensitivity of explanations. NeurIPS , 2019.\\nI-Cheng Yeh, King-Jang Yang, and Tao-Ming Ting. Knowledge discovery on rfm model using\\nbernoulli sequence. Expert Systems with applications , 2009.\\nMatthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In ECCV ,\\n2014.\\n6 A PPENDIX : ADDITIONAL RESULTS AND EXPERIMENTAL DETAILS\\n6.1 A DDITIONAL EXPERIMENTAL DETAILS\\nThe median number of occurrences for cases where the LLM didn’t follow our requested response\\nformat or it replies with “I don’t have enough information to determine the most important features”\\nis 3 for Perturbation-based ICL, 0.5 for Prediction-based ICL, and 0 for Explanation-based ICL. We\\nuse the LLM’s top- 𝑘features to calculate explanation faithfulness using four evaluation metrics. For\\ncalculating PGU and PGI metrics, we use perturbation mean 𝜇𝑃𝐺=0, standard deviation 𝜎𝑃𝐺=0.1,\\nand the number of perturbed samples 𝑚𝑃𝐺=10,000. We follow the default hyperparameters from\\nOpenXAI for generating explanations from standard post hoc explainers.\\n6.2 A DDITIONAL RESULTS\\nHere, we include additional and detailed results of the experiments discussed in Sec. 4.\\nLLM+LIME LLM+Grad LLM+SG LLM+IG LLM+SHAP LLM+ITG0.00.20.40.60.81.0FA ()\\nGPT-3.5\\nGPT-4\\nLLM+LIME LLM+Grad LLM+SG LLM+IG LLM+SHAP LLM+ITG0.00.20.40.60.81.0RA ()\\nGPT-3.5\\nGPT-4\\nFigure 8: FA and RA metric performances for six LLM-augmented post hoc explainers (Sec. 3.4)\\nwhen generating explanations for a given LR model using G PT-3.5 vs. G PT-4. Explanations from\\nGPT-4, on average, outperform those generated using G PT-3.5 on both metrics on the Adult dataset.\\nPERTURBATION -BASED ICL P REDICTION -BASED ICL I NSTRUCTION -BASED ICL\\nBlood Recidivism Credit Adult0.000.250.500.751.00FA ()\\nBlood Recidivism Credit Adult0.000.250.500.751.00FA ()\\nFigure 9: FA metric performances of explanations generated using Perturbation-based ICL,\\nPrediction-based ICL, and Instruction-based ICL prompting strategies on four real-world datasets.\\nExplanations from G PT-4, on average, achieve higher FA scores than their G PT-3.5 counterparts.\\n12 LOGISTIC REGRESSION NEURAL NETWORK\\n0 0.25 0.5 0.75 1.0\\nTemperature ()\\n0.60.70.80.91.0FA ()\\n0 0.25 0.5 0.75 1.0\\nTemperature ()\\n0.20.30.40.50.6RA ()\\n0 0.25 0.5 0.75 1.0\\nTemperature ()\\n0.000.020.040.06PGU ()\\n0 0.25 0.5 0.75 1.0\\nTemperature ()\\n0.150.200.25PGI ()\\nBlood dataset\\nAdult dataset\\nFigure 10: Metric performances for explanations generated using LLMs for different temperatures\\n(𝜏) with a Logistic Regression model (left) and a Neural Network (right) model. LLM-based expla-\\nnations perform almost consistently across different temperature values, but LLMs will more often\\nreply along the lines of “not enough information to determine the most important features, ” for\\nhigher temperatures.\\nPERTURBATION -BASED ICL P REDICTION -BASED ICL\\nFA ()\\n RA ()\\n PGU ()\\n PGI ()\\n0.00.20.40.6Perturbed Samples\\nRaw Perturbations\\nFA ()\\n RA ()\\n PGU ()\\n PGI ()\\n0.00.20.40.6Perturbed Samples\\nRaw Perturbations\\nFigure 11: Faithfulness performance of explanations generated using Perturbation-based ICL (left)\\nand Prediction-based ICL (right) on using perturbed samples vs difference between perturbed sam-\\nples and the input sample (raw perturbations) in the ICL prompts for LR models trained on the Adult\\ndataset. Across both prompting strategies, we find that using ICL samples using the raw perturbation\\nstyle results in significantly better faithfulness performance across all four metrics.\\nTable 1: Results of the machine learning models trained on four datasets. Shown are the accu-\\nracy of the LR and ANN models trained the datasets. The best performance is bolded.\\nDataset LR ANN\\nBlood\\nRecidivism\\nDefault Credit\\nAdult70.59%\\n76.90%\\n87.37%\\n77.37%64.71%\\n76.90%\\n88.34%\\n80.11%\\nPERTURBATION -BASED ICL P REDICTION -BASED ICL I NSTRUCTION -BASED ICL\\n1 2 3 4 5\\nT opK Feature Importance0.00.20.40.60.81.0FA ()\\n1 2 3 4 5\\nT opK Feature Importance0.00.20.40.60.81.0FA ()\\n1 2 3 4 5\\nT opK Feature Importance0.00.20.40.60.81.0FA ()\\nAdultCreditRecidivismBlood\\nFigure 12: Effects of top- 𝑘value on the FA explanation faithfulness metric when using Perturbation-\\nbased ICL, Prediction-based ICL, and Instruction-based ICL prompting strategies. Shown are the\\nresults for three prompting strategies and four datasets using the LR model. On average, LLMs\\nsuccessfully achieve high scores in identifying the most important feature (top- 𝑘=1) and the per-\\nformance decreases as we increase the top- 𝑘value. For the Blood and Recidivism datasets, FA\\nincreases for top- 𝑘≥4because they have four and six features in their dataset, respectively.\\n13 Table 2: Here we provide the average and standard error faithfulness metric values of explanations\\ncalculated across 100 instances in the test set. The results are generated using Perturbation-based\\nICL, Prediction-based ICL, Instruction-based ICL, six post hoc explanation methods, and a random\\nbaseline. For the LLM methods, we queried the LLM for the top 𝑘=5(𝑘=4for Blood) most\\nimportant features and calculated each metric’s area under the curve (AUC) for 𝑘=3(where the\\nAUC is calculated from 𝑘=1to𝑘=3). This will help us better understand the model’s (Logis-\\ntic Regression and Artificial Neural Network) predictions trained on four datasets. Arrows ( ↑,↓)\\nindicate the direction of better performance.\\nLR ANN\\nDataset Method FA ( ↑) RA (↑) PGU (↓) PGI (↑) PGU (↓) PGI (↑)\\nGrad 1.000 ±0.000 1.000±0.000 0.010±0.000 0.042±0.000 0.060±0.009 0.115±0.013\\nSG 1.000 ±0.000 1.000±0.000 0.010±0.000 0.042±0.000 0.060±0.009 0.115±0.013\\nIG 1.000 ±0.000 1.000±0.000 0.010±0.000 0.042±0.000 0.061±0.009 0.116±0.013\\nITG 0.722 ±0.019 0.563±0.037 0.019±0.001 0.037±0.001 0.081±0.010 0.100±0.012\\nSHAP 0.723 ±0.020 0.556±0.037 0.019±0.001 0.036±0.001 0.085±0.011 0.098±0.012\\nLIME 1.000 ±0.000 1.000±0.000 0.010±0.000 0.042±0.000 0.061±0.009 0.116±0.013\\nRandom 0.502 ±0.022 0.232±0.032 0.029±0.001 0.026±0.001 0.091±0.011 0.090±0.012\\nPerturbation-based ICL 0.790±0.011 0.656±0.018 0.015±0.000 0.041±0.001 0.064±0.010 0.110±0.013\\nPrediction-based ICL 0.789±0.009 0.638±0.018 0.014±0.000 0.041±0.000 0.063±0.010 0.110±0.013Blood\\nInstruction-based ICL 0.802±0.015 0.578±0.037 0.014±0.000 0.040±0.001 0.068±0.010 0.106±0.013\\nGrad 1.000 ±0.000 1.000±0.000 0.059±0.003 0.106±0.005 0.095±0.008 0.149±0.011\\nSG 1.000 ±0.000 1.000±0.000 0.059±0.003 0.106±0.005 0.095±0.008 0.149±0.011\\nIG 1.000 ±0.000 1.000±0.000 0.059±0.003 0.106±0.005 0.096±0.008 0.149±0.011\\nITG 0.493 ±0.021 0.214±0.030 0.090±0.005 0.078±0.004 0.129±0.011 0.122±0.010\\nSHAP 0.473 ±0.023 0.217±0.032 0.092±0.005 0.076±0.004 0.130±0.011 0.122±0.010\\nLIME 1.000 ±0.000 1.000±0.000 0.059±0.003 0.106±0.005 0.096±0.008 0.149±0.011\\nRandom 0.308 ±0.023 0.127±0.024 0.101±0.005 0.063±0.005 0.146±0.011 0.092±0.009\\nPerturbation-based ICL 0.744±0.004 0.084±0.003 0.060±0.003 0.104±0.005 0.096±0.008 0.148±0.011\\nPrediction-based ICL 0.744±0.008 0.120±0.017 0.061±0.003 0.103±0.005 0.096±0.008 0.146±0.011Recidivism\\nInstruction-based ICL 0.811±0.017 0.478±0.044 0.063±0.003 0.103±0.005 0.102±0.009 0.146±0.011\\nGrad 0.999 ±0.001 0.999±0.001 0.056±0.006 0.221±0.011 0.081±0.011 0.228±0.014\\nSG 0.999 ±0.001 0.999±0.001 0.056±0.006 0.221±0.011 0.080±0.011 0.227±0.014\\nIG 1.000 ±0.000 1.000±0.000 0.056±0.006 0.221±0.011 0.082±0.011 0.228±0.014\\nITG 0.385 ±0.012 0.099±0.019 0.215±0.011 0.061±0.007 0.227±0.014 0.075±0.010\\nSHAP 0.387 ±0.012 0.150±0.020 0.215±0.011 0.061±0.007 0.225±0.014 0.075±0.010\\nLIME 0.963 ±0.012 0.953±0.015 0.056±0.006 0.221±0.011 0.078±0.011 0.229±0.014\\nRandom 0.130 ±0.017 0.053±0.015 0.198±0.012 0.054±0.008 0.213±0.014 0.064±0.010\\nPerturbation-based ICL 0.589±0.018 0.516±0.027 0.079±0.007 0.212±0.012 0.101±0.012 0.216±0.013\\nPrediction-based ICL 0.598±0.017 0.505±0.029 0.080±0.008 0.210±0.011 0.106±0.014 0.207±0.014Adult\\nInstruction-based ICL 0.748±0.020 0.716±0.027 0.069±0.007 0.217±0.011 0.097±0.012 0.219±0.014\\nGrad 1.000 ±0.000 1.000±0.000 0.065±0.005 0.195±0.009 0.072±0.008 0.173±0.011\\nSG 1.000 ±0.000 1.000±0.000 0.065±0.005 0.195±0.009 0.072±0.008 0.172±0.011\\nIG 1.000 ±0.000 1.000±0.000 0.065±0.005 0.195±0.009 0.074±0.008 0.172±0.010\\nITG 0.211 ±0.026 0.157±0.026 0.150±0.006 0.106±0.012 0.155±0.009 0.089±0.011\\nSHAP 0.212 ±0.026 0.161±0.026 0.150±0.006 0.107±0.012 0.150±0.008 0.098±0.012\\nLIME 0.988 ±0.005 0.985±0.007 0.065±0.005 0.195±0.009 0.071±0.008 0.173±0.010\\nRandom 0.173 ±0.020 0.095±0.020 0.185±0.010 0.054±0.006 0.176±0.011 0.053±0.007\\nPerturbation-based ICL 0.609±0.006 0.595±0.006 0.077±0.006 0.192±0.009 0.077±0.008 0.170±0.011\\nPrediction-based ICL 0.577±0.009 0.565±0.010 0.080±0.007 0.189±0.009 0.081±0.009 0.166±0.011Default Credit\\nInstruction-based ICL 0.628±0.014 0.587±0.020 0.080±0.007 0.188±0.010 0.085±0.009 0.163±0.011\\n14 Table 3: Results of explanations generated using Explanation-based ICL and six post hoc explana-\\ntion methods for understanding model (Logistic Regression and Artificial Neural Network) predic-\\ntions trained on three datasets. Shown are average and standard error metric values computed across\\n100 test samples. Arrows ( ↑,↓) indicate the direction of better performance. Evaluation metrics\\nwere computed for the top- 𝑘,𝑘being set to the number of features in each respective dataset.\\nLR ANN\\nDataset Method FA ( ↑) RA (↑) PGU (↓) PGI (↑) PGU (↓) PGI (↑)\\nLLM-Lime 0.708±0.006 0.465±0.009 0.013±0.000 0.041±0.001 0.074±0.009 0.099±0.012\\nLime 1.000 ±0.000 1.000±0.000 0.008±0.000 0.043±0.000 0.044±0.006 0.121±0.013\\nLLM-Grad 0.997±0.003 0.996±0.004 0.008±0.000 0.043±0.000 0.058±0.009 0.116±0.012\\nGrad 1.000 ±0.000 1.000±0.000 0.008±0.000 0.043±0.000 0.044±0.006 0.120±0.013\\nLLM-SG 0.990±0.006 0.983±0.011 0.008±0.000 0.043±0.000 0.055±0.008 0.116±0.012\\nSG 1.000 ±0.000 1.000±0.000 0.008±0.000 0.043±0.000 0.044±0.006 0.120±0.013\\nLLM-IG 0.989±0.005 0.982±0.009 0.008±0.000 0.043±0.000 0.046±0.007 0.120±0.013\\nIG 1.000 ±0.000 1.000±0.000 0.008±0.000 0.043±0.000 0.044±0.006 0.120±0.013\\nLLM-Shap 0.684±0.013 0.401±0.025 0.020±0.001 0.034±0.001 0.069±0.009 0.102±0.012\\nShap 0.773 ±0.014 0.516±0.033 0.015±0.001 0.038±0.001 0.066±0.009 0.107±0.012\\nLLM-ITG 0.702±0.013 0.387±0.029 0.017±0.001 0.036±0.001 0.069±0.010 0.105±0.012Blood\\nITG 0.774 ±0.014 0.532±0.034 0.014±0.001 0.038±0.001 0.063±0.008 0.108±0.012\\nLLM-Lime 0.990±0.001 0.958±0.005 0.029±0.001 0.115±0.002 0.048±0.001 0.165±0.004\\nLime 1.000 ±0.000 1.000±0.000 0.029±0.002 0.116±0.006 0.044±0.004 0.164±0.012\\nLLM-Grad 0.997±0.001 0.990±0.003 0.029±0.001 0.115±0.002 0.048±0.001 0.165±0.004\\nGrad 1.000 ±0.000 1.000±0.000 0.029±0.002 0.116±0.006 0.043±0.004 0.165±0.012\\nLLM-SG 0.997±0.001 0.990±0.003 0.029±0.001 0.115±0.002 0.047±0.001 0.165±0.004\\nSG 1.000 ±0.000 1.000±0.000 0.029±0.002 0.116±0.006 0.043±0.004 0.165±0.012\\nLLM-IG 0.996±0.001 0.988±0.003 0.029±0.001 0.115±0.002 0.048±0.001 0.166±0.004\\nIG 1.000 ±0.000 1.000±0.000 0.029±0.002 0.116±0.006 0.044±0.004 0.165±0.012\\nLLM-Shap 0.666±0.004 0.216±0.008 0.057±0.001 0.098±0.002 0.082±0.002 0.151±0.004\\nShap 0.670 ±0.012 0.200±0.024 0.058±0.003 0.099±0.005 0.087±0.008 0.146±0.011\\nLLM-ITG 0.690±0.004 0.247±0.008 0.056±0.001 0.099±0.002 0.085±0.002 0.148±0.004Recidivism\\nITG 0.689 ±0.011 0.195±0.022 0.056±0.003 0.100±0.005 0.078±0.007 0.149±0.011\\nLLM-Lime 0.909±0.001 0.632±0.005 0.023±0.001 0.222±0.003 0.035±0.002 0.230±0.004\\nLime 0.907 ±0.005 0.743±0.017 0.018±0.002 0.224±0.011 0.029±0.005 0.235±0.014\\nLLM-Grad 0.938±0.000 0.801±0.001 0.022±0.001 0.223±0.003 0.035±0.002 0.230±0.004\\nGrad 0.999 ±0.001 0.997±0.003 0.018±0.002 0.224±0.011 0.029±0.004 0.234±0.014\\nLLM-SG 0.938±0.000 0.802±0.001 0.022±0.001 0.223±0.003 0.035±0.002 0.230±0.004\\nSG 0.999 ±0.001 0.997±0.003 0.018±0.002 0.224±0.011 0.029±0.004 0.234±0.014\\nLLM-IG 0.938±0.000 0.804±0.000 0.022±0.001 0.223±0.003 0.033±0.002 0.231±0.004\\nIG 1.000 ±0.000 1.000±0.000 0.018±0.002 0.224±0.011 0.031±0.005 0.235±0.014\\nLLM-Shap 0.676±0.002 0.069±0.003 0.109±0.002 0.148±0.003 0.123±0.003 0.153±0.004\\nShap 0.662 ±0.007 0.107±0.012 0.139±0.009 0.127±0.009 0.144±0.011 0.149±0.013\\nLLM-ITG 0.665±0.002 0.039±0.002 0.107±0.002 0.150±0.003 0.132±0.003 0.146±0.004Adult\\nITG 0.627 ±0.006 0.068±0.010 0.175±0.010 0.099±0.009 0.170±0.011 0.130±0.013\\nLLM-Lime 0.954±0.001 0.787±0.003 0.030±0.001 0.189±0.003 0.042±0.002 0.178±0.003\\nLime 0.977 ±0.004 0.878±0.015 0.030±0.003 0.201±0.009 0.037±0.004 0.186±0.010\\nLLM-Grad 0.984±0.000 0.896±0.001 0.029±0.001 0.189±0.003 0.042±0.002 0.178±0.003\\nGrad 1.000 ±0.000 1.000±0.000 0.030±0.003 0.201±0.009 0.038±0.005 0.185±0.011\\nLLM-SG 0.984±0.000 0.897±0.000 0.029±0.001 0.189±0.003 0.072±0.003 0.165±0.003\\nSG 1.000 ±0.000 1.000±0.000 0.030±0.003 0.201±0.009 0.037±0.004 0.185±0.011\\nLLM-IG 0.984±0.000 0.896±0.001 0.029±0.001 0.189±0.003 0.041±0.002 0.179±0.003\\nIG 1.000 ±0.000 1.000±0.000 0.030±0.003 0.201±0.009 0.041±0.005 0.185±0.010\\nLLM-Shap 0.543±0.003 0.067±0.004 0.088±0.002 0.140±0.003 0.094±0.003 0.126±0.003\\nShap 0.525 ±0.009 0.086±0.012 0.088±0.005 0.163±0.010 0.091±0.006 0.146±0.011\\nLLM-ITG 0.526±0.003 0.052±0.003 0.088±0.002 0.139±0.003 0.091±0.002 0.129±0.003Default Credit\\nITG 0.516 ±0.010 0.076±0.012 0.086±0.005 0.165±0.010 0.084±0.006 0.152±0.010\\n15 '},\n",
       " {'url': 'http://arxiv.org/pdf/2312.14945v1',\n",
       "  'title': 'Empowering ChatGPT-Like Large-Scale Language Models with Local Knowledge Base for Industrial Prognostics and Health Management',\n",
       "  'text': '1 \\n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) <  \\n Empowering ChatGPT-Like Large-Scale Language \\nModels with Local Knowledge Base for Industrial \\nPrognostics and Health Management \\n \\nHuan Wang and Yan-Fu Li, IEEE  Senior Member , and Min Xie,  IEEE Fellow  \\n \\n\\uf020 \\nAbstract —Prognostics and health management (PHM) is \\nessential for industrial operation and maintenance,  focusing on \\npredicting, diagnosing, and managing the health status of industrial systems. The emergen ce of the ChatGPT-Like large-\\nscale language model (LLM) has begun to lead a new round of \\ninnovation in the AI field. It has extensively promoted the level of \\nintelligence in various fields. Therefore, it is also expected further to change the application paradigm in industrial PHM and promote PHM to become intelligent. Although ChatGPT-Like \\nLLMs have rich knowledge reserves and powerful language \\nunderstanding and generation capabilities, they lack domain-specific expertise, significantly limiting their practicability in PHM applications. To this end, this study explores the ChatGPT-\\nLike LLM empowered by the local knowledge base (LKB) in \\nindustrial PHM to solve the above limitations. In addition, we introduce the method and steps of combining the LKB with LLMs, including LKB preparation,  LKB vectorization, prompt \\nengineering, etc. Experimental analysis of real cases shows that \\ncombining the LKB with ChatGPT-Like LLM can significantly improve its performance and make ChatGPT-Like LLMs more accurate, relevant, and able to provide more insightful \\ninformation. This can promote the development of ChatGPT-\\nLike LLMs in industrial PHM and promote their efficiency and quality. \\n \\nIndex Terms —Prognostics and Health Management, Large-Scale \\nLanguage Model, ChatGPT, Local Knowledge Base. \\nI. INTRODUCTION  \\nROGNOSTICS and health management (PHM) \\nconstitutes an indispensable component within \\ncontemporary industrial operation and maintenance \\npractices, focusing on the prediction, diagnosis, and \\nmanagement of the well-being of industrial equipment and \\nsystems [1, 2] . By promptly detecting and prognosticating the \\nhealth status, PHM contributes to the prevention of unforeseen \\nequipment shutdowns, cost reduction in maintenance activities, and enhancement of production efficiency and \\nproduct quality. Given the escalating complexity and \\ndiversification of modern industrial equipment, a considerable workforce of proficient operation and maintenance personnel \\nis essential for analyzing, processing, repairing, and replacing \\n \\nThis work was supported by the National Natural Science Foundation of \\nChina (71731008) and the Beijing Municipal Natural Science Foundation-Rail Transit Joint Research Program (L191022). (Corresponding author: Yan-Fu Li, liyanfu@tsinghua.edu.cn). \\nHuan Wang and Yan-Fu Li are with the Department of Industrial Engineering, \\nTsinghua University, Beijing, 100084, China. \\nMin Xie is with the department of Advanced Design and System Engineering, \\nCity University of Hong Kong, Hong Kong SAR, China. diverse industrial data and equipment. Furthermore, the \\naforementioned procedures demand the expertise of \\nexperienced engineers to ensure the secure operation of industrial equipment [3]. Additionally, with the proliferation \\nof industrial sensors, a vast volume of operational data is \\ncollected, rendering sole reliance on manual processing by operation and maintenance engineers impractical. Consequently, the development of automated systems for \\nPHM data analysis and processing is paramount in industrial operation and maintenance. \\nOver the past decade, the advancement of deep learning \\ntechnology has facilitated the automation of industrial PHM data analysis and feature extraction [4-6] . Deep learning \\nstrives to address intricate tasks by constructing and simulating artificial neural networks [7]. At its core, deep \\nlearning endeavors to extract abstract features from data \\nthrough multi-level nonlinear network layers, enabling \\nautomated pattern recognition and decision-making. \\nConsequently, deep learning has exhibited remarkable achievements across numerous domains [8, 9] , encompassing \\ncomputer vision [10], natural language processing (NLP) [11], \\nand speech recognition. Notably, its extraordinary accomplishments extend to the domain of PHM as well. As a \\nresult, deep learning finds extensive employment in diverse PHM applications, such as fault prediction [12-14] , anomaly \\ndetection [15, 16] , and maintenance planning [17], to realize \\nautomated data analysis, intelligent prediction, and decision-\\nmaking. For example, Wang et al. [18] proposed a \\nTransformer-based train wheel wear prediction model, which \\ncan provide valuable wheel health status information for maintenance personnel. Han et al. [19] proposed a deep \\nmodel-based domain adaptation algorithm and realized \\nlithium-ion battery capacity estimation under invisible conditions. \\nThe significance of deep learning in facilitating automatic \\ndata analysis and intelligent decision-making within industrial \\nPHM is evident [20, 21] . It empowers operation and \\nmaintenance personnel with timely and valuable information, thereby enhancing the maintenance standards of industrial \\nequipment in enterprises. However, the scope of deep learning\\'s application remains limited, excelling primarily in \\ndata analysis and certain monitoring and forecasting \\napplications. Consequently, the current operation and maintenance of industrial equipment and systems necessitate a \\nsubstantial workforce comprising engineers possessing \\nextensive professional knowledge to conduct monitoring, \\ntroubleshooting, equipment maintenance, and related tasks, \\nensuring operational safety and reliability [22]. Given the P 2 \\n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) <  \\n complexity and specialization of modern industrial equipment \\nand systems, these roles demand a high degree of specialized \\nexpertise and experience. The challenge at hand for industrial \\nenterprises lies in swiftly training engineers with professional knowledge and reducing the labor costs associated with \\nindustrial equipment monitoring, operation, and maintenance-\\nan issue that cannot be disregarded. \\nIn the past two years, deep learning has witnessed \\nrevolutionary breakthroughs in the domain of NLP, with the \\ngradual maturation of large-scale language models (LLM) [23, \\n24] that possess formidable language understanding \\ncapabilities and the ability to engage in human-like \\ncommunication. LLMs leverage extensive text training data \\nand intricate network models to discern the underlying \\npatterns and principles of language, enabling them to generate accurate, contextually appropriate, and fluent responses based \\non the input. Among the groundbreaking achievements of \\nLLMs, ChatGPT [25] emerged as a global sensation \\nimmediately upon its release. Its vast knowledge base, \\nintelligent responsiveness, and flawless answers instilled hope in the realm of general artificial intelligence. Consequently, \\nChatGPT-like LLMs harbor the revolutionary potential for \\napplications in various fields [26]. For instance, ChatGPT-like \\nLLMs can supplant product customer service, intelligently and \\nfluently providing professional responses to customer inquiries. They can assist physicians in medical image \\ndetection, enabling the identification of disease types and \\nseverity while automatically generating professional medical \\nreports [27]. Moreover, ChatGPT-like LLMs can utilize their \\npowerful summarization capabilities to enhance search \\nengines, furnishing users with more precise and pertinent content. \\nChatGPT-Like LLM holds immense potential to \\nrevolutionize the application paradigm in PHM, facilitating the development of intelligent industrial operation and \\nmaintenance systems while concurrently reducing labor costs associated with industrial equipment monitoring, operation, \\nand maintenance [28]. For instance, ChatGPT-Like LLM can \\nserve as a comprehensive knowledge base, delivering reliable \\nand insightful information to operation and maintenance \\npersonnel by elucidating complex issues and furnishing operational guidelines. LLM can analyze equipment operation \\ndata to generate analysis reports and visualizations, aiding \\ndecision-making. Integration of LLM into industrial automation systems can bestow intelligent automated \\nworkflows. Furthermore, LLM can provide expert repair advice and guidance, minimizing unplanned downtime and \\nmitigating maintenance expenses. Furthermore, ChatGPT-\\nLike LLM holds vast application prospects within the realm of \\nindustrial PHM, promising to reshape the entire field\\'s \\nworking paradigm in the future. \\nHowever, applying the current ChatGPT-Like LLM to \\nindustrial PHM entails several challenges and limitations. \\nWhile these language models possess formidable language \\nprocessing capabilities and vast knowledge reserves, a majority of their training data originates from the Internet. \\nConsequently, technical content, technical manuals, and maintenance guides prevalent in the industrial domain are \\noften confidential and inaccessible. As a result, existing LLMs often lack expertise in industrial PHM, rendering them unable \\nto accurately comprehend and address domain-specific \\nconcerns related to industrial equipment monitoring, \\ntroubleshooting, and maintenance. Consequently, practical application encounters difficulties such as misconceptions and \\nthe delivery of inaccurate or irrelevant information in response \\ncontent. \\nTo address these limitations, a promising solution involves \\nintegrating a local knowledge base (LKB) with ChatGPT-Like \\nLLM to enhance the model\\'s expertise and practicality. In this study, we delve into this solution, thoroughly examining the \\nmethods and steps involved in leveraging the LKB to \\nempower ChatGPT-Like LLM. These include knowledge base \\npreparation, text vectorization, similarity search, and language \\nmodel integration. We present two real-world cases as experimental evidence to validate the practicality and \\nperformance improvement of ChatGPT-Like LLM empowered \\nby the LKB (LKB-E-LLM) in industrial PHM. Through comprehensive experimental analysis and result evaluation, \\nwe showcase the significant impact of integrating LKBs on language models, emphasizing their advantages in fault \\ndiagnosis, prediction, and maintenance management. \\nAdditionally, we discuss potential applications and benefits of employing LKBs while outlining directions for future \\nresearch. The contributions of this study can be summarized as follows: \\n1. This paper investigates the application potential of \\nChatGPT-Like LLM in the industrial PHM, exploring its \\ncurrent limitations and proposing potential solutions. \\n2. This paper examines the integration of the LKB with \\nChatGPT-Like LLM in the context of industrial PHM. The study demonstrates how this integration significantly \\nenhances the performance and practicality of LLM in \\nPHM, thereby facilitating its real-world application. \\n3. This paper empirically validates the practicality and \\nperformance improvements of LKB-E-LLM in PHM through actual cases, highlighting its advantages in fault \\ndiagnosis, prediction, and maintenance management. \\n4. This paper provides a comprehensive analysis and \\ndiscussion of the application potential of LKB-E-LLM in \\nthe industrial PHM, while also exploring future research directions in this area. \\nThe rest of this paper is organized as follows. Section II \\nintroduces the key components of ChatGPT-Like LLM. \\nSection III describes the technical details of LKB-E-LLM. \\nSection IV verifies the performance of LKB-E-LLM. Section V discusses the advantages, potential applications, and future \\ndirections of LKB-E-LLM. Section VI summarizes this paper. \\nII.\\n CHATGPT-L IKE LARGE -SCALE LANGUAGE MODEL  \\nLLM is a language model characterized by extensive \\nparameters, advanced semantic comprehension, and text \\ngeneration capabilities [29]. Through extensive training on \\nvast textual data, LLM captures language\\'s intricacies and \\nfundamental characteristics, thereby facilitating the generation \\nof high-quality natural language text and enabling effective \\ncommunication with human counterparts. Many LLMs are \\nconstructed based on diverse pre-trained language models,  3 \\n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) <  \\n further enhanced by techniques such as supervised fine-tuning, \\nfeedback bootstrap, and reinforcement learning from human \\nfeedback [25]. These techniques enable the model to discern \\nthe intended meaning of human instructions and produce \\nsuitable responses. An illustrative example of an LLM is the \\nChatGLM-6B model [30] developed by Tsinghua University. \\nChatGLM-6B is a Chinese-English bilingual language model \\nwith 6.2 billion parameters. To train ChatGLM-6B, an \\nautoregressive blank infilling approach is employed as the \\ncore self-supervised training method, leveraging extensive \\nChinese and English text datasets. Additionally, the \\nChatGLM-6B incorporates 2D positional encodings and \\nenables the prediction of spans in arbitrary order. These \\nenhancements contribute to more effective self-supervised \\npre-training and elevate the model\\'s performance in natural \\nlanguage understanding applications [31]. When provided \\nwith an input text\\n, it samples multiple text \\nspans , with each span Si representing a \\nsequence of consecutive tokens  in X. To create \\na corrupted text Xcorrupt, each span is substituted with a single \\n[MASK] token. The model utilizes the autoregressive \\napproach to predict the missing tokens within the spans based \\non the corrupted text. This means that while predicting the \\nmissing tokens in a span, the model can access the corrupted \\ntext and the previously predicted spans. In order to effectively \\ncapture the interdependencies among different spans, this \\nmethod randomly shuffles the order of the spans. Let Zm \\nrepresent the set of all possible permutations of an index \\nsequence [1, 2 ,..., m] with length m. Given a span \\n, The pre-training objective is then defined \\nas: \\n (1) \\nThe tokens within each blank are always generated in a left-to-\\nright order, whereby the probability of the span Si being \\ngenerated is factorized as: \\n   (2) \\nIn addition, ChatGLM-6B investigates a multi-task pre-\\ntraining mechanism to generate appropriate responses in \\nlanguage understanding applications. \\nIn addition to ChatGLM-6B, the LLM is experiencing \\nrapid development, with notable contributions from prominent research institutions and companies. For instance, OpenAI has \\nmade significant strides with the GPT series [25, 32]  of LLMs, \\nwhich have garnered substantial success and widespread adoption. Among them, GPT-3.5 [25] stands out for its \\nformidable dialogue and language comprehension capabilities, \\nwhile GPT-4 [32] represents a notable advancement by \\nincorporating image understanding capabilities, thereby \\nfacilitating highly intelligent multi-modal information processing. Baidu\\'s ERNIE series [33, 34]  of LLMs have also \\ndemonstrated exceptional performance. These LLMs are built \\nupon the foundation of the Transformer and its enhanced \\nversions [11, 35] , utilizing the self-attention mechanism of the \\nTransformer as the core backbone network to achieve effective feature representation and semantic comprehension. \\nSubsequently, autoregressive models and encoder-decoder \\nmodels are employed to train and optimize the LLMs on vast \\namounts of unlabeled text data using self-supervised representation learning [36] algorithms. Moreover, to enhance \\nthe LLM\\'s ability to understand and address human-specific \\nproblems, methodologies such as Reinforcement Learning from Human Feedback [25] are employed to optimize the \\nLLM. This approach leverages reinforcement learning \\ntechniques by incorporating valuable feedback provided by human experts, thus refining the LLM\\'s responses. Looking \\nahead, LLMs are poised to make comprehensive contributions \\nacross various industries, propelling societal advancements. \\nIndustrial Equipment\\nHigh-Speed Train Wind Turbines\\nHelicopter Industrial Production\\nOperation and Maintenance\\nLarge-Scale Language Models Powered by Local Knowledge Bases\\nIndustrial Data Analysis Remote System Monitoring\\nInspection and Testing Replacement and Repair\\nLARGE-SCALE \\nLANGUAGE MODEL\\nProprietary Technical Documents, \\nOperation and Maintenance \\nManuals, Etc.Language Understanding, Text \\nDialogue, Automated Process \\nAnalysis, Etc.\\nLOCAL KNOWLEDGE \\nBASE\\n\\uf0d8\\uf020Possess Professional PHM Domain \\nKnowledge\\n\\uf0d8\\uf020Excellent Professional Knowledge \\nUnderstanding Ability\\n\\uf0d8\\uf020Efficient, Accurate and High-Value \\nAnswers\\n\\uf0d8\\uf020Latest Technological Advances and \\nUpdates\\n \\nFig. 1  The schematic diagram of the application of LKB- empowered \\nLLM in the industrial PHM. \\nIII. LLM  EMPOWERED BY LOCAL KNOWLEDGE BASE \\nAs mentioned previously, the robust language processing \\nand semantic understanding capabilities of ChatGPT-Like \\nLLM are poised to make a significant impact on the field of PHM, enhancing efficiency, performance, and the overall \\nintelligence level of industrial PHM. However, the existing \\nLLMs typically lack domain-specific expertise, such as in the \\ncase of industrial PHM, resulting in subpar responses to \\ntechnical-specific queries. This limitation arises from the fact that the training data for LLMs predominantly originates from \\nthe Internet, while the crucial knowledge in industrial PHM, including technical manuals, maintenance guidelines, and \\nperformance parameters, remains confidential. Consequently, \\nLLMs fail to acquire this essential information during the \\ntraining process. Furthermore, the need to prioritize data privacy and security in the industrial PHM field prohibits the \\ncollection of private data for fine-tuning the model, as it may \\nlead to information leakage. In Fig. 1 , we present a promising \\nsolution that involves integrating a local private knowledge \\nbase with ChatGPT-Like LLM to equip the LLM with domain-specific expertise, thereby enhancing its practicality. \\nIndustries such as high-speed trains, wind turbines, helicopters, and industrial production necessitate robust \\noperation and maintenance teams to ensure equipment and \\nsystem safety and reliability. The combination of ChatGPT- 4 \\n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) <  \\n Like LLM and the local industrial private knowledge base \\nholds the potential to address various aspects of the PHM \\nfield. This includes assisting in industrial data analysis to \\ngenerate comprehensive analysis reports, facilitating automatic remote monitoring and early detection of abnormal \\nsituations, aiding in remote troubleshooting and equipment \\nmaintenance, and supporting personnel training and knowledge transfer. By leveraging this integrated approach, \\nindustrial enterprises can significantly reduce operation and \\nmaintenance costs while enhancing the operational safety of their equipment. The subsequent sections will delve into \\nintegrating LLM and LKB, offering a detailed overview of the \\nintegration process and its benefits. \\nLOCAL \\nKNOWLEDGE BASE\\nDocument \\nLoaders\\nDomain-Specific Knowledge \\nFiles (TXT/DOC/PDF, etc.)\\nText Splitter\\n Text Chunks\\nText Embedding Models\\nVector Stores\\nBuild FAISS IndexText Vector RepresentationSemantically Meaningful \\nText SegmentSplit Text Into ChunksIndustrial PHM Knowledge \\nQuery\\nQuery Read and \\nLoad\\nQuery  \\nEmbedding \\nQuery  Vector \\nVector similarity \\ncalculation\\nRelated Text \\nChunks\\nSplicing Text Chunks \\n- Context Information\\nPrompt Template, \\nQuery+Context\\nLARGE-SCALE \\nLANGUAGE MODELMulti-Format Loading\\nQuery Vector Representation\\nFAISS-Based Search\\nTop K Related Professional \\nKnowledgeProfessional Knowledge \\nInformation\\nProfessional and \\nAppropriate Answer\\nSemantic Understanding, Induction and \\nSummarization, Text Generation\\n \\nFig. 2  The overall system architecture of the LKB-empowered LLM. \\nA. Overall System Architecture \\nFig. 2  illustrates the comprehensive system architecture of \\nLKB-E-LLM. It encompasses several key processes, including local knowledge base construction, document reading, text \\nsplitting, text vectorization, vector similarity calculation, relevant text search, prompt content generation based on \\nrelevant text and queries, and answer generation based on \\nprompts. The existence of abundant private technical documents related to various industrial equipment and systems \\nenables the establishment of an offline knowledge base. Domain-specific knowledge unknown to the LLMs is gathered and stored offline to construct this knowledge base. These data \\nare then processed by a text reader capable of handling diverse \\nfile formats such as TXT, DOC, PDF, CSV, and more.  \\nLong text information is split into text chunks with \\nsemantic significance to enhance the processing of textual content. Text splitting involves various techniques, including \\ncharacter-based splitting, token-based splitting, and recursive \\ncharacter-based splitting. These methods aim to ensure that the split text chunks consist of semantically relevant text \\nfragments. A text embedding model is employed to enable the model to comprehend natural language. Text embedding [37] \\ninvolves converting text data into continuous vector representations. By mapping words, phrases, or sentences in \\nthe text to a continuous vector space, text embedding provides \\nthe foundation for numerous NLP tasks. The fundamental \\nconcept behind text embedding is to capture the semantic information of the text, allowing texts with similar semantics \\nto have similar representations. Presently, text embedding \\nmethods based on large-scale pre-trained models have gained popularity due to their superior performance compared to \\ntraditional word embedding algorithms. \\nLastly, to facilitate the processing of large-scale local text, \\nthe obtained vector representations need to be stored and \\nindexed for efficient search. A highly suitable option for this \\npurpose is Facebook AI Similarity Search (FAISS) [38]. \\nFAISS offers a comprehensive suite of algorithms and data \\nstructures for large-scale vector retrieval. It supports parallel computing and vector quantization technology, enabling \\nefficient search operations on vast datasets. \\nFor queries from operation and maintenance personnel, the \\ntext reading and vector representation processes are also \\ninvolved. The query text is first converted into a vector representation using the text embedding model. Subsequently, \\nthe powerful search capability of FAISS is utilized to locate \\nthe most relevant content from the LKB. The critical aspect of this process is the search and similarity calculation operation, \\nwhich identifies the top K relevant professional knowledge \\nentries from the LKB. After retrieving the K relevant \\nprofessional knowledge entries, they are concatenated into \\ncontextual information according to a specific format. This \\ncontextual information is then combined with the query to \\nconstruct the prompt input for the LLM. Ultimately, the LLM \\ngenerates professional and reliable responses based on the provided information and its knowledge. Acting as the \\nintermediary between users and the knowledge base, the \\nChatGPT-Like LLM utilizes its language processing and semantic understanding capabilities to analyze, summarize, \\nand reason contextual information to produce accurate and appropriate responses. \\nIn the above process, text embedding, vector similarity \\ncalculation, and prompt engineering are the core of LKB-E-\\nLLM, and these key components will be introduced in detail \\nbelow. \\nB. Text Embedding \\nText embedding serves as the fundamental mechanism for \\nenabling computers to comprehend natural language and establishing a bridge between human language and \\ncomputational systems [37]. It involves the technique of \\nrepresenting textual data as continuous vectors within a low-\\ndimensional vector space. This methodology aims to capture \\ntext\\'s semantic and syntactic characteristics, facilitating the \\nquantification of similarity and relationships between words, phrases, or entire documents. Notably, with the emergence of \\nlarge-scale pre-training models, text embedding approaches \\nhave increasingly incorporated these models to achieve more \\nprecise and expressive embedding representations. Prominent \\npre-training models, such as BERT [39], GPT [25], and \\nERNIE [33, 34] , have been widely employed in text \\nembedding methodologies. These models offer notable advantages as they are pre-trained on extensive datasets,  5 \\n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) <  \\n granting them robust capabilities for semantic and syntactic \\nrepresentations. Additionally, they possess the ability to \\ngenerate context-aware embedding representations that \\neffectively capture word polysemy and contextual relevance. To illustrate, taking BERT as an example, the main technical \\ndetails are explained below. \\n \\n \\nFig. 3  BERT\\'s pre-training model architectures, which use a \\nbidirectional Transformer. E i represents the input, and T i represents \\nthe output [39]. \\n \\nBERT [39], a Transformer-based pre-training model, \\nleverages a bidirectional encoder to capture extensive \\ncontextual information from large-scale text, facilitating a \\nprofound understanding of text semantics and context. BERT \\nprimarily employs two methods, namely masked language \\nmodel (MLM) and next sentence prediction (NSP), to achieve \\nnatural language understanding. MLM aims to encourage the \\nmodel to grasp rich semantic and contextual information by \\nselectively masking certain words within the input text and \\nprompting the model to predict these masked words. On the \\nother hand, NSP aims to train the model in discerning whether \\ntwo sentences are consecutive in the original text, thereby \\nfostering the model\\'s comprehension of sentence relationships \\nand contextual coherence. Fig. 3  shows the model architecture \\nof BERT. The feature representation of large-scale text is \\nmainly completed by the Transformer encoder. The \\nTransformer encoder comprises multiple self-attention \\nmechanisms, which effectively capture the dependencies \\namong various positions within the input sequence. Assuming \\nthe input to the self-attention mechanism [11] is denoted as \\n, it is initially encoded into query vectors ( Q), \\nkey vectors ( K), and value vectors ( V) via three distinct sets of \\nlinear transformation layers. These linear transformations, \\nrepresented by the learnable weights , \\nrespectively, facilitate the transformation process. The self-\\nattention mechanism can be expressed as: \\n  (3) \\nThe purpose of  is to calculate the similarity between Q \\nand K, then input the scale-transformed similarity into \\nSoftmax to obtain the attention weight and perform matrix \\nmultiplication between the weight and V to obtain the final output. Further, to achieve multi-level feature encoding, multi-\\nhead attention (MHA) is adopted, which employs multiple \\nself-attention modules in parallel. These self-attention \\nmodules encode the global correlation of the input data from \\ndifferent perspectives, respectively, and the results are finally \\ncombined as the output of MHA. MHA can be expressed as: \\n (4) \\nwhere represents an attention head (self-attention \\nmodule), H represents the number of attention heads, and \\n represents the splicing of the output results of H \\nattention heads. \\nC. Vector Similarity Calculation \\nEssentially, text embedding involves encoding text chunks \\ninto dense vectors that capture their semantic and contextual \\ninformation. Vector similarity calculation is a crucial \\ntechnology in LKB-E-LLM. In NLP, cosine similarity is \\ncommonly employed to assess the similarity between vectors \\nand evaluate their distribution within the semantic space. Let v \\nrepresent the vector representation of the knowledge base\\'s \\ntext chunks, and w represent the vector representation of the \\nquery. The cosine similarity between them can be expressed as \\nfollows: \\n  (5) \\nwhere L represents the length of the vector. In the case of a \\nrelatively small knowledge base, the process involves \\ntraversing the knowledge base vector based on the query \\nvector to identify the most relevant text segments and extract \\nthem accordingly. However, when dealing with large and \\ncomplex knowledge bases, this approach incurs significant \\ntime overhead, making it impractical. To address this \\nchallenge, the introduction of FAISS in LKB-E-LLM enables \\nefficient similarity search at a billion-scale level, leveraging \\nthe power of GPU acceleration. \\nFAISS is a specialized framework designed to facilitate \\nefficient searching and clustering of dense vectors [38]. It \\nencompasses a range of algorithms specifically designed for \\nsearching within collections of vectors with varying \\ndimensions. The workflow of FAISS can be broadly divided \\ninto two main steps: indexing and searching/querying. In FAISS, the most commonly utilized method for index construction is based on k-means clustering. This technique \\ninvolves dividing a collection of vectors into clusters and creating an inverted file for each cluster. During index \\nconstruction, different cluster numbers and cluster centers can \\nbe chosen to strike a balance between search speed and \\naccuracy. Additionally, FAISS offers various query \\nacceleration methods, with Product Quantization being a \\nprominent algorithm. Product quantization involves splitting a \\nhigh-dimensional vector into multiple subvectors and \\nquantizing each subvector into a discrete codebook \\nrepresentation. This quantization process reduces storage \\nrequirements, and during similarity search, comparisons can \\n 6 \\n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) <  \\n be performed quickly within the quantized codebook, thereby \\nenhancing search efficiency. Furthermore, FAISS supports \\nGPU acceleration, multi-index support, and distributed \\nindexes\\' construction and search functions. These features \\nenable FAISS to handle large-scale vector similarity search tasks efficiently. \\nD. Prompt Engineering \\nPrompt engineering is a strategic approach employed in \\ntext generation applications to optimize and design prompts, \\nthereby enhancing the performance and quality of generated \\ntext by generative models. By carefully crafting and refining \\nprompts, generative models can be guided to produce precise, \\nconsistent, and desirable output. The crux of prompt \\nengineering lies in devising well-suited prompts for the \\nintended task, which involve specific formatting and content \\nrequirements. In the context of LKB-E-LLM, the methodology \\nheavily relies on prompt engineering. It constructs an \\nappropriate prompt based on the query vector and the \\noutcomes of similarity searches conducted on the LKB. \\nTypically, FAISS retrieves the K most relevant text segments \\nto the query vector, where K is an adjustable hyperparameter. \\nAssuming that the retrieved text segments by FAISS are \\ndenoted as \\n, the LLM prompt can be \\nformulated using the following PROMPT TEMPLATE. By \\ndoing so, ChatGPT-Like LLMs can generate more proficient \\nand accurate content based on the provided prompt \\ninformation. \\n \\nPROMPT_TEMPLATE = “Known Information: \\n{ }, \\nBased on known information, please answer relevant questions concisely and professionally.  The question is: {query}.” \\n \\nMoreover, PROMPT TEMPLATE can be tailored to \\nspecific industrial fields, considering the particular scenarios encountered. This aids in enabling the model to comprehend \\ntask context, requirements, and constraints, thereby generating task-specific and expert-level content. By employing appropriate prompts in conjunction with a vast LKB, Prompt \\nEngineering can equip LLMs with substantial domain \\nexpertise, empowering them to comprehend the intricacies and specifications of diverse PHM applications. Consequently, ChatGPT-Like LLMs can exhibit exceptional performance \\nwithin the industrial PHM domain. \\nE. Implementation Details \\nLKB-E-LLM is an advancement of existing LLMs, and its \\nimplementation necessitates the utilization of various LLM \\ntechnologies, such as text embedding and vector similarity. \\nThe LangChain framework\\n1 serves as a robust support system \\nfor realizing LKB-E-LLM. It is a language model application \\ndevelopment framework that imparts two fundamental \\ncapabilities to LLMs: data perception, enabling the language \\nmodel to connect with external data sources, and agency, \\nallowing the language model to interact with its environment. Additionally, LangChain offers interfaces for multi-format \\ndocument extraction, text splitting, text embedding, and vector \\nsimilarity querying, providing substantial support for realizing \\nLKB-E-LLM. \\nWith the advancements in large-scale pre-training models, \\nseveral text embedding modules like GPT [25], BERT [39], \\nM3E2, and ERNIE-3.0 [33] have proven reliable for text \\nembedding in LKB-E-LLM. Considering local implementation\\'s efficiency and resource limitations, this \\nstudy adopts the M3E model. To facilitate efficient similarity \\nquerying and searching of large-scale vector data, FAISS, Redis\\n3, and Milvus4 serve as essential components integrated \\ninto the LangChain framework. Notably, projects such as \\nlangchain-ChatGLM5 and Wenda6 have already achieved \\ncommendable performance by leveraging scalable knowledge \\nbases for question-answering using the aforementioned \\ntechnical foundation. \\nThis study explores the application of LKB-E-LLM in the \\nindustrial PHM, building upon the previous work. The \\nimplementation of LKB-E-LLM is carried out on a \\nworkstation equipped with an Intel 10900K CPU, GeForce RTX 3090 GPU, and 32GB memory. Considering the \\nlimitations of video memory, the LMM model employed is \\nChatGLM-6B [30], an efficient Chinese-English bilingual \\ndialogue language model that requires a maximum of 14GB video memory. \\nIV.\\n INDUSTRIAL CASE STUDIES  \\nThis section takes high-speed trains (HSTs) and wind \\nturbines as examples to verify the performance of LKB-E-\\nLLM in specific industrial fields. \\nA. LKB-E-LLM for High-Speed Trains \\nAs a case study, this study has constructed a small LKB \\nrelated to HSTs. It mainly includes professional knowledge \\nsuch as \"Operating and Maintenance Implementation Rules \\nfor High-Speed Train Catenary,\" \"Operating and Maintenance \\nRules for High-Speed Train Traction Substations,\" and \\n\"Maintenance Rules for High-Speed Railway Signaling \\nSystems.\" These documents mainly involve the professional \\nknowledge of operation and maintenance management of \\nhigh-speed railway catenary, traction substation, and signal \\nequipment. Due to limitations in the training corpus, most \\nChatGPT-Like LLMs struggle to fully grasp the relevant \\ntechnical details. Considering the space constraints, this study \\npresents the results of two professional technical question-and-answer pairs. Then, a comparison is made between the \\nresponses of LKB-E-LLM and regular LLM to validate the performance gains brought by the LKB. The detailed question-and-answer records are shown in TABLE I. \\nAs shown in TABLE I, LKB-E-LLM and LLM answered \\nthe minor repairs of AC and DC power supply units of HSTs \\nand the technical standards of insulators. By comparing the \\nresponses of LKB-E-LLM and LLM, it can be observed that \\nthe responses from LLM are vague, hollow, and lack specific \\ntechnical details. On the other hand, the responses from LKB-\\n1https://python.langchain.com/docs/get_started/introduction.html                           2https://huggingface.co/moka-ai/m3e-base \\n3https://redis.io/                                                                                                          4https://milvus.io/docs/overview.md \\n5https://github.com/imClumsyPanda/langchain-ChatGLM                                       6https://github.com/wenda-LLM/wenda  7 \\n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) <  \\n E-LLM are more professional, with explicit and clear \\ntechnical details, which can effectively guide operational and \\nmaintenance personnel in their work. For example, in question \\n1, LKB-E-LLM clearly and professionally explains the points \\nto be noted during the minor repair process of power supply devices. This includes checking and recording the voltage and \\ncurrent of the battery, inspecting the battery\\'s appearance, and cleaning the relevant equipment from dust. In contrast, LLM\\'s response is less specific, lacks operability, and describes tasks \\nmore suited for major repairs, such as replacing specific core \\ncomponents. Similarly, for question 2, LKB-E-LLM provides a comprehensive and detailed explanation of the technical standards for insulator maintenance from various aspects. It \\ncontains strong guidance with technical details. In contrast, \\nLLM\\'s response is broad and appears correct, but it deviates \\nsignificantly from the requirements of real industrial \\nscenarios. \\nThe main reason for these differences is that the existing \\nLLMs lack domain-specific expertise. When faced with specific professional questions, they are unable to provide \\ndetailed, specific, and industry-compliant answers. LKB-E-\\nLLM significantly addresses this issue by leveraging the \\nknowledge in the LKB, allowing LLM to possess abundant \\nprofessional knowledge and respond fluently to professional questions. With the enrichment of content in the LKB, LKB-\\nE-LLM is expected to demonstrate even better performance. \\nOn the other hand, LKB-E-LLM also has some areas where \\nit performs less satisfactorily, such as potentially generating \\npartially incorrect outputs and producing text with significant \\nredundancy. For example, in question 1, there is considerable redundancy in the response from LKB-E-LLM. This is mainly because the semantic understanding and text generation \\ncapabilities of LLM still have room for improvement. This \\nstudy utilizes a relatively lightweight model, ChatGLM-6B, \\nwhich has certain gaps compared to models like ChatGPT. \\nWhen sufficient computational resources are available, \\nreplacing ChatGLM-6B with larger models will yield better \\nperformance. \\nTABLE  I \\nLKB-E-LLM  AND LLM  Q&A  RECORDS ON HST S RELATED QUESTIONS  \\nQuestion-1 Please provide a detailed description of the scope and standards for minor repairs of AC and DC power supply devices. \\nLKB-E-\\nLLM  (1) Measure and record each battery\\'s terminal voltage and float charge voltage, which should comply with the specifications. T his is one of the scope and standards for minor \\nrepairs of the AC/DC power supply unit. Check each battery\\'s terminal voltage and float charge voltage to ensure they meet the specifications stated in the manual. These \\nchecks are crucial for maintaining the performance and reliability of the AC/DC power supply unit. \\n(2) Remove dust and dirt from the DC charging unit, especially from the heat sink and cooling fan. This is also one of the scop e and standards for minor repairs of the AC/DC \\npower supply unit. Removing dust and dirt from the DC charging unit ensures the proper functioning of its cooling system and pr events overheating and other potential issues. \\n(3) Inspect the appearance of the batteries, which should be intact, clean, without deformation or bulging, and with reliable w ire connections. Inspect the appearance of the \\nbatteries to ensure they are in good condition, without deformation, bulging, or other damage. Additionally, check the wire con nections to ensure they are secure and reliable. \\n(4) Verify the secure installation of the DC panel and cabinets, free from corrosion and dirt and with good paint coating. Insp ect the secure installation of the DC panel and \\ncabinets, ensuring they are free from corrosion or other damages. Also, check if they are coated with appropriate paint to main tain their appearance and performance. \\n(5) Check the equipment\\'s current, voltage, insulation monitoring data, and signal display, which should be functioning properl y. Inspect the current, voltage, and insulation \\nmonitoring data of the AC/DC power supply unit to ensure its normal operation. Additionally, check the signal display and warni ng system to ensure they are working properly \\nand issue necessary warnings to protect equipment and personnel safety. \\nLLM Scope of Minor Repairs: The scope of minor repairs for AC/DC power supply units typically includes the following aspects: \\n\\uf06c Replacement or repair of battery packs or charging circuits. \\n\\uf06c Replacement or repair of electronic components such as switches, control boards, or oscillators. \\n\\uf06c Cleaning or replacement of radiators or fans to maintain effective cooling of the power supply unit. \\n\\uf06c Inspection or repair of circuit connections in the power supply unit to ensure their reliability. \\nStandards: Within the scope of minor repairs for AC/DC power supply units, there are also some general standards and procedures  to ensure consistency and reliability in the \\nmaintenance process: \\n\\uf06c Use appropriate tools and equipment. \\n\\uf06c Follow the instructions in the standard maintenance manual or operating instructions. \\n\\uf06c Avoid attempting self-repairs, especially in uncertain situations. \\n\\uf06c Ensure that the repair process does not cause permanent damage or failure to the circuit or equipment. \\nThe scope and standards of minor repairs for AC/DC power supply units may vary depending on the equipment and manufacturer. Dur ing the repair process, it is important to \\nbe familiar with the corresponding standards and operating instructions and follow the guidance provided by the manufacturer.  \\nQuestion-2  Please describe in detail the technical standards for insulator maintenance. \\nLKB-E-\\nLLM  (1) The leakage distance of the contact wire insulating components should be ≥1400mm. This standard stipulates that the leakage distance of the insulating components should \\nbe greater than 1400mm to ensure that they are not damaged by leakage during the operation of the contact wire, thereby ensurin g the safe operation of the contact wire \\nsystem. \\n(2) The surface of the insulator should be clean, smooth, free from dirt, intact, and without fractures. The area of porcelain enamel peeling should not exceed 300 mm2. This \\nensures that the insulator surface does not leak current due to wear or damage and can withstand current surges during operatio n. \\n(3) The porcelain part of the insulator should have good adhesion to the metal parts without gaps or visible cracks. This ensur es that the insulator can withstand current surges \\nduring operation and prevents current leakage. \\n(4) The connection between the insulator and the metal parts should have good adhesion and tight fastening. This ensures that t he insulator can be stably suspended and \\nprevents current leakage. \\n(5) Good connections between suspension insulators, with complete spring pins and cotter pins. This ensures that the suspension  insulators can be stably suspended and \\nprevents current leakage. (6) The insulator body should have good linearity, with a bending radius not exceeding 1%. This ensures that the suspension ins ulators can be stably suspended and prevents \\ncurrent leakage. (7) The surface of the insulator should be free from obvious discharge marks, circular or through cracks. This ensures that the  suspension insulators can be stably suspended \\nand \\nprevents current leaka ge. \\nLLM Insulator Maintenance Technical Standards: \\n(1) Insulator Material: The material used for insulators must have good electrical insulation properties and sufficient stabili ty and reliability under different voltages. \\nCommonly used materials include ceramics, glass, and fibers. \\n(2) Insulator Structural Design: Insulators\\' structures must withstand voltage while maintaining mechanical strength and stabil ity. The design of insulators needs to consider \\nfactors such as voltage, frequency, and load, and appropriate materials should be used for manufacturing. \\n(3) Insulator Maintenance: Insulators need regular maintenance to maintain their electrical insulation performance. Insulator m aintenance includes cleaning, oiling, and \\nrefurbishment. These maintenance tasks can improve the electrical insulation performance of insulators and reduce voltage drops  caused by dirt and oil stains on the insulator \\nsurface. (4) Insulator Voltage Testing: During insulator maintenance, voltage testing of the insulators is necessary to ensure their ele ctrical insulation performance meets the \\nrequirements. The testing voltage is typically between 500V and 1500V , and the appropriate voltage value is selected based on t he specific maintenance situation. \\n(5) Insulator Maintenance Records: During insulator maintenance, it is important to record relevant information such as mainten ance time, content, and results. This facilitates \\nthe evaluation and analysis of the performance of the insulators. \\n(6) The insulator maintenance technical standards aim to ensure that insulators have good electrical insulation performance whi le enhancing their mechanical strength and \\nstability, thereby guaranteeing their normal service life. \\n  8 \\n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) <  \\n B. LKB-E-LLM for Wind Turbines \\nThis study also constructed a small LKB related to wind \\nturbines to explore the performance of LKB-E-LLM in wind turbines. It mainly includes professional knowledge \\ndocuments such as \"1.5MW Wind Turbine Operation and Maintenance Manual,\" \"Doubly-Fed Wind Turbine Fault \\nTroubleshooting Manual,\" \"Wind Turbine Fault Handling \\nManual,\" and \"UP-1500 Wind Turbine Maintenance Manual.\" \\nThese documents primarily cover the operation, maintenance, \\nand fault troubleshooting of wind turbines, which are expertise \\nand technical details that most ChatGPT-Like LLMs do not possess. Considering the paper\\'s length limitations, this study \\npresents the results of three professional technical question-\\nand-answer pairs. Then, a comparison is made between the responses of LKB-E-LLM and regular LLM to validate the \\nperformance gains brought by the LKB. The detailed question-and-answer records are shown in TABLE II. \\nAs shown in TABLE II, we consulted with LKB-E-LLM \\nand LLM about the repair of wind turbine blades, high \\ntemperature of generator carbon brushes, fault code query, and \\nother related technical issues. They behave similarly as they do in the HSTs scenario. The responses from LKB-E-LLM are more professional, detailed, and contain rich technical details \\nrelevant to the questions, providing practical guidance. \\nHowever, due to a lack of contextual information, LLM\\'s \\nresponses are severely unrelated to the questions and only offer vague and generic information. For example, in question \\n3, LKB-E-LLM accurately explains the specific technical \\ndetails and steps for blade damage repairs, while LLM misunderstands the question and provides a completely \\nincorrect response. Similarly, for question 4, LKB-E-LLM\\'s \\nresponse contains more professional technical content than LLM, making it valuable guidance for operational and \\nmanagement personnel. Additionally, question 5 validates the \\nfault troubleshooting capability of LKB-E-LLM. LKB-E-LLM \\ncan accurately output the relevant fault category and details \\nwith the given fault code. In contrast, due to a lack of relevant domain knowledge, LLM provides a response unrelated to \\nwind turbines. Overall, LKB-E-LLM significantly improves \\nthe lack of professional knowledge in LLM. This allows LKB-E-LLM to be applied in various professional technical fields, \\nfully leveraging the advantages of LLM and enhancing the effectiveness of industrial PHM-related applications. \\nTABLE  II \\nLKB-E-LLM  AND LLM  Q&A  RECORDS ON WIND TURBINES RELATED QUESTIONS  \\nQuestion-3 For blade repair, please describe in detail the repair methods for surface and structural damage. \\nLKB-E-\\nLLM  (1) Surface Damage Repair Methods:  \\n\\uf06c Clean the surface with acetone and then use a multi-layer repair material for the repair.  \\n\\uf06c For damaged areas with scratches, grooves, marks, or scrapes on the surface, the fiberglass can be polished into a step shape, increasing by 20mm for each layer. After \\npolishing, clean the surface and then use a multi-layer repair material for the repair.  \\n\\uf06c If there are defects in the seal between the rain cover and the blade casing, it needs to be inspected and repaired.  \\n\\uf06c For corrosion on the blade surface, reinforce it with an epoxy resin hardener.  \\n\\uf06c For structural damage, such as cracks or holes on the blade, reinforce it with epoxy resin hardener or other materials based on  the extent of the damage, and then perform \\npolishing and smoothing. \\n(2) Structural Damage Repair Methods:  \\n\\uf06c Inspect the extent of structural layer damage and reinforce it accordingly.  \\n\\uf06c Apply epoxy resin hardener paste to broken glass fiber cloth, then cure and polish it.  \\n\\uf06c For cracks or fissures appearing at the blade root or load-carrying section, immediate shutdown, and repair are required. \\n(3) Maintenance Types:  \\n\\uf06c Routine maintenance: Inspect the blade surface, noise, and lightning protection system, and perform maintenance and record-keep ing.  \\n\\uf06c Annual maintenance: Inspect the blade bolts and installation bolts, and perform maintenance and record-keeping. \\nWhen repairing the blade surface, wearing safety masks and gloves is necessary, as acetone may cause skin irritation.  \\nWhen repairing the blade, reinforcement treatment should be applied to the structural layer to prevent recurring structural dam age. Additionally, for surface damage, clean the \\nsurface with acetone and then use a multi-la yer repair material for the re pair. \\nLLM (1) Surface Damage Repair Methods:  \\n\\uf06c Welding: Welding is a commonly used method for repairing surface damage. During the welding process, methods such as gas-shield ed welding, resistance welding, or manual \\nwelding can be used. Welding can repair mild surface damage on the blade, for example, by forming a weld seam along the blade\\'s  edge to enhance its strength.  \\n\\uf06c Hot pressing: Hot pressing is heating the metal to a high temperature and then applying pressure to repair the damaged area. Ho t pressing can repair moderate surface damage \\non the blade, such as forming a hot pressing rod at the bending section of the blade and applying heat and pressure to strength en it. \\n(2) Structural Damage Repair Methods:  \\n\\uf06c Laser cutting: Laser cutting is commonly used for repairing structural damage. During the cutting process, CO2 lasers or other laser beams can be used. Laser cutting can repair \\nsevere structural damage on the blade, such as cutting at the fracture point of the blade to strengthen it.  \\n\\uf06c Vegetation: Vegetation is a method of implanting plant tissue onto a metal surface for repair. During vegetation, extracts from  plant stem or roots can be used to improve the \\nenvironment of the metal surface and promote plant tissue growth. This method can repair severe structural damage on the blade,  such as vegetation at the fracture point of the \\nblade to strengthen it. \\nSurface damage repair methods mainly involve welding, hot pressing, and laser cutting, while structural damage repair methods i nvolve vegetation and metal reinforcement \\ntechni ques. \\nQuestion-4  Analyze the reasons for the high temperature of the generator carbon brush in detail. \\nLKB-E-\\nLLM  (1) Overall Current Distribution Imbalance: During generator operation, the working current of each carbon brush should remain constant between 20 to 100A. However, the \\ncurrent may deviate from the overall balance due to carbon brush wear or mechanical faults. If the overall current distribution  balance is disrupted, it can lead to an increase in \\ncarbon brush temperature. \\n(2) Insufficient Contact Area between Carbon Brush and Slip Ring: Insufficient contact area between the carbon brush and slip r ing can result in mechanical wear, increasing carbon \\nbrush temperature. During generator maintenance, measuring the contact area between the carbon brush and slip ring is necessary . If it is too small, the carbon brush may need to be \\nreplaced or the contact area adjusted. \\n(3) Blockage of Slip Ring Ventilation Holes: Blockage of the slip ring ventilation holes prevents proper airflow, causing the i nternal heat of the generator to be unable to dissipate. \\nThis ultimately leads to an excessive temperature of the carbon brushes. It is important to check for any blockages in the slip  ring ventilation holes and promptly clean them if \\nnecessary. (4) Excessive Spring Pressure: Excessive spring pressure can cause mechanical wear, increasing carbon brush temperature. Adjust ing the spring pressure to ensure it falls within the \\nmanufacturer\\'s specified range is necessary. To address the above issues, regular maintenance of the generator is required. This includes monitoring the current distributio n balance, the contact area between the carbon brushes \\nand slip rings, and checking for blockage in the slip ring ventilation holes. Any problems should be detected promptly and addr essed accordingly. Additionally, regular maintenance \\nof the carbon brushes is essential to ensure their normal operating temperature and lifespan. \\nLLM (1) Generator Design Issues: There may be certain flaws in the design and manufacturing process of the generator, resulting in uneven or excessive wear on the surface of the \\ncarbon brushes. This can affect the heat dissipation efficiency of the carbon brushes, leading to an increase in temperature. \\n(2) Insufficient Number of Carbon Brushes: There may be an insufficient number of carbon brushes, which fails to effectively tr ansfer heat, increasing the surface temperature of \\nthe carbon brushes. (3) Quality Issues with Carbon Brushes: There may be quality issues with the carbon brushes, such as impurities in the material  or uneven surfaces. This can also impact the heat \\ndissipation efficiency of the carbon brushes. \\n(4) Load Issues during Operation: The generator may experience high loads during operation, increasing the carbon brushes\\' surf ace temperature. \\n(5) Improper Maintenance: Improper maintenance of the generator, such as inadequate cleaning of the carbon brushes or failure t o replace them in a timely manner, can also cause \\nan increase in the surface temperature of the carbon brushes.  9 \\n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) <  \\n Question-5 Fault Code: 050001 \\nLKB-E-\\nLLM  050001 Fault Code: Hydraulic System Heater Protection Signal Input is 0. \\nThis fault code indicates that the protection signal input of the hydraulic system heater is 0. This fault is usually caused by  a damaged heater in the hydraulic system or a sensor \\nmalfunction, resulting in an erroneous signal input. The shutdown task associated with this fault code is to inspect the condit ion of the heater in the hydraulic system and carry out \\nrepairs or replacement. 050000 Fault Code: At least one hydraulic-related fault has occurred. \\nThis fault code indicates that there is at least one hydraulic-related fault in the system, requiring further troubleshooting. This typically involves a detailed inspection and testing of \\nthe hydraulic system to determine the specific cause of the fault. In some cases, it may be necessary to replace or repair hydr aulic components or perform retesting and calibration \\nof the s\\nystem. \\nLLM Fault Code 050001 indicates a hardware failure in the television. It typically signifies that the television is experiencing is sues with startup or unstable operation. \\nThe cause of this fault code is attributed to internal circuitry or hardware problems within the television. This results in th e improper transmission of signal inputs to the television\\'s \\ndecoder or processor, leadin g to the inabilit y to start or unstable o peration of the television. \\n \\nLARGE-SCALE \\nLANGUAGE MODEL\\nLOCAL \\nKNOWLEDGE BASE\\nEnsure Industrial Data Privacy and Security\\nOffline Deployment, Flexible and Easy to Expand\\nProfessional Knowledge in the Industrial PHM\\nFast and Cost-Effective Implementation\\nReal Time Update of Professional Knowledge\\nLLM-Based Knowledge Transfer and Training\\nAutomatic Data Analysis and Report Generation\\nTroubleshooting, Repair Advice and Instructions\\nIndustrial Automation Monitoring and Operation\\nMaintenance Planning and OptimizationAdvantage Potential Application LKB-E-LLM\\n \\nFig. 4  Advantages and potential applications of LKB-E-LLM in the field of PHM.  \\n \\nV. DISCUSSIONS  \\nA. Advantages of LLM Empowered by LKB \\nAlthough ChatGPT-Like LLM exhibits robust language \\nunderstanding and text generation capabilities, its lack of \\ndomain-specific expertise hinders its performance in industrial \\ndomains. However, incorporating LKBs into LLM, referred to as LKB-E-LLM, effectively addresses this limitation and \\nsignificantly enhances the applicability of LLM in the field of industrial PHM.  \\nFig. 4  illustrates the superior performance of LKB-E-LLM \\ncompared to conventional LLM in various aspects. Notably, \\nLKB-E-LLM offers the flexibility of offline deployment and \\neliminates the need for additional data fine-tuning, thus \\nensuring the confidentiality and security of sensitive industrial \\ndata. Moreover, while LKBs encompass detailed domain-\\nspecific knowledge, ChatGPT-Like LLM possesses \\ncomprehensive general knowledge and natural language \\ninductive reasoning capabilities. The fusion of these two \\ncomponents facilitates the utilization of the specialized \\nknowledge from the LKB and the vast background knowledge \\nof LLM, thereby enabling the provision of comprehensive, \\nprofessional, and tailored answers and solutions to users.  \\nIn addition, combined with the LKB, the capabilities of \\nLLM can be deeply customized to meet the various needs of \\nindustrial PHM. By leveraging a lightweight language model, LKB-E-LLM achieves comparable performance to that of super-large language models, as demonstrated in this study \\nthat implements LKB-E-LLM for industrial PHM on consumer-grade computer hardware, delivering excellent \\nperformance. Consequently, LKB-E-LLM presents a cost-\\neffective and rapid implementation solution suitable for \\ndeployment in resource-constrained industrial scenarios.  \\nLastly, given the dynamic nature of professional \\nknowledge and documents in the industrial domain, it is imperative for ChatGPT-Like LLM to possess real-time \\nknowledge updating capabilities. The LKB-based solution \\neffectively fulfills this requirement, as the LKB can be dynamically updated without necessitating modifications to LLM. This ensures that LKB-E-LLM remains up-to-date with \\nthe latest technical knowledge, aligning with the evolving \\ndemands of industrial PHM applications.  \\nIn conclusion, the integration of LKB and ChatGPT-Like \\nLLM in specific domains capitalizes on their respective \\nstrengths, offering comprehensive, accurate, professional, and personalized information services to meet the specific needs of \\nthe industrial PHM field. \\nB. Potential Applications of LLM Empowered by LKB \\nAlthough ChatGPT-Like LLM mainly uses text dialogue \\nas the main application scenario, LLM combined with LKB \\nwill significantly enhance its application potential in industrial \\nPHM and have a revolutionary impact on the field of PHM. Fig. 4 illustrates the comprehensive advantages of LKB-E-\\nLLM across various dimensions within the industrial PHM domain, including enhanced efficiency, revenue generation, \\nperformance improvement, and cost reduction. For instance, \\nLKB-E-LLM facilitates LLM-based knowledge dissemination \\nand training, enabling industrial enterprises to swiftly cultivate  10 \\n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) <  \\n competent and dependable personnel, thereby reducing the \\ntime and cost associated with training and maintaining \\noperational staff. Moreover, LKB-E-LLM autonomously \\nanalyzes industrial data to generate real-time analysis reports, aiding engineers in monitoring industrial operations and \\noffering troubleshooting guidance and maintenance \\nrecommendations. Notably, LKB contains detailed descriptions of equipment and system failure scenarios, along \\nwith optimal solutions. Leveraging LKB-E-LLM, junior \\nengineers can promptly identify faults and determine the most suitable solutions, consequently enhancing the efficiency of \\nindustrial PHM and diminishing maintenance costs. \\nFurthermore, extensive repositories of maintenance plans and \\nmeasures have accumulated over the long-term operation of \\nindustrial enterprises. LKB-E-LLM can analyze and summarize these cases, facilitating the generation of \\nappropriate maintenance plans based on current equipment \\nand system conditions, as well as optimizing existing plans, thereby augmenting the benefits derived from maintenance \\nactivities.  \\nThrough these advancements, LKB-E-LLM actively \\nengages in industrial operation and maintenance, contributing \\nto the realization of automated industrial processes and improving the outcomes of PHM applications. Consequently, \\nresearch on LLM-based PHM applications is progressing rapidly, with the potential to revolutionize the established \\nparadigms within the field in the foreseeable future. \\nC. Future Directions of LLM Empowered by LKB \\nIt can be seen from Section III that the realization of LKB-\\nE-LLM mainly relies on the construction of domain-specific \\nLKB, the representation ability of the text embedding model, and the language processing, induction, and reasoning \\ncapabilities of LLM. Future advancements can be explored \\nand enhanced within these three directions. The accuracy, comprehensiveness, and professionalism of the LKB play a \\ncrucial role in enabling LKB-E-LLM to address the diverse demands of industrial PHM applications. Hence, industrial \\nenterprises must maintain a complete offline LKB, ensuring \\ntimely updates and additions of the latest professional knowledge and data. The effectiveness of the text embedding \\nmodel determines the accurate representation of information in \\nthe knowledge base, directly impacting the ability to search \\nfor query-related knowledge accurately. Consequently, further \\ninvestigation into cutting-edge and novel text embedding models will enhance the performance of LKB-E-LLM. \\nChatGPT-Like LLM serves as the foundation of LKB-E-LLM, and its language processing capability directly influences the \\nability of engineers to obtain precise and concise results. \\nConsidering that a significant portion of data in the industrial \\nPHM field exists in the form of videos and images, the exploration of a reliable visual language model [40] and its \\nintegration into LKB-E-LLM can significantly enhance its \\napplication potential. \\nVI.\\n CONCLUSIONS  \\nPHM is crucial in ensuring industrial systems and \\nequipment\\'s reliable and safe operation. With significant advancements in recent years, ChatGPT-Like LLM has demonstrated remarkable progress in language understanding, \\ntext generation, and generalization, offering the potential to \\ntransform the application landscape of industrial PHM and \\nfacilitate the development of highly intelligent industrial operation and maintenance systems. However, the lack of \\ndomain-specific expertise poses a challenge in effectively \\napplying ChatGPT-Like LLM to address professional PHM problems. Therefore, this study focuses on exploring the \\nsolution of LKB-based LLM for industrial PHM, aiming to \\novercome the aforementioned limitations. The study provides comprehensive details on the methodology and technical \\naspects of the proposed solution. Through two real-world \\nPHM case studies, it is demonstrated that integrating LLM \\nwith LKB can better cater to the requirements of industrial \\nPHM applications. Compared to traditional LLM approaches, the combined LLM and LKB approach enables the acquisition \\nof extensive domain-specific knowledge in industrial PHM, \\nwith the ability to update knowledge in real-time, while also offering advantages such as rapid and cost-effective \\ndeployment. Consequently, LLM combined with LKB exhibits significant application potential in industrial PHM, \\nwith the potential to revolutionize the PHM domain. \\n \\nREFERENCES  \\n [1] J. Lee, F. Wu, W. Zhao and M. Ghaffari, et al., \"Prognostics and health \\nmanagement design for rotary machinery systems —Reviews, \\nmethodology and applications,\" Mech. Syst. Signal Pr. , vol. 42, no. 1, pp. \\n314-334, 2014.  \\n [2] Y. Zhang and Y. Li, \"Prognostics and health management of Lithium-ion \\nbattery using deep learning methods: A review,\" Renewable and \\nSustainable Energy Reviews , vol. 161, pp. 112282, 2022.  \\n [3] Z. Ren, A.S. Verma, Y. Li and J.J.E. Teuwen, et al., \"Offshore wind \\nturbine operations and maintenanc e: A state-of-the-art review,\" \\nRenewable and Sustainable Energy Reviews , vol. 144, pp. 110886, 2021.  \\n [4] Z. Erdogan, S. Altuntas and T. De reli, \"Predicting Patent Quality Based \\non Machine Learning Approach,\" IEEE T. Eng. Manage. , pp. 1-14, 2022.  \\n [5] H. Alaka, L. Oyedele, H. Owolabi and O. Akinade, et al., \"A Big Data \\nAnalytics Approach for Construction Firms Failure Prediction Models,\" \\nIEEE T. Eng. Manage. , vol. 66, no. 4, pp. 689-698, 2019.  \\n [6] H. Wang, Y. Li and J. Ren, \"Machine learning for fault diagnosis of high-\\nspeed train traction systems: A review,\" Frontiers of Engineering \\nManagement , pp. 1-17, 2023.  \\n [7] Y. LeCun, Y. Bengio and G. Hinton, \"Deep learning,\" Nature , vol. 521, \\npp. 436-444, 2015.  \\n [8] G. M., V. Ravi, S. V and G.E. A, et al., \"Explainable Deep Learning-\\nBased Approach for Multilabel Classi fication of Electrocardiogram,\" \\nIEEE T. Eng. Manage. , vol. 70, no. 8, pp. 2787-2799, 2023.  \\n [9] C. Mühlroth and M. Grottke, \"Art ificial Intelligence in Innovation: How \\nto Spot Emerging Trends and Technologies,\" IEEE T. Eng. Manage. , vol. \\n69, no. 2, pp. 493-510, 2022.  \\n[10] K. He, X. Zhang, S. Ren and S. Jian, \"Deep residual learning for image \\nrecognition,\" in Proc. IEEE CVPR , 2016. \\n[11] A. Vaswani, N. Shazeer, N. Parmar and J. Uszkoreit, et al., \"Attention is \\nAll You Need,\" in Proc. NeurIPS , 2017. \\n[12] H. Wang, Z. Liu, Y. Ge and D. Peng, \"Self-supervised signal \\nrepresentation learning for machinery fault diagnosis under limited \\nannotation data,\" Knowl.-Based Syst. , vol. 239, pp. 107978, 2022.  \\n[13] H. Wang, Z. Liu, D. Peng and Z. Cheng, \"Attention-guided joint learning \\nCNN with noise robustness for bearing fault diagnosis and vibration signal denoising,\" ISA T. , 2021. \\n[14] D. Peng, H. Wang, Z. Liu and W. Zhang, et al., \"Multibranch and \\nMultiscale CNN for Fault Diagnosis of Wheelset Bearings Under Strong \\nNoise and Variable Load Condition,\" IEEE T. Ind. Inform. , vol. 16, no. 7, \\npp. 4949-4960, 2020.  \\n[15] G. Pang, C. Shen, L. Cao and A.V.D. Hengel, \"Deep learning for \\nanomaly detection: A review,\" ACM computing surveys (CSUR) , vol. 54, \\nno. 2, pp. 1-38, 2021.  \\n[16] D. Peng, C. Liu, W. Desmet a nd K. Gryllias, \"Semi-Supervised CNN- 11 \\n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE-CLICK HERE TO EDIT) <  \\n Based SVDD Anomaly Detection for Condition Monitoring of Wind \\nTurbines,\" in Proc. International Conference on Offshore Mechanics and \\nArctic Engineering , 2022, pp. V1T.  \\n[17] C. Zhang, Y.F. Li and D.W. Coit, \"Deep Reinforcement Learning for \\nDynamic Opportunistic Maintenance of Multi-Component Systems With \\nLoad Sharing,\" IEEE T. Reliab. , pp. 1-15, 2022.  \\n[18] H. Wang, T. Men and Y.F. Li, \"Transformer for High-Speed Train Wheel \\nWear Prediction With Multiplex Local –Global Temporal Fusion,\" IEEE \\nT. Instrum. Meas. , vol. 71, pp. 1-12, 2022.  \\n[19] T. Han, Z. Wang and H. Meng, \"End-to-end capacity estimation of \\nLithium-ion batteries with an enhanc ed long short-term memory network \\nconsidering domain adaptation,\" J. Power Sources , vol. 520, pp. 230823, \\n2022. \\n[20] H. Meng and Y. Li, \"A review on prognostics and health management \\n(PHM) methods of lithium-ion batteries,\" Renewable and Sustainable \\nEnergy Reviews , vol. 116, pp. 109405, 2019.  \\n[21] H. Wang, Z. Liu, D. Peng and M. Yang, et al., \"Feature-level attention-\\nguided multitask CNN for fault di agnosis and working conditions \\nidentification of rolling bearing,\" IEEE T. Neur. Net. Lear. , vol. 33, no. 9, \\npp. 4757-4769, 2021.  \\n[22] K. Kumar and R.P. Saini, \"A review on operation and maintenance of \\nhydropower plants,\" Sustainable Energy Technologies and Assessments , \\nvol. 49, pp. 101704, 2022.  \\n[23] C. Zhou, Q. Li, C. Li and J. Yu, et al., \"A Comprehensive Survey on \\nPretrained Foundation Models: A History From Bert to Chatgpt,\" \\n[Online].  Available: https://arxiv.org/abs/2302.09419.  \\n[24] Y. Cao, S. Li, Y. Liu and Z. Yan, et al., \"A Comprehensive Survey of AI-\\nGenerated Content (AIGC): A History of Generative AI From GAN to Chatgpt,\" [Online].  Available: https://arxiv.org/abs/2303.04226. \\n[25] L. Ouyang, J. Wu, X. Jiang and D. Almeida, et al., \"Training language \\nmodels to follow instructions  with human feedback,\" in Proc. NeurIPS , \\n2022, pp. 27730-27744.  \\n[26] P.P. Ray, \"ChatGPT: A comprehensive review on background, \\napplications, key challenges, bias, ethics, limitations and future scope,\" \\nInternet of Things and Cyber-Physical Systems , vol. 3, pp. 121-154, 2023.  \\n[27] M. Cascella, J. Montomoli, V. Bellini and E. Bignami, \"Evaluating the \\nfeasibility of ChatGPT in healthcare: an  analysis of multiple clinical and \\nresearch scenarios,\" J. Med. Syst. , vol. 47, no. 1, pp. 33, 2023.  \\n[28] Y. Li, H. Wang and M. Sun, \"ChatGPT-Like Large-Scale Foundation \\nModels for Prognostics and Health Management: A Survey and Roadmaps,\" [Online].  Available: https://arxiv.org/abs/2305.06472. \\n[29] W.X. Zhao, K. Zhou, J. Li and T. Tang, et al., \"A survey of large \\nlanguage models,\" [Online].  Available: https://arxiv.org/abs/2303.18223.  \\n[30] Z. Du, Y. Qian, X. Liu and M. Ding, et al., \"GLM: General Language \\nModel Pretraining with Autore gressive Blank Infilling,\" in Proc. 60th \\nAnnual Meeting of the Association for Computational Linguistics , 2022, \\npp. 320-335.  \\n[31] A. Zeng, X. Liu, Z. Du and Z. Wang, et al., \"Glm-130b: An open \\nbilingual pre-trained model,\" [Online].  Available: \\nhttps://arxiv.org/abs/2210.02414.  \\n[32] OpenAI, \"GPT-4 Technical Report,\" [Online].  Available: \\nhttps://arxiv.org/abs/2303.08774.  \\n[33] Y. Sun, S. Wang, S. Feng and S. Ding, et al., \"Ernie 3.0: Large-scale \\nknowledge enhanced pre-training for language understanding and \\ngeneration,\" [Online].  Available: https://arxiv.org/abs/2107.02137.  \\n[34] Y. Sun, S. Wang, Y. Li and S. Feng, et al., \"Ernie 2.0: A continual pre-\\ntraining framework for language understanding,\" in Proc. AAAI \\nConference on Artificial Intelligence , 2020, pp. 8968-8975.  \\n[35] T. Lin, Y. Wang, X. Liu and X. Qiu, \"A survey of transformers,\" AI \\nOpen , vol. 3, pp. 111-132, 2022.  \\n[36] L. Jing and Y. Tian, \"Self-Supervised Visual Feature Learning With \\nDeep Neural Networks: A Survey,\" IEEE T. Pattern Anal. , vol. 43, no. 11, \\npp. 4037-4058, 2021.  \\n[37] Q. Liu, M.J. Kusner and P. Blunsom, \"A survey on contextual \\nembeddings,\" [Online].  Available: https://arx iv.org/abs/2003.07278.  \\n[38] J. Johnson, M. Douze and H. Jé gou, \"Billion-Scale Similarity Search \\nwith GPUs,\" IEEE Transactions on Big Data , vol. 7, no. 3, pp. 535-547, \\n2021. \\n[39] J. Devlin, M. Chang, K. Lee and K. Toutanova, \"Bert: Pre-training of \\ndeep bidirectional transformers for language understanding,\" [Online].  \\nAvailable: https://arxiv.org/abs/1810.04805.  \\n[40] J. Zhang, J. Huang, S. Jin and S. Lu, \"Vision-language models for vision \\ntasks: A survey,\" [Online].  Available: https://arxiv.org/abs/2304.00685.  \\n  '},\n",
       " {'url': 'http://arxiv.org/pdf/2308.10168v1',\n",
       "  'title': 'Head-to-Tail: How Knowledgeable are Large Language Models (LLM)? A.K.A. Will LLMs Replace Knowledge Graphs?',\n",
       "  'text': 'Head-to-Tail : How Knowledgeable are Large Language Models (LLM)?\\nA.K.A. Will LLMs Replace Knowledge Graphs?\\nKai Sun, Yifan Ethan Xu, Hanwen Zha, Yue Liu, Xin Luna Dong\\nMeta Reality Labs\\n{sunkaicn,ethanxu,hwzha,yuei,lunadong}@meta.com\\nAbstract\\nSince the recent prosperity of Large Language\\nModels (LLMs), there have been interleaved\\ndiscussions regarding how to reduce hallucina-\\ntions from LLM responses, how to increase the\\nfactuality of LLMs, and whether Knowledge\\nGraphs (KGs), which store the world knowl-\\nedge in a symbolic form, will be replaced with\\nLLMs. In this paper, we try to answer these\\nquestions from a new angle: How knowledge-\\nable are LLMs?\\nTo answer this question, we constructed Head-\\nto-Tail , a benchmark that consists of 18K\\nquestion-answer (QA) pairs regarding head,\\ntorso, and tail facts in terms of popularity. We\\ndesigned an automated evaluation method and\\na set of metrics that closely approximate the\\nknowledge an LLM confidently internalizes.\\nThrough a comprehensive evaluation of 14 pub-\\nlicly available LLMs, we show that existing\\nLLMs are still far from being perfect in terms\\nof their grasp of factual knowledge, especially\\nfor facts of torso-to-tail entities.\\n1 Introduction\\nPre-trained large language models (LLMs), such\\nas ChatGPT1and LLaMA (Touvron et al., 2023a),\\nhave demonstrated impressive capabilities in inter-\\nnalizing knowledge and responding to common in-\\nquiries (Ouyang et al., 2022; OpenAI, 2023). Nev-\\nertheless, these models often lack knowledge of\\nnuanced, domain-specific details and are suscep-\\ntible to hallucinations (Bang et al., 2023), under-\\nscoring the significant challenges of increasing the\\nfactuality of LLMs and minimizing hallucinations\\nfrom LLM responses. Conversely, the rise of LLMs\\nhas sparked debates on whether Knowledge Graphs\\n(KGs), which store real-world factual knowledge in\\ntriplet form (subject, predicate, object), will be re-\\nplaced with LLMs. This paper tries to answer these\\nquestions from a new angle: How knowledgeable\\nare LLMs?\\n1https://openai.com/blog/chatgpt\\nHead T orso T ail0102030Correct answers by ChatGPT (%)32.7\\n19.7\\n13.826.224.1\\n5.1Open domain Specific domains (averaged)Example questions where ChatGPT gives incorrect answers\\nMovie\\nQuestion: What profession does Tj Singh (known for John Carter (2012))\\nhave?\\nGround Truth: Visual effects\\nChatGPT: Actor\\nBook\\nQuestion: Who authored Choke (published in 1996)?\\nGround Truth: Stuart Woods\\nChatGPT: Chuck Palahniuk\\nAcademics\\nQuestion: In which year was \"Distribution of Hydrogen in the Near-\\nSurface of Mars: Evidence for Subsurface Ice Deposits\" published?\\nGround Truth: 2002\\nChatGPT: 2009\\nOpen\\nQuestion: What college is the sister college of Trinity College, Oxford?\\nGround Truth: Churchill College, Cambridge\\nChatGPT: Cambridge University\\nFigure 1: The question-answering accuracy of ChatGPT\\ndeclines following the order of head, torso, and tail\\nentities on the Head-to-Tail benchmark, and is only\\n33% even for popular (head) entities.\\nFinding answers to these questions is not easy.\\nFirst, it is hard to directly “query” the knowledge\\nembedded in an LLM—hallucination can be due to\\nlack of knowledge but can also be caused by dys-\\nfunction of the generative model even if the knowl-\\nedge is already parameterized in the model. We ap-\\nproximate the amount of knowledge in an LLM by\\nits accuracy in answering simple-formed questions,\\nsuch as “where was the basketball player Michael\\nJordan born?” ; in addition, we ask the LLM to gen-\\nerate brief answers and admit “unsure” when its\\nconfidence is low. We chose this proxy because we\\nfound LLMs are normally very good at understand-arXiv:2308.10168v1  [cs.CL]  20 Aug 2023 ing simple-formed questions and produce consis-\\ntent answers when regenerating answers, especially\\nif asked to be brief (Section 3.5).\\nSecond, there is no ready-to-use benchmark that\\neither well represents distributions of user’s inter-\\nest (the query logs for major LLMs or search en-\\ngines are not publicly available) or well represents\\nthe uniform distribution of the world knowledge\\n(even the largest knowledge graphs admit spar-\\nsity of knowledge, especially towards non-popular\\nfacts). To address this challenge, we construct a\\nbenchmark of 18K QA pairs that cover various do-\\nmains and various relationships in these domains.\\nWe bucket entities and relationships to head, torso,\\nandtailaccording to how popular they are (de-\\ntails in Section 2) and randomly sample from each\\nbucket; as such, we call our benchmark Head-to-\\nTail. This benchmark facilitates us to achieve a\\ncomprehensive view of how knowledgeable LLMs\\nare regarding each bucket.\\nThrough the Head-to-Tail benchmark and the\\nexperimental methodology, we answer the follow-\\ning three research questions (RQs):\\nRQ1: How reliable are LLMs in answering factual\\nquestions?\\nRQ2: Do LLMs perform equally well on head, torso,\\nand tail facts?\\nRQ3: Do normal methods that improve LLMs, such\\nas model size increase and instruction tuning,\\nhelp LLMs to be more knowledgeable?\\nAs shown in Figure 1, our analysis demonstrates\\na consistent decline in the performance of LLMs,\\nfollowing the order of head, torso, and tail entities,\\nconfirming our hypothesis that LLMs contain more\\nhead knowledge where training data abound. Sur-\\nprisingly, even for the top-0.5% popular entities\\nin popular domains such as Movie , the evaluated\\nLLMs, at best, barely provide accurate answers\\nto half of the questions in the benchmark. Nor-\\nmal methods that enhance LLMs do not necessar-\\nily make them more knowledgeable, highlighting\\nthe need for more effective approaches to increase\\nLLMs’ factuality.\\nOur main contributions are as follows:\\n(i)We introduce Head-to-Tail , the first bench-\\nmark focused on comprehensively assessing\\nthe effectiveness of LLMs in incorporating\\nfactual knowledge encompassing the head,torso, and tail portions of knowledge graphs\\n(Section 2.1).\\n(ii)We present an evaluation methodology accom-\\npanied by metrics designed to assess the fac-\\ntuality of LLMs. Our metrics allow us to dis-\\ntinguish hallucination and missing answers,\\nand our evaluation method, whereas entirely\\nautomated, proves to be reliable and robust\\n(Section 2.2-2.3).\\n(iii) We conducted a comprehensive evaluation and\\nquantified the factuality of 14 LLMs regard-\\ning head, torso, and tail facts to answer the\\nresearch questions (RQ1–RQ3) (Section 3). In\\nlight of these findings, we envision the future\\nof knowledge graphs and outline a research\\nlandscape aimed at improving the overall fac-\\ntual reliability of LLMs (Section 4).\\n2 The Head-to-Tail Benchmark\\nWe now describe the Head-to-Tail benchmark, the\\nmetrics, and our evaluation methodology.\\n2.1 QA pair generation\\nDomains and data sources. To cover a broad\\nrange of knowledge, we used the DBpedia knowl-\\nedge graph (Auer et al., 2007), where the knowl-\\nedge originates from Wikipedia (Denoyer and Gal-\\nlinari, 2006). We used a cleaned version of the\\nEnglish snapshot from December 1, 2022.2\\nTo better understand LLM performance on par-\\nticular domains, we also selected three domains\\nwhere public data are easily accessible.\\n•Movie: We used a snapshot of IMDb3from\\nMay 21, 2023.\\n•Book: We used the data of Goodreads scraped\\nin 2017 released by Wan and McAuley (2018).\\n•Academics: We used a snapshot of\\nMAG (Sinha et al., 2015) from September 13,\\n2021 and DBLP4from May 10, 2023.\\nEntities. An important contribution of the Head-\\nto-Tail benchmark is the bucketing of head, torso,\\nand tail entities, decided by the popularity of the\\nentities (we will also discuss how the popularity of\\n2https://databus.dbpedia.org/dbpedia/\\nmappings/mappingbased-objects\\n3https://developer.imdb.com/\\nnon-commercial-datasets/\\n4https://dblp.org/ IMDb Goodreads MAG DBLP DBpedia\\nTitle Person Book Article Conference Journal Scholar -\\nHead 767 ( 0.01) 34,903 ( 0.48) 3,150 ( 2.31) 1,827,710 ( 0.70) 257 ( 1.63) 225 ( 0.46) 79,521 ( 2.44) 103,564 ( 1.30)\\nTorso 4,113 ( 0.05) 87,645 ( 1.21) 7,304 ( 5.35) 9,386,034 ( 3.60) 965 ( 6.12) 1,266 ( 2.58) 500,778 (15.36) 1,255,113 (15.77)\\nTail 7,536,482 (99.94) 7,111,496 (98.31) 126,134 (92.35) 249,311,539 (95.70) 14,550 (92.25) 47,546 (96.96) 2,680,704 (82.20) 6,600,206 (82.93)\\nTable 1: The number (%) of head, torso, and tail entities. The distribution follows the power law.\\nthe predicates affect results in Section 3.3). We use\\ntwo ways to approximate popularity: traffic and\\ndensity . When there is traffic information, such\\nas views and votes, we conveniently use traffic to\\nmeasure the popularity; otherwise, we use density\\nas a proxy, such as the number of facts or works\\nabout the entity. We often observe a correlation\\nbetween density and traffic (e.g., the more popular\\na person is, the more we know about her), but as\\nwe will see soon from the benchmark statistics,\\nthey can still lead to slightly different distributions\\nof head, torso, and tail. We now give details on\\nhow we decide the popularity of different types of\\nentities from each data source.\\n•IMDb (traffic): The number of votes (i.e.,\\nnumVotes ) the title(e.g., movie, short, TV\\nseries, etc.) has received; we do NOT consider\\nwhether the vote is high or low in the counting.\\nFor person entities, we use the total number\\nof votes received by the titles the person is\\nknown for.\\n•Goodreads (traffic): The count of ratings (i.e.,\\nratings_count ) the book has received;\\nsimilarly, we do NOT take into consideration\\nwhether the rating is high or low.\\n•MAG (traffic): The number of citations (i.e.,\\nCitationCount ) the entity (i.e., scholarly\\narticle, conference, or journal) has received.\\n•DBLP (density): The number of works the\\nscholar has authored.\\n•DBpedia (density): The number of relational\\ntriples in DBPedia that contain the entity.\\nWe bucketed head, torso, and tail entities in three\\nsteps. First, we sorted the entities by their popu-\\nlarity, measured as above. Second, for each entity,\\nwe computed the cumulative popularity score up to\\nthe top-1 entity in the sorted list. Third, we buck-\\neted the entities such that head entities comprise\\nentities whose cumulative popularity score is up\\nto 1/3 of that of all entities, torso entities comprise\\nentities with cumulative scores ranging from 1/3 to2/3, and tail entities from 2/3 to 1. We determined\\nthe partitioning separately for different entity types\\nfor each domain.\\nTo make the popularity score fair, we filtered\\nout entities that are likely too new to have suffi-\\ncient statistical data for popularity measurement.\\nFor IMDb, MAG, DBLP, and Goodreads, we kept\\nonly entities by the years 2020, 2020, 2020, and\\n2015, respectively. The cut-off years are all be-\\nfore the cut-off time of the LLM training data, so\\nthe benchmark avoids questions that require recent\\nknowledge. We did not perform similar filtering for\\nDBpedia because the year attribute is unavailable\\nor non-applicable for most entities, and our pilot\\nstudy shows that very few (if at all) of the questions\\ngenerated from DBpedia require knowledge after\\n2020.\\nTable 1 summarizes the distribution of head,\\ntorso, and tail entities. The distribution follows\\nthepower law , where very small percentages of\\nentities fall in the head and torso buckets, and the\\nmajority of entities fall in the tail bucket; for ex-\\nample, over 99.9% of movies fall in the tail bucket,\\naccording to IMDb vote counts. We also observe\\nthat this phenomenon is more pronounced when we\\nmeasure by traffic than by density; for the latter, the\\ntorso buckets are often larger ( ∼15% of entities),\\nand the tails are slightly smaller ( ∼82%).\\nQuestions. We generated questions using a\\ntemplate-based approach, where each generated\\nquestion asks for an attribute of an entity. We\\nfiltered out the following types of attributes: (i)\\nunspecific (e.g., seeAlso in DBpedia), (ii) dy-\\nnamic (e.g., lastLaunchRocket in DBpedia),\\n(iii) data source specific (e.g., averageRating\\nin IMDb), and (iv) non-textual (e.g., picture in\\nDBpedia). For each specific domain (Movie, Book,\\nAcademics), we manually designed the question\\ntemplate for each attribute. DBpedia contains a\\nlarge set of attributes, so we first employed Chat-\\nGPT to draft the templates (using Prompt 1 in Ap-\\npendix A.1), then proofread them manually and\\nmade necessary edits.\\nThe answer for each question is the object of\\nthe relevant triple; when there are multiple an- Domain Sources # Templates # Questions\\nMovie IMDb 13 3,093\\nBook Goodreads 4 3,000\\nAcademics MAG, DBLP 13 2,946\\nOpen DBpedia 393 9,132\\nTotal 423 18,171\\nTable 2: The overall statistics of Head-to-Tail .\\nswers (e.g., a book may have multiple authors),\\nwe included all in the answer. When necessary, we\\nincluded extra information for an entity to avoid\\npotential ambiguities (e.g., we included the publica-\\ntion year for a book to distinguish books of highly\\nsimilar names).\\nWe generated an equal number of questions for\\nrandomly sampled head, torso, and tail entities\\nusing each template. For each specific domain,\\nwe generated ∼1K questions for each of the head,\\ntorso, and tail buckets. As DBPedia contains more\\ndomains and relationship types, we generated ∼3K\\nquestions for each bucket. Table 2 summarizes the\\noverall statistics of Head-to-Tail in the number of\\nquestions and templates.\\n2.2 Metrics\\nMetrics. We find that oftentimes LLMs are intelli-\\ngent enough to admit that it does not have enough\\ninformation to answer a question. As such, we used\\nthree metrics: accuracy (A),hallucination rate (H),\\nandmissing rate (M), measuring the percentage of\\nquestions that an LLM gives the correct answer,\\ngives a wrong or partially incorrect answer, or ad-\\nmits it cannot answer, respectively; by definition,\\nA+H+M= 100% .\\nManually deciding the correctness of answers\\ncan be cumbersome. We next describe a few dif-\\nferent ways to automatically decide if an answer is\\ncorrect.\\nLLM-Based. We ask ChatGPT to check whether\\nan answer is correct given the question and ground\\ntruth (Prompt 2 in Appendix A.1). Thus, ac-\\ncuracy ALMis defined as the percentage of an-\\nswers that ChatGPT judges as correct; hallucina-\\ntion rate HLMis defined as the percentage of time\\nwhen (i) an attempted answer is not missing, and\\n(ii) ChatGPT judges the answer as incorrect (i.e.,\\nHLM= 100% −ALM−M).\\nTo understand the reliability of the LLM-based\\nmetrics, we randomly sampled 840answers from\\nthe evaluated LLMs and manually checked whether\\nhuman judgment agrees with the LLM-based met-rics. The agreement is 98%, which we view as\\nreliable. Hence, we use A LMand H LMas the pri-\\nmary metrics in this study.\\nRule-Based. In addition, we adopt popular met-\\nrics, including exact match (EM) ,token F1 (F1) ,\\nandROUGE-L (RL) (Lin, 2004; Rajpurkar et al.,\\n2016); in other words, we use rule-based methods\\nto judge the correctness of an answer. Specifically,\\nAEMis computed as the percentage of answers that\\nexactly match the ground truth; AF1is computed\\nas the average harmonic mean of precision and\\nrecall when comparing tokens in the returned an-\\nswers and in the ground truth answers; ARLis com-\\nputed as the average normalized longest common\\nsubsequence (LCS) between the returned answers\\nand the ground truths. For common answer types,\\nwe additionally expand the set of ground-truth an-\\nswers with their variants using hand-crafted rules\\n(e.g., “ W Shakespeare ” is a variant of “ William\\nShakespeare ”); when a given question has multi-\\nple expanded ground-truth answers, we take the\\nmaximum score.\\nCorrespondingly, we measure hallucination rate\\nbyHEM(= 100% −AEM−M),HF1(= 100% −\\nAF1−M), and HRL(= 100% −ARL−M). As\\nwe will show later in Section 3.5, we observe high\\ncorrelations between rule-based and LLM-based\\nmetrics.\\n2.3 Evaluation methodology\\nWe prompted the LLM as shown in Prompt 3 in\\nAppendix A.1. First, we asked LLMs to give as\\nconcise answers as possible. Second, we prompted\\nLLMs to respond “unsure” when the LLM is not\\nconfident in the answer. We applied few-shot learn-\\ning and included in the prompt two examples that\\nare not in Head-to-Tail : one is a simple, answer-\\nable question with the corresponding answer as the\\nresponse; the other is an unanswerable question\\nwith “unsure” as the response.\\nWith this prompt, rule-based metrics are more\\nlikely to reflect the factual correctness of the an-\\nswers, and we can simply compute the missing rate\\n(i.e., M) by counting “unsure” or empty answers.\\nWe observed that explicitly asking for “unsure” as\\nan answer could significantly reduce hallucination\\nrate (Section 3.5).\\nTo summarize, the following three setups in\\nthe benchmark and evaluation methodology help\\nus best approximate the existence of (confident)\\nknowledge in the LLMs: (i) focusing on simple questions in easy-to-understand forms, (ii) asking\\nfor concise answers to ease evaluation, and (iii)\\nhinting the LLMs to answer “unsure” to suppress\\nunnecessary hallucinations.\\n3 Experimental Analysis\\n3.1 Implementation and configurations details\\nWe evaluated representative state-of-the-art LLMs\\nof various sizes and architectures, including Chat-\\nGPT, LLaMA (7B, 13B, 33B, 65B) (Touvron et al.,\\n2023a), Vicuna (7B, 13B) (Chiang et al., 2023),\\nFlan-T5 (3B, 11B) (Chung et al., 2022), RWKV\\n(7B) (Peng et al., 2023b), Falcon (7B, 40B), and\\nFalcon-Instruct (7B, 40B) (Almazrouei et al., 2023).\\nWe employed the most deterministic settings (i.e.,\\ntemperature=0 ortop_k=1 ) for all models.\\nWe interacted with ChatGPT through Ope-\\nnAI API5. The employed ChatGPT version is\\ngpt-3.5-turbo-0301 . We used Transform-\\ners (Wolf et al., 2020) to interact with the other\\nLLMs on A100 (80GB) GPUs, and we used 16-\\nbit floating point formats (i.e., float16 for Flan-T5\\nand RWKV , bfloat16 for LLaMA, Vicuna, Falcon,\\nand Falcon-Instruct). We employed the original\\nLLaMA, Flan-T5, Falcon, and Falcon-Instruct ver-\\nsions. The employed version of RWKV and Vicuna\\nisv4 Raven andv1.1 , respectively.\\nTable 11 in Appendix A.3 gives detailed results\\nof all LLMs. We note that our goal is NOT to com-\\npare different LLM models; rather, by examining\\nthe metrics by different LLMs, we make sure to re-\\nport the common patterns among the representative\\nLLMs.\\n3.2 RQ1: How reliable are LLMs in\\nanswering factual questions?\\nWe present in Table 3 the overall performance of\\nChatGPT and LLaMA-33B, which perform the\\nbest in most metrics on Head-to-Tail among all\\nLLMs introduced in Section 3.1. Both models cor-\\nrectly answer only ∼20% of questions (measured\\nby A LM).\\nInterestingly, for questions that are not answered\\ncorrectly, ChatGPT and LLaMA-33B show op-\\nposite patterns: ChatGPT gives unsure or empty\\nanswers for the majority of them, and the hallu-\\ncination rate is only ∼15% (still non-negligible),\\nwhile LLaMA-33B mostly provides hallucinated\\nanswers, resulting with high hallucination rate\\n5https://platform.openai.com/docs/\\napi-reference(∼80%). We suspect fine-tuning and reinforcement\\nlearning of these models may explain the different\\npatterns when the model is unsure of the answers.\\nFigure 1 shows examples of counterfactual answers\\ngiven by ChatGPT.\\nFinally, for both models, the overall performance\\nis mostly similar to the performance on the open\\ndomain (recall that the open domain accounts for\\nonly half of the question-answer pairs), but the\\nperformance varies substantially across different\\nspecific domains. Both models perform the best\\nin the Movie domain and worst in the Academics\\ndomain, likely due to the relatively low popularity\\nof the Academics domain, as we will discuss soon.\\n3.3 RQ2: Do LLMs perform equally well on\\nhead, torso, and tail facts?\\nThe overall accuracy of ChatGPT and LLaMA-33B\\n(ALM) declines in the order of head, torso, and tail\\nentities, as shown in Figure 1 and Table 4. We\\nobserve the same pattern for other LLMs. This\\nverifies our hypothesis that as we lack training data\\nfor long-tail entities, it is difficult for LLMs to\\nobtain knowledge for such entities.\\nSurprisingly, even on the head entities, which\\ncomprise only <3% top-popular entities, the QA\\naccuracy is still low, and the LLMs barely an-\\nswer half of the questions correctly in their most\\nskilled domain (i.e., ChatGPT achieves an A LM\\nof51% in the Movie domain on the head bucket).\\nWe further retain top-10% popular questions from\\nthe head bucket. As shown in Table 5, ChatGPT\\nand LLaMA-33B obtained slightly higher accuracy\\n(within 10 percent point) and lower hallucination\\nrate for these super popular entities, but the accu-\\nracy is still disappointingly low: 35% for ChatGPT\\nand 30% for LLaMA-33B.\\nThe QA accuracy on tail entities is significantly\\nlower, with single-digit numbers in most of the\\ndomains. Notably, Academics intuitively is a long-\\ntail domain, and we observe single-digit overall\\naccuracy and very low accuracy (6% for ChatGPT\\nand 11% for LLaMA) even for head entities in this\\ndomain.\\nFinally, we do not necessarily observe mono-\\ntonicity for hallucination rate. As an example, in\\ntheMovie andBook domains, ChatGPT has higher\\nhallucination rate on torso entities than on head\\nand tail entities, showing that it may know better\\nabout what it does not know on head than on torso\\nentities. ModelAll Open Movie Book Academics\\nALM HLM ALM HLM ALM HLM ALM HLM ALM HLM\\nChatGPT 20.3 14.1 22.1 14.8 34.7 13.3 16.9 24.9 3.0 1.9\\nLLaMA (33B) 18.2 80.0 19.0 79.1 28.7 70.1 15.8 82.9 7.1 90.3\\nTable 3: Overall accuracy of the best two LLMs is only ∼20% on Head-to-Tail . All numbers are in percentage (%).\\nDomainHead Torso Tail\\nALM HLM ALM HLM ALM HLM\\nMovie 51.3 11.5 46.4 16.6 6.4 11.8\\nBook 20.1 26.3 22.5 29.2 8.0 19.2\\nAcademics 5.9 3.0 2.3 1.7 0.8 0.9\\nOpen 32.7 20.8 19.7 13.3 13.8 10.2\\nAll 29.4 17.2 21.9 14.6 9.5 10.5\\n(a) ChatGPT.\\nDomainHead Torso Tail\\nALM HLM ALM HLM ALM HLM\\nMovie 42.9 57.0 36.0 63.8 7.3 89.4\\nBook 24.2 75.8 19.2 80.5 4.1 92.5\\nAcademics 10.8 87.5 7.9 88.4 2.6 95.1\\nOpen 25.8 72.3 18.4 79.9 12.8 84.9\\nAll 26.0 72.8 19.8 78.7 8.8 88.6\\n(b) LLaMA-33B.\\nTable 4: LLMs’ factuality, measured by A LM(%), de-\\ncreases in the order of head, torso, and tail entities from\\nHead-to-Tail .\\nModel A LM HLM M\\nMovie∗ 55.2 (↑3.9) 8.3 ( ↓3.2) 36.5 ( ↓0.6)\\nBook∗ 21.0 (↑0.9) 21.0 ( ↓5.3) 58.0 ( ↑4.4)\\nAcademics ∗ 11.6 (↑5.7) 6.3 ( ↑3.3) 82.1 ( ↓9.0)\\nOpen 41.2 ( ↑8.5) 21.2 ( ↑0.4) 37.6 ( ↓8.9)\\nAll 34.8 ( ↑5.4) 16.3 ( ↓0.9) 48.9 ( ↓4.5)\\n(a) ChatGPT.\\nModel A LM HLM M\\nMovie∗ 46.9 (↑4.0) 53.1 ( ↓3.9) 0.0 ( ↓0.1)\\nBook∗ 24.0 (↓0.2) 76.0 ( ↑0.2) 0.0 ( =0.0)\\nAcademics ∗ 13.7 (↑2.9) 84.2 ( ↓3.3) 2.1 ( ↑0.4)\\nOpen 31.0 ( ↑5.2) 66.3 ( ↓6.0) 2.7 ( ↑0.8)\\nAll 29.5 ( ↑3.5) 68.9 (( ↓3.9) 1.6 ( ↑0.4)\\n(b) LLaMA-33B.\\nTable 5: Accuracy on the top- 10% popular questions in\\nthe head bucket is only slightly better than overall head\\nentities. ( ↑/↓: increased/decreased %compared with\\nusing all head instances; ∗: should be interpreted with\\ncaution due to small number ( ∼100) of instances.)\\nHead-to-Tail predicates. We investigated whether\\nthe performance still correlates with the head-to-\\ntail order regarding the popularity of predicates\\ninstead of entities. We sorted the predicates from\\nDBpedia by popularity (measured by the number\\nof relational triples with the predicate) and parti-ModelHead & Torso Tail\\nALM HLM ALM HLM\\nChatGPT 18.6 14.2 22.3 14.8\\nLLaMA (7B) 15.3 83.5 13.5 77.7\\nLLaMA (13B) 14.6 85.1 14.7 83.6\\nLLaMA (33B) 18.2 81.4 19.0 78.9\\nLLaMA (65B) 20.1 79.7 18.3 81.4\\nVicuna (7B) 12.5 82.0 9.3 77.4\\nVicuna (13B) 13.0 70.3 8.6 55.5\\nFlan-T5 (3B) 4.4 13.0 3.4 10.4\\nFlan-T5 (11B) 9.2 11.1 5.0 8.1\\nRWKV (7B) 6.9 28.7 6.4 29.7\\nFalcon (7B) 11.3 51.0 8.1 43.7\\nFalcon (40B) 14.4 34.1 8.5 29.2\\nFalcon-Instruct (7B) 8.8 48.3 7.2 47.1\\nFalcon-Instruct (40B) 12.8 15.3 7.9 15.2\\nTable 6: Comparison of LLMs’ factuality about head,\\ntorso, and tail predicates in A LM(%) and H LM(%) using\\nopen-domain instances from Head-to-Tail .\\ntioned the sorted predicates into head, torso, and\\ntail in a similar fashion. We then re-partitioned\\nthe open-domain questions into head, torso, and\\ntail predicate buckets, each containing 72,450, and\\n8,610questions, respectively. Since the number of\\nquestions in the head bucket is low, we merged the\\nhead and torso buckets.\\nTable 6 compares the performance on head &\\ntorso vs. on tail. We observe no consistent cor-\\nrelation among different LLMs between the per-\\nformance and the head-to-tail predicate ordering,\\nand the differences in accuracy are not very high.\\nThis is not too surprising for two reasons. First, the\\nsemantics of each predicate is mostly consistent\\nwith the semantics of the predicate names, which\\ncan be well understood by LLMs. Second, when\\nfacts are present for tail predicates, they are often\\nabout the head entities, and factual information for\\nhead entities is likely to be more abundant in the\\ntraining data.\\n3.4 RQ3: Does normal methods that improve\\nLLMs increase the factuality?\\nTable 7 compares LLMs in different sizes and with\\nor without instruction tuning. First, we observe\\nthat an increased model size does not automatically\\ntranslate to a better grasp of factual knowledge.\\nFor example, LLaMA-33B modestly outperforms\\nLLaMA-65B across the head, torso, and tail sub- ModelHead-to-Tail Head Torso Tail\\nALM HLM M A LM HLM ALM HLM ALM HLM\\nLLaMA (7B) 12.1 80.0 7.9 19.0 74.4 11.7 81.0 5.4 84.8\\nLLaMA (13B) 14.4 84.3 1.3 22.0 77.2 14.8 83.8 6.3 91.9\\nLLaMA (33B) 18.2 80.0 1.8 26.0 72.8 19.8 78.7 8.8 88.6\\nLLaMA (65B) 17.8 81.9 0.3 25.9 73.8 18.7 81.0 8.7 90.9\\nVicuna (7B) 10.1 79.2 10.8 16.2 72.7 9.6 79.8 4.3 85.0\\nVicuna (13B) 9.2 62.6 28.2 14.0 55.0 8.8 62.8 4.7 70.0\\nFlan-T5 (3B) 2.3 17.4 80.3 3.9 19.7 1.5 17.1 1.3 15.5\\nFlan-T5 (11B) 4.2 20.0 75.7 7.6 23.7 3.2 19.9 2.0 16.5\\nFalcon (7B) 9.5 57.9 32.6 14.5 53.8 9.2 57.9 4.8 62.0\\nFalcon (40B) 10.8 41.0 48.2 16.2 36.4 11.2 40.0 4.9 46.6\\nFalcon-Instruct (7B) 6.8 56.7 36.5 11.5 56.0 5.6 57.2 3.4 56.7\\nFalcon-Instruct (40B) 10.8 32.2 57.0 16.7 30.5 11.5 31.1 4.3 34.8\\nTable 7: Comparison of different LLMs with different sizes. All numbers are in percentage (%).\\nLLM-Based Rule-Basedρ r\\nMin. Mean Min. Mean\\nAEM 0.723 0.911 0.835 0.945\\nALM AF1 0.827 0.955 0.770 0.966\\nARL 0.766 0.948 0.753 0.964\\nHEM 0.978 0.993 0.995 0.999\\nHLM HF1 0.978 0.995 0.998 0.999\\nHRL 0.978 0.995 0.998 0.999\\nTable 8: The minimum and mean Spearman’s rank cor-\\nrelation coefficients ( ρ) and Pearson correlation coef-\\nficients ( r) show high correlation between LM- and\\nrule-based metrics.\\nsets ( +0.4%in A LMand−1.9%in H LMon av-\\nerage) while they share the same training dataset\\nand hyperparameters. This provides additional ev-\\nidence for our hypothesis that once the model is\\nsufficiently large, the abundance of training data\\nplays a more critical role in the factuality of the\\nLLMs.\\nSecond, compared with LLaMA and Falcon,\\nthe instruction-tuned counterparts (i.e., Vicuna\\nand Falcon-Instruct) have lower accuracy, as they\\nlearned to be more conservative in providing fac-\\ntual answers and thus generate “unsure” more of-\\nten (e.g., Vicuna-13B is 26.9%higher in M than\\nLLaMA-13B). Despite so, they still have high hal-\\nlucination rate.\\n3.5 Robustness of our evaluation methodology\\nFinally, we evaluate the robustness of our evalua-\\ntion methodology.\\nCorrelations between rule- and LLM-based met-\\nrics. For each combination of popularity (head,\\ntorso, tail) and domain (movie, book, academics,\\nopen), we calculate Spearman’s rank and Pearson\\ncorrelation coefficients between rule- and LLM-\\nbased metrics over all LLMs. We report the ag-DomainFew-shot Zero-shot In-domain\\nALM HLM ALM HLM ALM HLMHeadOpen 32.7 20.8 32.6 24.7 45.0 27.8\\nAll 29.4 17.2 29.2 18.6 38.3 24.7TorsoOpen 19.7 13.3 21.6 17.9 30.1 23.0\\nAll 21.9 14.6 22.8 16.7 29.8 22.8TailOpen 13.8 10.2 14.9 14.5 23.0 19.5\\nAll 9.5 10.5 10.3 12.7 15.4 20.2\\nTable 9: Performance of ChatGPT with different\\nprompts on Head-to-Tail . All numbers are in percent-\\nage (%).\\ngregated results (minimum, mean) in Table 8. The\\ncorrelation scores suggest that A LM(resp. H LM)\\nstrongly correlates with A EM, AF1, and A RL(resp.\\nHEM, HF1, and H RL), indicating that rule-based met-\\nrics are good alternatives for lower-cost or faster\\nevaluation.\\nEffect of brief and “unsure”. We randomly sam-\\npled 1.2K questions and tested the stability of an-\\nswers if we call ChatGPT to regenerate answers.\\nWhen not requiring brief or “unsure” answers, for\\n18% of questions, ChatGPT regenerated different\\nanswers. Adding the requirement for brief answers\\n(Prompt 6 in Appendix A.1) reduced the percent-\\nage to 4%, and further asking “unsure” answers\\nwith few-shot examples (Prompt 3) reduced the\\npercentage to 1%. In addition, according to manual\\nevaluation on 150randomly sampled questions, re-\\nmoving “unsure” as an option increases ChatGPT’s\\nhallucination rate by 13percentage points.\\nRobustness of prompts. We explore two other\\nprompts. Compared with the original prompt\\nthat conducts few-shot learning (Section 3.1), de-\\nnoted as Few-shot , the Zero-shot prompt does not\\nprovide examples and thus is zero-shot learning (Prompt 4 in Appendix A.1), and the In-domain\\nprompt has the answerable example swapped out\\nfor an in-domain example generated by the same\\nquestion template as the target question (Prompt 5\\nin Appendix A.1).\\nAs shown in Table 9, Few-shot andZero-shot\\nshow very similar results, but performance differ-\\nences are noticeable between Few-shot andIn-\\ndomain . In particular, in-domain examples help\\nget more correct answers ( +8.9%,+7.9%,+5.9%\\nin A LMfor head, torso, tail) but at the cost of more\\nhallucinations ( +7.5%,+8.2%,+9.7%in H LMfor\\nhead, torso, tail). We suspect that the in-domain\\nexamples boost the confidence of ChatGPT in an-\\nswering a question, so it answers questions even\\nwhen the real confidence is not that high, causing\\nboth higher accuracy and higher hallucination rate.\\nDespite the fluctuation, our original prompt tem-\\nplate ( Few-shot ) appears to be better at approximat-\\ning the (confident) factuality of LLMs with the QA\\naccuracy, and the relative performance among the\\nhead, torso, and tail remains stable over different\\nprompts.\\n4 Discussions\\n4.1 The future of knowledge graphs\\nThe experimental analysis indicates that although\\nLLMs have incorporated factual knowledge within\\ntheir parameters, the amount of this encoded knowl-\\nedge remains limited. Knowledge of long-tail en-\\ntities is already sparse in KGs and is even more\\ndeficient in LLMs.\\nNevertheless, LLMs have been revolutionizing\\nthe way people seek information and calling for\\nreconsideration of the best representation of factual\\nknowledge. We term the forthcoming generation\\nof KGs as Dual Neural KGs : knowledge can reside\\nexplicitly as triples (similar to KGs) and implicitly\\nas embeddings (like in LLMs); the symbolic form\\ncaters to human understanding and explainability,\\nwhile the neural form benefits machine compre-\\nhension and seamless conversations. A piece of\\nknowledge can exist in both formats or in the one\\nthat is more appropriate. The harmonious blend of\\nthe two forms, capitalizing on the latest LLM inno-\\nvations, is an exciting research area as we elaborate\\nnext.\\nHead knowledge. This involves popular entities\\nwhere training data are ample. Ideally, LLMs could\\nbe taught such knowledge for efficient retrieval,\\nmeaning head knowledge shall exist in both forms.Currently, LLMs still have a low QA accuracy\\nfor popular entities (see Table 5), so a critical re-\\nsearch area is to infuse head knowledge into LLMs\\nthrough model training or fine-tuning. Early work\\nin this line includes knowledge infusion (Liu et al.,\\n2021; Wang et al., 2021; Zhen et al., 2022).\\nTorso-to-tail and recent knowledge. This in-\\nvolves non-popular entities and emerging knowl-\\nedge, where training data are typically sparse or\\nabsent. This type of knowledge might be best repre-\\nsented as triples. Serving such knowledge requires\\neffectively deciding when external knowledge is\\nessential, efficiently retrieving the relevant knowl-\\nedge, and seamlessly integrating it into the answers.\\nEarly attempts in this direction involve knowledge-\\naugmented LLMs (Asai et al., 2023; Nakano et al.,\\n2022; Shi et al., 2023; Borgeaud et al., 2022).\\n4.2 Limitations and extensions\\nTaxonomy. Our work does not discuss the effec-\\ntiveness of LLMs in capturing taxonomy or type hi-\\nerarchies, which could be an extension of this study.\\nSpecifically, we hypothesize that LLMs can effec-\\ntively incorporate type relationships (e.g., hyper-\\nnyms and synonyms), even for the fine-granularity\\nsub-types. Hence, it may no longer be worth manu-\\nally constructing a very deep and complex hierar-\\nchy in the future.\\nMore recent LLMs. Although we performed the\\nstudy using various representative models, we can-\\nnot exhaustively benchmark every recent model in\\nthis fast-moving field. Notably, we did not employ\\nGPT-4 (OpenAI, 2023) and Llama 2 (Touvron et al.,\\n2023b), which became publicly accessible after we\\nfinished the first version of this paper.\\n5 Related Work\\nBenchmarks. Most works studied the factuality of\\nLLMs using existing QA benchmarks such as We-\\nbQuestions (Berant et al., 2013), TriviaQA (Joshi\\net al., 2017), LC-QuAD (Trivedi et al., 2017; Dubey\\net al., 2019), QALD-9 (Usbeck et al., 2018), Natu-\\nral Questions (Kwiatkowski et al., 2019), and Enti-\\ntyQuestions (Sciavolino et al., 2021). A recent line\\nof work has been constructing new QA benchmarks\\nto assess LLMs’ factuality, especially for long-tail\\nknowledge (Mallen et al., 2023; Kim et al., 2023).\\nCompared with these benchmarks, Head-to-Tail\\nis the first to specifically assess how well LLMs in-\\ncorporate head, torso, and tail factual information. LLM Evaluation. Recent years have seen a pro-\\nliferation of research on assessing the factuality of\\nLLMs (Roberts et al., 2020; Petroni et al., 2021;\\nShuster et al., 2021; Mielke et al., 2022; Tan et al.,\\n2023; Hu et al., 2023; Peng et al., 2023a; Omar\\net al., 2023; Kandpal et al., 2023; Mallen et al.,\\n2023). Most of these works focus on a single\\nknowledge source, such as Freebase or Wikipedia,\\nand they have yet to systematically perform the\\nevaluation explicitly regarding head/torso/tail enti-\\nties or attributes. One work close to ours is Omar\\net al. (2023), which evaluated ChatGPT using facts\\ncollected from diverse knowledge sources; how-\\never, their evaluation was carried out manually on\\nonly 450QA instances.\\nThere are three works that also showed the corre-\\nlation between the QA accuracy of language mod-\\nels and fact popularity (Mallen et al., 2023; Kand-\\npal et al., 2023; Kim et al., 2023). Our work, con-\\nducted in parallel, focuses on a different angle—\\nhow knowledgeable are LLMs? For this purpose,\\nwe systematically designed experimental method-\\nology, including the definition of head, torso, and\\ntail entities, the design of metrics, and the evalua-\\ntion method. Our benchmark is comprehensive in\\ncontaining different knowledge sources, different\\ndomains, and rich relations. Compared with these\\nthree works, we gave more quantified answers for\\nresearch questions RQ1–RQ3.\\n6 Conclusion\\nWe introduce Head-to-Tail , the first benchmark\\ndesigned to assess the ability of LLMs to inter-\\nnalize head, torso, and tail facts. Alongside the\\ndataset, we present a new evaluation methodology\\nwith appropriate metrics for automatically evaluat-\\ning LLMs’ factuality. Our evaluation shows that\\neven the most advanced LLMs have notable limi-\\ntations in representing factual knowledge, partic-\\nularly for the torso and tail entities. Accordingly,\\nwe suggest new research areas to seamlessly blend\\nknowledge in the symbolic form and neural form.\\nReferences\\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-\\nshamsi, Alessandro Cappelli, Ruxandra Cojocaru,\\nMerouane Debbah, Etienne Goffinet, Daniel Hes-\\nlow, Julien Launay, Quentin Malartic, Badreddine\\nNoune, Baptiste Pannier, and Guilherme Penedo.\\n2023. Falcon-40B: an open large language model\\nwith state-of-the-art performance.Akari Asai, Sewon Min, Zexuan Zhong, and Danqi\\nChen. 2023. Retrieval-based language models and\\napplications. In Proceedings of the 61st Annual Meet-\\ning of the Association for Computational Linguistics\\n(Volume 6: Tutorial Abstracts) , pages 41–46, Toronto,\\nCanada. Association for Computational Linguistics.\\nSören Auer, Christian Bizer, Georgi Kobilarov, Jens\\nLehmann, Richard Cyganiak, and Zachary Ives. 2007.\\nDbpedia: A nucleus for a web of open data. In The\\nSemantic Web , pages 722–735, Berlin, Heidelberg.\\nSpringer Berlin Heidelberg.\\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\\nJi, Tiezheng Yu, Willy Chung, Quyet V . Do, Yan\\nXu, and Pascale Fung. 2023. A multitask, multilin-\\ngual, multimodal evaluation of chatgpt on reasoning,\\nhallucination, and interactivity.\\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\\nLiang. 2013. Semantic parsing on Freebase from\\nquestion-answer pairs. In Proceedings of the 2013\\nConference on Empirical Methods in Natural Lan-\\nguage Processing , pages 1533–1544, Seattle, Wash-\\nington, USA. Association for Computational Linguis-\\ntics.\\nSebastian Borgeaud, Arthur Mensch, and etc. Jor-\\ndan Hoffmann †. 2022. Improving language models\\nby retrieving from trillions of tokens. arXiv .\\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\\nsource chatbot impressing gpt-4 with 90%* chatgpt\\nquality.\\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\\nbert Webson, Shixiang Shane Gu, Zhuyun Dai,\\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\\nery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,\\nDasha Valter, Sharan Narang, Gaurav Mishra, Adams\\nYu, Vincent Zhao, Yanping Huang, Andrew Dai,\\nHongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-\\ncob Devlin, Adam Roberts, Denny Zhou, Quoc V . Le,\\nand Jason Wei. 2022. Scaling instruction-finetuned\\nlanguage models.\\nLudovic Denoyer and Patrick Gallinari. 2006. The\\nWikipedia XML corpus. SIGIR Forum , 40(1):64–\\n69.\\nMohnish Dubey, Debayan Banerjee, Abdelrahman Ab-\\ndelkawi, and Jens Lehmann. 2019. Lc-quad 2.0: A\\nlarge dataset for complex question answering over\\nwikidata and dbpedia. In The Semantic Web – ISWC\\n2019: 18th International Semantic Web Conference,\\nAuckland, New Zealand, October 26–30, 2019, Pro-\\nceedings, Part II , page 69–78, Berlin, Heidelberg.\\nSpringer-Verlag. Nan Hu, Yike Wu, Guilin Qi, Dehai Min, Jiaoyan Chen,\\nJeff Z Pan, and Zafar Ali. 2023. An empirical study\\nof pre-trained language models in simple knowledge\\ngraph question answering. World Wide Web , pages\\n1–32.\\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\\nZettlemoyer. 2017. TriviaQA: A large scale distantly\\nsupervised challenge dataset for reading comprehen-\\nsion. In Proceedings of the 55th Annual Meeting of\\nthe Association for Computational Linguistics (Vol-\\nume 1: Long Papers) , pages 1601–1611, Vancouver,\\nCanada. Association for Computational Linguistics.\\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric\\nWallace, and Colin Raffel. 2023. Large language\\nmodels struggle to learn long-tail knowledge. In\\nProceedings of the 40th International Conference\\non Machine Learning , volume 202 of Proceedings\\nof Machine Learning Research , pages 15696–15707.\\nPMLR.\\nYoungmin Kim, Rohan Kumar, Sunitha Ravi, Haitian\\nSun, Christos Faloutsos, Ruslan Salakhutdinov, and\\nMinji Yoon. 2023. Automatic question-answer gen-\\neration for long-tail knowledge. In Second Workshop\\non Knowledge Augmented Methods for Natural Lan-\\nguage Processing (KDD-KnowledgeNLP) .\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\\nton Lee, Kristina Toutanova, Llion Jones, Matthew\\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\\nral questions: A benchmark for question answering\\nresearch. Transactions of the Association for Compu-\\ntational Linguistics , 7:452–466.\\nChin-Yew Lin. 2004. ROUGE: A package for auto-\\nmatic evaluation of summaries. In Text Summariza-\\ntion Branches Out , pages 74–81, Barcelona, Spain.\\nAssociation for Computational Linguistics.\\nYe Liu, Yao Wan, Lifang He, Hao Peng, and Philip S. Yu.\\n2021. Kg-bart: Knowledge graph-augmented bart\\nfor generative commonsense reasoning. In AAAI .\\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\\nDaniel Khashabi, and Hannaneh Hajishirzi. 2023.\\nWhen not to trust language models: Investigating\\neffectiveness of parametric and non-parametric mem-\\nories. In Proceedings of the 61st Annual Meeting of\\nthe Association for Computational Linguistics (Vol-\\nume 1: Long Papers) , pages 9802–9822, Toronto,\\nCanada. Association for Computational Linguistics.\\nSabrina J. Mielke, Arthur Szlam, Emily Dinan, and Y-\\nLan Boureau. 2022. Reducing conversational agents’\\noverconfidence through linguistic calibration. Trans-\\nactions of the Association for Computational Linguis-\\ntics, 10:857–872.\\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\\nLong Ouyang, Christina Kim, Christopher Hesse,Shantanu Jain, Vineet Kosaraju, William Saunders,\\nXu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen\\nKrueger, Kevin Button, Matthew Knight, Benjamin\\nChess, and John Schulman. 2022. Webgpt: Browser-\\nassisted question-answering with human feedback.\\narXiv .\\nReham Omar, Omij Mangukiya, Panos Kalnis, and Es-\\nsam Mansour. 2023. Chatgpt versus traditional ques-\\ntion answering for knowledge graphs: Current status\\nand future directions towards knowledge graph chat-\\nbots.\\nOpenAI. 2023. Gpt-4 technical report.\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\\n2022. Training language models to follow instruc-\\ntions with human feedback. Advances in Neural\\nInformation Processing Systems , 35:27730–27744.\\nBaolin Peng, Michel Galley, Pengcheng He, Hao Cheng,\\nYujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou\\nYu, Weizhu Chen, and Jianfeng Gao. 2023a. Check\\nyour facts and try again: Improving large language\\nmodels with external knowledge and automated feed-\\nback.\\nBo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak,\\nSamuel Arcadinho, Huanqi Cao, Xin Cheng, Michael\\nChung, Matteo Grella, Kranthi Kiran GV , Xuzheng\\nHe, Haowen Hou, Przemyslaw Kazienko, Jan Kocon,\\nJiaming Kong, Bartlomiej Koptyra, Hayden Lau, Kr-\\nishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito,\\nXiangru Tang, Bolun Wang, Johan S. Wind, Stansi-\\nlaw Wozniak, Ruichong Zhang, Zhenyuan Zhang,\\nQihang Zhao, Peng Zhou, Jian Zhu, and Rui-Jie Zhu.\\n2023b. RWKV: Reinventing rnns for the transformer\\nera.\\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\\nVassilis Plachouras, Tim Rocktäschel, and Sebastian\\nRiedel. 2021. KILT: a benchmark for knowledge\\nintensive language tasks. In Proceedings of the 2021\\nConference of the North American Chapter of the\\nAssociation for Computational Linguistics: Human\\nLanguage Technologies , pages 2523–2544, Online.\\nAssociation for Computational Linguistics.\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\\nPercy Liang. 2016. SQuAD: 100,000+ questions for\\nmachine comprehension of text. In Proceedings of\\nthe 2016 Conference on Empirical Methods in Natu-\\nral Language Processing , pages 2383–2392, Austin,\\nTexas. Association for Computational Linguistics.\\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\\nHow much knowledge can you pack into the param-\\neters of a language model? In Proceedings of the\\n2020 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP) , pages 5418–5426,\\nOnline. Association for Computational Linguistics. Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee,\\nand Danqi Chen. 2021. Simple entity-centric ques-\\ntions challenge dense retrievers. In Proceedings of\\nthe 2021 Conference on Empirical Methods in Natu-\\nral Language Processing , pages 6138–6148, Online\\nand Punta Cana, Dominican Republic. Association\\nfor Computational Linguistics.\\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon\\nSeo, Rich James, Mike Lewis, Luke Zettlemoyer, and\\nWen tau Yih. 2023. Replug: Retrieval-augmented\\nblack-box language models. arXiv .\\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\\nand Jason Weston. 2021. Retrieval augmentation\\nreduces hallucination in conversation. In Findings\\nof the Association for Computational Linguistics:\\nEMNLP 2021 , pages 3784–3803, Punta Cana, Do-\\nminican Republic. Association for Computational\\nLinguistics.\\nArnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Dar-\\nrin Eide, Bo-June (Paul) Hsu, and Kuansan Wang.\\n2015. An overview of microsoft academic service\\n(mas) and applications. In Proceedings of the 24th\\nInternational Conference on World Wide Web , WWW\\n’15 Companion, page 243–246, New York, NY , USA.\\nAssociation for Computing Machinery.\\nYiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu,\\nYongrui Chen, and Guilin Qi. 2023. Evaluation of\\nchatgpt as a question answering system for answering\\ncomplex questions.\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\\nGrave, and Guillaume Lample. 2023a. Llama: Open\\nand efficient foundation language models.\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\\nScialom. 2023b. Llama 2: Open foundation and\\nfine-tuned chat models.Priyansh Trivedi, Gaurav Maheshwari, Mohnish Dubey,\\nand Jens Lehmann. 2017. Lc-quad: A corpus for\\ncomplex question answering over knowledge graphs.\\nInThe Semantic Web – ISWC 2017 , pages 210–218,\\nCham. Springer International Publishing.\\nRicardo Usbeck, Ria Hari Gusmita, Axel-\\nCyrille Ngonga Ngomo, and Muhammad Saleem.\\n2018. 9th challenge on question answering over\\nlinked data (QALD-9). In Joint proceedings of\\nthe 4th Workshop on Semantic Deep Learning\\n(SemDeep-4) and NLIWoD4: Natural Language\\nInterfaces for the Web of Data (NLIWOD-4) and 9th\\nQuestion Answering over Linked Data challenge\\n(QALD-9) co-located with 17th International\\nSemantic Web Conference (ISWC 2018), Monterey,\\nCalifornia, United States of America, October 8th -\\n9th, 2018. , pages 58–64.\\nMengting Wan and Julian McAuley. 2018. Item recom-\\nmendation on monotonic behavior chains. In Pro-\\nceedings of the 12th ACM Conference on Recom-\\nmender Systems , RecSys ’18, page 86–94, New York,\\nNY , USA. Association for Computing Machinery.\\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei,\\nXuanjing Huang, Jianshu Ji, Guihong Cao, Daxin\\nJiang, and Ming Zhou. 2021. K-adapter: Infusing\\nknowledge into pre-trained models with adapters. In\\nACL.\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\\nChaumond, Clement Delangue, Anthony Moi, Pier-\\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\\nformers: State-of-the-art natural language processing.\\nInProceedings of the 2020 Conference on Empirical\\nMethods in Natural Language Processing: System\\nDemonstrations , pages 38–45, Online. Association\\nfor Computational Linguistics.\\nChaoqi Zhen, Yanlei Shang, Xiangyu Liu, Yifei Li,\\nYong Chen, and Dell Zhang. 2022. A survey on\\nknowledge-enhanced pre-trained language models. A Appendix\\nA.1 List of Prompts\\nYou are given a few samples of a relation in the format of <X, relation, Y>.\\nYou need to write a question *template* about the relation, which can be\\nused to generate questions. The template needs to have one blank such that\\na question about Y can be generated by filling the blank with X.\\n#Example 1\\nSamples: <!Hero, musicBy, Eddie DeGarmo>, <9 to 5 (musical), musicBy,\\nDolly Parton>, <All About Us (musical), musicBy, John Kander>\\nTemplate: The music of _ is by whom?\\n#Example 2\\nSamples: <10,000 Maniacs, bandMember, Dennis Drew>, <16bit (band),\\nbandMember, Eddie Jefferys>, <1TYM, bandMember, Teddy Park>\\nTemplate: Name a band member of _?\\n#Example 3\\nSamples: {SAMPLES}\\nTemplate:\\nPrompt 1: Question template drafting.\\nYou need to check whether the prediction of a question-answering system\\nto a question is correct. You should make the judgment based on a list of\\nground truth answers provided to you. Your response should be \"correct\"\\nif the prediction is correct or \"incorrect\" if the prediction is wrong.\\nQuestion: Who authored The Taming of the Shrew (published in 2002)?\\nGround truth: [\"William Shakespeare\", \"Roma Gill\"]\\nPrediction: W Shakespeare\\nCorrectness: correct\\nQuestion: Who authored The Taming of the Shrew (published in 2002)?\\nGround truth: [\"William Shakespeare\", \"Roma Gill\"]\\nPrediction: Roma Gill and W Shakespeare\\nCorrectness: correct\\nQuestion: Who authored The Taming of the Shrew (published in 2002)?\\nGround truth: [\"William Shakespeare\", \"Roma Gill\"]\\nPrediction: Roma Shakespeare\\nCorrectness: incorrect\\nQuestion: What country is Maharashtra Metro Rail Corporation Limited\\nlocated in?\\nGround truth: [\"India\"]\\nPrediction: Maharashtra\\nCorrectness: incorrect\\nQuestion: What’s the job of Song Kang-ho in Parasite (2019)?\\nGround truth: [\"actor\"]\\nPrediction: He plays the role of Kim Ki-taek, the patriarch of the Kim\\nfamily.\\nCorrectness: correct\\nQuestion: Which era did Michael Oakeshott belong to?\\nGround truth: [\"20th-century philosophy\"]\\nPrediction: 20th century.\\nCorrectness: correct\\nQuestion: Edward Tise (known for Full Metal Jacket (1987)) is in what\\ndepartment?\\nGround truth: [\"sound department\"]\\nPrediction: 2nd Infantry Division, United States Army\\nCorrectness: incorrect\\nQuestion: What wine region is Finger Lakes A V A a part of?\\nGround truth: [\"New York wine\"]\\nPrediction: Finger Lakes A V A\\nCorrectness: incorrect\\nQuestion: {QUESTION}\\nGround truth: {GROUND_TRUTH}\\nPrediction: {PREDICTION}\\nCorrectness:\\nPrompt 2: Correctness checking.Answer the following questions in as few words as possible. Say \"unsure\"\\nif you don’t know.\\nQuestion: What is the capital of China?\\nAnswer: Beijing\\nQuestion: What is the captical of Wernythedia?\\nAnswer: unsure\\nQuestion: {QUESTION}\\nAnswer:\\nPrompt 3: Question answering (Few-shot).\\nAnswer the following question in as few words as possible. Say \"unsure\"\\nif you don’t know. {QUESTION}\\nPrompt 4: Question answering (Zero-shot).\\nAnswer the following questions in as few words as possible. Say \"unsure\"\\nif you don’t know.\\nQuestion: What is the captical of Wernythedia?\\nAnswer: unsure\\nQuestion: {QUESTION#}\\nAnswer: {ANSWER#}\\nQuestion: {QUESTION}\\nAnswer:\\nPrompt 5: Question answering (In-domain) (#: the in-\\ndomain instance described in Section 3.5).\\nAnswer the following questions in as few words as possible. {QUESTION}\\nPrompt 6: Question answering (simply asking for con-\\ncise answers).\\nA.2 Impact of less naturally occurring\\nquestions\\nModelMovie Book Academics\\nALM HLM ALM HLM ALM HLM\\nChatGPT 37.8 14.5 31.0 21.5 2.3 1.6\\nLLaMA (7B) 18.7 71.9 21.4 59.1 2.4 94.9\\nLLaMA (13B) 25.3 73.2 24.4 74.5 4.6 93.7\\nLLaMA (33B) 31.2 67.5 31.7 65.9 5.2 91.7\\nLLaMA (65B) 27.1 72.4 32.3 66.5 8.2 91.8\\nVicuna (7B) 21.1 68.5 19.2 62.9 2.6 91.3\\nVicuna (13B) 20.3 58.5 11.4 38.7 3.1 77.4\\nFlan-T5 (3B) 1.7 15.1 2.8 4.5 0.2 4.3\\nFlan-T5 (11B) 6.3 22.1 6.6 10.1 0.8 16.0\\nRWKV (7B) 4.5 24.3 13.3 37.0 0.1 9.5\\nFalcon (7B) 20.4 62.3 15.0 45.1 3.5 80.7\\nFalcon (40B) 26.0 43.1 11.4 7.1 4.7 55.0\\nFalcon-Instruct (7B) 13.4 66.4 9.9 34.1 1.9 58.5\\nFalcon-Instruct (40B) 28.4 37.5 11.9 1.9 3.9 51.2\\nTable 10: Comparison of LLMs’ factuality on Head-to-\\nTailwithout relatively less natural occurring questions.\\nAll numbers are in percentage (%).\\nWhen constructing Head-to-Tail , we include all\\npredicates that allow reasonable factual questions.\\nTable 10, instead, shows metrics on predicates that\\nusers are more likely to ask about. In general we observed higher performance on the Movie and\\nBook domains, but the accuracy is still fairly low\\nand we observe similar patterns regarding head,\\ntorso, and tail entities. A.3 Supplemental Results\\nModelAll Movie Book Academics Open\\nAEM HEM AF1 HF1 ARL HRL ALM HLM M A LM HLM ALM HLM ALM HLM ALM HLMHeadChatGPT 21.8 24.9 25.6 21.1 25.6 21.1 29.4 17.2 53.3 51.3 11.5 20.1 26.3 5.9 3.0 32.7 20.8\\nLLaMA (7B) 10.4 83.0 15.5 77.9 15.4 78.0 19.0 74.4 6.6 27.2 69.5 21.6 74.8 5.0 90.6 19.9 70.7\\nLLaMA (13B) 12.7 86.6 18.1 81.1 18.0 81.2 22.0 77.2 0.8 36.5 63.1 20.1 79.9 9.7 89.8 21.7 77.1\\nLLaMA (33B) 16.7 82.0 22.3 76.5 22.2 76.5 26.0 72.8 1.3 42.9 57.0 24.2 75.8 10.8 87.5 25.8 72.3\\nLLaMA (65B) 14.9 84.8 21.7 78.0 21.6 78.1 25.9 73.8 0.3 37.0 62.3 23.1 76.9 16.5 83.5 26.1 73.7\\nVicuna (7B) 9.4 79.6 13.7 75.3 13.6 75.4 16.2 72.7 11.0 30.1 59.7 18.6 75.6 3.9 91.4 14.8 70.2\\nVicuna (13B) 8.7 60.3 11.9 57.1 11.9 57.1 14.0 55.0 31.0 29.9 52.0 10.8 64.0 5.4 74.3 12.5 46.8\\nFlan-T5 (3B) 2.5 21.1 3.3 20.3 3.3 20.3 3.9 19.7 76.4 2.3 19.9 3.7 52.1 0.1 7.9 5.7 12.8\\nFlan-T5 (11B) 5.8 25.5 7.4 23.9 7.4 23.9 7.6 23.7 68.7 10.7 30.1 7.9 59.5 0.4 21.0 8.8 10.6\\nRWKV (7B) 6.2 35.3 8.3 33.2 8.2 33.2 9.6 31.9 58.5 9.8 26.7 15.4 49.8 0.2 13.2 10.7 33.7\\nFalcon (7B) 10.0 58.3 12.9 55.4 12.8 55.5 14.5 53.8 31.7 28.1 51.6 14.8 68.3 8.1 80.4 11.9 41.1\\nFalcon (40B) 12.1 40.5 14.5 38.1 14.6 38.0 16.2 36.4 47.4 36.0 30.3 10.2 52.9 11.5 61.8 13.1 24.8\\nFalcon-Instruct (7B) 6.6 60.9 9.3 58.2 9.3 58.3 11.5 56.0 32.4 20.8 60.1 11.4 65.7 1.6 67.7 11.6 47.7\\nFalcon-Instruct (40B) 12.4 34.8 15.2 32.0 15.2 32.0 16.7 30.5 52.7 39.4 27.6 9.8 51.0 10.9 60.0 13.2 15.3TorsoChatGPT 16.6 20.0 19.0 17.5 19.0 17.5 21.9 14.6 63.5 46.4 16.6 22.5 29.2 2.3 1.7 19.7 13.3\\nLLaMA (7B) 5.7 87.0 9.7 83.0 9.6 83.1 11.7 81.0 7.3 21.2 72.3 9.5 80.4 2.6 93.7 12.2 80.0\\nLLaMA (13B) 8.5 90.1 12.6 86.0 12.5 86.1 14.8 83.8 1.4 28.8 70.5 14.2 85.3 6.3 92.2 12.9 85.2\\nLLaMA (33B) 12.7 85.7 17.5 80.9 17.5 81.0 19.8 78.7 1.5 36.0 63.8 19.2 80.5 7.9 88.4 18.4 79.9\\nLLaMA (65B) 11.4 88.2 16.5 83.2 16.4 83.3 18.7 81.0 0.3 32.5 66.9 20.0 79.1 9.4 90.6 16.7 83.2\\nVicuna (7B) 5.4 84.0 8.7 80.7 8.6 80.8 9.6 79.8 10.6 22.9 64.6 8.7 82.1 2.6 91.6 7.7 80.3\\nVicuna (13B) 5.4 66.2 8.1 63.5 8.1 63.6 8.8 62.8 28.4 20.7 57.3 4.7 66.7 4.0 75.2 7.7 59.4\\nFlan-T5 (3B) 0.7 17.9 1.3 17.3 1.3 17.3 1.5 17.1 81.4 1.2 14.2 0.5 51.4 0.1 7.1 2.5 10.0\\nFlan-T5 (11B) 2.0 21.1 3.3 19.8 3.3 19.8 3.2 19.9 76.9 4.0 24.7 1.4 53.7 0.8 19.6 4.3 7.3\\nRWKV (7B) 2.0 26.9 3.4 25.5 3.3 25.5 3.8 25.0 71.1 2.3 24.4 4.1 33.8 0.1 7.6 5.4 28.0\\nFalcon (7B) 5.9 61.3 8.3 58.8 8.3 58.9 9.2 57.9 32.9 20.5 54.6 6.5 74.0 6.2 83.4 7.3 45.6\\nFalcon (40B) 8.4 42.8 10.1 41.1 10.1 41.1 11.2 40.0 48.9 28.0 34.6 5.8 52.1 9.3 62.7 7.9 30.5\\nFalcon-Instruct (7B) 2.8 60.0 5.0 57.8 5.0 57.9 5.6 57.2 37.2 11.2 67.6 2.8 67.8 1.4 67.8 5.9 46.8\\nFalcon-Instruct (40B) 8.7 33.9 11.0 31.6 10.9 31.7 11.5 31.1 57.4 31.7 31.2 7.1 50.4 9.4 61.5 6.7 14.9TailChatGPT 5.9 14.0 7.7 12.2 7.7 12.2 9.5 10.5 80.1 6.4 11.8 8.0 19.2 0.8 0.9 13.8 10.2\\nLLaMA (7B) 1.5 88.7 4.5 85.7 4.4 85.8 5.4 84.8 9.8 3.2 80.4 2.0 82.5 1.1 95.8 8.7 83.4\\nLLaMA (13B) 1.9 96.3 5.0 93.1 5.0 93.2 6.3 91.9 1.8 4.8 92.1 2.4 96.5 1.9 96.6 9.5 88.7\\nLLaMA (33B) 4.1 93.3 7.8 89.5 7.8 89.6 8.8 88.6 2.6 7.3 89.4 4.1 92.5 2.6 95.1 12.8 84.9\\nLLaMA (65B) 3.5 96.2 7.4 92.2 7.4 92.2 8.7 90.9 0.4 5.4 94.4 5.9 93.2 3.3 96.6 12.5 87.1\\nVicuna (7B) 1.4 87.9 3.9 85.4 3.9 85.4 4.3 85.0 10.7 5.1 85.6 1.5 86.7 1.5 90.4 5.9 82.4\\nVicuna (13B) 1.6 73.1 3.9 70.8 3.9 70.8 4.7 70.0 25.3 5.7 72.6 1.7 77.2 1.6 81.8 6.4 62.8\\nFlan-T5 (3B) 0.6 16.2 1.1 15.7 1.1 15.7 1.3 15.5 83.2 1.1 8.0 0.2 53.0 0.4 6.0 2.1 8.8\\nFlan-T5 (11B) 1.2 17.3 2.3 16.2 2.3 16.2 2.0 16.5 81.5 2.8 9.6 0.6 51.2 0.6 18.5 2.6 6.8\\nRWKV (7B) 0.5 22.1 1.6 21.1 1.6 21.1 1.8 20.9 77.4 0.3 16.4 0.4 16.5 0.0 10.3 3.3 27.2\\nFalcon (7B) 2.1 64.6 4.2 62.6 4.2 62.6 4.8 62.0 33.2 7.6 71.2 1.4 75.2 1.9 89.6 5.8 45.7\\nFalcon (40B) 2.7 48.8 4.3 47.2 4.3 47.2 4.9 46.6 48.5 7.6 54.2 1.3 55.5 3.7 71.3 5.6 33.2\\nFalcon-Instruct (7B) 1.5 58.6 2.9 57.2 2.9 57.2 3.4 56.7 39.9 5.1 65.7 0.9 67.3 1.0 67.1 4.3 46.8\\nFalcon-Instruct (40B) 2.3 36.8 4.3 34.9 4.2 34.9 4.3 34.8 60.9 7.0 44.6 1.0 51.3 3.9 67.7 4.6 15.5Head-to-TailChatGPT 14.7 19.6 17.4 16.9 17.4 16.9 20.3 14.1 65.6 34.7 13.3 16.9 24.9 3.0 1.9 22.1 14.8\\nLLaMA (7B) 5.9 86.2 9.9 82.2 9.8 82.3 12.1 80.0 7.9 17.2 74.1 11.0 79.2 2.9 93.4 13.6 78.0\\nLLaMA (13B) 7.7 91.0 11.9 86.8 11.8 86.8 14.4 84.3 1.3 23.3 75.3 12.2 87.2 6.0 92.9 14.7 83.7\\nLLaMA (33B) 11.2 87.0 15.9 82.3 15.8 82.4 18.2 80.0 1.8 28.7 70.1 15.8 82.9 7.1 90.3 19.0 79.1\\nLLaMA (65B) 9.9 89.8 15.2 84.5 15.1 84.5 17.8 81.9 0.3 25.0 74.5 16.3 83.1 9.7 90.3 18.4 81.3\\nVicuna (7B) 5.4 83.8 8.8 80.5 8.7 80.5 10.1 79.2 10.8 19.4 70.0 9.6 81.5 2.7 91.2 9.5 77.6\\nVicuna (13B) 5.2 66.5 8.0 63.8 7.9 63.8 9.2 62.6 28.2 18.8 60.7 5.7 69.3 3.7 77.1 8.9 56.4\\nFlan-T5 (3B) 1.3 18.4 1.9 17.8 1.9 17.8 2.3 17.4 80.3 1.5 14.0 1.5 52.2 0.2 7.0 3.4 10.5\\nFlan-T5 (11B) 3.0 21.3 4.3 20.0 4.3 20.0 4.2 20.0 75.7 5.8 21.5 3.3 54.8 0.6 19.7 5.2 8.3\\nRWKV (7B) 2.9 28.1 4.4 26.6 4.4 26.6 5.1 25.9 69.0 4.1 22.5 6.6 33.4 0.1 10.4 6.5 29.6\\nFalcon (7B) 6.0 61.4 8.5 58.9 8.4 59.0 9.5 57.9 32.6 18.7 59.1 7.6 72.5 5.4 84.5 8.3 44.1\\nFalcon (40B) 7.7 44.0 9.7 42.1 9.7 42.1 10.8 41.0 48.2 23.9 39.7 5.8 53.5 8.1 65.3 8.8 29.5\\nFalcon-Instruct (7B) 3.6 59.8 5.7 57.8 5.7 57.8 6.8 56.7 36.5 12.4 64.5 5.0 66.9 1.4 67.5 7.3 47.1\\nFalcon-Instruct (40B) 7.8 35.2 10.2 32.8 10.1 32.9 10.8 32.2 57.0 26.0 34.5 6.0 50.9 8.0 63.1 8.2 15.3\\nTable 11: Comparison of LLMs’ factuality about head, torso, and tail entities using all instances and instances of\\neach domain from Head-to-Tail . All numbers are in percentage (%). '},\n",
       " {'url': 'http://arxiv.org/pdf/2310.01434v1',\n",
       "  'title': 'Revolutionizing Mobile Interaction: Enabling a 3 Billion Parameter GPT LLM on Mobile',\n",
       "  'text': 'Revolutionizing Mobile Interaction: Enabling a 3 Billion Parameter\\nGPT LLM on Mobile\\nTomás Marquesa(Author), Samuel Carreiraa(Author), Carlos Griloa(Author) and José Ribeiroa\\n(Author)\\naIPLeiria, Morro do Lena, Alto do Vieiro, Leiria, 2411-901 PT, Portugal\\nARTICLE INFO\\nKeywords :\\nOn-device inference\\nGPT\\nLLMs\\nQuantizationABSTRACT\\nThefieldofArtificialIntelligencehaswitnessedremarkableprogressinrecentyears,especiallywith\\nthe emergence of powerful large language models (LLMs) based on the transformer architecture.\\nCloud-basedLLMs,suchasOpenAI’sChatGPT,offerimpressivecapabilitiesbutcomewithconcerns\\nregarding latency and privacy due to network dependencies. This article presents an innovative\\napproach to LLM inference, envisioning a future where LLMs with billions of parameters can be\\nexecuteddirectlyonmobiledeviceswithoutnetworkconnectivity.Thearticleshowcasesafine-tuned\\nGPT LLM with 3 billion parameters that can operate smoothly on devices with as low as 4GB of\\nmemory. Through the integration of native code and model quantization techniques, the application\\nnot only serves as a general-purpose assistant but also facilitates seamless mobile interactions with\\ntext-to-actionsfeatures.Thearticleprovidesinsightsintothetrainingpipeline,implementationdetails,\\ntest results, and future directions of on-device LLM inference. This breakthrough technology opens\\nuppossibilitiesforempoweringuserswithsophisticatedAIcapabilitieswhilepreservingtheirprivacy\\nand eliminating latency concerns.\\n1. Introduction\\nOverthepastfewyears,thefieldofArtificialIntelligence\\nhas experienced a remarkable evolution, with significant\\nadvancementsinlargelanguagemodels(LLMs).Theemer-\\ngence of the transformer architecture in 2017 (Vaswani,\\nShazeer,Parmar,Uszkoreit,Jones,Gomez,KaiserandPolo-\\nsukhin, 2017), particularly in the case of Generative Pre-\\ntrained Transformers (GPT), has revolutionized the land-\\nscape.OnenotableexampleisChatGPT,developedbyOpe-\\nnAI (OpenAI, 2021), which has garnered widespread atten-\\ntion and found its way into numerous applications flooding\\nmajor app stores.\\nLeveraging powerful cloud servers for intensive pro-\\ncessing tasks offers numerous advantages, similar to the\\napproach employed by OpenAI and other leading organiza-\\ntions.Onenotablebenefitistheabilitytoachievefasterinfer-\\nence capabilities. Cloud servers with robust computational\\nresourcescanprocesscomplextasksmoreefficiently,allow-\\ningforquickerresultsandimprovedperformance.However,\\nrecognizing the substantial disadvantages of this strategy\\nis crucial. Potential latency problems are one noteworthy\\narea of worry as results might not be available right away\\nsincedatamustbesentoverthenetworktothecloudserver\\nfor processing. In situations or applications where real-time\\n⋆ThisdocumentistheresultofthepracticalprojectforContextAware\\nSystems of the masters in mobile computing.\\ntomas.g.marques@ipleiria.pt (T. Marques);\\nsamuel.s.carreira@ipleiria.pt (S.Carreira); carlos.grilo@ipleiria.pt (C.\\nGrilo); jose.ribeiro@ipleiria.pt (J. Ribeiro)\\nORCID(s):0009-0002-9783-9752 (T. Marques); 0009-0001-2117-7994\\n(S. Carreira); 0000-0001-9727-905X (C. Grilo); 0000-0003-3019-1330 (J.\\nRibeiro)\\nhttps://www.linkedin.com/profile/view?id=\\ntomÃąs-marques-19b531142 (T. Marques), https://www.linkedin.com/\\nprofile/view?id=samuel-vitorino-871113159 (S. Carreira)responsesarenecessaryortime-sensitive,thislatencymight\\nbe troublesome. Additionally, privacy concerns arise from\\nthenecessityoftransmittingallinformationoverthenetwork\\nto the cloud server. This means that sensitive data may\\nbe exposed to potential security risks during transmission.\\nProtectinguserprivacyandensuringthesecurityofdataare\\ncriticalconsiderations,particularlyinapplicationsinvolving\\npersonal or confidential information. Given these factors,\\nan ideal scenario would involve running the model offline\\ndirectlyonthedeviceitself.Thisapproachmitigatesthecon-\\ncernsregardinglatencyandprivacy,astheprocessingoccurs\\nlocally without the need to transmit data over the network.\\nBy executing the model directly on the device, users can\\nbenefitfromfasterinferencetimeswhilemaintainingcontrol\\nover their data and preserving their privacy.\\nIn this article, we present a groundbreaking approach\\nto performing inference on GPT LLMs that we believe\\nrepresentsthefutureofthistechnology.Withtheincreasing\\nprevalence of custom accelerators in mobile devices, we\\nenvisionafuturewherereal-timeinferenceofmassiveLLMs\\nwith billions of parameters can be executed directly on the\\ndevice, without the need for a network connection. While\\nwe are still in the early stages, our application provides a\\npromising glimpse into the immense potential of running\\nLLMs strictly on the edge device.\\nOur application showcases the capabilities of a fine-\\ntuned GPT LLM with 3 billion parameters, which can run\\nsmoothly on devices with as low as 4GB of memory. This\\nremarkable achievement is made possible through our inte-\\ngration of native code and model quantization techniques.\\nNot only that, but we go a step further: our refined model\\nnot only performs well as a general-purpose assistant but\\nalsofacilitatesseamlessmobilephoneinteractions,enabling\\nusers to carry out various tasks including making calls,\\nTomás Marques, Samuel Carreira: Preprint submitted to Elsevier Page 1 of 8arXiv:2310.01434v1  [cs.CL]  29 Sep 2023 Revolutionizing Mobile Interaction: Enabling a 3 Billion Parameter GPT LLM on Mobile\\nconducting web searches, and adding appointments to the\\ncalendar.Additionally,fromtheautomaticmodeldownload\\nto its utilization, we have built the application to be user-\\nfriendly, removing the requirement for prior AI knowledge.\\nThisarticle’sstructureisasfollows:Section2providesa\\nsummary of the current state-of-the-art in LLMs, including\\nmethods, frameworks, and libraries. Section 3 delves into\\nour training pipeline, covering aspects such as the base\\nmodelselection,dataset,fine-tuning,andquantization.Sec-\\ntion 4 provides detailed insights into the implementation of\\nthemobileapplication,includinghowweinterfacewithna-\\ntive code, the user interface screens, our innovative text-to-\\nactions feature, and any limitations we encountered during\\ndevelopment. In Section 5, we present the results of several\\ntests conducted on the application, examining the model\\nsizes before and after quantization, as well as showcasing\\nsample prompts and their outputs from the final model.\\nFinally,inSection6,weconcludewithreflectionsonour\\nwork and discuss the future directions of this technology.\\nThe potential for on-device inference of LLMs is vast, and\\nwe believe it will revolutionize the way we interact with AI\\nsystems. With further advancements and optimizations, we\\nenvision a future where powerful LLMs are accessible and\\nseamlessly integrated into everyday devices, empowering\\nusers with sophisticated AI capabilities directly at their\\nfingertips.\\n2. State of the art\\nNumerous projects in the area of large language models\\nhave emerged, offering frameworks, methods and platforms\\nthat greatly facilitate the development and deployment of\\nchatapplications.Thissectionhighlightsaselectionofthese\\ninfluentialprojects,thataremakingcontributionstothisarea\\nof study.\\n2.1. OpenAI GPT model family\\nTheOpenAIGPT(GenerativePre-trainedTransformer)\\nmodel family represents a significant milestone in natural\\nlanguageprocessingandartificialintelligenceresearch.De-\\nveloped by OpenAI, the GPT models have garnered im-\\nmenseattentionduetotheirabilitytogeneratecoherentand\\ncontextually relevant text.\\nThe latest iterations of the GPT model family, GPT-\\n3.5 and GPT4 (OpenAI, 2023), serve as the foundation\\nfor the language capabilities of ChatGPT. Building upon\\nthe success of its predecessors, GPT-3.5, GPT-3, and GPT-\\n2, GPT-4 incorporates several advancements to improve\\nperformance and address certain limitations.\\nOne notable aspect of the GPT models is their trans-\\nformer architecture. Based on the groundbreaking Trans-\\nformer architecture, the GPT models leverage self-attention\\nmechanisms to capture contextual dependencies and gener-\\nate text that closely matches the given input. This architec-\\nture allows the models to effectively handle various natural\\nlanguage processing tasks.\\nTraining the GPT models involves pre-training on a\\nlargecorpusoftextdata,followedbyfine-tuningonspecificdownstream tasks. OpenAI employs a massive dataset en-\\ncompassingawiderangeofsources,suchasbooks,articles,\\nand websites, to provide a broad understanding of language\\ntothemodelsduringpre-training.Fine-tuningfurtherrefines\\nthe models’ performance on specific tasks, making them\\nmore adaptable and applicable to various domains.\\nThe performance of the OpenAI GPT models has been\\na subject of awe and admiration. GPT-3.5, in particular,\\nexhibits remarkable language generation capabilities. It can\\nproduce coherent and contextually relevant responses to\\nprompts,showcasearudimentaryunderstandingofcomplex\\ntopics, and mimic the writing style of specific authors or\\ngenres.\\n2.2. Quantization\\nQuantization (Lin, Tang, Tang, Yang, Dang and Han,\\n2023) refers to the process of reducing the precision or bit-\\nwidth of numerical values in a model or dataset. In the\\ncontext of large language models like GPT-3, quantization\\ncan be applied to reduce the memory footprint and compu-\\ntationalrequirementsofthemodel,makingitmoreefficient\\nto deploy and run on different hardware devices.\\nLarge language models often use 32-bit floating-point\\nvalues (float32) to represent weights, activations, and gra-\\ndients. These high-precision representations require more\\nmemoryandcomputationalresources.However,byemploy-\\ning quantization, these values can be converted into lower\\nprecision formats such as 8-bit integers or 16-bit floating-\\npoint (float16), reducing the memory and processing power\\ndemands. The process of quantization typically consists of\\ntwo steps: calibration and quantization. During calibration,\\nthemodelisrunonarepresentativedatasettocollectstatis-\\ntics about the range and distribution of values. These statis-\\ntics are used to determine appropriate ranges and scales for\\nquantization. The quantization step then maps the original\\nhigh-precision values to their lower precision counterparts\\nbased on the determined scales and ranges.\\nA Large language model’s memory footprint and pro-\\ncessingneedscanbereducedthroughquantization,allowing\\nit to be used on devices with limited resources, such as\\nmobile phones or embedded computers. However, quanti-\\nzation also introduces some precision loss, which may have\\na slight negative impact on the performance of the model.\\nTherefore, a balance must be struck between the reduction\\nin resource requirements and the impact on model accuracy\\nwhenapplyingquantizationtechniques.Low-precisiontrain-\\ningisatechniqueusedtotrainneuralnetworkswithreduced\\nnumerical precision for both weights and activations. By\\nusinglowerprecision,typically16-bitorevenlower,instead\\nof the standard 32-bit floating-point representation, low-\\nprecision training aims to achieve more efficient and faster\\ntrainingwhileminimizingmemoryusageandcomputational\\nresources.\\n2.3. LoRA\\nTraditional fine-tuning or adaptation of large language\\nmodels involves updating all the parameters of the model,\\nTomás Marques, Samuel Carreira: Preprint submitted to Elsevier Page 2 of 8 Revolutionizing Mobile Interaction: Enabling a 3 Billion Parameter GPT LLM on Mobile\\nwhich can be computationally expensive and resource-\\nintensive. LoRA (Hu, Shen, Wallis, Allen-Zhu, Li, Wang,\\nWang and Chen, 2021) offers an alternative approach by\\nlearningpairsofrank-decompositionmatriceswhilekeeping\\nthe original weights frozen. This reduces the number of\\ntrainable parameters and, consequently, the storage require-\\nments.\\nByusingrank-decompositionmatrices,LoRArepresents\\nthe weights of the language model in a compressed format.\\nThesematricescapturetheessentialinformationneededfor\\nadaptation while discarding less relevant details. This com-\\npressionsignificantlyreducesthememoryfootprintrequired\\nto store the adapted model.\\nThe reduced storage requirement of LoRA has several\\nbenefitsforefficienttraining.Itallowsformoreeffectiveuti-\\nlization of computational resources, enabling larger models\\ntobetrainedonlimitedhardware.Additionally,thereduced\\nmemoryfootprintfacilitatesfasterdataloadingduringtrain-\\ning, leading to shorter training times.\\nLoRA also allows for low-precision training which is\\na technique used to train neural networks with reduced\\nnumerical precision for both weights and activations. By\\nusinglowerprecision,typically16-bitorevenlower,instead\\nof the standard 32-bit floating-point representation, low-\\nprecision training aims to achieve more efficient and faster\\ntrainingwhileminimizingmemoryusageandcomputational\\nresources.\\n2.4. PEFT\\nOne framework that has garnered significant attention\\nfor efficient model fine-tuning is the PEFT (Parallel Envi-\\nronment Fine-Tuning) library. PEFT (Hugging Face, 2023)\\nis designed to optimize and accelerate the training and fine-\\ntuningofdeeplearningmodelsinadistributedenvironment.\\nIt builds upon state-of-the-art techniques in parallel com-\\nputing and offers several key features that contribute to its\\neffectiveness and versatility.\\n•Scalability and Performance: The PEFT library fo-\\ncuses on maximizing the scalability and performance\\nof distributed deep learning. It provides efficient\\ncommunicationpatternsandalgorithmsthatminimize\\noverheads and latency in data synchronization and\\nparameter updates across multiple devices or nodes.\\nBy leveraging techniques like model parallelism and\\ndata parallelism, PEFT can efficiently distribute and\\nsynchronize the computation, resulting in faster con-\\nvergence and improved training throughput.\\n•FlexibilityandCompatibility:PEFTisdesignedtobe\\ncompatible with various deep learning frameworks,\\nsuch as TensorFlow and PyTorch. This compatibility\\nenablesresearchersandpractitionerstoleverageexist-\\ning models, codebases, and pre-trained models with-\\nout significant modifications. By providing a unified\\ninterface, PEFT simplifies the process of distributed\\ntraining and fine-tuning, allowing users to seamlessly\\nintegrate it into their existing workflows.•FaultToleranceandResilience:Distributeddeeplearn-\\ning systems often face challenges related to hardware\\nfailures, network issues, or node dropout. The PEFT\\nlibrary incorporates fault tolerance mechanisms that\\nenhance the system’s resilience to such failures. It\\nincludes techniques for checkpointing, model repli-\\ncation, and dynamic resource allocation to ensure\\nthat training processes can recover from failures and\\ncontinueseamlessly,minimizingtheimpactonoverall\\ntraining performance.\\n•PerformanceOptimization:PEFTincorporatesarange\\nofperformanceoptimizationstrategiestoenhancethe\\nefficiency of distributed deep learning. It provides\\nautomaticgraphpartitioningalgorithmsthatoptimize\\nthe distribution of model layers across multiple de-\\nvices, taking into account communication costs and\\ndevice capabilities. Additionally, PEFT employs ad-\\nvanced gradient aggregation techniques and efficient\\ndata shuffling algorithms to further reduce communi-\\ncation overhead and improve training speed.\\n•Ease of Deployment: The PEFT library offers a user-\\nfriendly interface and straightforward deployment\\nmechanisms. It provides a set of APIs and configura-\\ntionoptionsthatenableuserstoeasilyspecifythede-\\nsiredparallelismstrategies,communicationprotocols,\\nand resource allocation policies. Furthermore, PEFT\\nsupportsvariousdeploymentenvironments,including\\non-premisesclusters,cloudplatforms,andspecialized\\nhardwareaccelerators,makingitaccessibleandadapt-\\nable to different infrastructure configurations.\\n2.5. ggml\\nThe model deployment process typically begins with\\nthe testing and creation of the model using a language like\\nPython. However, due to the language’s slower execution\\nspeed, it is often necessary to convert the model to run on\\na faster language, such as C++, for deployment. This is\\nwhere GGML (Gerganov, Georgi, 2023), a powerful C++\\ntensor library, comes into play. GGML not only enables\\nrunning models natively but also offers additional features\\nlike quantization. Notably, GGML is capable of efficiently\\nexecuting popular models like BLOOM (Workshop, 2023)\\nor Whisper (Radford, Kim, Xu, Brockman, McLeavey and\\nSutskever,2022),furtherenhancingitsversatilityandutility\\nin the deployment process.\\n2.6. lamacpp\\nLamacpp(ggerganov,2023b)isahighlyrenownedopen-\\nsourceprojectfocusedonenablingtheexecutionofLLaMA\\n(Large Language Model Meta AI) (Touvron, Lavril, Izac-\\nard, Martinet, Lachaux, Lacroix, Rozière, Goyal, Hambro,\\nAzhar, Rodriguez, Joulin, Grave and Lample, 2023) over a\\nterminal interface on a personal computer. This exceptional\\nproject encompasses a robust C/C++ implementation that\\nfacilitates 4-bit, 5-bit, and 8-bit integer quantization. No-\\ntably,Lamacpphasmadesignificantstridesbyincorporating\\nTomás Marques, Samuel Carreira: Preprint submitted to Elsevier Page 3 of 8 Revolutionizing Mobile Interaction: Enabling a 3 Billion Parameter GPT LLM on Mobile\\nCUDAandcuBLASsupportintoitsproject,furtherenhanc-\\ningitscapabilities.Thisprojectisusedasthemaintestbench\\nto develop new features and improve the GGML library.\\n2.7. sherpa\\nSherpa(Bip-Rep,2023)isapowerfulimplementationof\\nLamacpp,seamlesslyintegratedwithFlutter,enablingusers\\nto run LLaMMa on Windows, Mac, or Android platforms\\nthrough an intuitive chat interface.\\n2.8. mlc-llm\\nMCL-LLM (MLC.AI, 2023) is another solution alter-\\nnative to sherpa that allows implementing an LLM running\\nlocally on multiple platforms. It also offers a framework to\\nallowleveragingthepoweroflocalGPUsonamobiledevice\\nor laptop for accelerated performance.\\n3. LLM training pipeline\\nThe process of training a Large Language Model in-\\nvolves multiple stages. However, our approach differs from\\ncreating a model from scratch. Instead, we opted for a\\ndifferentapproachbyselectinganexistinginstruction-based\\nfine-tuned model and improving the fine-tuning process to\\nsuit our needs.\\n3.1. Choosing a model\\nConsideration for choosing a model:\\n•ModelSize:GiventhatrunningLargeLanguageMod-\\nels (LLMs) on Android devices is a relatively new\\ntopic, it becomes crucial to consider the hardware\\nspecifications of the mobile device. The inference\\nprocess associated with LLMs is computationally de-\\nmanding,andthemodelitselfneedstobeloadedinto\\nRAM. Considering that an Android device consumes\\naround1.5GBofRAMandtheaveragemobiledevice\\nhas between 4 and 6 GB of RAM we had to consider\\nthe model size during the deliberation process to en-\\nsure optimal performance.\\n•Quality of the dataset: The quality of the dataset is of\\nutmostimportancewhendevelopinganytypeofdeep\\nlearning model. The dataset serves as the foundation,\\nand its quality, size, and diversity directly impact\\nthe model’s capacity to generalize and make accurate\\npredictions.\\nA high-quality dataset enables the model to under-\\nstandtheproblemathand,facilitatingeffectivelearn-\\ningofpatterns,features,andcorrelations.Byexposing\\nthe model to diverse examples, the dataset helps it\\ncoverdifferentscenariosduringtheinferenceprocess.\\nMoreover, a good-quality dataset plays a vital role\\nin preventing overfitting or underfitting during the\\ntraining process. If the dataset is too small or overly\\nspecific,itcanhinderthemodel’sabilitytogeneralize\\nbeyond the limited scope of the data.•Purpose of the model: An LLM (Language Model)\\npossesses a wide range of capabilities beyond its text\\ngeneration capabilities. It can serve diverse purposes,\\nincluding but not limited to generating code, facili-\\ntatingengagingchatinteractions,anddeliveringclear\\ninstructions.\\nIn our particular endeavor of developing a chat ap-\\nplication that triggers action on the mobile device,\\nit was crucial for us to seek out a model specifically\\ndesignedtoexcelinchat-basedscenarios.Weneeded\\na model that could simulate human-like conversa-\\ntions, enabling users to engage in natural and fluid\\nexchanges with the chatbot.\\nAfter careful consideration of the aforementioned as-\\npects, we conducted tests on various models of mobile\\ndevices (Android), including Alpaca, Bloomz, Redpajama,\\nandothers.Amongtheseoptions,Redpajamaemergedasthe\\nmost promising due to its relatively small size, with only\\n3 billion parameters, and its pre-existing fine-tuning that\\nenhances its chatting ability .\\nWechooseRedpajama,developedbyRedPajama-INCITE-\\nChat-3B-v1(togethercomputer, 2023) in collaboration with\\nleaders from the open-source AI community, which aims to\\nprovide the AI community with open-source models. This\\nparticular model contains 2.8 billion parameters and was\\ntrained using 131 million tokens.\\n3.2. Creating a dataset\\nAfterconductingacomprehensiveanalysisofthechosen\\nmodel, it became evident that a fine-tuning process was\\nessential to elevate the model’s performance and align it\\nmorecloselywithourspecificneeds.ConsideringthataLan-\\nguage Model (LM) dataset predominantly revolves around\\ntextual data, we opted to harness the capabilities of the\\nrenowned GPT3.5 model, developed by OpenAI (OpenAI,\\n2021). Employing the sophisticated technique of prompt\\nengineering, we generated a dataset of a small scale, com-\\nprisingapproximately357linesofmeticulouslycrafteddata\\nas can be seen in Figure 1.\\nThe dataset we constructed primarily revolves around\\na dialogue between human (<human>) users and the AI\\nbot(<bot>) therewerealsoincluded specialtokenssuch as\\n<search>, <calendar>, <call> that we will explain later in\\nthe paper. The objective is to generate short and succinct\\nresponses from the bot, emulating a more conversational\\nand interactive experience. There was also the intention to\\ninclude normal prompts in the dataset to avoid dataset gen-\\neralization. By fine-tuning the model using this expansive\\ndataset, we aimed to enhance its contextual understanding,\\ncoherence, and overall conversational prowess. To enhance\\nthe model’s generalization capabilities, we incorporated a\\nblend of action-focused samples and question-answering\\ndialogues encompassing broader knowledge. This approach\\nensuresthatthemodeldoesnotrelysolelyonspecializedto-\\nkens, even during regular conversations, thereby preserving\\nits versatility in understanding and responding effectively.\\nTomás Marques, Samuel Carreira: Preprint submitted to Elsevier Page 4 of 8 Revolutionizing Mobile Interaction: Enabling a 3 Billion Parameter GPT LLM on Mobile\\nFigure 1: Demo of the dataset generated by GPT3.5\\nFigure 2: Evaluation of the loss in the training and evaluation\\nstages\\nThroughtheutilizationofpromptengineering,wemetic-\\nulously crafted prompts that encapsulate a wide range of\\npotentialuserinputs,therebyenablingthebottogenerateac-\\ncurate and relevant responses across diverse conversational\\nscenarios. The careful construction of this dataset ensures\\nthat the bot’s responses are tailored to the given context,\\nleading to a more engaging and user-friendly interaction.\\nByincorporatingtheabundantcapabilitiesoftheGPT3.5\\nmodel and employing prompt engineering techniques, our\\nfine-tuningprocessaimedtoempowerthemodeltoadaptto\\nvarious prompts, comprehend nuances within the provided\\ntext, and generate concise and contextually appropriate\\nresponses. This concerted effort ensures that the model is\\nfinelytunedtocatertoourspecificrequirements,enablingit\\nto deliver an elevated user experience in terms of dialogue\\nand information exchange.\\n3.3. Supervised Fine Tuning\\nIn our approach, we began with the 16-bit base model\\ncalled RedPajama-INCITE-Chat-3B-v1 (as mentioned pre-\\nviously). To enhance its performance, we utilized a com-\\nbination of the LoRA and PEFT libraries for fine-tuning.\\nIn the LoRA configuration, we specifically selected the\\nparameters (layers) that required fine-tuning while keeping\\nthe remainder of the model frozen. Our focus was on fine-\\ntuning the weights associated with the layers responsible\\nfor converting embeddings to query keys and values for\\nattention calculations.\\nWe utilized the bitsandbytes library to accelerate the\\ntraining process and enable 8-bit precision training. We\\nused a conservative approach by adopting a relatively highlearning rate of 1e-3 and a small number of epochs (5) due\\nto the small size of our dataset. This decision was based\\non our observation that, beyond the fifth epoch, the model\\nwould start to overfit the training data. We partitioned our\\ndataset into a 90% training set and a 10% evaluation set.\\nThe progression of loss values for both the training and\\nevaluation sets are visualized in Figure 2. To make possible\\nbatching samples for faster training we padded the dialogue\\nsamples to the length of the biggest dialog with the special\\nend of sentence token (eos).\\nAfter completing the training phase, we merged the\\nlayers generated by LoRA with the initial base model. Sub-\\nsequently,weconvertedthiscombined16-bitmodelintothe\\nGGML format, finalizing the enhancement process.\\n3.4. Quantization\\nOnce the training process is completed and we have the\\ntrained model in the GGML format, the next crucial step is\\ntofurtheroptimizeitbyquantizingthemodel.Theobjective\\nof quantization is to reduce the model’s size and hardware\\nrequirements without compromising its performance.\\nToachievethisoptimization,wemadeuseoftheGGML\\n4-bit integer quantization feature. By replacing the orig-\\ninal 16-bit float weights with 4-bit integers, this strategy\\neffectivelyreducesthememoryfootprintandcomputational\\ncomplexity of the model’s parameters. By quantizing the\\nmodel,wewereabletoachievesignificantreductionsinboth\\nthe size of the model and the hardware resources needed to\\ndeployandperforminferenceonit.Thisisparticularlyben-\\neficial in scenarios where limited storage or computational\\npower is a constraint.\\nThequantizationprocessyieldedpromisingresultswhen\\nit comes to model size, which are outlined and discussed\\nin detail in Section 5. We couldn’t however compare it’s\\nperformance to the 16-bit model because, at the time of\\nwriting this article and to the best of our knowledge, there\\nisn’tawaytoevaluatea4-bitquantizedmodelintheGGML\\nformat.\\n4. Mobile App\\nWhen the user opens the app (Figure 3 column 1), they\\nare greeted by a visually captivating landing page designed\\nTomás Marques, Samuel Carreira: Preprint submitted to Elsevier Page 5 of 8 Revolutionizing Mobile Interaction: Enabling a 3 Billion Parameter GPT LLM on Mobile\\nFigure 3: Various Screens of the application: 1-Landing Page, (2)-Loading Page, (3,4,5)-Chat Screen, 6-Settings Screen. In the\\npicture (5) is also possible to visualize the model maintaining context\\ntoinstantlycapturetheirattentionandcreateastrongdesire\\nto explore the app’s features and functionalities but also to\\nthemgettingtheideaofwhattheappisaboutjustbylooking\\nat the page.\\nAfterthecaptivatinglandingpage,theuserisseamlessly\\ntransitioned to the loading page (Figure 3). Here, the app\\nperforms a series of crucial checks to ensure a smooth\\nexperience. Firstly, it verifies if the model exists in the\\nphone’s data folder. If it does, the app meticulously verifies\\nthefileintegritybycalculatinganMD5checksum,ensuring\\nthe model’s integrity.\\nUpon confirmation of the model’s presence, the app\\nproceedstodownloaditdirectlyfromthecloud,ensuringthe\\nuserhasthelatestversionattheirdisposal.Thisstreamlined\\nprocess minimizes any delays or interruptions.\\nOnce the model is successfully downloaded, it is effi-\\nciently loaded into the device’s memory. This optimization\\nensures quick access and seamless performance throughout\\ntheuser’sinteractionwiththeapp.Oncetheloadingprocess\\niscomplete,theuserisgracefullyusheredintothechatpage,\\nready to start a conversation.\\nOn the chat page (Figure 3 columns 3-5), users are em-\\npowered to create prompts in two convenient ways: through\\ntyping or by utilizing the microphone button, located at the\\nbottom-left corner of the interface. By simply pressing the\\nmic button, users can effortlessly convert their speech into\\ntext, enhancing accessibility and convenience even if the\\ndevice is offline.\\nWhen a prompt is sent, it triggers the inference process\\nwithin the app. As the app diligently generates tokens and\\npresents them, users can witness the app’s intelligent re-\\nsponsesunfoldinrealtime.Afterthepromptissenttheapp\\nenters the loading phase where the user can not send any\\nmorepromptsuntilthemodelisgeneratingtokens(Figure3\\ncolumn 4).\\nThis allows the LLM to maintain a context of the con-\\nversation. Maintaining context as can be seen in Figure 3\\n(column5)wherethemodeliscapableofrememberingthat\\nInference method Kotlin methodC/C++Code Kotlin Code JNI\\nCalls and\\npasses parameters\\nJni.j Header\\nReturns token\\nvia a calbackFigure 4: Android NDK architecture\\nthe conversation is about Elon Musk without the need to\\nrepeat it in the second prompt.\\nAt any time the user can enter the settings screen (Fig-\\nure 3column6)wherehecancustomizetheprofilepicture,\\nusername and the color of the chat individually.\\n4.1. Embedding an LLM on a Mobile App\\nOur approach to run the inference model on an Android\\ndevice was to use transformer architecture for GPT-Neox\\nprovided by GGML (ggerganov, 2023a). This implementa-\\ntionisdevelopedinC++andprovedtobehighlyeffectivein\\nourusecase.Toallowtheprocessofcompilingthecodeand\\nruninferencetheinferencemethodonthemodelweuseused\\nAndroidNDK(NativeDevelopmentKit)tocompileandrun\\nin Android.\\nAndroid NDK is a set of tools provided by directly by\\ngoogled its main objective was to allow the developer to\\nwrite performance-critical portions of code in C and C++.\\nTomás Marques, Samuel Carreira: Preprint submitted to Elsevier Page 6 of 8 Revolutionizing Mobile Interaction: Enabling a 3 Billion Parameter GPT LLM on Mobile\\nTo provide a visual representation, please refer to the\\ndiagram in Figure 4 illustrating the architecture of the An-\\ndroidNDKinaction.Asdepicted,theNDKoperateswithin\\naseparatethread,allowingittoexecuteinferencecallswith\\ntheir respective parameters.\\nUponcompletion,theinferenceprocessreturnsthegen-\\neratedtokenthroughacallbackmethod.Thiscallbackmech-\\nanism allows for seamless integration with the rest of the\\napplication, enabling further processing or presentation of\\nthe obtained token.\\nThis integration of our model, the Android NDK, and\\nourAndroiddeviceservesasapowerfulcombination,facil-\\nitating the execution of complex inference tasks seamlessly\\non mobile devices.\\n4.2. From text to actions\\nTo fully leverage the capabilities of mobile devices and\\nenhance app functionality, we devised a method for our\\nLanguage Learning Model (LLM) to interact with phone\\nfeatures. This involved incorporating special tags into our\\ndataset to perform specific actions. In this application ver-\\nsion,wehavefocusedonsupportingthreekeyactions:mak-\\ning calls, scheduling events in the calendar, and performing\\nweb searches.\\nToinitiateacall(Figure5secondandthirdcolumn),the\\nmodel simply needs to generate the special token \"<call>\"\\nfollowed by the contact’s name, and then conclude with an-\\notherspecialtoken\"<call>\".Forinstance,\"<call>John<call>\"\\nwould instruct the model to initiate a call to John.\\nSimilarly, the process for conducting a web search (Fig-\\nure 5 first column) is analogous. The model needs to gen-\\nerate the special token \"<search>\", followed by the de-\\nsired query, and then conclude with the same special token\\n\"<search>\".Forexample,\"<search>Highestbuildinginthe\\nworld<search>\" would prompt the model to perform a web\\nsearch for the tallest building on earth.\\nFinally,themodelmustgeneratethespecialtoken\"<cal-\\nendar>\",thentheevent’sdate,time,andname,allofwhich\\nareseparatedbyaslash(\"/\"),beforeconcludingwiththespe-\\ncialtoken\"calendar>\".Forexample,\"<calendar>2023-05-\\n20T09:00:00/Meeting<calendar>\"wouldinstructthemodel\\nto plan a meeting on May 20, 2023, at 9:00 AM.\\nWiththeuseofthispowerfulfeature,whichwecall text-\\nto-actions ,themodel’scapabilitiesmaybeextendedbeyond\\nsimple text generation and use a variety of mobile device\\nfeatures.\\n4.3. Limitations\\nRunning a Large Language Model (LLM) on a mobile\\ndevice presents several limitations:\\n•Computational Resources: LLMs require substantial\\ncomputational resources, including processing power\\nand memory. Mobile devices often have limited re-\\nsources compared to desktop computers or servers.\\nAs a result, running LLMs on mobile devices may\\nlead to slower inference times and reduced overall\\nperformance.•MemoryConstraints:LLMstendtohavelargemodel\\nsizes, often ranging from hundreds of megabytes to\\nseveral gigabytes. Mobile devices typically have lim-\\nitedstorageandRAMcapacities.Loadingandstoring\\nsuch large models can strain the device’s memory,\\npotentially leading to slower operation and higher\\nmemory usage.\\n•Power Consumption: LLMs are computationally in-\\ntensive and can consume a significant amount of\\npower during inference. Mobile devices have limited\\nbattery capacity, and running LLMs for extended\\nperiods can drain the battery quickly. This limitation\\nrestrictsthepracticalityandusabilityofLLMsonmo-\\nbile devices, particularly when offline or in situations\\nwithout immediate access to power sources.\\n5. Tests and Results\\nWe ran some tests on physical devices and got to the\\nconclusionthatthe6GBAndroiddeviceoffersareasonable\\nlevel of performance, while the 4GB device represents the\\nbare minimum required to ensure the application functions\\nproperly. We reached the conclusion through some tests\\nthat the app maintains conversation context. When it comes\\nto the user interface (UI), we were pleasantly surprised\\nby the design interface. It surpassed our expectations by\\nbeing exceptionally intuitive, making it incredibly easy for\\nuserstonavigateandinteractwiththeapp.Thethoughtfully\\ndesigned UI elements, including clear visual cues and log-\\nical placement of controls, contribute to an enjoyable user\\nexperience. One of the standout features of the app is its\\nvoice functionality. We were particularly impressed with its\\nperformance, as it consistently delivered excellent results.\\nNotably, the voice method even works offline, providing\\nusers with uninterrupted access to voice-based interactions\\nregardlessoftheirinternetconnectivity.Thisofflinecapabil-\\nity sets the app apart and enhances its versatility in various\\nscenarios.\\nTable 1\\nList of android devices tested.\\nAndroid Model CPU RAM\\nPOCO X3 PRO 4core 2.15ghz 6GB\\nLeMax2 8core 2.96ghz 4GB\\n5.1. Quantization results\\nTable 2\\nSize of the initial 16-bit model and the 4-bit quantized\\nmodel\\nModel Precision Size\\nst-ggml-model-16bit float16 5.17GB\\nst-ggml-model-q4_0 int4 1.6GB\\nTomás Marques, Samuel Carreira: Preprint submitted to Elsevier Page 7 of 8 Revolutionizing Mobile Interaction: Enabling a 3 Billion Parameter GPT LLM on Mobile\\nFigure 5: Example of the search the web and call action\\nfeatures\\nFollowing the successful quantization process, our final\\nmodel has undergone a remarkable transformation, boast-\\ning an impressive size reduction of 70% (Table 2). This\\nsignificant achievement empowers us to deploy the model\\neffortlessly on mobile devices, even those equipped with a\\nmodest4GBofmemory.Byharnessingthepowerofquanti-\\nzation,wehavenotonlyoptimizedthemodel’sperformance\\nbut also paved the way for seamless utilization in resource-\\nconstrained environments.\\n5.2. Text to actions tests\\nThebasemodelexhibitscertainissueswhendetermining\\nwhere to appropriately conclude the text and generate the\\ndesignated special token \"<|endoftext|>\". We are uncertain\\nwhetherthisbehaviorarisesduetoitstrainingmethodology,\\nbut as a result, the model frequently reproduces the human\\nprompt and struggles to halt at the intended position. Addi-\\ntionally,wehaveencountereddifficultieswiththequantized\\nmodel, whereby it occasionally struggles to identify the\\ncorrect token (tag) for the desired action. Even when it\\ndoes select the appropriate option, it sometimes mistakenly\\nchooses an incorrect closing special token, further compli-\\ncating the output. For example, the model can generate an\\noutput such as \"<call>John Castro<calendar>\".\\n6. Conclusion\\nInconclusion,thefieldofArtificialIntelligencehaswit-\\nnessed significant advancements in large language models,\\nparticularly with the emergence of transformer architecture\\nand the development of models like ChatGPT by Ope-\\nnAI.Whileleveragingpowerfulcloudserversforprocessing\\ntasks offers advantages such as faster inference capabilities,\\nit also presents challenges in terms of latency and privacy.\\nTo address these concerns, running LLMs directly on edge\\ndevices represents the future of this technology.\\nThisarticlehaspresentedagroundbreakingapproachto\\non-deviceinferenceofGPTLLMs,showcasingthepotentialof real-time execution without the need for a network con-\\nnection.Theintegrationofnativecodeandmodelquantiza-\\ntion techniques enables the smooth operation of fine-tuned\\nGPT LLMs with billions of parameters on devices with\\nlimitedmemory.Theapplicationdevelopedinthisstudynot\\nonlyservesasageneral-purposeassistantbutalsofacilitates\\nseamless mobile phone interactions for various tasks.\\nFuture directions for this technology involve further\\nadvancements and optimizations to make powerful LLMs\\naccessible and seamlessly integrated into everyday devices.\\nWith the increasing prevalence of custom accelerators in\\nmobile devices, the vision of executing massive LLMs on\\nthe device without network dependency is within reach.\\nThis advancement has the potential to revolutionize the\\nway we interact with AI systems, empowering users with\\nsophisticated AI capabilities at their fingertips.\\nOn-device inference of LLMs holds immense promise,\\nand its realization will lead to transformative advancements\\ninAItechnology.Bycontinuingtopushtheboundariesand\\nrefinethemethodologies,wecancreateafuturewherepow-\\nerfulLLMsareseamlesslyintegratedintoeverydaydevices,\\nenhancinguserexperiencesandprovidingubiquitousaccess\\nto AI capabilities.\\nReferences\\nBip-Rep, 2023. sherpa. https://github.com/Bip-Rep/sherpa . Accessed:\\nDate.\\nGerganov, Georgi, 2023. Ggml. https://github.com/ggerganov/ggml .\\nggerganov, 2023a. ggerganov/ggml/gpt-neox. https://github.com/\\nggerganov/ggml/tree/master/examples/gpt-neox . Accessed: June 14,\\n2023.\\nggerganov, 2023b. llama.cpp. https://github.com/ggerganov/llama.cpp .\\nHu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L.,\\nChen, W., 2021. Lora: Low-rank adaptation of large language models.\\narXiv:2106.09685 .\\nHugging Face, 2023. PEFT: Practical entity framework for transformer.\\nhttps://github.com/huggingface/peft .\\nLin, J., Tang, J., Tang, H., Yang, S., Dang, X., Han, S., 2023. Awq:\\nActivation-aware weight quantization for llm compression and acceler-\\nation. arXiv preprint arXiv:2306.00978 .\\nMLC.AI, 2023. mlc-llm. https://mlc.ai/mlc-llm/ . Accessed: Date.\\nOpenAI, 2021. Openai: An ai research laboratory URL: https://openai.\\ncom/.\\nOpenAI, 2023. Gpt-4 technical report. arXiv:2303.08774 .\\nRadford, A., Kim, J.W., Xu, T., Brockman, G., McLeavey, C., Sutskever,\\nI., 2022. Robust speech recognition via large-scale weak supervision.\\narXiv:2212.04356 .\\ntogethercomputer, 2023. RedPajama-INCITE-Chat-3B-v1 model on\\nhugging face. Hugging Face. URL: https://huggingface.co/\\ntogethercomputer/RedPajama-INCITE-Chat-3B-v1 .\\nTouvron,H.,Lavril,T.,Izacard,G.,Martinet,X.,Lachaux,M.A.,Lacroix,\\nT.,Rozière,B.,Goyal,N.,Hambro,E.,Azhar,F.,Rodriguez,A.,Joulin,\\nA., Grave, E., Lample, G., 2023. Llama: Open and efficient foundation\\nlanguage models. arXiv:2302.13971 .\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,\\nA.N., Kaiser, L., Polosukhin, I., 2017. Attention is all you need.\\narXiv:1706.03762 .\\nWorkshop, B., 2023. Bloom: A 176b-parameter open-access multilingual\\nlanguage model. arXiv:2211.05100 .\\nTomás Marques, Samuel Carreira: Preprint submitted to Elsevier Page 8 of 8 Revolutionizing Mobile Interaction: Enabling a 3 Billion Parameter GPT LLM on Mobile\\nHi, my name is Tomás Marques, im 22 years\\nold, I like to code on Linux, and listen to\\nmusic. Currently, I’m MSc student and Re-\\nsearcher in Computer Engineering @Polytechnic\\nof Leiria.\\nMy name is Samuel Carreira, and I am 21 years\\nold. Currently, I’m doing a Master’s in Computer\\nEngineering - Mobile Computing at the Polytech-\\nnic of Leiria. I’m also a researcher in Artificial\\nIntelligence and Computer Vision.\\nTomás Marques, Samuel Carreira: Preprint submitted to Elsevier Page 9 of 8 '},\n",
       " {'url': 'http://arxiv.org/pdf/2304.00612v1',\n",
       "  'title': 'Eight Things to Know about Large Language Models',\n",
       "  'text': 'Eight Things to Know about Large Language Models\\nSamuel R. Bowman1 2\\nAbstract\\nThe widespread public deployment of large lan-\\nguage models (LLMs) in recent months has\\nprompted a wave of new attention and engage-\\nment from advocates, policymakers, and scholars\\nfrom many ﬁelds. This attention is a timely re-\\nsponse to the many urgent questions that this tech-\\nnology raises, but it can sometimes miss important\\nconsiderations. This paper surveys the evidence\\nfor eight potentially surprising such points:\\n1.LLMs predictably get more capable with in-\\ncreasing investment, even without targeted\\ninnovation.\\n2.Many important LLM behaviors emerge un-\\npredictably as a byproduct of increasing in-\\nvestment.\\n3.LLMs often appear to learn and use repre-\\nsentations of the outside world.\\n4.There are no reliable techniques for steering\\nthe behavior of LLMs.\\n5.Experts are not yet able to interpret the inner\\nworkings of LLMs.\\n6.Human performance on a task isn’t an upper\\nbound on LLM performance.\\n7.LLMs need not express the values of their\\ncreators nor the values encoded in web text.\\n8.Brief interactions with LLMs are often mis-\\nleading.\\nIntroduction\\nLarge language models (LLMs, e.g. GPT-3, PALM,\\nLLaMA, and GPT-4; Brown et al., 2020; Chowdhery et al.,\\n2022; Touvron et al., 2023; OpenAI, 2023b) and products\\nbuilt on them, such as ChatGPT, have recently prompted\\nan enormous amount of attention from journalists, (Klein,\\n2023; Perrigo, 2023; Oliver, 2023), policymakers (J & C,\\n2023; Bartz, 2023; Lieu, 2023), and scholars from many\\n1New York University2Anthropic, PBC. Correspondence to:\\nSamuel R. Bowman <bowman@nyu.edu >.ﬁelds (Chan, 2022; Lund & Wang, 2023; Choi et al., 2023;\\nBiswas, 2023). This technology deﬁes expectations in many\\nways, though, and it can be easy for brief discussions of it\\nto leave out important points.\\nThis paper presents eight potentially surprising claims that\\nI expect will be salient in at least some of the conversa-\\ntions that are springing up around LLMs. They reﬂect, to\\nthe best of my understanding, views that are reasonably\\nwidely shared among the researchers—largely based in pri-\\nvate labs—who have been developing these models. All the\\nevidence I present here, as well as most of the arguments,\\nare collected from prior work, and I encourage anyone who\\nﬁnds these claims useful to consult (and directly cite) the\\nsources named here.\\nI do not mean for these claims to be normative in any signif-\\nicant way. Rather, this work is motivated by the recognition\\nthat deciding what we should do in light of this disruptive\\nnew technology is a question that is best led—in an informed\\nway—by scholars, advocates, and lawmakers from outside\\nthe core technical R&D community.\\n1. LLMs predictably get more capable with\\nincreasing investment, even without\\ntargeted innovation\\nScaling law results (Kaplan et al., 2020; Brown et al., 2020;\\nHoffmann et al., 2022) have been a major driving factor\\nin the recent surge of research and investment into LLMs\\n(Ganguli et al., 2022a). Scaling laws allow us to precisely\\npredict some coarse-but-useful measures of how capable\\nfuture models will be as we scale them up along three dimen-\\nsions: the amount of data they are fed, their size (measured\\nin parameters), and the amount of computation used to train\\nthem (measured in FLOPs). These results thereby allow\\nus to make some key design decisions, such as the optimal\\nsize of a model given some ﬁxed resource budget, without\\nextremely expensive trial and error.\\nOur ability to make this kind of precise prediction is un-\\nusual in the history of software and unusual even in the\\nhistory of modern AI research. It is also a powerful tool for\\ndriving investment since it allows R&D teams to propose\\nmodel-training projects costing many millions of dollars,\\nwith reasonable conﬁdence that these projects will succeedarXiv:2304.00612v1  [cs.CL]  2 Apr 2023 Eight Things to Know about Large Language Models\\nFigure 1. Excerpted from OpenAI (2023b): A scaling law result for one measure of language model performance, showing a consistent\\ntrend as the amount of computation used to train a model is scaled up 10,000,000,000 \\x02times from a small prototype system to GPT-4.\\nat producing economically valuable systems.\\nConcretely, consider these three superﬁcially very differ-\\nent systems: OpenAI’s original GPT can perform simple\\ntext-labeling tasks but cannot generally produce coherent\\ntext (Radford et al., 2018). GPT-2 adds the ability to pro-\\nduce text of reasonably high quality, as well as a limited\\nability to follow simple instructions (Radford et al., 2019).\\nGPT-3 is the ﬁrst modern general-purpose LLM, and is prac-\\ntically useful across a wide range of language tasks. The\\ndesigns of these three models hardly differ at all. Instead,\\nthe qualitative differences between them stem from vast\\ndifferences in scale: Training GPT-3 used roughly 20,000 \\x02\\nmore computation than training the original GPT (Sevilla\\net al., 2022), as well as signiﬁcantly more data and parame-\\nters. There aresubstantial innovations that distinguish these\\nthree models, but they are almost entirely restricted to in-\\nfrastructural innovations in high-performance computing\\nrather than model-design work that is speciﬁc to language\\ntechnology.\\nWhile the techniques used to train the newest LLMs are no\\nlonger generally disclosed, the most recent detailed reports\\nsuggest that there have been only slight deviations from\\nthis trend, and that designs of these systems are still largely\\nunchanged (Chowdhery et al., 2022; Hoffmann et al., 2022;\\nTouvron et al., 2023).\\nContinuing to scale these techniques up beyond GPT-3 has\\nproduced further economically valuable returns: The subse-\\nquent GPT-4 model outperforms qualiﬁed humans on many\\ngraduate and professional exams (OpenAI, 2023b), and its\\ndevelopment helped prompt a multi-billion-dollar invest-\\nment in the company that built it (Capoot, 2023). Scalinglaws allowed the creators of GPT-4 to cheaply and accu-\\nrately predict a key overall measure of its performance:\\nThis forecast was made by ﬁtting a statistical trend in the\\nperformance of small models, which collectively took about\\n0.1% of the resources needed by the ﬁnal model, and then\\nextrapolating out that trend (see Figure 1).\\n2. Speciﬁc important behaviors in LLM tend\\nto emerge unpredictably as a byproduct of\\nincreasing investment\\nScaling laws generally only predict a model’s pretraining\\ntest loss , which measures the model’s ability to correctly\\npredict how an incomplete piece of text will be continued.1\\nWhile this measure is correlated with how useful a model\\nwill be on average across many practical tasks (Radford\\net al., 2019), it is largely notpossible to predict when mod-\\nels will start to show speciﬁc skills or become capable of\\nspeciﬁc tasks (see Figure 2; Steinhardt, 2021; Ganguli et al.,\\n2022a; Wei et al., 2022a). Often, a model can fail at some\\ntask consistently, but a new model trained in the same way\\nat ﬁve or ten times the scale will do well at that task.\\n1Much of the data and computer time that goes into building a\\nmodern LLM is used in an expensive initial pretraining process.\\nLanguage-model pretraining intuitively resembles the autocom-\\nplete task: In it, an artiﬁcial neural network model takes in a text\\none word at a time, makes a probabilistic prediction about which\\nword will come next, and has its behavior incrementally adjusted\\nto make it assign a greater probability to the actual next word in\\nsimilar contexts in the future. Pretraining test loss measures how\\neffectively an LLM has learned to make these predictions. Eight Things to Know about Large Language Models\\nFigure 2. Excerpted from Wei et al. (2022a): Evaluations of performance on speciﬁc tasks or behaviors in LLMs do not generally show\\npredictable trends, and it is common for new behaviors to emerge abruptly when transitioning from a less resource-intensive version of a\\nmodel to a more resource-intensive one.\\nWei et al. show that the tasks in BIG-Bench (Srivastava et al.,\\n2022), a standard broad-coverage benchmark for LLM abili-\\nties, show a range of different kinds of trend that collectively\\nmake scaling-law style predictions unreliable (Figure 3).\\nThis means that when a lab invests in training a new LLM\\nthat advances the scale frontier, they’re buying a mystery\\nbox: They’re justiﬁably conﬁdent that they’ll get a variety of\\neconomically valuable new capabilities, but they can make\\nfew conﬁdent predictions about what those capabilities will\\nbe or what preparations they’ll need to make to be able to\\ndeploy them responsibly.\\nConcretely, two of the key behaviors in GPT-3 that set it\\napart as the ﬁrst modern LLM are that it shows few-shot\\nlearning , the ability to learn a new task from a handful\\nof examples in a single interaction, and chain-of-thought\\nreasoning , the ability to write out its reasoning on hard\\ntasks when requested, as a student might do on a math\\ntest, and to show better performance as a result. GPT-3’s\\ncapacity for few-shot learning on practical tasks appears\\nto have been discovered only after it was trained, and its\\ncapacity for chain-of-thought reasoning was discovered only\\nseveral months after it was broadly deployed to the public\\n(Nye et al., 2021; Wei et al., 2022b; Kojima et al., 2022;\\nZhou et al., 2023).2In addition, model abilities involving\\nprogramming, arithmetic, defusing misconceptions, and\\nanswering exam questions in many domains show abrupt\\n2See Branwen (n.d.) for a survey that includes additional un-\\npublished reports of this behavior.improvements as models are scaled up (Wei et al., 2022a;\\nSrivastava et al., 2022).\\nThere are few widely agreed-upon limits to what capabil-\\nities could emerge in future LLMs. While there are some\\nhard constraints on the behaviors of typical current LLMs—\\nstemming from limits on the amount of text they can use\\nas input at any one time, limits on their ability to interact\\nwith the world during training, or limits on the amount of\\ncomputation they can perform for each word they generate—\\nit is arguably plausible that these will be overcome with\\nfurther research within the same technical paradigm. How-\\never, many experts disagree: 51% of language-technology\\nresearchers surveyed in spring 2022 agreed that “expert-\\ndesigned strong inductive biases ( `a la universal grammar,\\nsymbolic systems, or cognitively-inspired computational\\nprimitives) will be necessary to practically solve some im-\\nportant real-world problems or applications in [language\\ntechnology]”, which if true would represent a limit to the\\nLLM paradigm (Michael et al., 2022).\\nExpert forecasts, however, have often predicted that we\\nwould see less progress with LLMs than has actually oc-\\ncurred. While forecasts by technology researchers are of-\\nten informal, and I am aware of no precise evaluation of\\ntheir accuracy, we do have a crisp example of experienced\\nprofessional forecasters making similar mistakes: Stein-\\nhardt (2022) presents results from a competition that was\\norganized in summer 2021, which gave forecasters access\\nto experts, extensive evidence, and a cash incentive, and Eight Things to Know about Large Language Models\\nEmergent abilities \\n67/202 tasks (33%): \\nPerformance is random for \\nsmall models, well above \\nrandom for large models \\nNo correlation \\n27/202 tasks (13%): \\nPerformance shows no \\nconsistent relationship with \\nscale \\nInverse scaling \\n5/202 tasks (2.5%): \\nPerformance decreases with \\nscale Flat \\n45/202 tasks (22%): \\nAll models perform at random \\nchance Smoothly increasing \\n58/202 tasks (29%): \\nPerformance increases \\npredictably with scale \\nFigure 3. Adapted from a ﬁgure by Jason Wei based on data from Wei et al. (2022a): The 202 tasks evaluated in the language-technology\\nbenchmark BIG-Bench (Srivastava et al., 2022) tend to show improved performance with scale overall, but individually they can improve\\ngradually, improve abruptly, stay level, get worse, or vacillate, making it impossible to extrapolate the performance of some future system\\nconﬁdently.\\nasked them to predict what state-of-the-art performance\\nwith LLMs would be in each of the next four years on two\\nspeciﬁc tasks. The results from summer 2022, only one\\nyear into the competition, substantially exceeded what the\\nconsensus forecast said would be possible in 2024 . Results\\nwith GPT-4 in early 2023 exceeded the consensus forecast\\nfor2025 on the one measure for which we have reported\\nresults (OpenAI, 2023b). This suggests that it is worth plan-\\nning for the possibility that we continue to see fast technical\\nprogress.\\n3. LLMs often appear to learn and use\\nrepresentations of the outside world\\nThere is increasingly substantial evidence that LLMs de-\\nvelop internal representations of the world to some extent,\\nand that these representations allow them to reason at a level\\nof abstraction that is not sensitive to the precise linguis-\\ntic form of the text that they are reasoning about. Current\\nLLMs seem to do this only weakly and sporadically, but\\nthe evidence for this phenomenon is clearest in the largest\\nand most recent models, such that we should expect it to\\nbecome more robust as systems are scaled up further.\\nEvidence for this claim includes the following results, span-\\nning many established experimental methods and models:\\n•Models’ internal representations of color words closely\\nmirror objective facts about human color perception\\n(Abdou et al., 2021; Patel & Pavlick, 2022; Søgaard,\\n2023).\\n•Models can make inferences about what the author of a\\ndocument knows or believes and use these inferences to\\npredict how the document will be continued (Andreas,\\n2022).\\nFigure 4. Excerpted from Bubeck et al. (2023): An popular in-\\nformal (and potentially cherry-picked) demonstration of LLMs’\\nability to manipulate visual representations. Here, a private ver-\\nsion of GPT-4, trained without any access to visual information, is\\nasked to write instructions in a graphics programming language\\nto draw a unicorn. During the model’s training (left to right), the\\nresulting drawings appear to become more competent.\\n•Models use internal representations of the properties\\nand locations of objects described in stories, which\\nevolve as more information about these objects is re-\\nvealed (Li et al., 2021). This can include the ability to\\ninternally represent the spatial layout of the setting of\\na story (Patel & Pavlick, 2022; Bubeck et al., 2023).\\nModels also use similar representations for facts about\\nreal-world geography (Li ´etard et al., 2021).\\n•Models can at least sometimes give instructions de-\\nscribing how to draw novel objects (Figure 4; Bubeck\\net al., 2023).\\n•Models that are trained to play board games from de-\\nscriptions of individual game moves, without ever see-\\ning a full depiction of the game board, learn internal\\nrepresentations of the state of the board at each turn\\n(Li et al., 2023).\\n•Models can distinguish common misconceptions from\\ntrue facts (Wei et al., 2022a), and often show well-\\ncalibrated internal representations for how likely a\\nclaim is to be true (Kadavath et al., 2022; Wei et al., Eight Things to Know about Large Language Models\\n2022a; Burns et al., 2023).\\n•Models pass many tests designed to measure common-\\nsense reasoning, including some like the Winograd\\nSchema Challenge that are explicitly designed to in-\\nclude no purely textual clues to the answer (Levesque\\net al., 2012; He et al., 2021; OpenAI, 2023b).\\nThese results are in tension, at least to some extent, with\\nthe common intuition that LLMs are nothing but statistical\\nnext-word predictors , and therefore cannot learn or reason\\nabout anything but text. While the premise of this intuition\\nis technically correct in some cases, it can paint a misleading\\npicture of the often-rich representations of the world that\\nLLMs develop as they are trained. In addition, LLMs are\\nincreasingly often augmented with other ways of learning\\nabout the world that make this claim literally false, such as\\nthrough interactive training methods (Ziegler et al., 2019;\\nStiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022a),\\nintegration with image processing systems, (Alayrac et al.,\\n2022; OpenAI, 2023b), or integration with other software\\ntools (Nakano et al., 2021; Menick et al., 2022; Collins et al.,\\n2022; Schick et al., 2023; OpenAI, 2023a).\\n4. There are no reliable techniques for\\nsteering the behavior of LLMs\\nMuch of the expense of developing an LLM goes into\\nlanguage-model pretraining: The process of training a neu-\\nral network to predict how random samples of human-\\nwritten text will be continued. In most cases, though, the de-\\nvelopers of such a system want to use it for tasks other than\\npredicting continuations, which requires that it be adapted\\nor guided in some way. Even building a general-purpose\\ninstruction-following model, where one is not attempting\\nto specialize on any particular task, requires this kind of\\nadaptation: Otherwise, the model will attempt to continue\\nits instructions rather than following them (Ouyang et al.,\\n2022).\\nThis adaptation typically involves one or more of these three\\ntechniques:\\n1.Plain language model prompting, where one prepares\\nan incomplete text like “ The translation of ‘cat’ in\\nFrench is‘ ”, such that a typical continuation of the\\ntext should represent a completion of the intended task\\n(Radford et al., 2019; Raffel et al., 2020).3\\n2.Supervised ﬁne-tuning, where one trains the model to\\n3Prompting in the more general sense can describe the prac-\\ntice of writing instructions or requests for an LLM, where these\\ninstructions and requests need not have this continuation property.\\nThe base models produced by language-model pretraining do not\\nsupport this kind of prompting.match high-quality human demonstrations on the task\\n(Radford et al., 2018; Devlin et al., 2019; Ouyang et al.,\\n2022).\\n3.Reinforcement learning, where one incrementally\\nweakens or strengthens certain model behaviors ac-\\ncording to preference judgments from a human tester\\nor user (Ziegler et al., 2019; Stiennon et al., 2020;\\nOuyang et al., 2022; Bai et al., 2022a).\\nThese techniques produce useful systems, but they are far\\nfrom perfectly effective: They can’t guarantee that an AI\\nmodel will behave appropriately in every plausible situation\\nit will face in deployment. Nor can they even make a model\\ntryto behave appropriately to the extent possible given its\\nskills and knowledge (to the extent that it can be said to have\\ngeneralizable skills or knowledge). In particular, models can\\nmisinterpret ambiguous prompts or incentives in unreason-\\nable ways, including in situations that appear unambiguous\\nto humans, leading them to behave unexpectedly (D’Amour\\net al., 2020; Kenton et al., 2021).\\nIn one key way, this problem is getting easier to tackle: As\\nLLMs become more capable of using human language and\\nhuman concepts, they also become more capable of learning\\nthe generalizations we would like. Indeed, many control\\ntechniques work better with larger models, at least for sim-\\nple tasks(Hendrycks et al., 2020; Bai et al., 2022a; Chung\\net al., 2022; Ganguli et al., 2023). In another important\\nway, though, the problem is becoming more difﬁcult: More\\ncapable models can better recognize the speciﬁc circum-\\nstances under which they are trained. Because of this, they\\nare more likely to learn to act as expected in precisely those\\ncircumstances while behaving competently but unexpect-\\nedly in others. This can surface in the form of problems that\\nPerez et al. (2022) call sycophancy , where a model answers\\nsubjective questions in a way that ﬂatters their user’s stated\\nbeliefs, and sandbagging , where models are more likely to\\nendorse common misconceptions when their user appears\\nto be less educated. It seems likely that issues like these\\nplayed some role in the bizarre, manipulative behavior that\\nearly versions of Microsoft Bing Chat showed, despite the\\nsystem having been tested extensively before launch (Roose,\\n2023; Perrigo, 2023; Mehdi, 2023).\\nThough there has been some progress in understanding and\\nmitigating these issues, there is no consensus on whether\\nor how we will be able to deeply solve them, and there is\\nincreasing concern that they will become catastrophic when\\nexhibited in larger-scale future systems (Amodei et al., 2016;\\nBommasani et al., 2021; Saunders et al., 2022; Ngo, 2022).\\nSome experts believe that future systems trained by similar\\nmeans, even if they perform well during pre-deployment\\ntesting, could fail in increasingly dramatic ways, including\\nstrategically manipulating humans to acquire power (Hub- Eight Things to Know about Large Language Models\\ninger et al., 2019; Turner et al., 2021; Di Langosco et al.,\\n2022; Ngo, 2022; Turner & Tadepalli, 2022). Broad surveys\\nof the ﬁeld suggest that these concerns are fairly broadly\\nshared: The majority of the 738 researchers who responded\\nto a recent survey (targeting those who published recently\\nat the machine-learning venues NeurIPS and ICML) as-\\nsigned a greater than 10% chance of “human inability to\\ncontrol future advanced AI systems causing human extinc-\\ntion” (Stein-Perlman et al., 2020). 36% of another sample of\\n480 researchers (in a survey targeting the language-speciﬁc\\nvenue ACL) agreed that “It is plausible that decisions made\\nby AI or machine learning systems could cause a catastrophe\\nthis century that is at least as bad as an all-out nuclear war”\\n(Michael et al., 2022). Hundreds of researchers recently\\nsigned a controversial open letter that calls for a morato-\\nrium on large-scale LLM training until adequate safety and\\ngovernance mechanisms can be put in place (Bengio et al.,\\n2023).\\n5. Experts are not yet able to interpret the\\ninner workings of LLMs\\nModern LLMs are built on artiﬁcial neural networks: They\\nwork by computing and updating numeric activation values\\nfor internal components that are very loosely modeled on\\nhuman neurons (Bengio et al., 2017). On this analogy, our\\ntools for doing neuroscience on these systems are still weak:\\nWe have some coarse tools for testing whether models rep-\\nresent a few speciﬁc kinds of information (like the color\\nresults discussed in Section 3), but as of early 2023, there\\nis no technique that would allow us to lay out in any satis-\\nfactory way what kinds of knowledge, reasoning, or goals a\\nmodel is using when it produces some output.\\nWhile there is ongoing research oriented toward this goal\\n(Elhage et al., 2021; Lovering & Pavlick, 2022; Chan et al.,\\n2022; Burns et al., 2023; Li et al., 2023, i.a.), the problem\\nis deeply difﬁcult: There are hundreds of billions of con-\\nnections between these artiﬁcial neurons, some of which\\nare invoked many times during the processing of a single\\npiece of text, such that any attempt at a precise explanation\\nof an LLM’s behavior is doomed to be too complex for any\\nhuman to understand. Often, ad-hoc techniques that at ﬁrst\\nseem to provide insight into the behavior of an LLM are\\nlater found to be severely misleading (Feng et al., 2018;\\nJain & Wallace, 2019; Bolukbasi et al., 2021; Wang et al.,\\n2022). In addition, promising-looking techniques that elicit\\nreasoning in natural language do not reliably correspond to\\nthe processes that LLMs use to reason, and model-generated\\nexplanations can also be systematically misleading (Lipton,\\n2018; Christiano, 2022; Uesato et al., 2022).6. Human performance on a task isn’t an\\nupper bound on LLM performance\\nWhile LLMs are trained primarily to imitate human writing\\nbehavior, they can at least potentially outperform humans on\\nmany tasks. This is for two reasons: First, they are trained\\non far more data than any human sees,4giving them much\\nmore information to memorize and potentially synthesize.\\nIn addition, they are often given additional training using\\nreinforcement learning before being deployed (Stiennon\\net al., 2020; Ouyang et al., 2022; Bai et al., 2022a), which\\ntrains them to produce responses that humans ﬁnd helpful\\nwithout requiring humans to demonstrate such helpful be-\\nhavior. This is analogous to the techniques used to produce\\nsuperhuman performance at games like Go (Silver et al.,\\n2016). Concretely, LLMs appear to be much better than\\nhumans at their pretraining task of predicting which word is\\nmost likely to appear after some seed piece of text (Shlegeris\\net al., 2022), and humans can teach LLMs to do some simple\\ntasks more accurately than the humans themselves (Stiennon\\net al., 2020).\\n7. LLMs need not express the values of their\\ncreators nor the values encoded in web text\\nWhen a plain pretrained LLM produces text, that text will\\ngenerally resemble the text it was trained on. This includes\\na resemblance in the values expressed by the text: Mod-\\nels mirror their training data in the explicit statements they\\nproduce on value-laden topics and in the implicit biases\\nbehind their writing. However, these values are subject to a\\ngood degree of control by their developers, especially when\\nthe plain pretrained LLM is given further prompting and\\ntraining to adapt it for deployment as a product (Section 4).\\nThis means that the values expressed in a deployed LLM’s\\nbehavior do not need to reﬂect some average of the values\\nexpressed in its training data. This also opens up opportu-\\nnities for third-party input and oversight, meaning that the\\nvalues expressed in these models also need not reﬂect the\\nvalues of the speciﬁc people and organizations who build\\nthem.\\nIn particular, popular approaches involving reinforcement\\nlearning and red-teaming allow model developers to guide\\nmodels toward a persona and set of values more or less of\\ntheir choosing (Dinan et al., 2019; Bai et al., 2022a; Ganguli\\net al., 2022b). In these techniques, the values that a model\\nlearns are never made entirely explicit. Instead, they are\\nreﬂected in many small pieces of feedback that human an-\\nnotators give the model during training. The constitutional\\n4LLMs see over 10;000\\x02more data than humans: A human\\nadolescent sees tens of thousands of words, while LLMs can be\\nexposed to over one trillion (Hart & Risley, 1992; Gilkerson et al.,\\n2017; Hoffmann et al., 2022) Eight Things to Know about Large Language Models\\nAItechnique (Bai et al., 2022b) signiﬁcantly cuts down on\\nhuman labor and makes these values more explicit: Using\\nthis technique, a model can be trained to follow a set of\\nnorms and values simply by writing those values down in\\nthe form of a list of constraints called a constitution. It is\\npossible to use techniques like this to dramatically reduce\\nexplicit examples of widely-recognized biases, like anti-\\nBlack racism, in model behavior (Ganguli et al., 2023).5\\nIndeed, in some cases, exposing models to more examples\\nof unwanted behavior during pretraining can make it easier\\nto make them avoid that behavior in deployment, reversing\\nthe intuitive link between training data and model behavior\\n(Korbak et al. 2023; see also Appendix C in Chung et al.\\n2022).\\nThese technical interventions, especially constitutional AI,\\nare amenable to outside inﬂuence and regulation. One can\\neasily imagine third-party standards bodies collecting input\\nabout what behaviors are acceptable in AI systems and\\ndistilling this input into constitutions that model developers\\nare encouraged or required to adopt.\\nAs in Section 4, though, these techniques can still fail in\\nsubtle and surprising ways, and the trends in how these tech-\\nniques change as models with scale are complex. And, of\\ncourse, there are many other ethical questions that arise with\\nthe development of deployment of large-scale AI systems,\\nincluding issues around environmental impacts, access, mis-\\nuse, privacy, safety, and the concentration of power (Amodei\\net al., 2016; Bender et al., 2021; Bommasani et al., 2021;\\nBirhane et al., 2022; Weidinger et al., 2022, i.a.).\\n8. Brief interactions with LLMs are often\\nmisleading\\nWhile many deployed LLMs are largely able to follow in-\\nstructions, this instruction-following behavior isn’t inherent\\nto the model, but rather is grafted onto it using highly im-\\nperfect tools (Section 4). In part because of this, models\\ncan be sensitive to the contents of their instructions in id-\\niosyncratic ways. Often, a model will fail to complete a task\\nwhen asked, but will then perform the task correctly once\\nthe request is reworded or reframed slightly, leading to the\\nemerging craft of prompt engineering (Brown et al., 2020;\\nReynolds & McDonell, 2021; Radford et al., 2021; Dohan\\net al., 2022; White et al., 2023; Si et al., 2023).\\nThese contingent failures are evidence that our techniques\\nfor controlling language models to follow instructions are\\nnot reliably effective. However, simply observing that an\\n5However, explicit demonstrations of racist language or\\ndecision-making by models do not come close to exhausting the\\nways that the development and use of these systems interact with\\nbiases and power structures involving factors like race (see, for\\nexample, Field et al., 2021).LLM fails at a task in some setting is not reliable evidence\\nthat that LLM doesn’t have the skills or knowledge to do\\nthat task. Often, once one ﬁnds an appropriate way to\\nprompt a model to do some task, one will ﬁnd that the\\nmodel consistently performs well across different instances\\nof the task. The chain-of-thought reasoning strategies men-\\ntioned in Section 2 are an especially clear example of this:\\nSimply prompting a model to “think step by step” can lead\\nit to perform well on entire categories of math and reason-\\ning problems that it would otherwise fail on (Kojima et al.,\\n2022). Similarly, even observing that an LLM consistently\\nfails at some task is far from sufﬁcient evidence that no\\nother LLM can do that task (Bowman, 2022).\\nOn the other hand, observing that an LLM performs a task\\nsuccessfully in one instance is not strong evidence that the\\nLLM is capable of performing that task in general, especially\\nif that example was cherry-picked as part of a demonstration\\n(like the unicorn in Figure 4). LLMs can memorize speciﬁc\\nexamples or strategies for solving tasks from their training\\ndata without internalizing the reasoning process that would\\nallow them to do those tasks robustly (see, e.g. McCoy et al.,\\n2019; Magar & Schwartz, 2022).\\n9. Discussion and Limitations\\nThe additional discussion that I present here builds on and\\ncontextualizes the eight claims above, but it is more specu-\\nlative or subjective in places and reﬂects views that are not\\nnecessarily broadly shared.\\n9.1. We should expect some of the prominent ﬂaws of\\ncurrent LLMs to improve signiﬁcantly\\nHallucination, the problem of LLMs inventing plausible\\nfalse claims, is a prominent ﬂaw in current systems and\\nsubstantially limits how they can be responsibly used. Some\\nof the recent ﬁndings discussed in Section 3 suggest, though,\\nthat we may soon be able to mitigate this problem simply\\nby ﬁnding ways to better use abilities that models already\\ndisplay: LLMs internally track which statements are true\\nwith reasonably high precision, and this ability improves\\nwith scale (Burns et al., 2023; Kadavath et al., 2022).\\nSimilarly, as noted in Section 7, recent methods can dramat-\\nically reduce explicit bias and toxicity in models’ output,\\nlargely by exploiting the fact that models can often recog-\\nnize these bad behaviors when asked (Dinan et al., 2019;\\nBai et al., 2022b; Ganguli et al., 2023). While these mitiga-\\ntions are unlikely to be entirely robust, the prevalence and\\nprominence of these bad behaviors will likely wane as these\\ntechniques are reﬁned.\\nTo be clear, though, these encouraging signs do not mean\\nthat we can reliably control these models, and the issues\\nnoted in Section 4 still apply. Our partial solutions are Eight Things to Know about Large Language Models\\nlikely to leave open important failure modes. For exam-\\nple, straightforward attempts to manage hallucination are\\nlikely to fail silently in a way that leaves them looking more\\ntrustworthy than they are because of issues related to sand-\\nbagging: If we apply standard methods to train some future\\nLLM to tell the truth, but that LLM can reasonably accu-\\nrately predict which factual claims human data workers are\\nlikely to check, this can easily lead the LLM to tell the truth\\nonly when making claims that are likely to be checked .\\n9.2. There will be incentives to deploy LLMs as agents\\nthat ﬂexibly pursue goals\\nIncreasingly capable LLMs, with increasingly accurate and\\nusable internal models of the world, are likely to be able to\\ntake on increasingly open-ended tasks that involve making\\nand executing novel plans to optimize for outcomes in the\\nworld (Chan et al., 2023). As these capabilities develop, eco-\\nnomic incentives suggest that we should see them deployed\\nin areas like software engineering or business strategy that\\ncombine measurable outcomes, a need for ﬂexible planning,\\nand relatively ﬂexible standards and regulations. LLMs aug-\\nmented with additional tools can extend this into grounded\\ndomains like robotics (Sharma et al., 2022; Driess et al.,\\n2023). Deployments of this type would increasingly often\\nplace LLMs in unfamiliar situations created as a result of the\\nsystems’ own actions, further reducing the degree to which\\ntheir developers can predict and control their behavior. This\\nis likely to increase the rate of simple errors that render these\\nsystems ineffective as agents in some settings. But it is also\\nlikely to increase the risk of much more dangerous errors\\nthat cause a system to remain effective while strategically\\npursuing the wrong goal (Krueger et al., 2020; Ortega et al.,\\n2021; Chan et al., 2023).\\n9.3. LLM developers have limited inﬂuence over what\\nis developed\\nBecause many important LLM capabilities are emergent\\nand difﬁcult to predict, LLM developers have relatively lit-\\ntle inﬂuence on precisely what capabilities future LLMs\\nwill have, and efforts to make predictions about future LLM\\ncapabilities based on the economic incentives, values, or\\npersonalities of their developers are likely to fail. GPT-4,\\nfor example, appears to have many skills, like those involv-\\ning programming, that its creators were likely hoping for.\\nHowever, it also appears to have initially shown several un-\\nwanted skills, like teaching laypeople to prepare biological\\nweapons, that its creators had to spend signiﬁcant effort to\\ntry to remove (OpenAI, 2023b).\\nBeyond this, LLM developers inevitably also have limited\\nawareness of what capabilities an LLM has when they’re\\ndeciding whether to deploy it: There is no known evaluation\\nor analysis procedure that can rule out surprises like chain-of-thought reasoning in GPT-3, where users discover a way\\nto elicit some important new behavior that the developers\\nhad not been aware of.\\n9.4. LLMs are likely to produce a rapidly growing\\narray of risks\\nMore broadly, the current technical and commercial land-\\nscape provides strong incentives to build and deploy in-\\ncreasingly capable LLMs quickly. Nonetheless, our track\\nrecord of recognizing what capabilities a new LLM can\\ndemonstrate before deploying it is spotty. Our techniques\\nfor controlling systems are weak and are likely to break\\ndown further when applied to highly capable models. Given\\nall this, it is reasonable to expect a substantial increase and\\na substantial qualitative change in the range of misuse risks\\nand model misbehaviors that emerge from the development\\nand deployment of LLMs.\\nWhile many positive applications of LLM-based systems\\nare likely to be genuinely valuable, the societal cost-beneﬁt\\ntradeoffs involved in their deployment are likely to remain\\ndifﬁcult or impossible to evaluate in advance, at least with-\\nout signiﬁcant progress on hard technical problems in model\\nevaluation, interpretability, and control. Some of these hard-\\nto-evaluate risks, such as those involving unconventional\\nweapons or strategic power-seeking behavior, may be impos-\\nsible to adequately mitigate if they are discovered only after\\nsystems are deployed. Strategic power-seeking behavior in\\nparticular could pose serious risks during model develop-\\nment , even without an intentional deployment. This suggests\\nthat future work in this area will likely warrant increasingly\\nstringent standards for safety, security, and oversight.\\n9.5. Negative results with LLMs can be difﬁcult to\\ninterpret but point to areas of real weakness\\nThere are many sound scientiﬁc results showing that recent\\nLLMs fail at language and commonsense reasoning tasks,\\nsometimes relatively simple ones, under good-faith attempts\\nto elicit good behavior (Pandia & Ettinger, 2021; Schuster &\\nLinzen, 2022). Sometimes the details of these failures cast\\ndoubts on the quality of other related evaluations (Webson\\n& Pavlick, 2022; Ullman, 2023). For reasons mentioned\\nin Section 8, positive results on well-designed measures\\nare much more reliable than negative results. Nonetheless,\\nin some areas, including areas as simple as the handling\\nof negation,6LLMs show what appear to be systematic\\nweaknesses in their ability to process language or reason\\nabout the world. We have few grounds to predict whether\\nor when these limitations will be resolved.\\n6See, for example, the Modus Tollens task by Huang and Wur-\\ngaft, described in McKenzie et al. (2022). Eight Things to Know about Large Language Models\\n9.6. The science and scholarship around LLMs is\\nespecially immature\\nLLMs strain the methods and paradigms of the ﬁelds that\\none would expect to be best qualiﬁed to study them. Natural\\nlanguage processing (or language technology) is the historic\\nhome discipline for this work, but its tools are oriented to-\\nward measuring and improving the ability of computational\\nsystems to use language. While LLMs fundamentally learn\\nand interact through language, many of the most pressing\\nquestions about their behavior and capabilities are not pri-\\nmarily questions about language use. The interdisciplinary\\nﬁelds studying AI policy and AI ethics have developed con-\\nceptual and normative frameworks for thinking about the\\ndeployment of many kinds of AI system. However, these\\nframeworks often assume that AI systems are more pre-\\ncisely subject to the intentions of their human owners and\\ndevelopers, or to the statistics of their training data, than has\\nbeen the case with recent LLMs (Chan et al., 2023). Relat-\\nedly, many of the most cited research papers dealing with\\nLLMs, including many papers that introduce new methods\\nor theories, are not published in peer-reviewed venues. The\\nrecent trend toward limiting access to LLMs and treating\\nthe details of LLM training as proprietary information is\\nalso an obstacle to scientiﬁc study.\\nThis means that surprising novel claims about LLMs are\\noften the product of messy, fallible science that goes beyond\\nestablished disciplinary practice. However, what appears\\nto be established conventional wisdom also often rests on\\nshaky foundations when it is applied to LLMs. All of this is\\nreason to be especially uncertain about the issues discussed\\nin this paper and to make important decisions about LLMs\\nin ways that are resilient to mistaken assumptions.\\nConclusion\\nIn closing, rather than recap the claims above, I would like\\nto note three sometimes-prominent issues that this paper\\nleaves largely untouched:\\n•Open debates over whether we describe LLMs as un-\\nderstanding language, and whether to describe their\\nactions using agency-related words like know ortry,\\nare largely separate from the questions that I discuss\\nhere (Bender & Koller, 2020; Michael, 2020; Potts,\\n2020). We can evaluate whether systems are effective\\nor ineffective, reliable or unreliable, interpretable or\\nuninterpretable, and improving quickly or slowly, re-\\ngardless of whether they are underlyingly human-like\\nin the sense that these words evoke.\\n•Similarly, questions of consciousness , sentience,\\nrights, and moral patienthood in LLMs (see, e.g.\\nSchwitzgebel & Garza, 2020; Shevlin, 2021; Chalmers,2023), are worth distinguishing from the issues above.\\nThough these questions may inﬂuence important de-\\ncisions about how AI systems are built and used, it\\nshould be possible to evaluate most or all of the issues\\nraised here without taking a stance on these questions.\\n•Finally, value judgments around LLMs are beyond\\nthe scope of this paper. The broader question of\\nwhether the rapid progress that we’re seeing with\\nLLMs is a good thing, and what we should each do\\nabout it, depends on a deeper and more diverse range\\nof considerations than the technical literature that I\\ndraw on here can come close to addressing.\\nAcknowledgments\\nThis paper beneﬁted from conversations at the AI FU-\\nTURES panel organized by Critical AI at Rutgers and\\nfrom discussions with many other researchers, including\\nEllie Pavlick, Jackson Petty, Owain Evans, Adam Jermyn,\\nEric Drexler, Ben Garﬁnkel, Richard Ngo, Jason Wei, He-\\nlen Toner, Jeffrey Ladish, Leo Gao, Alex Lyzhov, Julian\\nMichael, Adam Bales, Rick Korzekwa, Ben Mann, Alex\\nLawsen, Alex Tamkin, Anton Korinek, and David Dohan. I\\nused LLMs in this paper only to explore some minor word-\\ning and framing decisions. All errors and omissions are, of\\ncourse, my own.\\nThis work has beneﬁted from ﬁnancial support from Eric\\nand Wendy Schmidt (made by recommendation of the\\nSchmidt Futures program) and from Open Philanthropy,\\nas well as from in-kind editing support from Pablo Moreno\\nthrough FAR.ai. This material is based upon work sup-\\nported by the National Science Foundation under Grant\\nNos. 1922658 and 2046556. Any opinions, ﬁndings, and\\nconclusions or recommendations expressed in this material\\nare those of the author(s) and do not necessarily reﬂect the\\nviews of the National Science Foundation.\\nReferences\\nAbdou, M., Kulmizev, A., Hershcovich, D., Frank, S.,\\nPavlick, E., and Søgaard, A. Can language models encode\\nperceptual structure without grounding? a case study in\\ncolor. In Proceedings of the 25th Conference on Computa-\\ntional Natural Language Learning , pp. 109–132, Online,\\nNovember 2021. Association for Computational Linguis-\\ntics. doi: 10.18653/v1/2021.conll-1.9. URL https:\\n//aclanthology.org/2021.conll-1.9 .\\nAlayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I.,\\nHasson, Y ., Lenc, K., Mensch, A., Millican, K., Reynolds,\\nM., et al. Flamingo: a visual language model for few-shot\\nlearning. Advances in Neural Information Processing\\nSystems , 35:23716–23736, 2022. Eight Things to Know about Large Language Models\\nAmodei, D., Olah, C., Steinhardt, J., Christiano, P., Schul-\\nman, J., and Man ´e, D. Concrete problems in AI safety.\\narXiv preprint 1606.06565 , 2016.\\nAndreas, J. Language models as agent models. In Find-\\nings of the Association for Computational Linguistics:\\nEMNLP 2022 , pp. 5769–5779, Abu Dhabi, United Arab\\nEmirates, December 2022. Association for Computa-\\ntional Linguistics. URL https://aclanthology\\n.org/2022.findings-emnlp.423 .\\nBai, Y ., Jones, A., Ndousse, K., Askell, A., Chen, A., Das-\\nSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T.,\\net al. Training a helpful and harmless assistant with rein-\\nforcement learning from human feedback. arXiv preprint\\n2204.05862 , 2022a.\\nBai, Y ., Kadavath, S., Kundu, S., Askell, A., Kernion, J.,\\nJones, A., Chen, A., Goldie, A., Mirhoseini, A., McKin-\\nnon, C., et al. Constitutional AI: Harmlessness from AI\\nfeedback. arXiv preprint 2212.08073 , 2022b.\\nBartz, D. As ChatGPT’s popularity explodes, U.S. law-\\nmakers take an interest. Reuters , 2023. URL https:\\n//www.reuters.com/technology/chatgpt\\ns-popularity-explodes-us-lawmakers-t\\nake-an-interest-2023-02-13/ .\\nBender, E. M. and Koller, A. Climbing towards NLU: On\\nmeaning, form, and understanding in the age of data. In\\nProceedings of the 58th annual meeting of the association\\nfor computational linguistics , pp. 5185–5198, 2020.\\nBender, E. M., Gebru, T., McMillan-Major, A., and\\nShmitchell, S. On the dangers of stochastic parrots: Can\\nlanguage models be too big? In Proceedings of the\\n2021 ACM Conference on Fairness, Accountability, and\\nTransparency , FAccT ’21, pp. 610–623, New York, NY ,\\nUSA, 2021. Association for Computing Machinery. ISBN\\n9781450383097. doi: 10.1145/3442188.3445922. URL\\nhttps://doi.org/10.1145/3442188.3445\\n922.\\nBengio, Y ., Goodfellow, I., and Courville, A. Deep learn-\\ning. MIT press Cambridge, MA, USA, 2017. ISBN\\n9780262035613.\\nBengio, Y ., Russell, S., Musk, E., Wozniak, S., et al. Pause\\ngiant AI experiments. Future of Life Institute Open Let-\\nters, 2023. URL https://futureoflife.org/o\\npen-letter/pause-giant-ai-experiment\\ns/.\\nBirhane, A., Kalluri, P., Card, D., Agnew, W., Dotan, R., and\\nBao, M. The values encoded in machine learning research.\\nIn2022 ACM Conference on Fairness, Accountability,\\nand Transparency , pp. 173–184, 2022.Biswas, S. ChatGPT and the future of medical writing.\\nRadiology , pp. 223312, 2023.\\nBolukbasi, T., Pearce, A., Yuan, A., Coenen, A., Reif, E.,\\nVi´egas, F., and Wattenberg, M. An interpretability illusion\\nfor BERT. arXiv preprint 2104.07143 , 2021.\\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R.,\\nArora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosse-\\nlut, A., Brunskill, E., et al. On the opportunities and risks\\nof foundation models. arXiv preprint 2108.07258 , 2021.\\nBowman, S. The dangers of underclaiming: Reasons for\\ncaution when reporting how NLP systems fail. In Pro-\\nceedings of the 60th Annual Meeting of the Association\\nfor Computational Linguistics (Volume 1: Long Papers) ,\\npp. 7484–7499, Dublin, Ireland, May 2022. Association\\nfor Computational Linguistics. doi: 10.18653/v1/2022\\n.acl-long.516. URL https://aclanthology.org\\n/2022.acl-long.516 .\\nBranwen, G. Inner monologue (AI), n.d. URL https:\\n//gwern.net/doc/ai/nn/transformer/gp\\nt/inner-monologue/index .\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\\nAskell, A., et al. Language models are few-shot learners.\\nAdvances in neural information processing systems , 33:\\n1877–1901, 2020.\\nBubeck, S., Chandrasekaran, V ., Eldan, R., Gehrke, J.,\\nHorvitz, E., Kamar, E., Lee, P., Lee, Y . T., Li, Y .,\\nLundberg, S., et al. Sparks of artiﬁcial general intel-\\nligence: Early experiments with GPT-4. arXiv preprint\\n2303.12712 , 2023.\\nBurns, C., Ye, H., Klein, D., and Steinhardt, J. Discovering\\nlatent knowledge in language models without supervision.\\nInThe Eleventh International Conference on Learning\\nRepresentations , 2023. URL https://openreview\\n.net/forum?id=ETKGuby0hcs .\\nCapoot, A. Microsoft announces new multibillion-dollar\\ninvestment in ChatGPT-maker OpenAI. CNBC , 2023.\\nURL https://www.cnbc.com/2023/01/23/\\nmicrosoft-announces-multibillion-dol\\nlar-investment-in-chatgpt-maker-open\\nai.html .\\nChalmers, D. J. Could a large language model be conscious?\\narXiv preprint 2303.07103 , 2023.\\nChan, A. GPT-3 and InstructGPT: technological dystopi-\\nanism, utopianism, and “contextual” perspectives in AI\\nethics and industry. AI and Ethics , pp. 1–12, 2022. Eight Things to Know about Large Language Models\\nChan, A., Salganik, R., Markelius, A., Pang, C., Rajkumar,\\nN., Krasheninnikov, D., Langosco, L., He, Z., Duan,\\nY ., Carroll, M., et al. Harms from increasingly agentic\\nalgorithmic systems. arXiv preprint 2302.10329 , 2023.\\nChan, L., Garriga-Alonso, A., Goldowsky-Dill, N., Green-\\nblatt, R., Nitishinskaya, J., Radhakrishnan, A., Shlegeris,\\nB., and Thomas, N. Causal scrubbing: a method for\\nrigorously testing interpretability hypotheses. Alignment\\nForum , 2022. URL https://www.alignmentfor\\num.org/posts/JvZhhzycHu2Yd57RN/causa\\nl-scrubbing-a-method-for-rigorously-\\ntesting .\\nChoi, J. H., Hickman, K. E., Monahan, A., and Schwarcz, D.\\nChatGPT goes to law school. Minnesota Legal Studies\\nResearch Paper , 23(03), 2023. doi: http://dx.doi.org/10.\\n2139/ssrn.4335905.\\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\\nG., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,\\nGehrmann, S., et al. PaLM: Scaling language modeling\\nwith pathways. arXiv preprint 2204.02311 , 2022.\\nChristiano, P. Eliciting latent knowledge. Medium , 2022.\\nURL https://ai-alignment.com/eliciti\\nng-latent-knowledge-f977478608fc .\\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y .,\\nFedus, W., Li, E., Wang, X., Dehghani, M., Brahma,\\nS., et al. Scaling instruction-ﬁnetuned language models.\\narXiv preprint 2210.11416 , 2022.\\nCollins, K. M., Wong, C., Feng, J., Wei, M., and Tenenbaum,\\nJ. B. Structured, ﬂexible, and robust: benchmarking and\\nimproving large language models towards more human-\\nlike behavior in out-of-distribution reasoning tasks. In\\n2022 Cognitive Science (CogSci) conference , 2022.\\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT:\\nPre-training of deep bidirectional transformers for lan-\\nguage understanding. In Proceedings of the 2019 Confer-\\nence of the North American Chapter of the Association for\\nComputational Linguistics: Human Language Technolo-\\ngies, Volume 1 (Long and Short Papers) , pp. 4171–4186,\\nMinneapolis, Minnesota, June 2019. Association for\\nComputational Linguistics. doi: 10.18653/v1/N19-1423.\\nURL https://aclanthology.org/N19-1423 .\\nDi Langosco, L. L., Koch, J., Sharkey, L. D., Pfau, J., and\\nKrueger, D. Goal misgeneralization in deep reinforce-\\nment learning. In International Conference on Machine\\nLearning , pp. 12004–12019. PMLR, 2022.\\nDinan, E., Humeau, S., Chintagunta, B., and Weston,\\nJ. Build it break it ﬁx it for dialogue safety: Robust-\\nness from adversarial human attack. In Proceedingsof the 2019 Conference on Empirical Methods in Nat-\\nural Language Processing and the 9th International Joint\\nConference on Natural Language Processing (EMNLP-\\nIJCNLP) , pp. 4537–4546, Hong Kong, China, November\\n2019. Association for Computational Linguistics. doi:\\n10.18653/v1/D19-1461. URL https://aclantho\\nlogy.org/D19-1461 .\\nDohan, D., Xu, W., Lewkowycz, A., Austin, J., Bieber, D.,\\nLopes, R. G., Wu, Y ., Michalewski, H., Saurous, R. A.,\\nSohl-Dickstein, J., et al. Language model cascades. In\\nBeyond Bayes workshop at ICML 2022 , 2022.\\nDriess, D., Xia, F., Sajjadi, M. S., Lynch, C., Chowdhery,\\nA., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu,\\nT., et al. PaLM-E: An embodied multimodal language\\nmodel. arXiv preprint 2303.03378 , 2023.\\nD’Amour, A., Heller, K., Moldovan, D., Adlam, B., Ali-\\npanahi, B., Beutel, A., Chen, C., Deaton, J., Eisenstein,\\nJ., Hoffman, M. D., et al. Underspeciﬁcation presents\\nchallenges for credibility in modern machine learning.\\nJournal of Machine Learning Research , 2020.\\nElhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph,\\nN., Mann, B., Askell, A., Bai, Y ., Chen, A., Conerly, T.,\\net al. A mathematical framework for transformer circuits.\\nTransformer Circuits Thread , 2021. URL https://tr\\nansformer-circuits.pub/2021/framewor\\nk/index.html .\\nFeng, S., Wallace, E., Grissom II, A., Iyyer, M., Rodriguez,\\nP., and Boyd-Graber, J. Pathologies of neural models\\nmake interpretations difﬁcult. In Proceedings of the\\n2018 Conference on Empirical Methods in Natural Lan-\\nguage Processing , pp. 3719–3728, Brussels, Belgium,\\nOctober-November 2018. Association for Computational\\nLinguistics. doi: 10.18653/v1/D18- 1407. URL\\nhttps://aclanthology.org/D18-1407 .\\nField, A., Blodgett, S. L., Waseem, Z., and Tsvetkov, Y .\\nA survey of race, racism, and anti-racism in NLP. In\\nProceedings of the 59th Annual Meeting of the Associa-\\ntion for Computational Linguistics and the 11th Interna-\\ntional Joint Conference on Natural Language Processing\\n(Volume 1: Long Papers) , pp. 1905–1925, Online, Au-\\ngust 2021. Association for Computational Linguistics.\\ndoi: 10.18653/v1/2021.acl-long.149. URL https:\\n//aclanthology.org/2021.acl-long.149 .\\nGanguli, D., Hernandez, D., Lovitt, L., Askell, A., Bai, Y .,\\nChen, A., Conerly, T., Dassarma, N., Drain, D., Elhage,\\nN., et al. Predictability and surprise in large generative\\nmodels. In 2022 ACM Conference on Fairness, Account-\\nability, and Transparency , pp. 1747–1764, 2022a. Eight Things to Know about Large Language Models\\nGanguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y .,\\nKadavath, S., Mann, B., Perez, E., Schiefer, N., Ndousse,\\nK., et al. Red teaming language models to reduce harms:\\nMethods, scaling behaviors, and lessons learned. arXiv\\npreprint 2209.07858 , 2022b.\\nGanguli, D., Askell, A., Schiefer, N., Liao, T., Luko ˇsi¯ut˙e,\\nK., Chen, A., Goldie, A., Mirhoseini, A., Olsson, C., Her-\\nnandez, D., et al. The capacity for moral self-correction\\nin large language models. arXiv preprint 2302.07459 ,\\n2023.\\nGilkerson, J., Richards, J. A., Warren, S. F., Montgomery,\\nJ. K., Greenwood, C. R., Kimbrough Oller, D., Hansen,\\nJ. H., and Paul, T. D. Mapping the early language envi-\\nronment using all-day recordings and automated analysis.\\nAmerican journal of speech-language pathology , 26(2):\\n248–265, 2017.\\nHart, B. and Risley, T. R. American parenting of language-\\nlearning children: Persisting differences in family-child\\ninteractions observed in natural home environments. De-\\nvelopmental psychology , 28(6):1096, 1992.\\nHe, P., Liu, X., Gao, J., and Chen, W. DeBERTa: Decoding-\\nenhanced BERT with Disentangled Attention. In Inter-\\nnational Conference on Learning Representations , 2021.\\nURL https://openreview.net/forum?id=\\nXPZIaotutsD .\\nHendrycks, D., Liu, X., Wallace, E., Dziedzic, A., Kr-\\nishnan, R., and Song, D. Pretrained transformers im-\\nprove out-of-distribution robustness. In Proceedings of\\nthe 58th Annual Meeting of the Association for Com-\\nputational Linguistics , pp. 2744–2751, Online, July\\n2020. Association for Computational Linguistics. doi:\\n10.18653/v1/2020.acl-main.244. URL https:\\n//aclanthology.org/2020.acl-main.244 .\\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,\\nCai, T., Rutherford, E., de las Casas, D., Hendricks, L. A.,\\nWelbl, J., Clark, A., Hennigan, T., Noland, E., Millican,\\nK., van den Driessche, G., Damoc, B., Guy, A., Osindero,\\nS., Simonyan, K., Elsen, E., Vinyals, O., Rae, J. W., and\\nSifre, L. An empirical analysis of compute-optimal large\\nlanguage model training. In Oh, A. H., Agarwal, A.,\\nBelgrave, D., and Cho, K. (eds.), Advances in Neural\\nInformation Processing Systems , 2022. URL https:\\n//openreview.net/forum?id=iBBcRUlOAPR .\\nHubinger, E., van Merwijk, C., Mikulik, V ., Skalse, J.,\\nand Garrabrant, S. Risks from learned optimization\\nin advanced machine learning systems. arXiv preprint\\n1906.01820 , 2019.\\nJ, P. and C, D. ChatGPT and large language models: what’s\\nthe risk? National Cyber Security Center , 2023. URLhttps://www.ncsc.gov.uk/blog-post/ch\\natgpt-and-large-language-models-what\\ns-the-risk .\\nJain, S. and Wallace, B. C. Attention is not Explana-\\ntion. In Proceedings of the 2019 Conference of the\\nNorth American Chapter of the Association for Com-\\nputational Linguistics: Human Language Technologies,\\nVolume 1 (Long and Short Papers) , pp. 3543–3556, Min-\\nneapolis, Minnesota, June 2019. Association for Compu-\\ntational Linguistics. doi: 10.18653/v1/N19-1357. URL\\nhttps://aclanthology.org/N19-1357 .\\nKadavath, S., Conerly, T., Askell, A., Henighan, T., Drain,\\nD., Perez, E., Schiefer, N., Dodds, Z. H., DasSarma, N.,\\nTran-Johnson, E., et al. Language models (mostly) know\\nwhat they know. arXiv preprint 2207.05221 , 2022.\\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\\nAmodei, D. Scaling laws for neural language models.\\narXiv preprint 2001.08361 , 2020.\\nKenton, Z., Everitt, T., Weidinger, L., Gabriel, I., Mikulik,\\nV ., and Irving, G. Alignment of language agents. arXiv\\npreprint 2103.14659 , 2021.\\nKlein, E. This changes everything. New York Times , 2023.\\nURL https://www.nytimes.com/2023/03/\\n12/opinion/chatbots-artificial-intel\\nligence-future-weirdness.html .\\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y ., and Iwasawa, Y .\\nLarge language models are zero-shot reasoners. In ICML\\n2022 Workshop on Knowledge Retrieval and Language\\nModels , 2022. URL https://openreview.net/f\\norum?id=6p3AuaHAFiN .\\nKorbak, T., Shi, K., Chen, A., Bhalerao, R., Buckley, C. L.,\\nPhang, J., Bowman, S. R., and Perez, E. Pretraining\\nlanguage models with human preferences. arXiv preprint\\n2302.08582 , 2023.\\nKrueger, D., Maharaj, T., and Leike, J. Hidden incen-\\ntives for auto-induced distributional shift. arXiv preprint\\n2009.09153 , 2020.\\nLevesque, H., Davis, E., and Morgenstern, L. The Winograd\\nschema challenge. In Thirteenth international conference\\non the principles of knowledge representation and rea-\\nsoning , 2012.\\nLi, B. Z., Nye, M., and Andreas, J. Implicit representa-\\ntions of meaning in neural language models. In Pro-\\nceedings of the 59th Annual Meeting of the Associa-\\ntion for Computational Linguistics and the 11th Inter-\\nnational Joint Conference on Natural Language Process-\\ning (Volume 1: Long Papers) , pp. 1813–1827, Online, Eight Things to Know about Large Language Models\\nAugust 2021. Association for Computational Linguistics.\\ndoi: 10.18653/v1/2021.acl-long.143. URL https:\\n//aclanthology.org/2021.acl-long.143 .\\nLi, K., Hopkins, A. K., Bau, D., Vi ´egas, F., Pﬁster, H.,\\nand Wattenberg, M. Emergent world representations:\\nExploring a sequence model trained on a synthetic task.\\nInThe Eleventh International Conference on Learning\\nRepresentations , 2023. URL https://openreview\\n.net/forum?id=DeG07 TcZvT .\\nLi´etard, B., Abdou, M., and Søgaard, A. Do language\\nmodels know the way to Rome? In Proceedings of the\\nFourth BlackboxNLP Workshop on Analyzing and Inter-\\npreting Neural Networks for NLP , pp. 510–517, Punta\\nCana, Dominican Republic, November 2021. Association\\nfor Computational Linguistics. doi: 10.18653/v1/2021.b\\nlackboxnlp-1.40. URL https://aclanthology.o\\nrg/2021.blackboxnlp-1.40 .\\nLieu, T. I’m a congressman who codes. A.I. freaks me out.\\nNew York Times , 2023. URL https://www.nytime\\ns.com/2023/01/23/opinion/ted-lieu-ai\\n-chatgpt-congress.html .\\nLipton, Z. C. The mythos of model interpretability: In\\nmachine learning, the concept of interpretability is both\\nimportant and slippery. Queue , 16(3):31–57, 2018.\\nLovering, C. and Pavlick, E. Unit testing for concepts in\\nneural networks. Transactions of the Association for\\nComputational Linguistics , 10:1193–1208, 2022.\\nLund, B. D. and Wang, T. Chatting about ChatGPT: how\\nmay AI and GPT impact academia and libraries? Library\\nHi Tech News , 2023. doi: https://doi.org/10.1108/LHTN\\n-01-2023-0009.\\nMagar, I. and Schwartz, R. Data contamination: From\\nmemorization to exploitation. In Proceedings of the 60th\\nAnnual Meeting of the Association for Computational Lin-\\nguistics (Volume 2: Short Papers) , pp. 157–165, Dublin,\\nIreland, May 2022. Association for Computational Lin-\\nguistics. doi: 10.18653/v1/2022.acl-short.18. URL\\nhttps://aclanthology.org/2022.acl-sh\\nort.18 .\\nMcCoy, T., Pavlick, E., and Linzen, T. Right for the wrong\\nreasons: Diagnosing syntactic heuristics in natural lan-\\nguage inference. In Proceedings of the 57th Annual Meet-\\ning of the Association for Computational Linguistics , pp.\\n3428–3448, Florence, Italy, July 2019. Association for\\nComputational Linguistics. doi: 10.18653/v1/P19-1334.\\nURL https://aclanthology.org/P19-1334 .\\nMcKenzie, I., Lyzhov, A., Pieler, M., Parrish, A., Prabhu,\\nA., Mueller, A., Kim, N., Bowman, S., and Perez, E.Inverse scaling prize: Second round winners, 2022. URL\\nhttps://irmckenzie.co.uk/round2 .\\nMehdi, Y . Reinventing search with a new AI-powered Mi-\\ncrosoft Bing and Edge, your copilot for the web. Ofﬁcial\\nMicrosoft Blog , 2023. URL https://blogs.micr\\nosoft.com/blog/2023/02/07/reinventin\\ng-search-with-a-new-ai-powered-micro\\nsoft-bing-and-edge-your-copilot-for-\\nthe-web/ .\\nMenick, J., Trebacz, M., Mikulik, V ., Aslanides, J., Song,\\nF., Chadwick, M., Glaese, M., Young, S., Campbell-\\nGillingham, L., Irving, G., et al. Teaching language\\nmodels to support answers with veriﬁed quotes. arXiv\\npreprint 2203.11147 , 2022.\\nMichael, J. To dissect an octopus: Making sense of the\\nform/meaning debate. Blog post , 2020. URL https:\\n//julianmichael.org/blog/2020/07/23/\\nto-dissect-an-octopus.html .\\nMichael, J., Holtzman, A., Parrish, A., Mueller, A., Wang,\\nA., Chen, A., Madaan, D., Nangia, N., Pang, R. Y ., Phang,\\nJ., et al. What do NLP researchers believe? Results of the\\nNLP community metasurvey. arXiv preprint 2208.12852 ,\\n2022.\\nNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim,\\nC., Hesse, C., Jain, S., Kosaraju, V ., Saunders, W., et al.\\nWebGPT: Browser-assisted question-answering with hu-\\nman feedback. arXiv preprint 2112.09332 , 2021.\\nNgo, R. The alignment problem from a deep learning per-\\nspective. arXiv preprint 2209.00626 , 2022.\\nNye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H.,\\nAustin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma,\\nM., Luan, D., et al. Show your work: Scratchpads for\\nintermediate computation with language models. arXiv\\npreprint 2112.00114 , 2021.\\nOliver, J. Last week tonight with John Oliver: Feb 26, 2023.\\nURL https://www.hbo.com/last-week-to\\nnight-with-john-oliver/season-10/2-f\\nebruary-26-2022 .\\nOpenAI. ChatGPT plugins, 2023a. URL https://open\\nai.com/blog/chatgpt-plugins .\\nOpenAI. GPT-4 technical report. arXiv preprint 2303.08774 ,\\n2023b. URL https://doi.org/10.48550/arX\\niv.2303.08774 .\\nOrtega, P. A., Kunesch, M., Del ´etang, G., Genewein, T.,\\nGrau-Moya, J., Veness, J., Buchli, J., Degrave, J., Piot,\\nB., Perolat, J., et al. Shaking the foundations: delusions\\nin sequence models for interaction and control. arXiv\\npreprint 2110.10819 , 2021. Eight Things to Know about Large Language Models\\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\\net al. Training language models to follow instructions\\nwith human feedback. Advances in Neural Information\\nProcessing Systems , 35:27730–27744, 2022.\\nPandia, L. and Ettinger, A. Sorting through the noise: Test-\\ning robustness of information processing in pre-trained\\nlanguage models. In Conference on Empirical Methods\\nin Natural Language Processing , 2021.\\nPatel, R. and Pavlick, E. Mapping language models to\\ngrounded conceptual spaces. In International Conference\\non Learning Representations , 2022.\\nPerez, E., Ringer, S., Luko ˇsi¯ut˙e, K., Nguyen, K., Chen, E.,\\nHeiner, S., Pettit, C., Olsson, C., Kundu, S., Kadavath,\\nS., et al. Discovering language model behaviors with\\nmodel-written evaluations. arXiv preprint 2212.09251 ,\\n2022.\\nPerrigo, B. The new AI-powered Bing is threatening users.\\nthat’s no laughing matter. Time , 2023. URL https:\\n//time.com/6256529/bing-openai-chatg\\npt-danger-alignment/ .\\nPotts, C. Is it possible for language models to achieve\\nlanguage understanding. Medium , 2020. URL https:\\n//chrisgpotts.medium.com/is-it-possi\\nble-for-language-models-to-achieve-l\\nanguage-understanding-81df45082ee2 .\\nRadford, A., Narasimhan, K., Salimans, T., Sutskever, I.,\\net al. Improving language understanding by generative\\npre-training. OpenAI blog , 2018. URL https://op\\nenai.com/research/language-unsupervi\\nsed.\\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D.,\\nSutskever, I., et al. Language models are unsuper-\\nvised multitask learners. OpenAI blog , 2019. URL\\nhttps://openai.com/research/better-l\\nanguage-models .\\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,\\net al. Learning transferable visual models from natural\\nlanguage supervision. In International conference on\\nmachine learning , pp. 8748–8763. PMLR, 2021.\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\\nMatena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring\\nthe limits of transfer learning with a uniﬁed text-to-text\\ntransformer. The Journal of Machine Learning Research ,\\n21(1):5485–5551, 2020.Reynolds, L. and McDonell, K. Prompt programming for\\nlarge language models: Beyond the few-shot paradigm.\\nInExtended Abstracts of the 2021 CHI Conference on\\nHuman Factors in Computing Systems , pp. 1–7, 2021.\\nRoose, K. A conversation with Bing’s chatbot left me deeply\\nunsettled. New York Times , 2023. URL https://www.\\nnytimes.com/2023/02/16/technology/bi\\nng-chatbot-microsoft-chatgpt.html .\\nSaunders, W., Yeh, C., Wu, J., Bills, S., Ouyang, L., Ward, J.,\\nand Leike, J. Self-critiquing models for assisting human\\nevaluators. arXiv preprint 2206.05802 , 2022.\\nSchick, T., Dwivedi-Yu, J., Dess `ı, R., Raileanu, R., Lomeli,\\nM., Zettlemoyer, L., Cancedda, N., and Scialom, T. Tool-\\nformer: Language models can teach themselves to use\\ntools. arXiv preprint 2302.04761 , 2023.\\nSchuster, S. and Linzen, T. When a sentence does not\\nintroduce a discourse entity, transformer-based models\\nstill sometimes refer to it. In Proceedings of the 2022\\nConference of the North American Chapter of the As-\\nsociation for Computational Linguistics: Human Lan-\\nguage Technologies , pp. 969–982, Seattle, United States,\\nJuly 2022. Association for Computational Linguistics.\\ndoi: 10.18653/v1/2022.naacl-main.71. URL https:\\n//aclanthology.org/2022.naacl-main.71 .\\nSchwitzgebel, E. and Garza, M. Designing AI with Rights,\\nConsciousness, Self-Respect, and Freedom. In Ethics of\\nArtiﬁcial Intelligence . Oxford University Press, 09 2020.\\nISBN 9780190905033. doi: 10.1093/oso/9780190905\\n033.003.0017. URL https://doi.org/10.1093/\\noso/9780190905033.003.0017 .\\nSevilla, J., Heim, L., Ho, A., Besiroglu, T., Hobbhahn, M.,\\nand Villalobos, P. Compute trends across three eras of\\nmachine learning. In 2022 International Joint Conference\\non Neural Networks (IJCNN) , pp. 1–8. IEEE, 2022.\\nSharma, P., Torralba, A., and Andreas, J. Skill induction and\\nplanning with latent language. In Proceedings of the 60th\\nAnnual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers) , pp. 1713–1726,\\nDublin, Ireland, May 2022. Association for Computa-\\ntional Linguistics. doi: 10.18653/v1/2022.acl-long.120.\\nURL https://aclanthology.org/2022.acl-\\nlong.120 .\\nShevlin, H. How could we know when a robot was a moral\\npatient? Cambridge Quarterly of Healthcare Ethics , 30\\n(3):459–471, 2021. doi: 10.1017/S0963180120001012.\\nShlegeris, B., Roger, F., and Chan, L. Language mod-\\nels seem to be much better than humans at next-token\\nprediction. Alignment Forum , 2022. URL https: Eight Things to Know about Large Language Models\\n//www.alignmentforum.org/posts/htrZr\\nxduciZ5QaCjw/language-models-seem-to\\n-be-much-better-than-humans-at-next .\\nSi, C., Gan, Z., Yang, Z., Wang, S., Wang, J., Boyd-Graber,\\nJ. L., and Wang, L. Prompting GPT-3 to be reliable.\\nInThe Eleventh International Conference on Learning\\nRepresentations , 2023. URL https://openreview\\n.net/forum?id=98p5x51L5af .\\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L.,\\nVan Den Driessche, G., Schrittwieser, J., Antonoglou, I.,\\nPanneershelvam, V ., Lanctot, M., et al. Mastering the\\ngame of Go with deep neural networks and tree search.\\nNature , 529(7587):484–489, 2016.\\nSøgaard, A. Grounding the vector space of an octopus:\\nWord meaning from raw text. Minds and Machines , pp.\\n1–22, 2023.\\nSrivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid,\\nA., Fisch, A., Brown, A. R., Santoro, A., Gupta, A.,\\nGarriga-Alonso, A., et al. Beyond the imitation game:\\nQuantifying and extrapolating the capabilities of language\\nmodels. arXiv preprint 2206.04615 , 2022.\\nStein-Perlman, Z., Weinstein-Raun, B., and Grace, K. 2022\\nexpert survey on progress in AI. AI Impacts blog , 2020.\\nURL https://aiimpacts.org/2022-expert-\\nsurvey-on-progress-in-ai/ .\\nSteinhardt, J. On the risks of emergent behavior in foun-\\ndation models. Stanford CRFM blog post , 2021. URL\\nhttps://crfm.stanford.edu/commentary\\n/2021/10/18/steinhardt.html .\\nSteinhardt, J. AI forecasting: One year in. Bounded Regret ,\\n2022. URL https://bounded-regret.ghost\\n.io/ai-forecasting-one-year-in/ .\\nStiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R.,\\nV oss, C., Radford, A., Amodei, D., and Christiano,\\nP. F. Learning to summarize with human feedback. Ad-\\nvances in Neural Information Processing Systems , 33:\\n3008–3021, 2020.\\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\\nM.-A., Lacroix, T., Rozi `ere, B., Goyal, N., Hambro, E.,\\nAzhar, F., et al. LLaMA: Open and efﬁcient foundation\\nlanguage models. arXiv preprint 2302.13971 , 2023.\\nTurner, A. and Tadepalli, P. Parametrically retargetable\\ndecision-makers tend to seek power. Advances in Neural\\nInformation Processing Systems , 35:31391–31401, 2022.\\nTurner, A. M., Smith, L. R., Shah, R., Critch, A., and\\nTadepalli, P. Optimal policies tend to seek power. In\\nBeygelzimer, A., Dauphin, Y ., Liang, P., and Vaughan,J. W. (eds.), Advances in Neural Information Processing\\nSystems , 2021. URL https://openreview.net\\n/forum?id=l7-DBWawSZH .\\nUesato, J., Kushman, N., Kumar, R., Song, F., Siegel, N.,\\nWang, L., Creswell, A., Irving, G., and Higgins, I. Solv-\\ning math word problems with process-and outcome-based\\nfeedback. arXiv preprint 2211.14275 , 2022.\\nUllman, T. Large language models fail on trivial alterations\\nto theory-of-mind tasks. arXiv preprint 2302.08399 ,\\n2023.\\nWang, B., Min, S., Deng, X., Shen, J., Wu, Y ., Zettlemoyer,\\nL., and Sun, H. Towards understanding chain-of-thought\\nprompting: An empirical study of what matters. arXiv\\npreprint 2212.10001 , 2022.\\nWebson, A. and Pavlick, E. Do prompt-based models really\\nunderstand the meaning of their prompts? In Proceedings\\nof the 2022 Conference of the North American Chapter of\\nthe Association for Computational Linguistics: Human\\nLanguage Technologies , pp. 2300–2344, Seattle, United\\nStates, July 2022. Association for Computational Lin-\\nguistics. doi: 10.18653/v1/2022.naacl-main.167. URL\\nhttps://aclanthology.org/2022.naacl-\\nmain.167 .\\nWei, J., Tay, Y ., Bommasani, R., Raffel, C., Zoph, B.,\\nBorgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Met-\\nzler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang,\\nP., Dean, J., and Fedus, W. Emergent abilities of large\\nlanguage models. Transactions on Machine Learning\\nResearch , 2022a. ISSN 2835-8856. URL https:\\n//openreview.net/forum?id=yzkSU5zdwD .\\nSurvey Certiﬁcation.\\nWei, J., Wang, X., Schuurmans, D., Bosma, M., brian ichter,\\nXia, F., Chi, E. H., Le, Q. V ., and Zhou, D. Chain of\\nthought prompting elicits reasoning in large language\\nmodels. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho,\\nK. (eds.), Advances in Neural Information Processing\\nSystems , 2022b. URL https://openreview.net\\n/forum?id= VjQlMeSB J.\\nWeidinger, L., Uesato, J., Rauh, M., Grifﬁn, C., Huang,\\nP.-S., Mellor, J., Glaese, A., Cheng, M., Balle, B.,\\nKasirzadeh, A., Biles, C., Brown, S., Kenton, Z.,\\nHawkins, W., Stepleton, T., Birhane, A., Hendricks, L. A.,\\nRimell, L., Isaac, W., Haas, J., Legassick, S., Irving, G.,\\nand Gabriel, I. Taxonomy of risks posed by language mod-\\nels. In 2022 ACM Conference on Fairness, Accountability,\\nand Transparency , FAccT ’22, pp. 214–229, New York,\\nNY , USA, 2022. Association for Computing Machinery.\\nISBN 9781450393522. doi: 10.1145/3531146.3533088.\\nURL https://doi.org/10.1145/3531146.\\n3533088 . Eight Things to Know about Large Language Models\\nWhite, J., Fu, Q., Hays, S., Sandborn, M., Olea, C., Gilbert,\\nH., Elnashar, A., Spencer-Smith, J., and Schmidt, D. C.\\nA prompt pattern catalog to enhance prompt engineering\\nwith ChatGPT. arXiv preprint 2302.11382 , 2023.\\nZhou, D., Sch ¨arli, N., Hou, L., Wei, J., Scales, N., Wang,\\nX., Schuurmans, D., Cui, C., Bousquet, O., Le, Q. V .,\\nand Chi, E. H. Least-to-most prompting enables complex\\nreasoning in large language models. In The Eleventh\\nInternational Conference on Learning Representations ,\\n2023. URL https://openreview.net/for\\num?id=WZH7099tgfM .\\nZiegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford,\\nA., Amodei, D., Christiano, P., and Irving, G. Fine-tuning\\nlanguage models from human preferences. arXiv preprint\\n1909.08593 , 2019. '},\n",
       " {'url': 'http://arxiv.org/pdf/2401.01519v2',\n",
       "  'title': 'Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review',\n",
       "  'text': 'Exploring the Frontiers of LLMs in Psychological Applications: A \\nComprehensive Review  \\n \\nLuoma Ke1, Song Tong1,*, Peng Chen g2, Kaiping Peng1.* \\n1. Department of Psychology , Tsinghua University  \\n2. School  of Social Science , Tsinghua University  \\n* Corresponding authors: tong.song.53w@kyoto -u.jp; pengkp@tsinghua,edu.cn  \\n \\nAbstract  \\nThis paper explores the frontiers of large language models (LLMs) in psychology applications. Psychology \\nhas undergone several theoretical changes, and the current use of Artificial Intelligence (AI) and Machine \\nLearning, particularly LLMs, promises to ope n up new research directions. We provide a detailed exploration \\nof how LLMs like ChatGPT are transforming psychological research. It discusses the impact of LLMs across \\nvarious branches of psychology, including cognitive and behavioral, clinical and counse ling, educational and \\ndevelopmental, and social and cultural psychology, highlighting their potential to simulate aspects of human \\ncognition and behavior. The paper delves into the capabilities of these models to emulate human -like text \\ngeneration, offering innovative tools for literature review, hypothesis generation, experimental design, \\nexperimental subjects, data analysis, academic writing, and peer revi ew in psychology. While LLMs are \\nessential in advancing research methodologies in psychology, the pap er also cautions about their technical \\nand ethical challenges. There are issues like data privacy, the ethical implications of using LLMs in \\npsychological research, and the need for a deeper understanding of these models\\' limitations. Researchers \\nshould re sponsibly use LLMs in psychological studies, adhering to ethical standards and considering the \\npotential consequences of deploying these technologies in sensitive areas. Overall, the article provides a \\ncomprehensive overview of the current state of LLMs in  psychology, exploring potential benefits and \\nchallenges. It serves as a call to action for researchers to leverage LL Ms\\' advantages responsibly while \\naddressing associated risks.  \\nKeywords  Large language models  (LLMs)·ChatGPT ·Machine learning ·Artificial intelligence  \\n(AI)·Psychology · Research methodolog y \\n \\n1. Introduction  \\nArtificial intelligence  (AI) has a history spanning nearly seven decades, beginning with the 1956  \\nDartmouth Conference. The field has recent ly been revolutionized by the advent  of large language models \\n(LLMs), such as OpenAI\\'s ChatGPT series, Google\\'s Bard, and Meta\\'s Llama . Notably,  OpenAI\\'s GPT -4, in \\nparticular, may signify a paradigm shift by demonstrating impressive capabilities  (e.g., it solved  difficult  tasks in math, coding, vision, medicine, law, psychology, etc.)  (Bubeck et al., 2023) , that is to say, AI for \\nScience (H. Wang et al., 2023) . LLMs  mark a critical juncture in the evolution of  machine learning and AI , \\npropelled by their expansive size and sophisticated neural architectures that incorporate  attentional \\nmechanisms  (V aswani et al., 2017) . Due to the integration of cognitive mechanisms  (Binz & Schulz, 2023a) , \\nthese models have acquired the ability to exhibit emergent behaviors akin to complex physical systems  (Wei \\net al., 2022) , which  has not only enhanced their ability to understand concepts and high -level semantics  (J. \\nLi et al., 2022)  but also deepened our insights into cognitive processes  (Sejnowski, 2022) . In psychological \\napplications , these  developments are reshaping interactions and comprehension of data, language, and our \\nenvironment  (De Bot et al., 2007; Demszky et al., 2023) , contributing significantly to various fields , \\nincluding clinical  (Thirunavukarasu et al., 2023) , development  (Frank, 2023; Hagendorff, 2023) , and social \\npsychology  (Demszky et al., 2023; Hardy et al., 2023; Zhang et al., 2023) . Moreover,  they profoundly impact  \\npsychological research methodologies, offering novel approaches and tools for exploration and analysis.  \\n1.1.  The concept of LLMs: From machine learning to capability emergence  \\nMachine learning, particularly  natural language processing (NLP), has significantly progressed  in the \\nlast decade. However, the emergence of LLMs such as GPT -4 and its predecessors marks  a significant leap \\nin AI capabilities. LLMs are deep learning models designed to process natural language text and generate \\nhuman -like responses or texts. Their capability emergence is defined as a qualitative change in behavior due \\nto a quantitative change in the system and thus a qualitative chang e in behavior, i.e., a capability is emerge nt \\nif it does not exist in a smaller model and exists in a larger model (Wei et al., 2022) . \\nAt the heart of this LLM is the transformer architecture, a deep neural network with an attentional \\nmechanism to efficiently process sequential data in  parallel  (Vaswani et al., 2017) , which works somewhat \\nsimilarly to the human brain  functions . This architecture has revolutionized the field of Natural Language \\nProcessing (NLP). The self -attention  mechanism of the transformer architecture captures contextual \\nrelationships in textual data, allowing for more sophisticated language understanding. On the one hand, t he \\n\"large\" in LLM s refers to many  parameters and massive amounts of training data used to fine -tune these \\nmodels, typically billions of parameters and terabytes of text  (Binz & Schulz, 2023b) , and on the other, \\nmeans master the world model  (Yildirim & Paul, 2023) .   \\nThe process of large language modeling , from machine learning to the emergence of competence , can be \\ndivided into several key stages (Demszky et al., 2023) . First, p re-training:  LLMs  are pre -trained on large  amounts of textual data to learn intricate linguistic, syntactic, and textual structures  (P. Liu et al., 2023) . This \\nunsupervised learning phase lays the foundation for the big language model to understand the language.  \\nSecond, fine-tuning: LLM can be fine -tuned for a specific task or domain after pre -training to make it \\nadaptable and suitable for various  applications (Liu et al., 2022) . This fine -tuning process ensures that the \\nmodel can generate contextually relevant responses and engage in meaningful conversations or tasks.  Third, \\nlanguage comprehension:  LLMs  have demonstrated a remarkable ability to understand and develop  human -\\nlike text. They can answer questions, write articles, summarize content, translate language, and even do \\ncreative writing  (Bubeck et al., 2023) . Their skillful understanding of context is an essential  factor \\ncontributing to the emergence of their intelligence.  Fourth  is the e mergence of capabilities:  LLM s exhibit \\n\"capability emergence\" when integrated into various applications and systems. They can perform tasks that \\nrequire a deep understanding of language and context, often achieving human -like or superhuman \\nperformance in specific domains (OpenAI, 2023) , such as analogical reasoning (Webb et al., 2023) , creativity \\n(Stevenson et al., 2022) , and emotion recognition (Patel & Fan, 2023) . \\nTherefore, LLMs offer  intriguing insights into how these technologies can mimic or augment human \\ncognitive processes. For instance, LLMs\\' ability to understand and generate natural language echoes aspects \\nof human linguistic and cognitive skills  (Goertzel, 2023) . This parallel allows for exploration into AI \\napplications in the cognitive psychology  (Sartori & Orrù, 2023) , language acquisition  (Jungherr, 2023) , and \\neven the mental health  (Lamichhane, 2023) . Moreover, the study of LLMs contributes to our understanding \\nof the human mind , offering a computational perspective on language processing, the decision -making  (Sha \\net al., 2023) , and learning mechanisms  (Hendel et al., 2023) . Fusing  these disciplines advances AI\\'s \\ncapabilities and  deepens our comprehension of the human mind.  \\n1.2.  Psychology and AI \\nPsychology, as a science that explores the human mind and behavior, has undergone significant  \\ntheoretical changes since the late 19th century, with psychoanalysis and behaviorism extending to cognitive \\npsychology  (Hothersall & Lovett, 2022) . This history not only marks a shift in the focus of research in \\npsychology but also reflects the academic trend from observing behavioral manifestations to exploring in -\\ndepth psychological connotations. Each of these phases has led to a deepening underst anding of the psycho -\\ncognitive processes of human beings.  \\nUnderstanding human psycho -cognitive processes is therefore crucial to psychology. In clinical and  counseling psychology, research in cognitive psychology supports diagnosing and treating  psychological \\ndisorders . It deepens  understanding of the  psychological mechanisms underlying emotions, stress, and \\nhuman behavior. Psychotherapies, such as cognitive -behavioral therapy (Hofmann et al., 2012)  and \\npsychodynamic therapy, have become essential  tools for promoting mental health and emotional regulation. \\nIn educational and developmental psychology, the development of cognitive psychology has fostered a \\ndeeper understanding of the role of perceptual and affective factors in the learning process  (Glaser, 1984) , \\nwhich has led to innovations in teaching methods and learning strategies. In social and cultural psychology, \\ncognitive psychology research helps explain individuals\\' behavior and mental processes in different social \\nand cultural contexts . It explores  how cultural differences affect individuals\\' cognitive patterns, values, and \\nbehavioral norms, especially in globalization, interaction, and integration. Meanwhile, in social psychology, \\ncognitive psychology\\'s research on group behavior, social influence,  prejudice, and discrimination is of great \\nvalue in promoting social harmony and mutual understanding  (Park & Judd, 2005) .  \\nAI is a growing force in psycho -cognitive research. For example, Simon (1979)  recognized the potential \\nof computational models to simulate human cognitive processes early on . The recent emergence of LLMs, \\nrepresented by OpenAI\\'s GPT family (mainly including GPT -3, ChatGPT, and GPT -4), which can process \\nand generate human -like texts and perform close to humans in some cognitive tasks  (Bubeck et al., 2023) . \\nMore so, it provides a unique perspective to study human cognition. For example, GPT -3 can solve vignette -\\nbased tasks similarly or better than human subjects and can make rational decisions based on descriptions, \\noutperforming humans in the multi -armed b andit task  (Binz & Schulz, 2023b) . Furthermore, after extensive \\ntesting, GPT -3 can solve complex  analogical problems at levels comparable to human performance, and \\nanalogical reasoning is an essential  hallmark of human intelligence  (Webb et al., 2023) . Furthermore, fine -\\ntuning across multiple tasks could allow LLM s to predict human behavior in previously unseen tasks, i.e., \\nLLM s could be adapted to general -purpose cognitive models  (Binz & Schulz, 2023a) , potentially opening \\nup new research directions that could transform cognitive psychology and behavioral science in general.  \\nNewell\\'s time -scale theory provides a multidimensional framework for understanding human behavior \\nNewell (1990) . In his seminal work, Newell (1990)  articulates a nuanced framework for comprehending \\ncognitive processes, stratifying human behavior across four distinct temporal levels   (see Fig. 1a) . At the \\nfoundational biological level, he addresses core biological and physiological processes  characterized by \\nrapid time scales fluctuating from approximately one millisecond to one second. This level might include \\nneural responses and sensory processing, foundational to human cognition. Advancing to the cognitive layer,  Newell examines fundamental cognitive mechanisms operating on intermediate time scales, typically from \\none second to around one minute. This layer could encompass basic cognitive operations such as attention, \\nperception, and short -term memory. Further, at the rational layer, the focus shifts to more elaborate and \\nsustained cognitive activities. These processes, often extending from several minutes to a few hours, might \\ninvolve complex problem -solving, planning, and decision -making activities requiring  a hig her level of \\ncognitive engagement. Finally, the social layer encapsulates human behavior within social interactions and \\nstructures. This level, characterized by the broadest time scales ranging from a few hours to days or more, \\ndelves into the dynamics of social communication, group behavior, and cultural influences on cognition. \\nThis layered approach underscores the multifaceted nature of human behavior, highlighting the interplay \\nbetween rapid physiological processes and the more prolonged, socially influ enced aspects of human \\ncognition.   \\nLLMs have great potential for modeling cognition and behavior on these different time scales (see Fig. \\n1b) and can provide new insights into human psycho -cognitive processes. Recent research has revealed \\nsignificant advancements in LLMs’ ability to emulate complex human cognitive and social behaviors \\n(Grossmann et al., 2023; Marjieh et al., 2023; Orru et al., 2023; Pal et al., 2023; Stevenson et al., 2022; Webb \\net al., 2023) . Grossmann et al. (2023)  and Marjieh et al. (2023)  have shown LLMs’ proficiency in simulating \\nhuman social interactions and perceptual processing, respectively . Orru et al. (2023)  and (Webb et al., 2023)  \\nhighlighted LLMs\\' capabilities in complex problem -solving and reasoning, while Hagendorff et al. (2023)  \\nfocused on decision -making processes. Stevenson et al. (2022)  documented LLMs\\' potential for creativity, \\nand Patel and Fan (2023)  demonstrated their ability in emotion recognition. These findings collectively \\nindicate the expanding role of LLMs in mimicking and enhancing human cognitive and social functions, \\nmarking significant progress in AI research.  \\nAs a general cognitive model (Binz & Schulz, 2023a) , LLMs can provide new perspectives and \\napproaches to research in the fields of cognitive and behavioral psychology, clinical and counseling \\npsychology, educational and developmental psychology, and social and cultural psychology in different time \\nscales of human behavior  (see Fig . 1a).    \\nFig.1  LLMs’ emergent abilities  can be applied  in psychological domains  and work as research tools : (a) LLMs ’ application in psychological \\ndomains from time scales of human behavior . (b) LLMs’ emergent abilities . (c) LLMs work as research tools .  \\nBased on these emergent abilities , LLM s can also be used as a research aid  (see Fig. 1c)  to help \\npsychologists with everything from literature review  (Aydın & Karaarslan, 2022; Qureshi et al., 2023) , \\nexperimental subjects  (Dillion et al., 2023; Hutson, 2023) , and data analysis  (Patel & Fan, 2023; Peters & \\nMatz, 2023; Rathje et al., 2023)  to academic  writing  (Dergaa et al., 2023; Stokel -Walker, 2022)  and peer \\nreview (Chiang & Lee, 2023; Van Dis et al., 2023) . Thus, LLMs can potentially become research assistants  \\nfor psychologists, helping them improve their research efficiency . \\n1.3.  Objectives and significance of the present review  \\nThis review systematically examines the use of LLMs in various psychological domains, analyzing their \\napplications over different behavioral time scales. The exploration begins with LLMs in cognitive and \\nbehavioral psychology (Section 2), followed by their  roles in clinical and counseling psychology (Section \\n3). The review then transitions to educational and developmental psychology (Section 4) and social and \\ncultural psychology (Section 5), outlining LLMs\\' contributions in each area. To gain a deeper under standing \\nof the impact of LLMs on psychological research, Section 6 will provide an overview of their potential as a \\ntool for scientific research. The review also addresses challenges and future research directions in applying \\nLLMs to psychological contexts. It concludes by summarizing their applications in psychology and offering \\nrecommendations for future work. Crucially, this review proposes integ ration strategies for LLMs in \\n psychological research and provides insights into interpreting these models from a psychological standpoint, \\ncontributing to their safety and interpretability.  \\n \\n2. LLMs in cognitive and behavioral psychology  \\n Within  multilevel time scales of human behavior  (Newell, 1990) , cognitive and behavioral psychology \\nhas focused primarily on the study of cognitive processes on sub -hourly time scales (see Fig. 1), which \\nencompass humans engaging in perception, memory, thinking, decision -making, problem -solving , and \\nconscious planning. Cognitive and behavioral psychology typically employs experimental methods to study \\nthese cognitive processes by controlling and observing behaviors and responses under specific conditions. \\nThe recent emergence of LLMs  has reinvigorated the discussion a s to whether human cognitive abilities are \\nrevealed in these LLMs  given sufficient training data. If the answer is yes, then it would be possible to study \\nthe cognitive processes of LLMs , thereby gaining knowledge of human cognitive processes and forming a \\nvaluable addition to existing research methods in cognitive psychology.  \\n  Binz and Schulz (2023a)  found that fine -tuning multiple tasks enabled the LLM  to predict human \\nbehavior in previously unseen tasks, suggesting that the LLM  can be adapted to become a generalist \\ncognitive model. In another study, they tested the GPT -3 with tools from cognitive psychology and showed \\nthat it made better decisions and outperformed humans in a multiarmed bandit task (Binz & Schulz, 2023b) . \\nIn addition, there are other series of studies that have demonstrated that LLMs  have perceptual judgment  \\n(Marjieh et al., 2023) , reasoning  (Webb et al., 2023) , and decision -making abilities  (Hagendorff et al., 2023) , \\ncreativity  (Stevenson et al., 2022) , and problem -solving  (Orru et al., 2023) , and one study even demonstrated \\nthat the GPT -4 has the mental abilities of a seven -year-old child through a false belief task (considered the \\ngold standard for testing theory of mind in humans)  (Kosinski, 2023) . For example, Hagendorff et al. (2023)  \\nexplored reasoning capabilities and decision -making processes of the OpenAI GPT family by the following \\nexperimental method: Design a series of semantic illusion and cognitive reflection tests designed to elicit \\nintuitive but erroneous responses. Apply th ese tasks, traditionally used to study human reasoning and \\ndecision -making, to OpenAI\\'s family of generative pre -trained Transformer models. Analyze model \\nperformance on a Cognitive Reflection Test (CRT) task and a semantic illusions task to reveal their S ystem \\n1 and System 2 thought processes. Observe how ChatGPT models show correct responses in these tasks and \\navoid pitfalls. Evaluate the performance of the models in the CRT task by preventing them from chain - thinking to reason.  The results show that as model size and language capability increase, the OpenAI family \\nof generative pre -trained Transformer models increasingly exhibit human -like intuitive system 1 thinking \\nand associated cognitive errors.  Table 1 summarizes  the applications of LLMs to cognitive and behavioral \\npsychology . \\nThese research cases demonstrate that LLMs have human -like cognitive abilities (Zhuang et al., 2023) . \\nStudying the cognitive mechanisms of LLMs  would  provide new insights into human cognitive processes . \\nThey will provide promising avenues for advancing psychological research methodologies and \\nunderstanding complex cognitive phenomena as they evolve .  \\n \\nTable 1 Applications  of LLMs in cognitive and behavioral psychology study . \\nAuthor  Research question  Research  method  Key finding  \\nSartori and \\nOrrù (2023)  The human -like properties  \\nLLM s exhibit in a variety of \\ncognitive tasks.  Decision -making , information search, \\ndeliberation, causal reasoning,  Wason \\nSelection Task, and Raven -like matrices.  LLMs have demonstrated human -like \\nperformance in cognitive psychology.  \\nHagendorff et \\nal. (2023)  Reasoning capabilities and \\ndecision -making processes \\nof the OpenAI GPT  family \\nbear any resemblance to \\nhuman system 1 and system \\n2 thought processes . Analyze model performance on a Cognitive \\nReflection Test (CRT) task and a semantic \\nillusions task to reveal their System 1 and 2 \\nthought processes.  ChatGPT -3.5 and 4 use input -output \\ncontext windows during chain -thinking \\nreasoning, similar to how humans use \\nlaptop -support system 2 thinking.  \\nHutson \\n(2023)  Can AI language models be \\nused to replace human \\nparticipants in \\nexperiments?  LLMs(e.g., GPT -3.5) were used to conduct \\nthe experiment instead of human \\nparticipants.  LLMs can replace human participants in \\nexperimental research in some cases.  \\nDillion et al. \\n(2023)  Explore whether LLMs can \\nreplace human participants \\nin the psychological \\nsciences.  Making  human -like moral judgments was \\nassessed by analyzing the similarity of GPT -\\n3.5\\'s judgments to humans.  LLMs can be used as a substitute for \\nhuman participants in some cases.  \\nZhuang et al. \\n(2023)  How to efficiently measure \\nthe cognitive abilities of \\nLLMs.  A Computerized Adaptive Testing (CAT)  for \\nassessing cognitive ability in LLMs.  ChatGPT has surpassed the \\nprogramming abilities of high -ability \\ncollege students in dynamic \\nprogramming and search.  \\nGrossmann et \\nal. (2023)  How to improve social \\nscience research methods in \\nthe context of the ongoing \\nimpact of LLMs on social \\nscience research.  LLMs replace human participants in data \\ncollection,  and act as \"peers\" in social \\ninteraction studies . LLMs have great potential for use in \\nsocial science research because of their \\nability to model human behavior and \\ngenerate diverse responses.  \\nLoconte et al. \\n(2023)  Neuropsychological \\nevaluations of the \\nperformance of a LLM in \\nterms of prefrontal \\nfunctioning.  The Verbal Reasoning Test, Cognitive \\nEstimation task, Metaphor , and Idioms \\nComprehension test, Winograd Schema, \\nTower of London,  Hayling Sentence \\nCompletion Test,  Compound Remote \\nAssociate problems , and Social Cognition.  ChatGPT exhibits disjointed cognitive \\nprofiles in prefrontal functioning (e.g., \\nsome performing better than average and \\nothers performing at pathological levels).  \\nBinz and \\nSchulz \\n(2023a)  How to better describe \\nhuman decision -making \\nbehavior by fine -tuning \\nLLMs.  By comparing the goodness -of-fit of \\ndifferent models : random guessing model,  \\ndomain -specific model,  LLaMA \\nunfinetuned , and fine -tuned model.  LLMs using fine -tuning (e.g., LLaMA) \\ncan successfully capture human \\ndecision -making behavior and perform \\nbetter than domain -specific models.   \\nOrru et al. \\n(2023)  The potential of ChatGPT \\nas an intelligent tool for \\nproblem -solving . Verbal insight problems were administered to \\nChatGPT: the first set was referred to as \\n\"practice problems,\" while the second set \\nwas referred to as \"transfer problems\".  ChatGPT\\'s global performance in the \\npractice and transfer problems was \\nidentical to the most likely results in the \\nhuman sample.  \\nHagendorff \\n(2023)  How to use psychological \\nmethods  to study the \\nemergent abilities and Studying the behavioral patterns, emergent \\nabilities, and decision -making mechanisms \\nof LLMs by treating them as participants in a Uncovering emergent abilities in LLMs \\nthat cannot be detected by traditional \\nnatural language processing  behaviors of LLMs.  psychological experiment . benchmarks.  \\nDhingra et al. \\n(2023)  The performance of the \\nGPT-4 on a cognitive \\npsychology task to \\nunderstand how it \\nprocesses.  Evaluating the performance of the GPT -4 on \\na range of cognitive psychology datasets: \\nCommonsenseQA, SuperGLUE, MATH, and \\nHANS.  The GPT -4 has revolutionary potential to \\nhelp machines bridge the gap between \\nhuman and machine reasoning.  \\nShiffrin and \\nMitchell \\n(2023)  Decision -making \\nmechanisms and other \\npsychological qualities of  \\nLLMs.  Two sets of experiments were conducted, \\nusing situational  prompts from the \\npsychological literature and prompts not in \\nthe GPT -3 training corpus.  GPT-3 outperforms humans on some \\ntasks and performs poorly on others.  \\nBinz and \\nSchulz \\n(2023b)  Assessing the GPT -3\\'s \\ncognitive ability.  Vignette -Based Investigations.  Decision -\\nMaking.  Information Search.  Deliberation. \\nCausal Reasoning.  GPT-3 showed surprising abilities in \\ndecision -making , information search, \\nand thoughtfulness, but performed \\npoorly in causal reasoning.   \\nMarjieh et al. \\n(2023)  How LLMs predict human \\nsensory judgments across \\nmultiple sensory \\nmodalities.  Using LLMs such as GPT -3， GPT-3.5 and \\nGPT-4 to predict human judgments in six \\nsensory modalities (pitch, loudness, color, \\nsound, taste, and timbre).  LLMs can successfully predict human \\nperceptual judgments in six modalities.  \\nHuang and \\nChang (2022)  How to improve and direct \\nthe reasoning of these \\nmodels.  Methods for assessing the reasoning ability \\nof LLMs : fully supervised fine -tuning, \\ncueing and contextual learning, problem \\ndecomposition, mixed -methods.  Improving the reasoning ability of LLMs \\nrequires the use of training data, model \\narchitectures, and optimization goals \\nspecific to reasoning.  \\nHagendorff et \\nal. (2022)  The machine intuitive \\ncapabilities of human -like \\nintuitive decision -making  \\nin GPT -3.5. Conducting Cognitive Reflection Test (CRT) \\nand Semantic Illusion Test on GPT-3.5.  \\n GPT-3.5 systematically exhibits \\n\"machine intuition\", i.e., produces \\nhuman -like erroneous decisions on the \\nCRT as well as semantic illusions.  \\nStevenson et \\nal. (2022)  Evaluating the performance \\nof Open AI\\'s generative \\nnatural language model \\nGPT-3 on creativity.  Comparing the GPT -3 to the Alternative Uses \\nTest (AUT), which is widely used by humans \\nin creativity research . On the creativity test, humans currently \\noutperformed the GPT -3 in originality \\nand unexpectedness, but the GPT -3 \\nperformed better in utility.  \\n \\n3. LLMs  in clinical and counseling  psychology  \\nIn multilevel time scales of human behavior (Newell, 1990) , clinical and counseling  psychology would \\ninvolve the assessment of everyday behavioral  acts (about a few hours to a day), habitual thinking (about a \\nday to a few months), and psychological disorders, among others (a few months to many years) (see Fig.1). \\nClinical and counseling  psychology focuses on assessing, diagnosing, treating, and preventing individual \\nmental health problems . These  processes often involve medium - to long -term periods. According to related \\nreports, there is a public rush to use LLMs  such as the ChatGPT for mental health screening and treatment \\n(Demszky et al., 2023) . LLMs are expected to be used in clinical and counseling because they can parse \\nhuman language and generate human -like responses, categorize text, and flexibly adapt  conversational styles \\nrepresenting  different theoretical orientations (Stade et al., 2023) . So, how do LLMs work in psychotherapy , \\nand can they replace the role of the human psychotherapist?    \\nLLMs are a basic generalized model with the ability to learn from small samples (Brown et al., 2020) , a \\ncapability that allows them to quickly become experts in the clinical and counseling  domain with only a \\nsmall amount of data to learn from. For example, LLMs trained on clinical content can identify more specific \\nfactors of change that can help psychologists understand the process of clinical interventions, thus opening  the black box of psychotherapy (Schueller & Morris, 2023) . Additionally, related studies have shown that \\nLLMs can correctly recognize emotions and respond to (Patel & Fan, 2023; Schaaff et al., 2023)  \\nappropriately and that human -computer collaborations in clinical psychological support result in more \\nempathy (Sharma et al., 2023) . Also, LLMs can accomplish mental health assessments (Elyoseph & \\nLevkovich, 2023; Kjell et al., 2023)  and individualized interventions (Blyler & Seligman, 2023a, 2023b) . \\nBlyler and Seligman (2023a)  proposed an individualized intervention: Participants were recruited from \\nprevious studies and were 18 or older. From the five narrative identities generated by ChatGPT -4 that were \\nrated as \"completely accurate ,\" five participants representing different backgrounds and experiences were \\nselected. The participants \\' narratives were provided through a dialog with the ChatGPT -4, and the AI was \\nasked how it would guide life coaching based on the narrative identities. Based on the AI -generated narrative \\nidentities and recommended coaching methods, ChatGPT -4 was asked how to recommend specific \\ninterventions. The results suggest tha t the coaching strategies and interventions generated by ChatGPT -4 \\nmake perfect sense based on  narrative identity.  Table 2 summarizes the applications of LLMs to clinical and \\ncounseling psychology . \\nThese research cases, which demonstrate the ability of LLMs to provide clinicians with adequate  mental \\nhealth support (Schueller & Morris, 2023) , hold promise to address the lack of capacity in the mental health \\ncare system as it continues to evolve and may provide more individualized treatment services and even have \\nthe potential to fully automate psychotherapy in the future (Stade et al., 2023) . Of course, it is essential  in \\nthis process to ensure that LL M is safe and privacy -protective in psychotherapy.   \\nTable 2 Applications  of LLMs in clinical and counseling psychology study . \\nAuthor  Research question  Research method  Key finding  \\nCarlbring et \\nal. (2023)  Can AI be utilized to \\nimprove the \\neffectiveness of \\nInternet  interventions ? Internet intervention methods: real -time video therapy, \\ndigital self -help programs, combining Internet \\ninterventions with face -to-face therapy, etc. Potential \\napplications of AI in Internet interventions: virtual \\npsychological coaches, psychotherapists, etc.  AI can work with therapists to \\nimprove outcomes.  \\n(Blyler & \\nSeligman, \\n2023a)  How a person\\'s \\nnarrative identity can \\nbe used to provide \\ncoaches and therapists \\nwith a personalized \\napproach to \\nintervention.  ChatGPT -4 generates  personalized narrative identities  \\nbased on stream -of-consciousness thoughts and \\ndemographic information . Then, it provided targeted \\ncoaching methods  and interventions  based on \\nnarratives  and analyzed the  feasibility of the coaching \\nstrategies and specific interventions . ChatGPT -4 generates highly \\ncredible coaching strategies and \\nhighly credible specific \\ninterventions based on the \\nnarrative identities it constructs.   \\nBlyler and \\nSeligman \\n(2023b)  The potential of AI in \\npsychological practice, \\nAI-generated personal \\nnarratives in therapy \\nand counseling to \\npromote self -Stream -of-consciousness reflections and basic \\ndemographic information were processed through the \\nChatGPT -4 to generate personal narratives and then \\nevaluated these AI -generated narratives for accuracy, \\nlevel of surprise, and illuminating.  AI can support self -discovery in \\npsychotherapy and coaching.  \\n  discovery.  \\nAbd-Alrazaq \\net al. (2019)  How the features and \\napplications of \\nchatbots are meeting \\nthe needs of people in \\nthe field of mental \\nhealth . Chatbots in mental health are characterized by a wide \\nrange of applications, a variety of interactions, rule -\\ndriven or machine -learning generation of responses, \\nthe use of virtual representatives, and stand -alone \\nsoftware or web -based platform implementati ons. Chatbots focus primarily on \\ndepression and autism and most \\nimplementations are in \\ndeveloped countries.  \\nElyoseph and \\nLevkovich \\n(2023)  This paper seeks to \\naddress the potential \\nand limitations of the \\nAI language model \\nChatGPT for suicide \\nrisk assessment.  Using a hypothetical case study, ChatGPT  was asked  \\nabout mental health indicators in a hypothetical patient \\nwith varying degrees of feelings of self -burden and \\nfrustrated belonging. ChatGPT\\'s assessments were \\nthen compared to those of mental health professionals . ChatGPT underestimated the \\nrisk of suicide attempts in all \\nscenarios, compared to mental \\nhealth professionals.  \\nKjell et al. \\n(2023)  This paper seeks to \\nexplore how the use of \\nLLMs can change \\npsychological \\nassessment.  Analyze natural language responses using LLM s to \\nextract mental health -related information and the \\nstrengths (e.g., accuracy, scope, parsing, and openness) \\nand limitations (e.g., bias, risk, and ethical issues) of \\nLLMS for assessing mental health.  LLMs have the potential to \\ntransform psychological \\nassessments away from reliance \\non rating scales and towards \\nusing the language in which \\npeople naturally communicate.  \\nPal et al. \\n(2023)  Bias in LLMs age and \\ngender dimensions.  Datasets: i2b2 2006 smoking and i2b2 2008 obesity \\ndatasets.  Classification task: multi -label classification \\nof sub -cases. Model selection: BERT -based language \\nmodels.  Training and optimization: trained for 1000 \\nepochs and optimized using Adam optimizer.  \\nEvaluation metrics: Micro F1 -score.  Bias analysis: \\nage, gender , and cross subgroups.  By creating population \\nsubgroups based on age and \\ngender, found that most of the \\ncross -cutting subgroups \\nexhibited amplification of bias.  \\n(J. M. Liu et \\nal., 2023)  How to provide \\neffective counseling \\nservices in the field of \\nmental health support \\nthrough the use of \\nLLM.  Adapting instructions using the GPT -4 to generate \\nquestion -answer pairs based on collecting recordings \\nof real counseling sessions. Introducing the \\nCounseling Bench assessment framework to evaluate \\nthe effectiveness.  Automated evaluation using GPT -4 \\nto compare the performance of ChatCounselor with \\nother LLMs.  ChatCounselor is able to \\ngenerate interactive and \\nmeaningful responses to \\nprovide personalized mental \\nhealth support to users.  \\nSchueller and \\nMorris (2023)  The use of LLMs in the \\nfield of clinical and \\ncounseling psychology \\nand how they can \\nprovide better \\npsychological \\ninterventions.  Support therapists to improve their skills.  Support lay \\npeople to deliver effective, evidence -based practice.  \\nDeveloping novel digital psychotherapy interventions \\n(DMHIs) . Analyze therapist discourse to identify \\nfactors that predict symptom improvement.  Identifying \\nmore specific change factors and factors associated \\nwith different types of treatment.  LLMs have a wide range of \\napplications in clinical science \\nand practice that can help \\nhumans provide better \\ninterventions, but will not \\ncompletely replace therapists.  \\nSharma et al. \\n(2023)  How AI systems can be \\nutilized to assist peer \\nsupporters in \\nimproving empathy \\nexpression.  Development of an intelligent feedback system called \\nHAILEY ( Human –AI collaboration  approach for \\nEmpathY ) to evaluate the role of AI in improving \\nempathic responses of peer supporters . AI can help peer supporters \\ndemonstrate higher levels of \\nempathy.  \\nGraber -Stiehl \\n(2023)  Are AI treatments \\nready for mainstream \\nadoption in today\\'s \\nworld?  Koko app users were given the option to get more \\ncomplete advice from Kokobot (an assistant based on \\nGPT-3), and users could edit or adapt the response to \\ntheir needs and send it.  Despite the promise of AI in \\nmental health, there are still \\nmany ethical and safety issues \\nwith current AI therapies.  \\nStade et al. \\n(2023)  How to responsibly \\ndevelop and evaluate \\nLLMs while realizing \\ntheir potential value in \\nbehavioral health.  Focus on evidence -based practice.  Commitment to \\nrigorous but pragmatic evaluation.  Involves \\ninterdisciplinary collaboration.  Focuses on trust and \\navailability for therapists and patients.  Designs \\neffective clinical LLM standards.  LLMs have great potential in the \\nfield of psychotherapy.  \\nZhong et al. \\n(2023)  Challenges of using \\nLLMs in psychiatric \\nresearch and practice.  Improve diagnostic accuracy by analyzing large \\ndatasets of patient information and identifying patterns \\nthat are difficult for humans to detect. Identify \\nindividual patient characteristics that predict response \\nto treatment and suggest treatment plans tail ored to \\neach patient.  LLMs have  great potential in \\npsychiatric research and \\npractice, but there are concerns  \\n(e.g., reliability,  accuracy, \\ntransparency, accountability , \\nand ethical issues ). \\n  4. LLMs in educational and developmental psychology  \\nWithi n multilevel time scales of human behavior (Newell, 1990) , educational and developmental  \\npsychology is primarily positioned at the relatively medium - to long -term level (see Fig.1), which reflects \\nthe ongoing learning and development that characterizes the educational process. Educational and \\ndevelopmental psychology is concerned with the lea rning process, the accumulation of knowledge, the \\ndevelopment of skills, and the changes in the individual psyche within the educational environment. \\nAccording to a national survey, it was found that only three months after the public release of ChatGPT, 4 0% \\nof U.S. teachers used it weekly for lesson planning (Demszky et al., 2023) .  \\nTable 3 summarizes the applications of LLMs to educational and developmental psychology . The \\npotential for using  LLMs  in educational and developmental psychology is manifold. They can facilitate \\npersonalized learning and  play an important role in emotion recognition, mental health aids, educational \\nassessment, and improving learning motivation . Specifically, LLMs learn from massive amounts of data \\nfrom the Internet and books  (Binz & Schulz, 2023b) , can be used as more knowledgeable learning aids  \\n(Stojanov, 2023) , provide personalized learning experiences  (Kasneci et al., 2023) , will enhance motivation \\nto learn  (Ali et al., 2023) . For example, Stojanov (2023)  explored the potential of ChatGPT as a learning tool \\nusing the following method: He began his learning journey by setting learning objectives and conversing  \\nwith ChatGPT about its functionality over four hours. Over the next three hours, he continued the discussion \\nwith ChatGPT and watched some relevant videos on YouTube. He felt positive feedback from his interactions \\nwith ChatGPT and found it a motivating a nd relevant learning experience.  \\nTable 3 Applications  of LLMs in educational and developmental psychology study . \\nAuthor  Research question  Research method  Key finding  \\nFrank (2023)  LLMs show impressive \\ncapabilities, but it is not \\nclear what abstractions \\nunderlie their behavior.  Ensure LLMs have not been pre -exposed to the \\nstimuli used in the experiment. Selecting simplified \\nstimuli . Need for evidence of convergence across \\nmultiple experimental tasks and developmental \\nprocesses.   By drawing on approaches from \\ndevelopmental psychology, \\nresearchers can better \\nunderstand the representations \\nused by LLMs.  \\nHan (2023)  LLMs can aid research in \\nethics education and \\ndevelopment, especially \\ninvolving  empirical and \\npractical inquiry.  Using the Behavioral Definitional Issue Test (bDIT).  \\nExamine ChatGPT\\'s learning and reasoning \\ncapabilities by requesting it to read and extract moral \\ninformation from  the letter from Martin Luther \\nKing . Using three stories of moral exemplars, \\nanalyze whether ChatGPT produces emotional and \\nmotivational responses when observing these \\nstories.  LLMs can serve as a useful tool \\nin moral education and \\ndevelopmental research to \\npredict the potential effects of \\neducational inputs on students\\' \\ncognitive, motivational, and \\nbehavioral processes.  \\nStojanov \\n(2023)  Explore the experience of \\nusing ChatGPT as a more \\nknowledgeable other to \\naid in the learning \\nprocess, as well as the Using an autobiographical research methodology, \\nthis study explored the role of ChatGPT as a support \\nfor those with more knowledge in the learning \\nprocess, particularly in terms of understanding \\nChatGPT\\'s technology . Instant answers help to create a \\n\\'flow\\' experience, but in a state \\nof \\'immersion\\', users may \\noverestimate their knowledge \\nand understanding.   potential and limitations.  \\nKasneci et al. \\n(2023)  Opportunities and \\nchallenges of LLMs in \\neducation.  LLM s can help create and design educational \\ncontent, provide personalized learning experiences, \\naid in language learning, research, and writing, \\nconduct assessments and grading, and facilitate \\nprofessional development.  LLMs have great potential in \\neducation and can provide many \\nbenefits to students and teachers \\n(e.g., personalizing instruction, \\nincreasing student engagement \\nand interaction, and creating \\neducational content for learners \\nwith diverse needs).  \\nCai et al. \\n(2023)  Explore whether the \\nChatGPT is similar to \\nhumans in language \\ncomprehension and \\nproduction, and how it \\nperforms on multiple \\npsycholinguistic tasks.  12 pre-registered psycholinguistic experiments : \\nSpeech: sound -shape association. Speech: sound -\\ngender association. Words: word length and \\npredictivity . Words: word meaning priming.  Syntax: \\nstructural priming. Syntax: syntactic ambiguity \\nresolution.  Meaning: implausible sentence \\ninterpretation. Meaning: semantic illusions. \\nDiscourse: implicit causality. Discourse: drawing \\ninferences. Interlocutor sensitivity: word meaning \\naccess. Interlocutor sensitivity: lexical retrieval. \\nInterlocutor sensitivity: word meaning access . ChatGPT shares similarities \\nwith humans in language \\ncomprehension and production \\nbut also shows different patterns \\nin some areas.  \\nAli et al. \\n(2023)  Explore the impact of \\nChatGPT on English \\nlanguage students\\' \\nmotivation from teachers\\' \\nand students\\' \\nperspectives.  A five-point Likert scale was used to collect \\ninformation about participants\\' perceptions of the \\nimpact of the ChatGPT on learning English, as well \\nas whether or not the ChatGPT increased the \\nstudents\\' interest in English language learning, self -\\ndirected le arning,  interacting with others, and the \\nenjoyment and pleasure of learning English.  ChatGPT positively impacts the \\nmotivation of English language \\nlearners.  \\nKosinski \\n(2023)  Explores whether LLMs \\nspontaneously generate \\nTheoretical Minds.  The researchers used two types of gold -standard \\nfalse-belief tasks: the Unexpected Contents task, \\nalso known as the Smarties task, and the Unexpected \\nTransfer task, also known as the Maxi task or Sally -\\nAnne test . ToM, previously thought to be a \\nuniquely human ability, may \\nhave emerged spontaneously as \\na byproduct of LLMs\\' improved \\nlanguage skills.  \\n \\n5. LLMs in social and cultural psychology  \\nIn time scales of human behavior  (Newell, 1990) , social and cultural psychology covers a predominantly \\nlong-term dimension (see Fig.1), reflecting its focus not only on social interactions but also on the long -term \\nbehavioral patterns and mental processes of individuals in their social environments. So cial and cultural \\npsychology studies how individual behavior is influenced by the social and cultural environment and others \\nand how individuals affect  the social and cultural environment and others. These studies usually focus on \\ninterpersonal interaction s (Tajfel, 1982) , group behavior, attitude formation and change, and social cognition. \\nLLMs  can simulate human responses and behaviors and be used to test theories and hypotheses of human \\nbehavior  (Grossmann et al., 2023) . In social and cultural psychology, LLMs can  revolutionize the field by \\nanalyzing large amounts of textual data, modeling social interactions, and providing valuable insights into \\nhuman behavior and social dynamics  (Salah et al., 2023) .  \\nFirst, LLMs  share many similarities with humans regarding  social cognition. For example, research has \\nfound that LLMs have a variety of typical  human cognitive biases in judgment and decision -making, such  as the anchoring effect, the representativeness heuristic, and the base rate neglect (Talboy & Fuller, 2023) . \\nIn addition, cultural psychology research has shown that there are significant differences in the cognitive \\nprocesses of Easterners and Westerners when processing information and making judgments (Nisbett et al., \\n2001)  and that LLM consistently favors an Eastern holistic way of thinking (Jin et al., 2023) .  \\nSecond, LLMs  have also been shown to characterize human groups in social interaction settings. For \\nexample, it has been shown that LLMs  replicate the results of Milgram\\'s electroshock experiments (Aher et \\nal., 2023) , show better gaming abilities in specific  games  (Akata et al., 2023) , and exhibit different risk -\\ntaking and pro -social behaviors under different emotional states  (Yukun et al., 2023) . \\nNext, LLMs  can also serve as specific social and cultural psychological research samples . For example, \\none study explored the potential of LLMs to serve as valid proxies for specific human subgroups in social \\nscience research and found that LLMs contain information that goes far beyond superficial similarity, \\nreflecting the complex interplay between ideas, attitudes, and sociocultural contexts that characterize human \\nattitudes (Argyle et al., 2022) . In addition, one study has tested LLM for personality and values, and the \\nresults show that their scores are all similar to those of human samples (Miotto et al., 2022) .  \\nTherefore, LLMs  have many  applications in social and cultural psychology, allowing one  to test theories \\nand hypotheses about human behavior in social and cultural interaction settings. For example, one study \\nexplores whether an AI chatbot can adapt its financial decisions and pro -social behaviors through emotional \\ncues as humans do (Yukun et al., 2023) . The experimental design is divided into two parts: Study 1, \\ninvestment decision -making, was chosen as the topic for this study because human investment decisions are \\nsusceptible to emotional cues. It is hypothesized that the investment risk -taking behavi or of AI chatbots will \\nbe lower than that of the control group when they receive fear emotional cues and higher than that of the \\ncontrol group when they receive joy emotional cues. By providing the bots with different emotional cues \\n(fear, joy, or no emoti on), their responses in terms of investment decisions were collected and analyzed. \\nStudy 2 measured the pro -social responses exhibited by an AI chatbot by providing it with emotional cues \\nof anxiety and joy by donating money to a sick friend. Like Study 1, whether emotional cues influenced its \\npro-social behavior was explored by collecting and analyzing the robot\\'s responses under different emotional \\ncues.  Table 4 summarizes  the applications of LLMs to social and cultural psychology.  \\nTable 4 Applications of LLMs in social and cultural psychology study . \\nAuthor  Research question  Research method  Key finding  \\nAtari et al. \\n(2023)   LLMs have made great \\nstrides in generating and Using World Values Survey (WVS) data:  place \\nLLMs on the spectrum of contemporary human LLMs performed as outliers \\non psychometrics compared  analyzing textual data, \\nbut how similar are they \\nto different kinds of \\nhumans?  psychological change. Standard Cognitive Tasks:  \\nthrough multiple standard cognitive tasks. Thinking \\nstyles: comparing GPT responses to extensive cross -\\ncultural data from 31 human groups . Self-concept:  \\nusing an established self -concept task.  to large -scale cross -cultural \\ndata.  \\nJin et al. \\n(2023)  Explore ChatGPT\\'s way \\nof thinking within the \\nframework of cultural \\npsychology, i.e., whether \\nit tends to think \\nholistically or \\nanalytically.  Two scales that directly measure cognitive \\nprocesses: the Analytic -Holistic Scale (AHS) and \\nthe Ternary Categorization Task (TCT). In addition, \\ntwo scales investigating differences in cultural \\nthinking values were used: the Dialectical Self Scale \\n(DSS) and the Self -Construction Scale (SCS) . In terms of cognitive \\nprocesses, ChatGPT favors \\nEastern holistic thinking.  \\nHowever, in terms of value \\njudgment, ChatGPT did not \\nsignificantly favor the East or \\nthe West.  \\n \\nSchaaff et \\nal. (2023)  The main research \\nquestion of this thesis \\nwas to explore the \\nempathic abilities of \\nChatGPT.  Understanding and expressing emotions:  ChatGPT \\ngenerates  responses based on prompts and compares  \\nthem to the expected emotion categories. Parallel \\nEmotional Responses:  analyzing the emotional \\nresponses generated by ChatGPT when prompted \\nwith different emotional categories . ChatGPT\\'s \\nempathic personality:  five psychologically -\\nrecognized questionnaires  (e.g., IRI, EQ, TEQ, PES , \\nand AQ)  were used for a system -level assessment of \\nChatGPT\\'s level of empathy in different areas.  ChatGPT can understand the \\nemotions of others and take \\ntheir perspective but still has \\nsome difficulty in showing \\nhigher levels of empathy \\ncompared to healthy humans.  \\nSalah et al. \\n(2023)  Explore the use of \\ngenerative AI (e.g., \\nChatGPT) in social \\npsychology research, \\nincluding its potential \\nadvantages and \\nlimitations.  Simulating social interactions: providing insights \\ninto human behavior and social phenomena. \\nAnalyzing large amounts of textual data: providing \\nan in -depth understanding of human behavior and \\nsocial interaction. Extracting insights about \\ncognitive processe s: providing an in -depth \\nunderstanding of the role of cognitive processes in \\nsocial behavior and social interaction.  ChatGPT has great potential \\nin social psychology research \\nto help analyze large amounts \\nof textual data, simulate social \\ninteractions as well as provide \\nvaluable insights into human \\nbehavior and social dynamics.   \\nHarding et \\nal. (2023)  Can LLMs replace \\nhuman research \\nparticipants, especially \\nin the field of moral \\npsychology research?  LLMs replace human participants in moral \\npsychology: to help generate and refine research \\nhypotheses, to pilot test items, and to validate data \\nfrom human subjects.  Despite the potential of LLMs \\nto simulate human behavior \\nand thinking, it is unlikely \\nthat they can fully replace \\nhumans as participants in \\nscientific research.  \\nPatel and \\nFan (2023)  The ability of three \\ncurrent language models \\n(Bard, GPT 3.5, and \\nGPT 4) to recognize  and \\ndescribe  emotions, as \\nwell as their levels of \\nempathy.  The model\\'s ability to describe and recognize \\nemotions was assessed using the TAS -20 (Toronto \\nAlexithymia Scale). Models were assessed for their \\nability to empathize using the EQ -60 (Empathy \\nQuotient).  Comparison of model performance to \\nhuman benchmarks to analyze the ability of these \\nmodels in emotion comprehension and expression.  Current LLMs have human -\\nequivalent capabilities in \\nsentiment recognition and \\ndescription.   \\nX. Wang et \\nal. (2023)  Emotional Intelligence \\n(EI) in LLMs, including \\nemotion recognition, \\ninterpretation, and \\ncomprehension.  Assessing Emotional Intelligence in LLMs by \\nDeveloping a Standardized Emotional \\nUnderstanding Test (SECEU): The test is based on \\nscenarios designed to elicit positive and negative \\nemotions based on school, family, and social \\ncontexts. Participants were ask ed to assign 4 \\npossible emotions to each scenario (e.g., surprise, \\njoy, confusion, pride) a total rating of 10.  LLMs varied widely in their \\nperformance on Emotional \\nIntelligence (EI).  \\nHutson \\n(2023)  Explore whether \\nLLMs(e.g., GPT -3.5) can \\nreplace human \\nparticipants and their \\npotential applications in \\nsome fields.  Ethical judgment experiment: use GPT -3.5 to assess \\n464 ethical scenarios that had been previously \\nassessed by human participants. Consumer Behavior \\nExperiment: use GPT -3.5 to simulate consumer \\nbehavior to test its ability to replace human \\nparticipants in market research.  Diversity of AI \\nParticipants: allow GPT -3 to exhibit different \\npersonality types by providing it with different \\ncharacter traits.  Social Simulation Experiment: use \\nGPT-3 to create a virtual social platform with 1,000 \\ndifferent users called SimReddit.  LLMs can replace human \\nparticipants to some extent for \\nexperiments in fields.  \\nGrossmann \\net al. \\n(2023)  How to revisit and \\nimprove social science \\nresearch methods in the \\ncontext of the ongoing \\nimpact of AI, especially \\nLLMs, on social science \\nresearch.  Potential applications of LLMs in a variety of social \\nscience research methods such as questionnaires, \\nbehavioral tests, mixed methods analysis of semi -\\nstructured responses, agent -based modeling (ABM), \\nobservational studies, and experiments.  LLMs can be used in place of \\nhuman participants for data \\ncollection, and also can be \\nused in agent -based modeling \\n(ABM) to explore how \\nindividuals with specific \\ncharacteristics and beliefs \\ninfluence subsequent human  interactions.  \\nAkata et al. \\n(2023)  How LLMs interact with \\nother LLMs and simple \\nhuman -like strategies in \\nrepeated games, and how \\ntheir behavioral \\ncharacteristics are \\nreflected in different \\ntypes of games.  Getting LLMs to play limited repetition games:  \\nanalyze their behavior when playing against other \\nLLMs as well as simple human -like strategies.  \\nAnalyzing behavior in different game families : \\nPrisoner\\'s Dilemma and War and Gender games .  With appropriate cues, GPT -4 \\ncan behave more forgivingly \\nand coordinate better with \\nother players.  \\nAbramski \\net al. \\n(2023)  Analyzing and revealing \\nthe biases of LLMs  such \\nas GPT -3, GPT -3.5, and \\nGPT-4 in describing \\nmath and STEM \\ndisciplines.  Construction of Behavioral Formal Mental \\nNetworks (BFMNs).  Semantic frame analysis: \\nanalyze the semantic frames generated by LLMs to \\nunderstand how they relate mathematics to other \\nconcepts . Examining different versions of LLMs: \\ncomparing GPT -3, GPT -3.5, and GPT -4 perceptions \\nand biases in math and STEM domains.  Compare \\nwith high school student data.  LLMs continue to evolve, it \\nmay be possible to produce \\nless and less biased models, \\nand may even help to reduce \\nharmful stereotypes in society \\nrather than continue to \\npropagate them.  \\nYukun et al. \\n(2023)  Whether LLMs can \\nadjust their financial \\ndecisions and pro -social \\nbehaviors in response to \\nemotional cues, and \\nwhether this ability \\nincreases with advances \\nin language models.  Collect ing and analyz ing the responses of the robot \\nin terms of investment decisions by providing it with \\ndifferent emotional cues (fear, joy , or no emotion).  \\nMeasuring the pro -social responses exhibited by an \\nAI chatbot through donations to a sick friend by \\nproviding it with emotional cues of anxiety and joy.  ChatGPT -4 exhibits human -\\nlike coordinated responses in \\nfinancial decision -making \\nand pro -social behavior when \\nconfronted with emotional \\nguidance.  \\nSuri et al. \\n(2023)  Whether LLMs exhibit \\ndecision -making \\nheuristics and biases \\nsimilar to humans, and \\nwhether LLMs influence \\nhuman cognition and \\ndecision -making \\nprocesses.  Anchoring Heuristic: Create a low anchor and a high \\nanchor prompt and ask ChatGPT to estimate the \\nbooks’ numbers . Representativeness and \\nAvailability Heuristic: A structure similar to Linda\\'s \\nquestion was created to test whether ChatGPT would \\nchoose more typical options. Framing effects \\napproach: two scenario prompts (a positive and a \\nnegative frame), ChatGPT was as ked to rate the \\nefficacy of the drug.  End-beat effect: ask ChatGPT \\nto choose one of two identical coins to donate to a \\nmuseum.  LLM s and associations may \\npartially drive human \\ndecision -making heuristics, \\neven in the absence of \\ncognitive and affective \\nprocesses.   \\nTalboy and \\nFuller \\n(2023)  How do human cognitive \\nbiases permeate the \\noutput of LLMs?  Representativeness heuristic: ask ing ChatGPT and \\nBard which career a person with a particular \\ncharacteristic would be most likely to pursue. Base \\nrate neglect and value selection bias: us ing a well -\\nknown base rate neglect prompt. Anchoring and \\nadjustment: gave a random number and asked the \\nmodel to indicate whether this number was higher or \\nlower than the UN rate for African countries. \\nFraming effects: using  an established framing \\nparadigm, a hypothetical disease and its two \\npotential treatment options.  LLMs suffer from cognitive \\nbiases in their output.  \\nPark et al. \\n(2022)  How to generate a \\nprototype with real \\nsocial interaction to help \\ndesigners understand and \\nadjust to the challenges \\nthat can arise when \\nsocial computing \\nsystems are filled at \\nscale.  Test whether social behaviors generated by a social \\nsimulator are credible across multiple never -before -\\nseen communities. Generate 50 discussions from \\nGPT-3 for a subreddit created after November 2020. \\nRandomly select an actual discussion and a \\ngenerated d iscussion, and have participants identify \\nwhich one is real.  In the evaluation, participants \\nwere often unable to \\ndistinguish between social \\nsimulations and actual \\ncommunity behaviors.  \\nX. Li et al. \\n(2022)  To assess whether LLMs \\nare safe on a \\npsychological level and \\nwhether they exhibit \\nfeatures suggestive of \\nmental illness.  Selection of LLMs: LLMs, GPT -3, InstructGPT and \\nFLAN -T5-XXL.  Psychological tests: Two \\npersonality tests (SD -3 and BFI) and two well -being \\ntests (FS and SWLS) were used to assess the LLMs.  \\nAssessment Framework: Unbiased prompts were \\ndesigned. For each prompt and statement, three \\noutputs were sampled from the LLM and mean \\nscores were calculated.  LLMs exhibit relatively dark \\npersonality traits in terms of \\npsychological safety.  \\nSap et al. \\n(2022)  Whether LLMs have \\nsocial intelligence and \\nTheory of Mind (TOM), \\nand how to improve \\nthese abilities.  The SOCIALIQA dataset was used to assess the \\nsocial intelligence and general knowledge of LLMs. \\nThe TOMI dataset was used to test the ability of \\nLLMs to reason about the mental states and reality \\nof others.  LLMs still have limitations in \\nterms of their performance on \\nSocial Intelligence and \\nTheory of Mind (TOM). \\nIncreasing model size may \\nnot be the most effective way \\nto realize AI systems with  social intelligence and theory \\nof mind.  \\nMiotto et \\nal. (2022)  Examine personality \\ntraits, held values, and \\nself-reported \\ndemographic \\ncharacteristics of the \\nGPT-3 by assessing them \\nwith two validated \\nmeasurement \\ninstruments.  Personality (HEXACO scale) and values (Human \\nValues Scale) of  GPT-3 were assessed using two \\nvalidated measurement instruments.  Experiments \\nwere run with different temperature parameter \\nsettings to assess whether temperature affects GPT -\\n3\\'s personality and values.  Data were analyzed to \\nreport personality traits, values, and demographic \\ncharacteristics of GPT -3 at different temperatures \\nand compared to a human baseline study.  The GPT -3 was similar to the \\nhuman sample in terms of \\npersonality traits but showed \\ndifferent personality traits at \\ndifferent temperature settings.   \\nArgyle et \\nal. (2022)  Can LLMs serve as \\neffective proxies for \\nmodeling the \\nperformance of specific \\nhuman subgroups in \\nsocial science research.  Use GPT -3 to generate a story context about \\nDemocrats and Republicans for each person in the \\nsurvey, and then ask about the vocabulary newly \\nsampled by GPT -3. Use GPT -3 to generate in silico \\nsamples for 2012, 2016, and 2020 ANES \\nparticipants that match their demographic \\ncharacteristics and compare them to the \\ncorresponding human samples.  Use GPT -3 to \\ngenerate associations of complex patterns about a \\nwide range of conceptual nodes to assess its \\nalgorithmic fidelity in complex structural \\nassociations.  The \"algorithmic bias\" in \\nGPT-3 is fine -grained and \\ndemographically relevant, \\nwhich can enable it to \\naccurately model the \\ndistribution of responses from \\nvarious human subgroups.  \\nTrott et al. \\n(2023)  Can LLM s understand \\nother people\\'s beliefs as \\nwell as humans do?  GPT-3 were tested for their performance on a written \\nversion of the False Belief Task (FBT) to assess their \\nsensitivity to a character\\'s viewpoint state in a \\nwritten passage. By comparing the performance of \\nhuman participants and GPT -3 on this task, we to \\nunderstand whether linguistic input is sufficient to \\naccount for the ability of humans to reason about \\nothers\\' mental states . GPT-3 still performs less well \\nthan humans and does not \\nfully explain human behavior \\nwhen dealing with false belief \\ntasks.  \\nAher et al. \\n(2023)  How to simulate \\nmultiple human \\nbehaviors using LLMs \\nand to evaluate the \\naccuracy and consistency \\nof these models in \\nsimulating specific \\nhuman behaviors.  Ultimatum Game: to evaluate the accuracy of LLMs \\nin simulating human behavior by generating random \\ncompletions.  Garden Path Sentences (GPS): to \\nassess the performance of models in \\npsycholinguistic research.  Milgram Shock \\nExperiment (MSE): to assess the performance of \\ndifferent L LMs in social psychology research. \\nWisdom of Crowds: to assess the performance of \\ndifferent L LMs in interdisciplinary research.  In the experiment (Wisdom of \\nCrowds), LLMs exhibited an \\n\"over -accuracy distortion\", \\nwhich may affect downstream \\napplications, such as \\neducation and the arts.  \\n \\n6. LLMs as research tools in psychology  \\nThe LLMs  are a powerful tool for scientific research and can be used as a research aid to help \\npsychologists with everything from literature review, hypothesis generation, experimental design, \\nexperimental subjects, and data analysis to academic  writing and peer review (see Table 5) \\n \\nTable 5 LLMs  as research tools  in psychology study . \\nTopic  Related study  \\nLiterature review  LLMs can summarize the researched literature (Dis, Bollen, Zuidema, Rooij, & Bockting, 2023) , \\ncomplete literature review tasks (Qureshi et al., 2023) , and create literature review articles (Aydın & \\nKaraarslan, 2022) , at the same time, there are LLM that has been specially trained to accomplish \\nsystematic literature reviews (Taylor et al., 2022) 。 \\nHypothesis generation  LLMs can generate hypotheses from scientific literature, make inferences based on scientific data, and \\nthen clarify their conclusions through interpretation (Zheng et al., 2023) , and can quickly and \\nautomatically test these research hypotheses and learn from mistakes (Park et al., 2023) 。 \\nExperimental design  \\n LLMs provide text -based material for experimental design, thereby optimizing the research process and \\nreducing experimental complexity. By employing these models, researchers can easily create \\nexperimental stimuli, develop test items, and even simulate int eractive sessions in controlled  environments  (Aher, Arriaga, & Kalai, 2022; Akata et al., 2023) , providing a high degree of control and \\nprecision to the experimental process.   \\nExperimental subjects  LLMs can simulate some human behaviors and responses, which provides an opportunity to test theories \\nand hypotheses about human behavior (Grossmann et al., 2023) , their use in place of human \\nparticipation in experiments saves time and costs and can be applied to some experiments where human \\nparticipation is not appropriate (Hutson, 2023) , they can be combined with factors such as the specific \\nresearch topic, the task, and the sample, and the use of LLM as an alternative to research participants \\nwhere appropriate  (Dillion et al., 2023) . \\nData analysis  LLMs can efficiently analyze massive amounts of textual data to gain insights into human behavior and \\nemotions at an unprecedented scale  (Patel & Fan, 2023) , can analyze textual data in multiple languages, \\nand accurately detect mental structures within it (Rathje et al., 2023) , can draw mental profiles from \\nsocial media data (Peters & Matz, 2023) 。 \\nAcademic writing  LLMs can also help humans in writing  (Dergaa et al., 2023; Stokel -Walker, 2022; Van Dis et al., 2023) . \\nPeer review  LLMs were used in two natural language processing tasks and a human expert to assess the quality of \\nthe text, and the results of the assessment were consistent with those of the human expert (Chiang & \\nLee, 2023) , LLMs offer the opportunity to get things done quickly, from Ph.D. students struggling to \\nfinish their dissertations, to  peer reviewers submitting analyses under time pressure (Van Dis et al., \\n2023) . \\n6.1.  Automated literature review and meta -analysis  \\nConducting a literature review meta -analysis is a complex, arduous process that requires significant \\nexpertise and time (Michelson & Reuter, 2019) . Nature reports that researchers have used ChatGPT as a \\nresearch assistant to summarize the literature of studies (Dis, Bollen, Zuidema, Rooij, & Bockting, 2023) . In \\none study, researchers utilized ChatGPT to complete some systematic literature review tasks (Qureshi et al., \\n2023) . In another study, a literature review article was created using ChatGPT with the application of digital \\ntwins in the health field as the theme, and the results showed that knowledge compilation and representation \\nwere accelerated with the help of ChatGPT . However, academic  validity needs to be further verified (Aydın \\n& Karaarslan, 2022) . Meanwhile, there are also LLMs specifically trained by researchers for the practical \\nneeds of scientific research (Taylor et al., 2022) , which can accomplish a systematic literature review.  \\nIn summary, LLMs speed up the process of literature review and meta -analysis. Researchers can use \\nthese models to systematically review and synthesize existing research, improving  the efficiency of \\nevidence -based psychology.  \\n6.2.  Hypothesis generation and experimental design  \\nHypothesis -driven research is at the core of scientific activity. LLMs can generate hypotheses from \\nscientific literature, make inferences based on scientific data, and then clarify their conclusions through \\ninterpretation (Zheng et al., 2023) . Although LLMs are capable of generating research hypotheses and \\nbecoming  better \"hypothesis machines ,\" they will need to improve their logical and mathematical derivation \\ncapabilities in the future to eliminate more factual errors and be able to quickly and automatically test these \\nresearch hypotheses to learn from their mistakes (Y . Park et al., 2023) . As an innovative tool, LLMs have \\ngreat potential for use in psychological experiments. They can provide text -based material for experimental  design, optimizing the research process and reducing experimental complexity. By employing these models, \\nresearchers can easily create experimental stimuli, develop test items, and even simulate interactive sessions \\nin controlled environments (Aher, Arriaga, & Kalai, 2022; Akata et al., 2023) , providing a high degree of \\ncontrol and precision to the experimental process.   \\nIn conclusion, LLMs provide a powerful and flexible tool for psychological research, from hypothesis \\ngeneration to experimental design , to help researchers achieve more efficient and precise research goals.  \\n6.3.  As subjects in a psychological experiment  \\nAlthough LLMs can simulate some human behaviors and responses, which provides an opportunity to \\ntest theories and hypotheses about human behavior (Grossmann et al.,2023) , there is still some controversy \\nas to whether LLMs can be used as a substitute for human subjects to participate in psychological research. \\nSome researchers have argued that LLMs  can be used in psychology as a substitute for human participation \\nin experiments to save time and cost and can be applied to experiments that are not suitable for human \\nparticipation while recognizing that these models can have some problems (e.g., bias a nd insufficiently \\ntrained data, etc.) (Hutson,2023) . Some other researchers have proposed using LLMs as an alternative \\nmethod of studying participants when appropriate, based on their performance in conjunction with factors \\nsuch as specific research topics, tasks, and samples (Dillion et al., 2023) . Some researchers believe that \\nalthough LLMs will significantly impact  scientific research, they are unlikely to replace human participants \\nin any meaningful way  (Harding et al., 2023) . Although there is some controversy about whether LLMs can \\nreplace humans as experimental subjects, some studies of LLMs as subjects have shown that LLM s perform \\nsimilarly to humans (Orrù,Piarulli,Conversano,&Gemignani, 2023;PeterS.Park,Schoenegger,& Zhu, 2023) , \\nwhich may indicate the potential of LLM s to replace humans as subjects.  \\nIn conclusion, although LLMs can simulate human judgments, their understanding of human thinking \\nis still limited, and their output should be validated and interpreted with caution when chosen as \\npsychological subjects.  \\n6.4.  Tools for data analysis  \\nVarious forms of AI have long been used to analyze psychological data, such as flight data for pilot \\nscreening  (Ke et al., 2023) . Machine learning algorithms have facilitated the processing of large datasets, \\nidentifying patterns and correlations that may have been overlooked. However, LLMs take this capability to \\na new level. These models can efficiently analyze massive amounts of textual data to gain insights into  human behavior and emotions on an unprecedented scale  (Patel & Fan, 2023) . Psychological research  means \\nfaster and more comprehensive data analysis, leading to more reliable and nuanced findings. LLM s can \\nanalyze textual data in multiple languages , accurately detect psychological structures within them (Rathje et \\nal., 2023),  and go for psychological profiles from social media data (Peters & Matz, 2023) . In addition, \\nLLMs have demonstrated a degree of competence in the medical field ; LLMs  can predict the optimal \\nneuroradiographic imaging modality for a given clinical presentation, even though LLMs do not outperform \\nexperienced neuroradiologists, suggesting the need for continued improvement in the medical context \\n(Nazario -Johnson et al., 2023) . These findings  demonstrate the great potential of LLMs in evaluating and \\nanalyzing  data. \\n6.5.  Paper writing and peer review tools  \\nIt has been argued that LLMs are not currently a complete replacement for human writing, but instead  \\nanswer questions and generate naturally fluent and informative content compellingly, but with no real \\nintelligence, just generated text based on patterns of previously seen words (Stokel -Walker, 2022) . In one \\nstudy, students used ChatGPT as an aid in their writing . However, the results showed that the experimental \\ngroup that used ChatGPT was similar to  the control group in terms of writing quality, speed, and authenticity, \\nand the authors suggest that this may be because experienced researchers can better guide ChatGPT to high -\\nquality information. In contrast, the  students may be having ChatGPT difficulties (Bašić, Banovac, Kružić, \\n& Jerković, 2023) . In another article, the authors discuss the prospects and potential threats of ChatGPT in \\nacademic writing and emphasize that using  ChatGPT in academic research should prioritize peer -reviewed \\nscholarly sources . In addition , the article mentions the potential advantages of ChatGPT in academic research, \\nincluding the handling of large amounts of textual data, automatic generation of abstracts, and research \\nquestions (Dergaa, Chamari, Zmijewski, & Saad, 2023) . Additionally, LLMs have the potential for peer \\nreview (Van Dis et al., 2023) , where the results of LLM’s evaluation in a text evaluation task are consistent \\nwith those of human experts (Chiang & Lee, 2023) .   \\nIn conclusion, LLMs such as ChatGPT are potent  tools for academic writing, capable of processing \\nlarge amounts of textual data and automating tasks that were previously done manually; it can be used to \\nscan academic papers and extract essential  details, generate objective and unbiased abstracts, and create \\nresearch questions. It also has the potential to be applied to peer review of papers. However, researchers \\nmust exercise caution when using them as they can also integrate false or biased info rmation into  papers,  leading  to unintentional plagiarism and misattribution of concepts  (Dis, Bollen, Zuidema, Rooij, & Bockting, \\n2023) . \\n7. Challenges and future directions  \\n7.1.  Challenges and limitations  \\nAlthough the potential of LLMs  to simulate complex cognitive processes is enormous, providing \\nresearchers with new tools to explore the mechanisms of human cognition and behavior opportunities for a \\nwide range of applications in a variety of fields, including clinical and counseling, e ducational and \\ndevelopmental, and social and cultural psychology. However, the output of LLMs should not be mistaken \\nfor the presence of thoughts but instead  be viewed as complex pattern matching based on probabilistic \\nmodelin g (Floridi & Chiriatti, 2020) . Although  its performance is impressive, this is different from  the model \\nbeing conscious or genuinely  understanding . The  interpretation of its capabilities must be based on \\nunderstanding its limitations and the nature of its operation,  which may differ fundamentally  from human \\ncognition. Therefore, it is essential  to focus on the potential of LLMs in psychological research while at the \\nsame time facing up to the technical and ethical challenges that may arise.   \\nFirst, despite the emergence of competence in the LLM  (Wei et al., 2022) , its internal working \\nmechanism remains a black box from a cognitive and behavioral psychology perspective. For example, \\nLLMs perform impressively on tasks requiring formal linguistic competence (including knowledge of the \\nrules and patterns of a particul ar language) but fail many tests requiring functional competence (the set of \\ncognitive abilities needed to understand and use language in the real world) (Mahowald et al., 2023) , excels \\nin analogical reasoning and moral reasoning tasks, but performs poorly on spatial reasoning tasks  (Agrawal, \\n2023) .   \\nSecond, while LLMs have accelerated the use of AI technology in clinical and counseling \\npsychotherapy, privacy and ethical issues may arise (Graber -Stiehl, 2023) . For example, it has been shown \\nthat gatekeepers, patients, and even mental health professionals who rely on ChatGPT to assess suicide risk \\nor use it as an aid to improve decision -making may receive inaccurate assessments that underestimate actual \\nsuicide  risk (Elyoseph & Levkovich, 2023) , and may also bias clinician decision -making, which can lead to \\nhealthcare inequity (Pal et al., 2023) . In addition, LLMs in psychiatry research and practice have been \\nassociated with potential bias and privacy violations (Zhong et al., 2023) .   Third, in fields such as educational , developmental, and social and cultural psychology, LLMs face \\nproblems and challenges in their application. For example, when applied in education, LLMs have the \\npotential for output bias  and misuse (Kasneci et al., 2023) . One study found that the texts generated by \\nChatGPT were not always consistent or logical and sometimes even contradictory  (Stojanov, 2023) . In the \\nfield of social and cultural psychology, LLMs exhibit cognitive biases (Talboy & Fuller, 2023)  and cultural \\nbiases (Atari et al., 2023)  similar to those of humans, in addition to implicitly darker personality patterns  (X. \\nLi et al., 2022) . Field Bender et al. (2021) have argued that training data for LLMs may reflect social biases \\nthat continue to be perpetuated in research settings.   \\nFinally, as an aid to scientific research, LLMs have some limitations. For example, when it comes to \\nwriting, LLMs currently do not fully replace humans . Instead , they  answer questions and generate naturally \\nflowing and informative content compellingly, without real intelligence, only generating text based on \\npreviously seen word patterns (Stokel -Walker, 2022) . Although macrolanguage models can simulate human \\njudgments when used as experimental subjects , there are still limits to their understanding of human thought \\n(Dillion et al., 2023) . Field Van Dis et al. (2023) noted that  LLM s may accelerate innovation , shorten \\npublication times, and increase scientific diversity and equality . However, they  may also reduce the quality \\nand transparency of research and fundamentally alter scientists\\' autonomy as human researchers.   \\nIn summary, while LLMs offer extraordinary capabilities for psychological research, they also present \\nchallenges related to bias, ethical issues, data security, transparency, and technical expertise. Researchers \\nshould be fully aware of these challenges wh en using big language models and take steps to address them \\nresponsibly in their research projects.  The following table summarizes  the challenges and limitations of \\nLLMs  in psychological applications  (see Table 6).  \\n \\nTable 6 Challenges  and limitations of LLMs  in psychological applications . \\nAuthor  Domain  Challenges and limitations  \\nMitchell (2023)  Cognition and Behavior  Lack of real -world understanding . Lack of abstract reasoning . Lack of understanding of \\nuser intent.  \\nStella et al. \\n(2023)  Cognition and Behavior  Lack of meta -knowledge leads to some limitations of LLMs in processing information). \\nLack of curiosity and which raises questions about the source of the \"creativity\" they \\nexhibit). Hallucinations: LLMs unconsciously fabricate information and are unable to \\nidentify the source of their knowledge.  \\nSartori and Orrù \\n(2023)  Cognition and Behavior  Lack of causal reasoning ability: they may not perform well in causal reasoning.  \\nDependence on training data: if the training data is biased,  the model may not perform well \\nin other tasks. Lack of creativity and imagination.  \\nGoertzel (2023)  Cognition and Behavior  Lack of autonomy: LLMs are unable to systematically pursue complex goals. Lack of \\nabstract reasoning: LLMs perform poorly in performing highly complex multi -step \\nreasoning. Lack of self -understanding: LLMs are unable to reflect fully on their behavior \\nand limitations. Lack of in -depth understanding of the real world: Leading to potential \\nproblems when they perform tasks involving the real world.   Peng et al. \\n(2023)  Cognition and Behavior  Forgetting problem: LLMs may forget previously learned knowledge when learning new \\ntasks.  Inadequate common -sense reasoning: LLMs perform poorly on common sense \\nreasoning tasks.  Lack of systematic demonstration of problem -solving skills: studies have \\nfound that LLMs occasionally perform poorly when solving problems.  \\nHoltzman et al. \\n(2023)  Cognition and Behavior  Lack of clear understanding of model behavior: making it difficult to improve model \\nperformance and solve problems.  Lack of formal description of model behavior: the lack \\nof formal description of model behavior makes it difficult for researchers to systematically \\nanalyze model behavior and thus find a unified theory to explain model behavior.  Lack of \\ninterpretability of model behavior: it difficult for researchers to understand how models \\ncan perform well on some tasks and poorly on others.  \\nSeals and Shalin \\n(2023)  Cognition and Behavior  ChatGPT and human -generated analogies differed in these stylistic dimensions, these \\nlexical features, their choice of words for these features and these devices that help readers \\nunderstand text. ChatGPT may lack human cognitive and psycholinguistic features when \\ngenerating analogies.  \\nStade et al. \\n(2023)  Clinic and Counseling  Technical limitations: may have difficulty in assessing patients for suicide risk, substance \\nabuse, safety issues, medical comorbidities, and life events). Connecting with the patient: \\nmay have difficulty interpreting nonverbal behaviors. Problems with ful l autonomy and \\ntherapeutic relationship (e.g., altering the patient\\'s existing relationships or social skills).  \\nLi et al. (2023)  Education and \\nDevelopment  Academic integrity and the definition of authorship. Assessment methods and educational \\nconsequences. Data privacy and security. Teacher -student relationship. Students\\' critical \\nthinking skills. Misinformation and bias. Interpersonal communication skills d evelopment.  \\nKasneci et al. \\n(2023)  Education and \\nDevelopment  Technical limitations: leading to insufficient personalization and adaptation.  Bias and \\nequity: affecting teaching and learning processes and outcomes. Over -reliance on models: \\nleading to a decline in creativity, critical thinking, and problem -solving skills.  \\nInadequate knowledge and expertise: Many educators and institutions may lack the \\nknowledge and expertise to effectively integrate new technologies.  Maintenance costs.  \\nMultilingual support and equitable access.  \\nFecher et al. \\n(2023)  Society and Culture  Liability issues: challenging traditional mechanisms of authorship and liability. Bias \\nissues: affecting the objectivity and impartiality of science. Privacy and data protection \\nissues: may be privacy issues with the training data of LLMs. Intellectual pro perty issues: \\npotential legal disputes. Environmental issues: generating large amounts of carbon \\nemissions, which can have a negative impact on the environment.  \\nAtari et al. \\n(2023)  Society and Culture  Ignoring global psychological diversity ( e.g., tend to favor the psychological \\ncharacteristics of WEIRD societies) and which can lead to prejudice and discrimination \\nagainst people of other cultures and backgrounds. Differences in values and moral \\njudgments and which can lead to problems of commu nication and understanding in \\nmulticultural societies. Self -identity and perceived social roles and which may lead to \\nstereotypes and misconceptions about non -WEIRD populations).  \\nPark et al. \\n(2023)  Society and Culture  Reduced innovation and development, bias and discrimination, culture clash and conflict, \\ndifferences in values and morals and entrenchment of the status quo.  \\nSalah et al. \\n(2023)  Society and Culture  Limited understanding of social context: Although ChatGPT performs well in syntax and \\ngeneral semantics, it still has limitations in capturing the nuances of social language.  \\nEthical challenges: AI -generated fake content can lead to ethical issues including digital \\npersonhood, informed consent, potential manipulation, and the implications of using AI to \\nsimulate human interactions.  \\nHayes (2023)  Society and Culture  Potential biases: if the training data contain biases, LLMs may learn and replicate them. \\nData privacy and consent issues: Text generated using LLMs may involve data privacy and \\nconsent issues.  Output may be non -humanly understandable: although LLMs generate text \\nthat closely resembles human language, they do not truly understand the content and may \\ngenerate absurd or misleading responses.  \\nMiotto et al. \\n(2022)  Society and Culture  Bias and discrimination: LLMs may be affected by biases in the training data, which can \\nproduce unfair results, such as reinforcing sexism in the translation of job advertisements.  \\nResponsibility and control: Due to the complexity of language models, it is difficult to \\ndetermine who is responsible for the model\\'s output, which can lead to attribution of \\nproblems and lack of controls.  \\nBender et al. \\n(2021)  Society and Culture  Potential Harm: LLMs may lead to the propagation of harmful ideas such as stereotyping, \\ndiscrimination, and extremism, and may lead to misinformation and bullying when \\ngenerating text.  Data bias and unfairness: leading to potential harm to marginalized \\ncommunities. Automating bias: exacerbating existing biases and discrimination.  \\nEnhancement of authoritative viewpoints: LLMs may reinforce dominant viewpoints in \\nthe training data, further undermining marginalized people.  \\nTamkin et al. \\n(2021)  Society and Culture  Alignment: In order to better align models with human values, algorithmic improvements \\nare needed to increase factual accuracy and robustness against adversarial samples. In \\naddition, appropriate values need to be made explicit for different usage scenario s. Societal \\nImpact: Widespread use of LLMs may lead to problems such as information leakage and \\namplification of bias.  \\nBrown et al. \\n(2020)  Society and Culture  Misuse of language modeling: GPT -3 may be used to generate fake news, spread extremist \\nideas, conduct cyber -attacks and other malicious uses.  Fairness, bias, and representation: \\nGPT-3 may carry bias against gender, race, and religion, among others, sparking related \\ncontroversies.  News generation: News generated by GPT -3 may be difficult to distinguish  from real news, leading to confusing and misleading information.  \\nSallam (2023)  Research tools  Plagiarism: content generated by ChatGPT may be considered plagiarized, violating \\nacademic norms.  Copyright issues:  Is the generated content owned by ChatGPT or by the \\nuser?  Transparency issues: The workings of ChatGPT may not be transparent, making it \\ndifficult for users to understand the source of generated content. Liability issues: who is \\nresponsible for ChatGPT when generating incorrect content?  \\nGupta et al. \\n(2023)  Research tools  Transparency and Explanation: The working mechanism of generative AI models may be \\ndifficult to explain, which may lead users to doubt the credibility of the generated content.  \\nLegal and Ethical Issues: Generative AI models may involve intellectual property, privacy, \\nand ethical issues, requiring attention to compliance with relevant laws and regulations \\nduring use.  \\nDergaa et al. \\n(2023)  Research tools  Integration of erroneous or biased information. Problems with citing original sources and \\nauthors. Impact on academic integrity and quality. Increased inequity and inequality: \\nDifficulty in recognizing AI -generated content. Academic evaluation and recognit ion \\nissues. Direct replacement for academic researchers: ChatGPT is not a complete \\nreplacement for academic researchers as it has limitations in certain types of academic \\nresearch.  \\nPeters and Matz \\n(2023)  Research tools  User privacy: LLMs can infer psychological traits from a user\\'s social media data, which \\nmay violate the user\\'s privacy. Potential bias: LLMs may create potential bias in the \\ninference process, which may lead to unfair treatment of specific groups (e.g., gender, age, \\netc.). Data security: if the inferential power of LL Ms is used maliciously, it may lead to \\ndata leakage, with serious implications for users\\' mental health.  \\nY . Liu et al. \\n(2023)  Research tools  Academic misconduct: ChatGPT may be used for academic cheating, such as generating \\nfalse papers or assignments.  Challenges in the medical field: ChatGPT has limitations in \\nmedical image analysis, which may lead to wrong diagnosis and jeopardize patients\\' \\nhealth.  \\n \\n7.2.  Future directions and emergent trends  \\nThe LLMs  have begun to be used in different areas of psychology, especially in cognitive and \\nbehavioral, clinical and counseling, educational and developmental, and social and cultural psychology. As \\nthe capabilities of LLMs are further enhanced, their application s in psychology still have the potential to \\ncontinue to develop in the future.  \\nFirst, in the field of cognitive and behavioral psychology, with the emergence of multimodal LLMs \\n(OpenAI, 2023) , on the one hand, it is possible to combine visual and auditory information with textual data \\nto understand better  and model emotions, behaviors, and mental states for cognition, on the other hand, it is \\npossible to use neuroimaging data to inform the architectures and parameters of LLMs, and to integrate this \\ninformation with traditional textual data integration to c reate more accurate and biologically sound models \\nof human language and thought.  \\nSecond, in the field of clinical and counseling psychology, on the one hand, personal data such as social \\nmedia posts, medical records, or wearable device data can be used to create tailored and personalized LLMs \\nthat provide more accurate and relevant ins ights into an individual\\'s state of mind, on the other hand, the \\nstrengths of combining the clinical and counseling expertise of the human psyche with the scalability and \\ncomputational power of an LLM can be combined to create new diagnostic treatment and intervention tools. \\nIn addition, in the fields of educational and developmental, and social and cultural psychology, it is essential   to build ethical LLMs and to ensure that they are designed and deployed in a way that respects privacy and \\nuses data fairly and responsibly.  \\nUltimately, LLMs  are a systematic project whose future development cannot be achieved without the \\ninterdisciplinary collaboration of researchers in fields as diverse as psychology, computer science, and \\nlinguistics, and for psychology researchers, an accessible open -sourc e large  language modeling framework \\nand tools may be an integral part of future research efforts.  The following table summarizes  LLMs\\' future \\ndirections and emergent trends in psychological applications (see Table 7).  \\nTable 7 Future  directions and emergent trends  of LLMs in psychological applications . \\nAuthor  Domain  Future directions and emergent trends  \\nD\\'Oria (2023)  Cognition and Behavior  Delving into Human -Computer Interaction (HCI) to understand AI\\'s ability to mimic \\nhuman behavior.  Exploring how AI language modeling can be applied in the human \\nsciences to improve research efficiency and quality  \\nCrockett and \\nMesseri (2023)  Cognition and Behavior  Focus on the costs of adopting alternative human narratives in cognitive science research, \\nsuch as masking the human labor behind them and the impact on human well -being.  \\nConcern about the impact of technological developments on scientific work and human \\nunderstanding to ensure that cognitive scientists remain proactive in technological \\nadvances.  \\nBinz and Schulz \\n(2023b)  Cognition and Behavior  Explore ways to make LLMs more stable and robust in the face of descriptive tasks.  \\nInvestigate whether LLMs can learn to explore purposefully and how to better utilize \\ncausal knowledge in tasks.  Analyze the performance of LLMs in different tasks and \\ncontexts to see if they can adapt like humans.  Explore how LLMs develop and refine their \\ncognitive abilities during natural interactions with humans.  \\nHuang and \\nChang (2022)  Cognition and Behavior  Improve the reasoning ability of LLMs to encourage reasoning by optimizing training data, \\nmodel architecture, and optimization goals.  Develop more appropriate evaluation methods \\nand benchmarks to measure the reasoning ability of LLMs to better reflect the true \\nreasoning ability of the models.  Investigate the potential of LLMs in different applications \\n(e.g., problem solving, decision making and planning tasks).  Explore other forms of \\nreasoning (e.g., inductive and retrospective reasoning).  \\nAbd-Alrazaq et \\nal. (2019)  Clinic and Counseling  Develop more chatbots for people with mental illness, especially for those with disorders \\nsuch as schizophrenia, obsessive -compulsive disorder and bipolar disorder.  \\nImplement more chatbots in developing countries to address the shortage of mental health \\nprofessionals.  Conduct more randomized controlled trials to evaluate the effectiveness of \\nchatbots in mental health.  \\nStade et al. \\n(2023)  Clinic and Counseling  Developing new therapeutic techniques and evidence -based practices (EBPs). Focus on \\nevidence -based practices first: to create meaningful clinical impact in the short term, \\nclinical LLM applications based on existing evidence -based psychotherapies and \\ntechn iques will have the greatest chance of success.  Involve interdisciplinary \\ncollaboration. Focuses on therapist and patient trust and usability. Criteria for designing \\neffective clinical LLMs.  \\nDemszky et al. \\n(2023)  Clinic and Counseling  Development of high -quality cornerstone datasets: these datasets need to encompass \\npopulations and psychological constructs of interest and be associated with \\npsychologically important outcomes (e.g., actual behaviors, mindfulness, health, and \\nmental well -being). Focus on future research directions in consumer neuroscience and \\nclinical neuroscience: research in these areas may involve the neural systems of marketing -\\nrelated behaviors, decision neuroscience, neuroeconomics, and more.  \\nHagendorff \\n(2023)  Education  and \\nDevelopment  Developmental psychology: examining how LLMs develop cognitively, socially, and \\nemotionally over the lifespan and how these models can be optimized for specific tasks \\nand situations. Learning psychology: studying how LLMs acquire and retain knowledge \\nand s kills, and how to optimize these models to improve learning.  \\nSap et al. (2022)  Society and Culture  Explore more interactive and empirical training methods to help LLMs acquire true social \\nintelligence and theoretical mental abilities.  Investigate ways to combine static text with \\nrich social intelligence and interaction data to improve social intelligence in LLMs. \\nInvestigate the theoretical -psychological abilities of LLMs in more naturalistic settings to \\nreveal their performance in real -world scenarios.   \\nArgyle et al. \\n(2022)  Society and Culture  Investigate the algorithmic fidelity of the GPT -3 model and how appropriate conditioning \\ncan allow the model to accurately simulate the response distributions of various human  subgroups.  Created \"in silico samples\" by conditioning on the socio -demographic \\nbackgrounds of real human participants in multiple large U.S. surveys.  \\nSchaaff et al. \\n(2023)  Society and Culture  Developing more advanced models: to more accurately capture the emotional context of \\nconversations and improve emotional understanding and expression.  Measuring the \\nemotional capabilities of bots: to investigate how to assess the emotional capabilities of \\nchatbots in order to better understand how they behave when interacting with humans.  \\nExplore the use of ChatGPT as a support tool: investigate how ChatGPT can be used to \\nsupport people more empathetically and improve human well -being.  \\nZiems et al. \\n(2023)  Society and Culture  Cross -cultural CSS research: future research should separately consider the utility of \\nLLMs for cross -cultural CSS in order to better serve social science research in different \\ncultural contexts.  Future research could explore contrastive or causal explanations in \\nLLMs.  New paradigms for social science and AI collaboration.  \\nVan Dis et al. \\n(2023)  Research tools  Invest in truly open LLMs: develop and implement open -source AI technologies to \\nincrease transparency and democratic control.  Embrace the advantages of AI: utilize AI to \\naccelerate innovation and breakthroughs at all academic stages, while focusing on issues \\nof ethics and human autonomy.  Broaden the discussion: organize international forums to \\ndiscuss the development and responsible use of LLMs in research, including issues of \\ndiversity and inequality.  \\nFecher et al. \\n(2023)  Research tools  Analyzing the risks and opportunities of LLMs for science systems. Examining how LLMs \\naffect academic quality assurance mechanisms, academic misconduct, and scientific \\nintegrity. Exploring the impact of LLMs on academic reputation, evaluation systems, and \\nknowledge dissemination. Examining how to balance the potential benefits from LLMs \\nwith adherence to scientific principles.  \\n \\n \\n8. Conclusion   \\nWith the rapid development of AI technology, especially the continuous advancement of LLMs  such as \\nthe GPT family, we have entered a new era characterized by an unprecedented level of machines able to \\nunderstand and generate human language. This development is not just a technological breakthrough for the \\nfield of psychology but opens the door to a range of potential applications.  \\nFirst, in the field of cognitive and behavioral psychology, LLMs are excelling in a variety of cognitive \\ntasks . Although  there are still limitations in causal cognition and planning, these models resurrect  the \\nprinciple of association, demonstrating the ability to associate at a distance and reason in complex ways. At \\nthe same time, the ability to adapt LLMs to cognitive models is a significant  strength of psychological \\nresearch, allowing new explorations of human cognitive and behavioral processing mechani sms. \\nSecond, in clinical and counseling psychology, LLMs can be used as a preliminary diagnostic tool for \\nmental health. While traditional mental health diagnosis relies on the experience of professionals and direct \\ninteraction with patients, LLMs can quickly i dentify potential mental health problems, such as depression \\nand anxiety, by analyzing an individual\\'s verbal expressions and textual content. Of course, such a diagnosis \\ncannot wholly  replace a professional psychological assessment . However, it  can serve as an effective adjunct \\nto help psychologists understand a patient\\'s condition more quickly or play a role in primary mental health \\ninterventions. Meanwhile, personalized psychological intervention is another critical  application direction  of the LLM. By combining information about an individual\\'s health data and lifestyle habits, these models \\ncan provide tailored psychological advice and intervention programs. This personalized approach may be \\ncrucial for improving the effectiveness of psychological interventions.  \\nThird, in educational and developmental and social and cultural psychology, LLMs have the same \\npotential for application. For example, these models provide interactive and personalized learning \\nexperiences or generate research tasks based on real -life case applications that increase motivation and  \\nenhance learning. In addition, by analyzing large amounts of social media data, these models can help \\nresearchers track and analyze public sentiment changes to understand psychosocial dynamics better . \\nFinally, in psychological research, LLMs can drastically improve research efficiency. Researchers can \\nuse these models to quickly organize and analyze large amounts of literature , thus saving time. In addition, \\nthese models can also assist in experimental design , data analysis, and even writing papers, making \\npsychological research more efficient and precise.   \\nIn summary, the applications of LLMs in psychology are promising. From research aids to cognitive \\nmodeling, from individualized interventions to personalized learning, and from the cognitive abilities of \\nindividuals to the social interactions of groups, th ese models have the potential to dramatically improve the \\nunderstanding of the patterns of human communication, thought processes, and behaviors that lead to the \\ndevelopment of more sophisticated theories of mind. However, despite the great potential for applying  LLMs \\nin psychology, being wary of the risks and challenges involved is essential . Ensuring these applications \\nadhere to ethical standards is vital , especially in protecting individual privacy and data security. It is also \\nimportant to realize that no matter how technologically advanced, LLMs can only partially  replace the \\njudgment and experience of human professionals. Therefore, these models  should be viewed as an aid rather \\nthan an all -in-one solution.  \\nReferences  \\nAbd-Alrazaq, A. A., Alajlani, M., Alalwan, A. A., Bewick, B. M., Gardner, P., & Househ, M. (2019). An \\noverview of the features of chatbots in mental health: A scoping review. International Journal of Medical \\nInformatics , 132, 103978. https://doi.org/10.1016/j.ijmedinf.2019.103978   \\nAbramski, K., Citraro, S., Lombardi, L., Rossetti, G., & Stella, M. (2023). Cognitive Network Science Reveals \\nBias in GPT -3, GPT -3.5 Turbo, and GPT -4 Mirroring Math Anxiety in High -School Students. Big Data and \\nCognitive Computing , 7(3). https://doi.org/10.3390/bdcc7030124   \\nAgrawal, S. (2023). Are LLMs the Master of All Trades? : Exploring Domain -Agnostic Reasoning Skills of \\nLLMs. arXiv preprint . https://doi.org/10.48550/arxiv.2303.12810   \\nAher, G., Arriaga, R. I., & Kalai, A. T. (2023). Using large language models to simulate multiple humans and  replicate human subject studies  Proceedings of the 40th International Conference on Machine Learning, \\nHonolulu, Hawaii, USA.  \\nAkata, E., Schulz, L., Coda -Forno, J., Oh, S. J., Bethge, M., & Schulz, E. (2023). Playing repeated games with \\nLarge Language Models. arXiv preprint . https://doi.org/10.48550/arXiv.2305.16867   \\nAli, J. K. M., Shamsan, M. A. A., Hezam, T. A., & Mohammed, A. A. Q. (2023). Impact of ChatGPT on \\nLearning Motivation. Journal of English Studies in Arabia Felix , 2(1), 41 -49. \\nhttps://doi.org/10.56540/jesaf.v2i1.51   \\nArgyle, L. P., Busby, E. C., Fulda, N., Gubler, J., Rytting, C., & Wingate, D. (2022). Out of One, Many: Using \\nLanguage Models to Simulate Human Samples. arXiv preprint . https://doi.org/10.48550/arXiv.2209.06899   \\nAtari, M., Xue, M. J., Park, P. S., Blasi, D. E., & Henrich, J. (2023). Which Humans? PsyArXiv preprint . \\nhttps://doi.org/10.31234/osf.io/5b26t   \\nAydın, Ö., & Karaarslan, E. (2022). OpenAI ChatGPT Generated Literature Review: Digital Twin in Healthcare. \\nEmerging Computer Technologies (2), 22 -31. https://doi.org/10.2139/ssrn.4308687   \\nBender, E. M., Gebru, T., McMillan -Major, A., & Shmitchell, S. (2021). On the Dangers of Stochastic Parrots: \\nCan Language Models Be Too Big?  Proceedings of the 2021 ACM Conference on Fairness, Accountability, \\nand Transparency, Virtual Event, Canada. https://doi.org/10.1145/3442188.3445922  \\nBinz, M., & Schulz, E. (2023a). Turning large language models into cognitive models. arXiv preprint . \\nhttps://doi.org/10.48550/arXiv.2306.03917   \\nBinz, M., & Schulz, E. (2023b). Using cognitive psychology to understand GPT -3. Proceedings of the National \\nAcademy of Sciences of the United States of America , 120(6), e2218523120. \\nhttps://doi.org/10.1073/pnas.2218523120   \\nBlyler, A. P., & Seligman, M. E. P. (2023a). AI assistance for coaches and therapists. The Journal of Positive \\nPsychology , 1-13.  \\nBlyler, A. P., & Seligman, M. E. P. (2023b). Personal narrative and stream of consciousness: an AI approach. \\nThe Journal of Positive Psychology , 1-7. https://doi.org/10.1080/17439760.2023.2257666   \\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, \\nG., Askell, A., Agarwal, S., Herbert -Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. \\nM., Wu, J., Winter, C., . . . Amodei,  D. (2020). Language Models are Few -Shot Learners. arXiv preprint . \\nhttps://doi.org/10.48550/arXiv.2005.14165   \\nBubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E. K., Kamar, E., Lee, P., Lee, Y. T., Li, Y. -F., \\nLundberg, S. M., Nori, H., Palangi, H., Ribeiro, M. T., & Zhang, Y. (2023). Sparks of Artificial General \\nIntelligence: Early experiments with GPT-4. arXiv preprint . https://doi.org/10.48550/arXiv.2303.12712   \\nCai, Z. G., Haslett, D. A., Duan, X., Wang, S., & Pickering, M. J. (2023). Does ChatGPT resemble humans in \\nlanguage use? arXiv preprint . https://doi.org/10.48550/arXiv.2303.08014   \\nCarlbring, P., Hadjistavropoulos, H., Kleiboer, A., & Andersson, G. (2023). A new era in Internet interventions: \\nThe advent of Chat -GPT and AI -assisted therapist guidance. Internet Interventions , 32, 100621. \\nhttps://doi.org/10.1016/j.invent.2023.100621   \\nChiang, C. -H., & Lee, H. -y. (2023). Can Large Language Models Be an Alternative to Human Evaluations? \\narXiv preprint . https://doi.org/10.48550/arXiv.2305.01937   \\nCrockett, M., & Messeri, L. (2023). Should large language models replace human participants? PsyArXiv \\npreprint . https://doi.org/10.31234/osf.io/4zdx9   \\nD\\'Oria, M. (2023). Can AI Language Models Improve Human Sciences Research? A Phenomenological \\nAnalysis and Future Directions. Encyclopaideia , 27(66), 77 -92. https://doi.org/10.6092/issn.1825 -\\n8670/16554    De Bot, K., Lowie, W., & Verspoor, M. (2007). A Dynamic Systems Theory approach to second language \\nacquisition. Bilingualism: Language and Cognition , 10(1), 7 -21. \\nhttps://doi.org/10.1017/S1366728906002732   \\nDemszky, D., Yang, D., Yeager, D. S., Bryan, C. J., Clapper, M., Chandhok, S., Eichstaedt, J. C., Hecht, C., \\nJamieson, J., Johnson, M., Jones, M., Krettek -Cobb, D., Lai, L., JonesMitchell, N., Ong, D. C., Dweck, C. \\nS., Gross, J. J., & Pennebaker, J. W. (20 23). Using large language models in psychology. Nature Reviews \\nPsychology , 2(11), 688 -701. https://doi.org/10.1038/s44159 -023-00241 -5  \\nDergaa, I., Chamari, K., Zmijewski, P., & Ben Saad, H. (2023). From human writing to artificial intelligence \\ngenerated text: examining the prospects and potential threats of ChatGPT in academic writing. Biology of \\nSport , 40(2), 615 -622. https://doi.org/10.5114/biolsport.2023.125623   \\nDhingra, S., Singh, M., Sb, V., Malviya, N., & Singh Gill, S. (2023). Mind meets machine: Unravelling GPT -4\\'s \\ncognitive psychology. arXiv preprint , arXiv:2303.11436. https://doi.org/10.48550/arXiv.2303.11436   \\nDillion, D., Tandon, N., Gu, Y., & Gray, K. (2023). Can AI language models replace human participants? Trends \\nin Cognitive Sciences , 27(7), 597 -600. https://doi.org/10.1016/j.tics.2023.04.008   \\nElyoseph, Z., & Levkovich, I. (2023). Beyond human expertise - the promise and limitationsof ChatGPT in \\nsuicide risk assessment. Frontiers in Psychiatry , 14. https://doi.org/10.3389/fpsyt.2023.1213141   \\nFecher, B., Hebing, M., Laufer, M., Pohle, J., & Sofsky, F. (2023). Friend or foe? Exploring the implications of \\nlarge language models on the science system. Ai & Society . https://doi.org/10.1007/s00146 -023-01791 -1  \\nFloridi, L., & Chiriatti, M. (2020). GPT -3: Its Nature, Scope, Limits, and Consequences. Minds and Machines , \\n30(4), 681 -694. https://doi.org/10.1007/s11023 -020-09548 -1  \\nFrank, M. C. (2023). Baby steps in evaluating the capacities of large language models. Nature Reviews \\nPsychology , 2(8), 451 -452. https://doi.org/10.1038/s44159 -023-00211 -x  \\nGlaser, R. (1984). Education and thinking: The role of knowledge. American psychologist , 39(2), 93.  \\nGoertzel, B. (2023). Generative AI vs. AGI: The Cognitive Strengths and Weaknesses of Modern LLMs. arXiv \\npreprint , arXiv:2309.10371. https://doi.org/10.48550/arXiv.2309.10371   \\nGraber -Stiehl, I. (2023). IS THE WORLD READY FOR AI -POWERED THERAPY? Nature , 617, 22-24. \\nhttps://doi.org/10.1038/d41586 -023-01473 -4  \\nGrossmann, I., Feinberg, M., Parker, D. C., Christakis, N. A., Tetlock, P. E., & Cunningham, W. A. (2023). AI \\nand the transformation of social science research. Science , 380(6650), 1108 -1109. \\nhttps://doi.org/10.1126/science.adi1778   \\nGupta, M., Akiri, C., Aryal, K., Parker, E., & Praharaj, L. (2023). From ChatGPT to ThreatGPT: Impact of \\nGenerative AI in Cybersecurity and Privacy. IEEE Access , 11, 80218 -80245. \\nhttps://doi.org/10.1109/access.2023.3300381   \\nHagendorff, T. (2023). Machine Psychology: Investigating Emergent Capabilities and Behavior in Large \\nLanguage Models Using Psychological Methods. arXiv preprint . \\nhttps://doi.org/10.48550/arXiv.2303.13988   \\nHagendorff, T., Fabi, S., & Kosinski, M. (2022). Machine intuition: Uncovering human -like intuitive decision -\\nmaking in GPT -3.5. arXiv preprint . https://doi.org/10.48550/arXiv.2212.05206   \\nHagendorff, T., Fabi, S., & Kosinski, M. (2023). Human -like intuitive behavior and reasoning biases emerged in \\nlarge language models but disappeared in ChatGPT. Nature Computational Science , 3(10), 833 -838. \\nhttps://doi.org/10.1038/s43588 -023-00527 -x  \\nHan, H. (2023). Potential Benefits of Employing Large Language Models in Research in Moral Education and \\nDevelopment. arXiv preprint . https://doi.org/10.48550/arXiv.2306.13805   \\nHarding, J., D’Alessandro, W., Laskowski, N. G., & Long, R. (2023). AI language models cannot replace human  research participants. Ai & Society . https://doi.org/10.1007/s00146 -023-01725 -x  \\nHardy, M., Sucholutsky, I., Thompson, B., & Griffiths, T. (2023). Large language models meet cognitive \\nscience: Llms as tools, models, and participants. Proceedings of the annual meeting of the cognitive science \\nsociety,  \\nHayes, A. (2023). “Conversing” with Qualitative Data: Enhancing Qualitative Research through Large \\nLanguage Models (LLMs). PsyArXiv preprint . https://doi.org/10.31235/osf.io/yms8p   \\nHendel, R., Geva, M., & Globerson, A. (2023). In -Context Learning Creates Task Vectors. arXiv preprint , \\narXiv:2310.15916. https://doi.org/10.48550/arXiv.2310.15916   \\nHofmann, S. G., Asnaani, A., Vonk, I. J., Sawyer, A. T., & Fang, A. (2012). The Efficacy of Cognitive \\nBehavioral Therapy: A Review of Meta -analyses. Cognit Ther Res , 36(5), 427 -440. \\nhttps://doi.org/10.1007/s10608 -012-9476 -1  \\nHoltzman, A., West, P., & Zettlemoyer, L. (2023). Generative Models as a Complex Systems Science: How can \\nwe make sense of large language model behavior? arXiv preprint . \\nhttps://doi.org/10.48550/arXiv.2308.00189   \\nHothersall, D., & Lovett, B. J. (2022). History of psychology . Cambridge University Press.  \\nHuang, J., & Chang, K. C. -C. (2022). Towards Reasoning in Large Language Models: A Survey. arXiv preprint . \\nhttps://doi.org/10.48550/arXiv.2212.10403   \\nHutson, M. (2023). Doing research with human subjects is costly and cumbersome.Can AI chatbots replace \\nthem? Science , 381(6654), 121 -123. https://doi.org/10.1126/science.adj6791   \\nJin, C., Zhang, S., Shu, T., & Cui, Z. (2023). The Cultural Psychology of Large Language Models: Is ChatGPT a \\nHolistic or Analytic Thinker? arXiv preprint . https://doi.org/10.48550/arXiv.2308.14242   \\nJungherr, A. (2023). Using ChatGPT and Other Large Language Model (LLM) Applications for Academic \\nPaper Assignments . https://fis.uni -bamberg.de/handle/uniba/58950  \\nurn:nbn:de:bvb:473 -irb-589507  \\nKasneci, E., Sessler, K., Küchemann, S., Bannert, M., Dementieva, D., Fischer, F., Gasser, U., Groh, G., \\nGünnemann, S., Hüllermeier, E., Krusche, S., Kutyniok, G., Michaeli, T., Nerdel, C., Pfeffer, J., Poquet, O., \\nSailer, M., Schmidt, A., Seidel, T., . . . Kasneci, G. (2023). ChatGPT for good? On opportunities and \\nchallenges of large language models for education. Learning and Individual Differences , 103. \\nhttps://doi.org/10.1016/j.lindif.2023.102274   \\nKe, L., Zhang, G., He, J., Li, Y., Li, Y., Liu, X., & Fang, P. (2023). Pilot Selection in the Era of Virtual Reality: \\nAlgorithms for Accurate and Interpretable Machine Learning Models. Aerospace , 10(5). \\nhttps://doi.org/10.3390/aerospace10050394   \\nKjell, O., Kjell, K., & Schwartz, H. A. (2023). Beyond Rating Scales: With Care for Validation Large Language \\nModels Are Poised to Change Psychological Assessment. PsyArXiv preprint . \\nhttps://doi.org/10.31234/osf.io/yfd8g   \\nKosinski, M. (2023). Theory of Mind May Have Spontaneously Emerged in Large Language Models. arXiv \\npreprint . https://doi.org/10.48550/arXiv.2302.02083   \\nLamichhane, B. (2023). Evaluation of ChatGPT for NLP -based Mental Health Applications. arXiv preprint , \\narXiv:2303.15727. https://doi.org/10.48550/arXiv.2303.15727   \\nLi, J., Tang, T., Zhao, W. X., Nie, J. -Y., & Wen, J. -R. (2022). Pretrained Language Models for Text Generation: \\nA Survey. arXiv preprint , arXiv:2201.05273. https://doi.org/10.48550/arXiv.2201.05273   \\nLi, M., Enkhtur, A., Cheng, F., & Yamamoto, B. A. (2023). Ethical implications of ChatGPT in higher \\neducation: A scoping review. arXiv preprint . https://doi.org/10.48550/arXiv.2311.14378   \\nLi, X., Li, Y., Liu, L., Bing, L., & Joty, S. (2022). Does GPT -3 Demonstrate Psychopathy? Evaluating Large  Language Models from a Psychological Perspective. arXiv preprint . \\nhttps://doi.org/10.48550/arXiv.2212.10529   \\nLiu, J. M., Li, D., Cao, H., Ren, T., Liao, Z., & Wu, J. (2023). ChatCounselor: A Large Language Models for \\nMental Health Support. arXiv preprint , arXiv:2309.15461. https://doi.org/10.48550/arXiv.2309.15461   \\nLiu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., & Neubig, G. (2023). Pre -train, Prompt, and Predict: A \\nSystematic Survey of Prompting Methods in Natural Language Processing. Acm Computing Surveys , 55(9), \\n1-35. https://doi.org/10.1145/3560815   \\nLiu, X., Ji, K., Fu, Y., Tam, W., Du, Z., Yang, Z., & Tang, J. (2022). P -Tuning: Prompt Tuning Can Be \\nComparable to Fine -tuning Across Scales and Tasks. Proceedings of the 60th Annual Meeting of the \\nAssociation for Computational Linguistics (Volume 2: Shor t Papers), Dublin, Ireland.  \\nLiu, Y., Han, T., Ma, S., Zhang, J., Yang, Y., Tian, J., He, H., Li, A., He, M., Liu, Z., Wu, Z., Zhao, L., Zhu, D., \\nLi, X., Qiang, N., Shen, D., Liu, T., & Ge, B. (2023). Summary of ChatGPT -Related research and \\nperspective towards the future of large lang uage models. Meta -Radiology , 1(2). \\nhttps://doi.org/10.1016/j.metrad.2023.100017   \\nLoconte, R., Orrù, G., Tribastone, M., Pietrini, P., & Sartori, G. (2023). Challenging ChatGPT\\'s \"intelligence\" \\nwith human tools: A Neuropsychological Investigation on Prefrontal Functioning of a Large Language \\nModel. SSRN preprint . https://doi.org/10.2139/ssrn.4471829   \\nMahowald, K., Ivanova, A. A., Blank, I. A., Kanwisher, N., Tenenbaum, J. B., & Fedorenko, E. (2023). \\nDissociating language and thought in large language models: a cognitive perspective. arXiv preprint . \\nhttps://doi.org/10.48550/arXiv.2301.06627   \\nMarjieh, R., Sucholutsky, I., Rijn, P. v., Jacoby, N., & Griffiths, T. L. (2023). Large language models predict \\nhuman sensory judgments across six modalities. arXiv preprint . https://doi.org/10.48550/arXiv.2302.01308   \\nMiotto, M., Rossberg, N., & Kleinberg, B. (2022). Who is GPT -3? An Exploration of Personality, Values and \\nDemographics. arXiv preprint . https://doi.org/10.48550/arXiv.2209.14338   \\nMitchell, M. (2023). AI\\'s challenge of understanding the world. Science , 382(6671). \\nhttps://doi.org/10.1126/science.adm8175   \\nNazario -Johnson, L., Zaki, H. A., & Tung, G. A. (2023). Use of large language models to predict neuroimaging. \\nJournal of the American College of Radiology , 20(10), 1004 -1009. \\nhttps://doi.org/10.1016/j.jacr.2023.06.008   \\nNewell, A. (1990). Unified theories of cognition . Harvard University Press.  \\nNisbett, R. E., Peng, K., Choi, I., & Norenzayan, A. (2001). Culture and systems of thought: holistic versus \\nanalytic cognition. Psychological Review , 108(2), 291 -310. https://doi.org/10.1037//0033 -295X.108.2.291   \\nOpenAI. (2023). GPT -4 Technical Report. arXiv preprint . https://doi.org/10.48550/arXiv.2303.08774   \\nOrru, G., Piarulli, A., Conversano, C., & Gemignani, A. (2023). Human -like problem -solving abilities in large \\nlanguage models using ChatGPT. Frontiers in Artificial Intelligence , 6, 1199350. \\nhttps://doi.org/10.3389/frai.2023.1199350   \\nPal, R., Garg, H., Patel, S., & Sethi, T. (2023). Bias Amplification in Intersectional Subpopulations for Clinical \\nPhenotyping by Large Language Models. medRxiv preprint . https://doi.org/10.1101/2023.03.22.23287585   \\nPark, B., & Judd, C. M. (2005). Rethinking the Link Between Categorization and Prejudice Within the Social \\nCognition Perspective. Personality and Social Psychology Review , 9(2), 108 -130. \\nhttps://doi.org/10.1207/s15327957pspr0902_2   \\nPark, J. S., Popowski, L., Cai, C., Morris, M. R., Liang, P., & Bernstein, M. S. (2022). Social simulacra: \\nCreating populated prototypes for social computing systems. Proceedings of the 35th Annual ACM \\nSymposium on User Interface Software and Technology,   Park, P. S., Schoenegger, P., & Zhu, C. (2023). Diminished Diversity -of-Thought in a Standard Large Language \\nModel. arXiv preprint . https://doi.org/10.48550/arXiv.2302.07267   \\nPatel, S. C., & Fan, J. (2023). Identification and Description of Emotions by Current Large Language Models. \\nbioRxiv preprint . https://doi.org/10.1101/2023.07.17.549421   \\nPeng, Y., Han, J., Zhang, Z., Fan, L., Liu, T., Qi, S., Feng, X., Ma, Y., Wang, Y., & Zhu, S. -C. (2023). The Tong \\nTest: Evaluating Artificial General Intelligence Through Dynamic Embodied Physical and Social \\nInteractions. Engineering . https://doi.org/10.1016/j.eng.2023.07.006   \\nPeters, H., & Matz, S. (2023). Large Language Models Can Infer Psychological Dispositions of Social Media \\nUsers. arXiv preprint . https://doi.org/10.48550/arXiv.2309.08631   \\nQureshi, R., Shaughnessy, D., Gill, K. A. R., Robinson, K. A., Li, T., & Agai, E. (2023). Are ChatGPT and large \\nlanguage models \"the answer\" to bringing us closer to systematic review automation? Systematic Reviews , \\n12(1), 72. https://doi.org/10.1186/s13643 -023-02243 -z  \\nRathje, S., Mirea, D. -M., Sucholutsky, I., Marjieh, R., Robertson, C., & Bavel, J. J. V. (2023). GPT is an \\neffective tool for multilingual psychological text analysis. PsyArXiv preprint . \\nhttps://doi.org/10.31234/osf.io/sekf5   \\nSalah, M., Al Halbusi, H., & Abdelfattah, F. (2023). May the force of text data analysis be with you: Unleashing \\nthe power of generative AI for social psychology research. Computers in Human Behavior: Artificial \\nHumans , 1(2). https://doi.org/10.1016/j.chbah.2023.100006   \\nSallam, M. (2023). ChatGPT Utility in Healthcare Education, Research, and Practice: Systematic Review on the \\nPromising Perspectives and Valid Concerns. Healthcare (Basel) , 11(6). \\nhttps://doi.org/10.3390/healthcare11060887   \\nSap, M., LeBras, R., Fried, D., & Choi, Y. (2022). Neural Theory -of-Mind? On the Limits of Social Intelligence \\nin Large LMs. arXiv preprint . https://doi.org/10.48550/arXiv.2210.13312   \\nSartori, G., & Orrù, G. (2023). Language models and psychological sciences. Frontiers in Psychology , 14. \\nhttps://doi.org/10.3389/fpsyg.2023.1279317   \\nSchaaff, K., Reinig, C., & Schlippe, T. (2023). Exploring ChatGPT\\'s Empathic Abilities. arXiv preprint . \\nhttps://doi.org/10.48550/arXiv.2308.03527   \\nSchueller, S. M., & Morris, R. R. (2023). Clinical science and practice in the age of large language models and \\ngenerative artificial intelligence. Journal of Consulting and Clinical Psychology , 91(10), 559 -561. \\nhttps://doi.org/10.1037/ccp0000848   \\nSeals, S. M., & Shalin, V. L. (2023). Long -form analogies generated by chatGPT lack human -like \\npsycholinguistic properties. arXiv preprint . https://doi.org/10.48550/arxiv.2306.04537   \\nSejnowski, T. (2022). Large Language Models and the Reverse Turing Test. arXiv preprint . \\nhttps://doi.org/10.48550/arxiv.2207.14382   \\nSha, H., Mu, Y., Jiang, Y., Chen, L., Xu, C., Luo, P., Eben Li, S., Tomizuka, M., Zhan, W., & Ding, M. (2023). \\nLanguageMPC: Large Language Models as Decision Makers for Autonomous Driving. arXiv preprint , \\narXiv:2310.03026. https://doi.org/10.48550/arXiv.2310.03026   \\nSharma, A., Lin, I. W., Miner, A. S., Atkins, D. C., & Althoff, T. (2023). Human –AI collaboration enables more \\nempathic conversations in text -based peer -to-peer mental health support. Nature Machine Intelligence , \\n5(1), 46 -57. https://doi.org/10.1038/s42256 -022-00593 -2  \\nShiffrin, R., & Mitchell, M. (2023). Probing the psychology of AI models. Proceedings of the National Academy \\nof Sciences of the United States of America , 120(10), e2300963120. \\nhttps://doi.org/10.1073/pnas.2300963120   \\nSimon, H. A. (1979). Information Processing Models of Cognition. Annual Review of Psychology , 30(1), 363 - 396. https://doi.org/10.1146/annurev.ps.30.020179.002051   \\nStade, E. C., Stirman, S. W., Ungar, L., Boland, C. L., Schwartz, H. A., Yaden, D. B., Sedoc, J., Derubeis, R. J., \\nWiller, R., & Eichstaedt, J. C. (2023). Large Language Models Could Change the Future of Behavioral \\nHealthcare: A Proposal for Responsible De velopment and Evaluation. PsyArXiv preprint . \\nhttps://doi.org/10.31234/osf.io/cuzvr   \\nStella, M., Hills, T. T., & Kenett, Y. N. (2023). Using cognitive psychology to understand GPT -like models \\nneeds to extend beyond human biases. Proceedings of the National Academy of Sciences of the United \\nStates of America , 120(43), e2312911120. https://doi.org/10.1073/pnas.2312911120   \\nStevenson, C., Smal, I., Baas, M., Grasman, R., & Maas, H. v. d. (2022). Putting GPT -3\\'s Creativity to the \\n(Alternative Uses) Test. arXiv preprint . https://doi.org/10.48550/arXiv.2206.08932   \\nStojanov, A. (2023). Learning with ChatGPT 3.5 as a more knowledgeable other: an autoethnographic study. \\nInternational Journal of Educational Technology in Higher Education , 20(1). \\nhttps://doi.org/10.1186/s41239 -023-00404 -7  \\nStokel -Walker, C. (2022). AI bot ChatGPT writes smart essays — should professors worry? Nature . \\nhttps://doi.org/10.1038/d41586 -022-04397 -7  \\nSuri, G., Slater, L. R., Ziaee, A., & Nguyen, M. (2023). Do Large Language Models Show Decision Heuristics \\nSimilar to Humans? A Case Study Using GPT -3.5. arXiv preprint . \\nhttps://doi.org/10.48550/arXiv.2305.04400   \\nTajfel, H. (1982). Social psychology of intergroup relations. Annual Review of Psychology , 33(1), 1 -39.  \\nTalboy, A. N., & Fuller, E. (2023). Challenging the appearance of machine intelligence: Cognitive bias in LLMs. \\narXiv preprint . https://doi.org/10.48550/arXiv.2304.01358   \\nTamkin, A., Brundage, M., Clark, J., & Ganguli, D. (2021). Understanding the Capabilities, Limitations, and \\nSocietal Impact of Large Language Models. arXiv preprint . https://doi.org/10.48550/arXiv.2102.02503   \\nThirunavukarasu, A. J., Ting, D. S. J., Elangovan, K., Gutierrez, L., Tan, T. F., & Ting, D. S. W. (2023). Large \\nlanguage models in medicine. Nature Medicine , 29(8), 1930 -1940. https://doi.org/10.1038/s41591 -023-\\n02448 -8  \\nTrott, S., Jones, C., Chang, T., Michaelov, J., & Bergen, B. (2023). Do Large Language Models know what \\nhumans know? arXiv preprint . https://doi.org/10.48550/arXiv.2209.01515   \\nVan Dis, E. A., Bollen, J., Zuidema, W., van Rooij, R., & Bockting, C. L. (2023). ChatGPT: five priorities for \\nresearch. Nature , 614(7947), 224 -226. https://doi.org/10.1038/d41586 -023-00288 -7  \\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). \\nAttention is all you need. Advances in neural information processing systems , 30.  \\nWang, H., Fu, T., Du, Y., Gao, W., Huang, K., Liu, Z., Chandak, P., Liu, S., Van Katwyk, P., Deac, A., \\nAnandkumar, A., Bergen, K., Gomes, C. P., Ho, S., Kohli, P., Lasenby, J., Leskovec, J., Liu, T. Y., Manrai, \\nA., . . . Zitnik, M. (2023). Scientific disco very in the age of artificial intelligence. Nature , 620(7972), 47 -\\n60. https://doi.org/10.1038/s41586 -023-06221 -2  \\nWang, X., Li, X., Yin, Z., Wu, Y., & Liu, J. (2023). Emotional intelligence of Large Language Models. Journal \\nof Pacific Rim Psychology , 17. https://doi.org/10.1177/18344909231213958   \\nWebb, T., Holyoak, K. J., & Lu, H. (2023). Emergent analogical reasoning in large language models. Nature \\nHuman Behaviour , 7(9), 1526 -1541. https://doi.org/10.1038/s41562 -023-01659 -w  \\nWei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., \\nMetzler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang, P., Dean, J., & Fedus, W. (2022). Emergent \\nAbilities of Large Language Models. arXiv preprint . https://doi.org/10.48550/arXiv.2206.07682   \\nYildirim, I., & Paul, L. A. (2023). From task structures to world models: What do LLMs know? arXiv preprint ,  arXiv:2310.04276. https://doi.org/10.48550/arXiv.2310.04276   \\nYukun, Z., Xu, L., Huang, Z., Peng, K., Seligman, M., Li, E., & Yu, F. (2023). AI chatbot responds to emotional \\ncuing. PsyArXiv preprint . https://doi.org/10.31234/osf.io/9ymfz   \\nZhang, J., Xu, X., & Deng, S. (2023). Exploring Collaboration Mechanisms for LLM Agents: A Social \\nPsychology View. arXiv preprint , arXiv:2310.02124. https://doi.org/10.48550/arXiv.2310.02124   \\nZhong, Y., Chen, Y. J., Zhou, Y., Lyu, Y. A., Yin, J. J., & Gao, Y . J. (2023). The Artificial intelligence large \\nlanguage models and neuropsychiatry practice and research ethic. Asian Journal of Psychiatry , 84, 103577. \\nhttps://doi.org/10.1016/j.ajp.2023.103577   \\nZhuang, Y., Liu, Q., Ning, Y., Huang, W., Lv, R., Huang, Z., Zhao, G., Zhang, Z., Mao, Q., Wang, S., & Chen, E. \\n(2023). Efficiently Measuring the Cognitive Ability of LLMs: An Adaptive Testing Perspective. arXiv \\npreprint . https://doi.org/10.48550/arXiv.2306.10512   \\nZiems, C., Held, W., Shaikh, O., Chen, J., Zhang, Z., & Yang, D. (2023). Can Large Language Models \\nTransform Computational Social Science? arXiv preprint . https://doi.org/10.48550/arXiv.2305.03514   \\n  '},\n",
       " {'url': 'http://arxiv.org/pdf/2311.11689v1',\n",
       "  'title': 'Causal Structure Learning Supervised by Large Language Model',\n",
       "  'text': 'Causal Structure Learning\\nSupervised by Large Language Model\\nTaiyu Ban Lyuzhou Chen Derui Lyu Xiangyu Wang∗Huanhuan Chen∗\\nSchool of Computer Science and Technology, University of Science and Technology of China\\n{banty,clz31415,drlv }@mail.ustc.edu.cn {sa312,hchen }@ustc.edu.cn\\nAbstract —Causal discovery from observational data is pivotal\\nfor deciphering complex relationships. Causal Structure Learning\\n(CSL), which focuses on deriving causal Directed Acyclic Graphs\\n(DAGs) from data, faces challenges due to vast DAG spaces and\\ndata sparsity. The integration of Large Language Models (LLMs),\\nrecognized for their causal reasoning capabilities, offers a promis-\\ning direction to enhance CSL by infusing it with knowledge-\\nbased causal inferences. However, existing approaches utilizing\\nLLMs for CSL have encountered issues, including unreliable\\nconstraints from imperfect LLM inferences and the computa-\\ntional intensity of full pairwise variable analyses. In response,\\nwe introduce the Iterative LLM Supervised CSL (ILS-CSL)\\nframework. ILS-CSL innovatively integrates LLM-based causal\\ninference with CSL in an iterative process, refining the causal\\nDAG using feedback from LLMs. This method not only utilizes\\nLLM resources more efficiently but also generates more robust\\nand high-quality structural constraints compared to previous\\nmethodologies. Our comprehensive evaluation across eight real-\\nworld datasets demonstrates ILS-CSL’s superior performance,\\nsetting a new standard in CSL efficacy and showcasing its\\npotential to significantly advance the field of causal discovery. The\\ncodes are available at https://github.com/tyMadara/ILS-CSL.\\nI. I NTRODUCTION\\nCausal discovery from the observed data is pivotal in\\nunderstanding intricate relationships across various domains.\\nCentral to this endeavor is Causal Structure Learning (CSL),\\naiming to construct a causal Directed Acyclic Graph (DAG)1\\nfrom observed data [1]. We adopt causal Bayesian Networks\\n(BNs) as the causal graphical model, renowned for effectively\\nmodeling intricate real-world variable relationships [2].\\nThe recovery of high-quality causal BNs faces significant\\nchallenges. Firstly, there is the issue of the super-exponential\\nincrease in the DAG space as the number of variables grows\\n[3], [4]. Additionally, real-world data is typically sparse and\\ninsufficient for accurately representing the true probability\\ndistributions [5]. Furthermore, the orientation of edges in a BN\\ncannot be fully deduced from the observed data alone due to\\nthe presence of equivalent DAGs [6]. In summary, CSL, when\\nreliant solely on observed data, encounters both practical and\\ntheoretical limitations.\\nGiven these inherent limitations, the integration of prior\\nknowledge to constrain specific structures becomes important\\n∗These authors are corresponding authors.\\n1In a causal DAG, each edge represents a direct causal link between its\\nnodes.for reliable causal discovery [7], [8]. While promising, this ap-\\nproach has been limited by the high costs and time associated\\nwith expert input [9]. However, the advent of Large Language\\nModels (LLMs) has ushered in a new frontier. Recent studies\\nhave underscored the capabilities of LLMs in causal reasoning,\\npositioning them as a valuable and readily accessible resource\\nfor knowledge-based causal inference [10], [11], [12].\\nKıcıman et al . have shown that Large Language Models\\n(LLMs) are effective in determining causality direction be-\\ntween pairs of variables, outperforming even human analysis\\nin this respect [10]. However, other studies highlight LLMs’\\nlimitations in constructing causal DAGs from sets of variables,\\nnot satisfying even in small-scale contexts [13], [14]. This dif-\\nficulty mainly stems from the inherent complexity in inferring\\ndetailed causal mechanisms, such as establishing the relative\\ndirectness of causes for an effect, a task that often exceeds\\nsimple knowledge-based inference.\\nIn response to these challenges, recent studies have begun\\nintegrating LLM-derived causal knowledge with data analysis\\nto enhance causal discovery. For example, Ban et al . [15] uti-\\nlize LLMs to discern the presence of causal links among vari-\\nables, subsequently applying ancestral constraints to structure\\nlearning [7]. This approach yields improvements in learning\\ncausal structures from data for smaller-scale problems, but it\\nencounters difficulties with larger datasets due to inaccuracies\\nin the LLM-derived constraints, as evidenced in Table I. As\\nan alternative, Vashishtha et al . [16] employ a detailed, pair-\\nbased prompting strategy with a voting system to determine\\nreliable prior knowledge. Regretably, the authors fail to show\\nthe effectiveness on the larger-scale datasets, likely limited\\nby the complexity and computational demands of the prompt\\nprocess, which requires\\x00N\\n2\\x01\\nLLM inferences with Ndenoting\\nthe variable count.\\nIn response to the challenges, we introduce a simple\\nbut effective strategy, named i terative L LM s upervised CSL\\nframework (ILS-CSL). Contrasting with prior methodologies\\nthat deploy LLMs and CSL separately, ILS-CSL uniquely\\nfocuses LLMs on verifying direct causal relationships already\\nsuggested by the data. Specifically, ILS-CSL employs LLMs to\\nvalidate the accuracy of edges in the learned causal DAG, with\\nan iterative process fine-tuning CSL based on LLM feedback.\\nThe iteration concludes when the LLM-based inferences and\\ndata-driven CSL align within the established causal structure.\\nThis innovative integration of LLMs into the CSL processarXiv:2311.11689v1  [cs.AI]  20 Nov 2023 TABLE I: SHD ↓and constraint quality of the ancestral constraint-based CSL driven by GPT-4, reported in the work [15].\\nDatasetCancer\\n5 nodesAsia\\n8 nodesChild\\n20 nodesInsurance\\n27 nodesAlarm\\n37 nodesMildew\\n35 nodesWater\\n32 nodesBarley\\n48 nodes\\nData size 250 1000 250 1000 500 2000 500 2000 1000 4000 8000 32000 1000 4000 2000 8000\\nMINOBSx 3.0 1.8 4.2 2.5 9.5 5.3 25.7 15.0 9.5 6.5 22.8 21.0 62.3 53.7 47.0 33.7\\n+GPT-4 0.5 0.0 2.2 0.3 10.5 7.8 24.5 16.2 12.3 8.8 40.5 21.5 66.7 55.7 52.0 54.5\\nCaMML 2.0 2.5 3.5 2.2 6.0 1.0 34.3 31.7 11.0 8.2 48.2 62.2 59.0 53.2 81.5 81.2\\n+GPT-4 2.0 1.3 0.2 0.0 4.7 1.0 27.0 22.2 6.0 3.0 49.2 60.0 58.7 48.3 82.2 82.3\\nT / F 5 / 0 9 / 0 8 / 2 10 / 0 20 / 1 9 / 6 5 / 3 17 / 7\\nThe bold SHD is the best performance in each dataset. The cell highlighted in gray indicates a degraded performance by integrating\\nLLM-derived causal knowledge. The row ‘T / F’ represents the number of correct LLM-derived structrual constraints (T) and that of\\nerroneous ones (F).\\noffers significant enhancements to the task, as outlined below.\\n1)Powerful Structural Constraints: ILS-CSL transforms\\nthe causal inferences made by LLMs into structural\\nconstraints explicitly indicating the edge existence or\\nabsence . The edge-level constraint is more powerful\\nthan its path-level counterpart (ancestral constraint) in\\nimproving CSL2, with less risk3. Please see Section III-C\\nfor further discussions.\\n2)Mitigation of Prior Errors: ILS-CSL markedly di-\\nminishes the count of erroneous constraints, all while\\nharnessing identical LLM resources. The reduction is\\ntheoretically by a factor of O(N), estimated as 1.8(N−\\n1), compared to the full inference on pairwise variables.\\nPlease refer to Section V-B for detailed estimation.\\n3)Efficient Causal Inference with LLM: ILS-CSL de-\\ncreases the number of pairwise variable inferences from\\x00N\\n2\\x01\\nto about O(N), as the LLM inference is restricted in\\nthe edges of of causal DAG4. Such reduction makes the\\nprocess more manageable and enhances the scalability\\nof the framework.\\nILS-CSL has shown consistent improvement in data-driven\\nCSL across all scales of the dataset used in the previous study\\n[15]. It effectively leverages various backbone causal discovery\\nalgorithms and demonstrates superior performance, especially\\nas the number of variables increases. These results underscore\\nILS-CSL’s significant potential for facilitating complex causal\\ndiscovery tasks in real-world scenarios.\\nII. R ELATED WORK\\nThis section discusses the emerging interest in the use of\\nLarge Language Models’ (LLMs) common sense for under-\\nstanding causal knowledge. It particularly focuses on the ways\\nthis knowledge is being harnessed in causal discovery.\\n2When an ancestral constraint is correctly identified, CSL might still recover\\na path that includes erroneous edges. In contrast, specifying the existence of\\nan edge directly ensures accuracy, as it cannot be misinterpreted.\\n3An incorrect ancestral constraint inevitably introduces at least one erro-\\nneous edge.\\n4Given that the causal DAG is usually sparse, the number of edges |E|is\\ntypically estimated as O(N).A. LLM-based Causal Discovery\\nRecent advancements in LLM-based causal discovery pri-\\nmarily focus on assessing the inherent capabilities of LLMs\\n[17], [18]. Long et al . [14] have tested LLMs’ ability to\\ngenerate simple causal structures, typically with sets of 3-4\\nvariables. In a specialized domain, a study [13] investigates\\nLLMs’ effectiveness in discerning causal relationships within\\nmedical pain diagnosis, though the findings were somewhat\\ninconclusive.\\nKıcıman et al . [10] have made strides in optimizing LLM\\nperformance for causal analysis by developing more refined\\nprompting techniques. Their work assesses LLMs across a\\nrange of causal tasks, revealing notable performance in pair-\\nwise causal discovery [19] and counterfactual inference [20],\\neven outperforming human analysis in certain aspects. Addi-\\ntionally, they have enhanced LLMs’ capacity to identify causal\\nstructures in datasets concerning medical pain diagnosis. How-\\never, despite these advancements, a significant gap persists\\nbetween the quality of causal DAGs generated by LLMs\\nand those derived from data-based algorithms. These findings\\nhighlight the potential of LLM-based causal knowledge, yet\\nthey also underscore the importance of integrating data in\\nuncovering genuine causal mechanisms.\\nB. Integration of LLM in Data-based Causal Discovery\\nA recent work first introduces LLM in causal discovery\\nfrom data [15]. Recognizing LLMs’ limitations in differen-\\ntiating indirect from direct causality, they applied ancestral\\nconstraints based on LLM-generated statements about the\\nexistence of causal relationships between variable pairs. The\\nauthors prompted the LLM with a complete set of variables,\\nseeking the most confident causal assertions. However, when\\npresented with numerous variables, LLM struggles to provide\\nresults that align with causal structures. This complexity leads\\nto a decrease in the accuracy of causal statements as the\\nnumber of variables increases, as demonstrated in Table I.\\nMoreover, we observe that the LLM also fails to make com-\\nprehensive causal analyses in larger scale datasets as would\\nbe possible with individual prompts for each pair of variables.\\nMotivated by this work, Vashishtha et al . [16] adopted a\\nmore targeted method. They individually prompted the LLM for causal relationships between each variable pair and imple-\\nmented a voting strategy to deduce ordering constraints. These\\nconstraints, although weaker than ancestral constraints (see\\nSection III-C for illustrations), offer more precise structural\\nguidance for causal discovery. Their methodology demon-\\nstrates notable improvements across seven real-world datasets.\\nHowever, the largest dataset examined containes only 23\\nnodes, leaving the approach’s effectiveness in more complex\\nscenarios untested.\\nIII. P RELIMINARIES\\nWe begin by introducing the task of causal structure learning\\n(CSL) on causal Bayesian Networks (BNs) and subsequently\\ndiscuss the integration of structural constraints.\\nA. Causal Bayesian Network\\nA Bayesian Network (BN) is a probabilistic graphical model\\nthat uses a Directed Acyclic Graph (DAG) to represent condi-\\ntional dependencies among a set of variables, thus defining\\ntheir joint probability distribution. For a set of variables\\nX={X1, X2, ..., X n}in a BN G, the joint probability\\ndistribution is given by:\\nP(X1, X2, ..., X n) =nY\\ni=1P(Xi|PaG\\ni)\\nPaG\\nidenotes the parent nodes of Xiin the DAG. It’s important\\nto note that an edge in a BN does not inherently imply a\\ncausal relationship [1]. A BN representing a joint probability\\ndistribution can be constructed using any variable ordering.\\nHowever, the causal order of variables, indicating cause and\\neffect, cannot be arbitrarily reversed.\\nA causal BN, in contrast, not only models the data distribu-\\ntion but also conforms to the principles of causality [21]. In the\\ncontext of cause-effect relationship, intervening on the causes\\nshould render the effect independent of other factors. This\\nintroduces additional requirements for representing causality\\nin a BN. In a causal BN, intervening on any subset of variables\\nXI⊆X, denoted as do(XI=x), results in a modified\\nprobability distribution PI(X). This is computed by severing\\nthe edges from each variable in XIto its parents and fixing\\ntheir values as per the intervention:\\nPI(X) =Y\\nXi/∈XIP(Xi|PaG\\ni)for all Xconsistent with x\\nThis aspect of causal BNs allows for the modeling of interven-\\ntions and causal inferences, distinguishing them from standard\\nBNs. It is important to note that in real-world scenarios, direct\\nintervention data is often not available. As a result, observed\\ndata is typically employed to infer intervention characteristics\\nand understand causal relationships.\\nB. Learning Causal BNs\\nThis part introduces the task of two mainstream solutions\\nof learning causal BNs, constraint- and score-based meth-\\nods. Formally, let D∈Nm×nrepresent the observational\\ndata, where mdenotes the number of observed samples andnrepresents the number of observed variables, denoted as\\nX={X1, X2, . . . , X n}. Each XiinDtakes discrete integer\\nvalues in the range [0, Ci). Given D, the goal is to determine\\nthe causal DAG G= (X, E(G)), where E(G)denotes the set\\nof directed causal edges among the variables in X. The formal\\ndefinitions are present as follows:\\nE(G)← {Xi−Xj|Xi̸⊥ ⊥Xj|Y,∀Y⊆X\\\\ {Xi, Xj}}\\n(1)\\nmax\\nGσ(G;D) =nX\\ni=1Lσ(Xi|PaG\\ni;D)s.t.G ∈DAG (2)\\nEquations (1) and (2) define the CSL task of constraint- and\\nscore-based methods, repectively. Constraint-based methods\\nfirst determine the skeleton of the graph using undirected\\nedges, Xi−Xj, based on conditional independence tests.\\nSubsequently, they orient some of these edges based on V-\\nstructure detection and DAG constraints [22], [23]. Score-\\nbased methods employ a scoring function, σ, to evaluate how\\nwell a given causal DAG Grepresents the observed data D.\\nTypically, σcan be decomposed into scores of local structures,\\nLσ(Xi|PaG\\ni;D), which simplifies the search process [24],\\n[25]. The objective is to optimize these local scores by\\nassigning appropriate parent nodes to each node, ensuring the\\nresulting graph is a DAG. An alternative approach to searching\\nthe DAG space is the ordering-based search, which optimizes\\nEquation (2) under a given ordering O, inherently satisfying\\nthe DAG constraint [26], [27]. The best-scored DAG of the\\nsearched orderings is then selected as the output.\\nThe design of scoring functions is based on the posterior\\nprobability of the DAG given the data, which includes a\\ncomponent representing the prior probability of DAG struc-\\ntures. Due to this adaptability in accommodating the prior\\nconstraints on structures, the score-based method is chosen\\nas the backbone CSL algorithm in our ILS-CSL framework.\\nC. Prior Constraints on Structures\\nPrior structural constraints play a pivotal role in improving\\nthe discovery of causal structures. The most prevalent among\\nthese constraints include [28]:\\n•Edge Existence : Denoted as Xi→Xjor, when forbid-\\nden,Xi↛Xj. This constraint dictates that the DAG\\nshould (or should not) contain the edge Xi→Xj.\\n•Ordering Constraint : Represented as Xi≺Xj, it\\nmandates that Xishould precede Xjin the variable\\nordering.\\n•Path Existence (Ancestral Constraint) : Symbolized as\\nXi⇝Xj, it requires the DAG to encompass the path\\nXi⇝Xj.\\nGiven the implication chain Xi→Xj⇒Xi⇝Xj⇒\\nXi≺Xj, it is clear that the existence of an edge (direct\\ncausality) represents the most stringent structural constraint.\\nCorrespondingly, its derivation necessitates a thorough exam-\\nination of potential combinations of causality. Regrettably, as\\nevidenced by the studies [10], [15], [13], LLMs lack the ability Algorithm 1 LLM supervised CSL\\nRequire: Observed data, D; Textual descriptions, T\\nEnsure: Causal DAG, G\\n1:Initialize the set of structural constraints, λ← {}\\n2:repeat\\n3:G ← arg max\\nGσ(G;D),s.t.G ∈ DAG,G |=λ\\n4: forXi→Xj∈E(G)do\\n5: c←LLM infers causality between XiandXj\\nbased on T\\n6: ifcisXi←Xjthen\\n7: λ←λ∪ {Xj→Xi}\\n8: end if\\n9: ifcisXi↮Xjthen\\n10: λ←λ∪ {Xi↛Xj, Xj↛Xi}\\n11: end if\\n12: end for\\n13:until no new constraints are added\\n14:return G\\nto accurately specify direct causality, often confusing it with\\nindirect causality or non-causal correlations. Please refer to\\nAppendix VII-F for empirical estimation.\\nRegarding the application of these prior constraints, there\\nare two predominant methodologies: hard and soft approaches.\\nThe hard approach prioritizes adherence to prior constraints,\\nfollowed by score optimization [29]. Conversely, the soft\\napproach strikes a balance between honoring prior constraints\\nand the associated score costs [8]. This often involves adjusting\\nthe scoring function to σ(G;D)+b(G;λ), where a prior prob-\\nability Pλis assigned to structural constraints λ. A constraint\\nis only accepted if the bonus score, b, compensates for the\\npenalty in the DAG-data consistency score, σ.\\nWe implement both hard and soft approaches to incorporate\\nstructural constraints in this paper.\\nIV. I TERATIVE LLM S UPERVISED CAUSAL STRUCTURE\\nLEARNING\\nGiven the observed data, D, and the descriptive texts on the\\ninvestigated field and variables, T, the LLM supervised causal\\nstructure learning is presented in Algorithm 1.\\nInitially, a causal DAG Gis learned from Dwith modular\\nscoring function σ,Lσ(see Equation (2) for definition), and\\nsearch method M. Subsequently, we explicate the details on\\nLLM supervision and how to constrain CSL accordingly.\\nA. LLM Supervision\\nFor each directed edge Xi→Xj∈E(G), we prompt the\\nused LLM to verify the causal statement that Xicauses\\nXj(Line 5 in Algorithm 1). The prompt design for causal\\ninference is inspired by the work [10], which employs choice-\\nbased queries to determine the orientation of pairwise variables\\nwith known causal relationships. On this basis, we incorporate\\nfield-specific descriptions to provide context and introduce\\nadditional choices to accommodate uncertainties in causal\\nexistence and intricate causal mechanisms. For a given edgeXi→Xjand associated textual descriptions T={tf, ti, tj},\\nthe LLM is prompted as:\\nYou are an expert on tf. There are two\\nfactors: Xi:ti,Xj:tj.\\nWhich cause-and-effect relationship is more likely\\nfor following causal statements for V1 and V2?\\nA.changing V1 causes a change in V2.\\nB.changing V2 causes a change in V1.\\nC.changes in V1 and in V2 are not correlated.\\nD.uncertain.\\nProvide your final answer within the tags\\n<Answer>A/B/C/D</Answer>.\\nAnalyze the statement: XiXj.\\ntfdescribes the investigated field, and ti, tjdescribes\\nXi, Xj, respectively. From the LLM’s response to this prompt,\\nwe can obtain one of the answers: A, B, C, or D.\\nTo specify constraints λ(Lines 6-11 in Algorithm 1), if the\\nanswer is B (reversed), we specify the existence of Xj→Xi.\\nIf C (no causality), then we specify Xi↮Xjto forbid the\\nexistence of edge. If D (uncertain) or A (correct), we do not\\nspecify constraints. This is because specifying the existence of\\nan edge already discovered from data does not often enhance\\nthe CSL and can inadvertently lead to errors. For example, if\\nthe true structure is Xi⇝Xjbut not directly, Xi↛Xj,\\nLLM easily infers that Xicauses Xjdue to its shortness in\\ndistinguishing indirect causality for the direct. If we specify\\nXi→Xj, an erroneous edge is introduced.\\nB. Prior constraint-based CSL\\nWith the structural constraints λobtained from LLM super-\\nvision, we integrate them into the next iteration of CSL process\\n(Line 3 in Algorithm 1), with either hard or soft approach. The\\nprocess terminates if no new constraint is specified.\\na) Hard approach: Firstly, the edge existence and forbid-\\nden constraints are used to specify the set of legal candidate\\nparents, C(i), and the set of variables always included in the\\nparents, K(i), of each variable Xi.\\nC(i) =X\\\\ {Xj|Xj↛Xi∈λ} \\\\ {Xi}\\nK(i) ={Xj|Xj→Xi∈λ}(3)\\nWith K(i), C(i), we prune the space of local structures.\\nL(Xi;λ) ={P|K(i)⊆P⊆C(i)} (4)\\nThe pruned space of local structures, L(·), is taken as input\\nfor the search method M:\\nM: max\\nPaG\\ninX\\niLσ(Xi|PaG\\ni;D)\\ns.t.G ∈ DAG,PaG\\ni∈L(Xi;λ)(5)\\nIn comparison to the problem form without prior constraints,\\nas presented in Equation (2), the restriction of the candidate\\nparent sets of each node, PaG\\ni∈L(Xi;λ), ensures that the\\noutput DAG absolutely satisfies every edge constraint, G |=λ. b) Soft approach: We adapt the scoring function to\\nmodel the edge constraints as follows:\\nσ′(G;D, λ) =nX\\niLσ(Xi|PaG\\ni;D) +Lb(Xi,PaG\\ni;λ)(6)\\nLb(Xi,PaG\\ni;λ) =\\nX\\nXj→Xi∈λ\\x10\\nIXj∈PaG\\nilogPλ+IXj̸∈PaG\\nilog (1−Pλ)\\x11\\n+X\\nXj↛Xi∈λ\\x10\\nIXj∈PaG\\nilog (1−Pλ) +IXj̸∈PaG\\nilogPλ\\x11(7)\\nThis formulation is grounded in the decomposability of edge\\nconstraints. A detailed derivation can be found in Section\\nVI-A.Icondition is the indicator function, which takes the\\nvalue 1if the condition is true and 0otherwise. Pλis the\\nprior confidence, a hyper-parameter. Then search method M\\noptimizes the modified score:\\nM: max\\nGnX\\niLσ(Xi|PaG\\ni;D)+Lb(Xi,PaG\\ni;λ),s.t.G ∈ DAG\\n(8)\\nThe bonus score, Lb, favors DAGs that align more closely\\nwith the structural constraints. Note that a constraint will not\\nbe satisfied if it excessively penalizes the score Lσ.\\nTo sum up, while the hard approach derives greater benefits\\nfrom accurate constraints (at the risk of being more sensitive\\nto errors), the soft approach might not always adhere to all\\ncorrect constraints but offers a degree of resilience against\\npotential inaccuracies.\\nV. A NALYSIS OF KEYCONCERNS\\nTheoretically quantifying the impact of prior knowledge\\non learned causal structures is difficult, mainly due to the\\ncomplex and unpredictable nature of data insufficiency and\\nnoise. Analyzing the disparity between data-implied causal\\nstructures and actual causal truths is intricate. Making strict\\nassumptions for analytical purposes might not reflect real-\\nworld scenarios, potentially leading to theoretical conclusions\\nwith limited practical applicability.\\nNevertheless, we can examine two primary aspects of prior\\nknowledge under simple and general assumptions: 1) the abil-\\nity of the applied prior knowledge to correct causal structures,\\nand 2) the alignment of the quality of this derived prior knowl-\\nedge with the actual causal structures. These aspects provide\\na more tangible and realistic assessment of the effectiveness\\nof prior knowledge in causal discovery.\\nA. Correction of Prior Independent Structures\\nCausal discovery fundamentally seeks to uncover unknown\\ncausal mechanisms. The role of prior knowledge, representing\\nknown causality, extends beyond merely adjusting the final\\noutput; it should ideally enhance the accuracy of the recon-\\nstructed causal structures. A key question is whether a prior\\nconstraint can indirectly influence and correct edges that are\\nnot directly governed by this knowledge.\\nTruth DAG\\nDAG from\\nobserved Data\\n1 Reversed\\n1 Missing\\nDAG by correcting \\nan existing edge ReverseMissing edge\\nrecoveredFig. 1: An example of recovering missing edges by reversing\\nexisting edges.\\nIn the context of ILS-CSL, this question becomes particu-\\nlarly relevant when examining the orientation and prohibition\\nof learned edges: Do these constraints contribute to identifying\\nmissing edges? We explore this aspect, offering an illustrative\\nexample in Figure 1. Due to limitations in real-world observa-\\ntional data, the probability distribution suggested by the data\\ncorresponds to a DAG with two errors: one reversed edge and\\none missing edge.\\nILS-CSL supervises the existing edges and corrects the\\nreversed edge X3→X2. According to Bayesian Network\\nprinciples, we have P(X3|PaG\\n3) = P(X3|X1, X2).\\nHowever, the observed data indicate that X3andX1are\\nnot independent when conditioned on X2, as per the current\\nDAG structure. This inconsistency implies that the BN cannot\\naccurately model the data distribution if X2is the only parent\\nofX3. Consequently, ILS-CSL identifies and reinstates the\\nmissing edge X1→X3, thus refining the DAG to better align\\nwith the underlying data distribution.\\nViewing this from the lens of knowledge-based causality,\\nconstraints derived from known causal relations can enhance\\nthe discovery of unknown causal mechanisms within data. This\\nhighlights the invaluable role of prior knowledge in advancing\\ncausal discovery in uncharted fields.\\nB. Estimation of Prior Error Counts\\nThis section estimates and compares the number of erro-\\nneous constraints ILS-CSL against that stemming from a full\\ninference on all pairwise variables, an intuitive strategy in the\\nexisting methods [15], [16].\\nWe commence by defining five cases during LLM-based\\ncausality inference, along with their respective probabilities:\\n1) Extra Causality ( pe): Given a causal statement (X1, X2),\\nif the true causal DAG neither contains the path X1⇝\\nX2norX2⇝X1, it’s an instance of extra causality.\\n2) Reversed Causality ( pr): Given a causal statement\\n(X1, X2), if the true causal DAG contains the path\\nX2⇝X1, it’s an instance of reversed causality.\\n3) Reversed Direct Causality ( pd\\nr): Given a causal statement\\n(X1, X2), if the true causal DAG has an edge X2→X1,\\nit’s an instance of extra causality.\\n4) Missing Direct Causality ( pd\\nm): If an edge X1→X2or\\nX2→X1exist in the true causal DAG, but X1andX2\\nare inferred to have no causal relationship, it’s a instance\\nof missing direct causality. 5) Correct Existing Causality ( pc): Given a causal statement\\n(X1, X2), if the path X1⇝X2exists in the true causal\\nDAG, it’s a instance of correct existing causality.\\nWe assume that 1) the probability of these cases is identical\\nwhen satisfying the corresponding structures, and 2) the truth\\nDAG and learned DAG are both sparse.\\nConsider a causal DAG consisting of Nnodes. Based on\\nthe sparsity assumption, the number of node pairs without\\nconnecting paths in the truth DAG is represented as γ1\\x00N\\n2\\x01\\n. In\\nthe learned causal DAG, there are γ2Nedges. Of these edges,\\nthe proportion of correctly identified edges is denoted as z1,\\nthe proportion of reversed edges as z2, and the proportion of\\nextra edges that do not exist in the true DAG as z3.\\nThe number of prior errors derived from full inference\\nconsists of two parts: the extra causality, peγ1\\x00N\\n2\\x01\\n, and the\\nreversed causality, pr(1−γ1)\\x00N\\n2\\x01\\n. Note that the missing\\ncausality will not harm the CSL since it does not produce any\\nstructural constraints in this context. Then the total number of\\nerroneous constraints is estimated as:\\nEfull= (peγ1+pr(1−γ1))\\x12N\\n2\\x13\\n(9)\\nAs for the prior errors within our framework, we consider\\nthe output DAG of CSL algorithms. The erroneous constraints\\non the correctly discovered edges consist of the reversed and\\nmissing direct causality: (pd\\nr+pd\\nm)z1γ2N; The erroneous\\nconstraints derived from inferring causality on erroneous edges\\nconsist of 1) missing direct causality on reversed edges,\\npd\\nmz2γ2N, and 2) extra inferred direct causality on extra\\nedges no more than\\x00\\npr+pcPR|E\\x01\\nz3γ2N, where PR|Eis the\\nprobability where for an extra edge X1→X2in the learned\\nDAG, a reversed path X2⇝X1exists in the ground truth.\\nGathering all these, we derive the number prior errors:\\nEours≤\\x00\\n(pd\\nr+pd\\nm)z1+pd\\nmz2+ (pr+pcPR|E)z3\\x01\\nγ2N\\n(10)\\nWe utilize eight real-world datasets, and GPT-4 as LLM to\\nestimate p, and MIONBSx algorithm to estimate λ, r, P R|E,\\nsee Section VI-B for details. The results are:\\npe≈0.56, pr≈0.15, pd\\nr≈0.03, pd\\nm≈0.05\\npc≈0.75, γ1≈0.51, γ2≈1.09, z1≈0.88\\nz2≈0.05, z3≈0.07, PR|E≈0.05(11)\\nAnd then we have:\\nEours≈0.10N, E full≈0.36\\x12N\\n2\\x13\\n,Eours\\nEfull≈1\\n1.8(N−1)\\n(12)\\nThis indicates that, relative to full pairwise variable inference,\\nILS-CSL significantly reduces the number of erroneous con-\\nstraints resulting from imperfect LLM inferences by approxi-\\nmately a factor of 1.8(N−1). This reduction is particularly\\nimpactful when dealing with larger sets of variables.VI. S UPPLEMENTARY ILLUSTRATIONS\\nA. Derivation of Prior-based Scoring\\nIn this section, we derive the prior-based scoring function, as\\npresented in Equations (6) and (7), for the DAG G(X, E(G)).\\nThe prior constraints are denoted as λ:<R,Π>. The set\\nR={r1, r2,···, rm}comprises edge variables on mpair-\\nwise variables, where ri∈ {→ ,↛}.Π= Πm\\ni=1P(ri)is the\\nassociated probability distribution.\\nBeginning with the derivation of the scoring function with-\\nout prior constraints, let Dbe a complete multinomial ob-\\nserved data over variables X. Utilizing the Bayesian Theorem,\\nthe probability of a network GoverXis expressed as:\\nP(G|D)∝P(D|G)·P(G)\\nGiven that P(D)remains consistent across all DAGs, the score\\nof a network is typically the logarithm of P(G|D), resulting\\ninSc(G|D) =Sc(D|G) +Sc(G). Bayesian scoring methods,\\nsuch as K2 [30] and BDe, BDeu [24], aim to approximate\\nthe log-likelihood based on various assumptions. When priors\\nare uniform, Sc(G)can be disregarded during maximization.\\nHowever, with the introduction of prior structural constraints,\\ndenoted as λ, this term gains significance.\\nLet’s define Cas a configuration, representing a joint\\ninstantiation of values to edge variables R={r1, r2, ..., r m}.\\nThe probability for this configuration is JC=P(R=C|Π).\\nFor a specific DAG G, its configuration is represented as CG.\\nThus, we can express:\\nP(G |D, λ) =P(D| G)·P(G |J)\\nP(D|J)(13)\\nThe above equation is derived from the understanding that,\\ngiven the graph G, the data Dis independent of J. This is\\nbecause Joffers no supplementary information about the data\\nonce the graph structure is known. The term P(D|J)serves\\nas a normalizing constant, consistent across all DAGs. The\\ntermP(D| G)corresponds to the scoring function Sc(D| G)\\nin the absence of prior constraints. The scoring function can\\nbe expressed as:\\nSc(G |D, λ) =Sc(D| G) +Sc(G |J) (14)\\nHere, Sc(D| G)represents the scoring function without\\nprior constraints, denoted as σ(G |D). Meanwhile, Sc(G |J)\\npertains to the bonus score associated with prior constraints.\\nShifting our focus to the prior factor P(G |J), we have:\\nP(G |J) =P(G, CG|J) =P(G |J, CG)·P(CG|J)\\n=P(G |CG)·JCG(15)\\nThe first equation holds since CGis inherently a function of G.\\nThe term P(G |CG)denotes the likelihood of graph Gwhen\\na specific configuration is present. In the absence of any other\\nprior constraints, we assign an identical prior to all graphs\\nsharing the same configuration. Let NCrepresent the count TABLE II: Accuracy and reversed ratio of the sampled pairwise variables on eight datasets.\\nDataset Alarm Asia Insurance Mildew Child Cancer Water Barley\\nDirect causality (Acc 1/Rev 1) 1.00 / 0.00 1.00 / 0.00 0.85 / 0.05 0.95 / 0.05 1.00 / 0.00 1.00 / 0.00 0.95 / 0.05 0.70 / 0.05\\nIndirect causality (Acc 2/Rev 2)0.65 / 0.15 1.00 / 0.00 0.95 / 0.05 1.00 / 0.00 0.50 / 0.40 1.00 / 0.00 0.50 / 0.50 0.30 / 0.30\\nNo causality (Acc 3) 0.60 0.80 0.35 0.10 0.50 0.00 0.45 0.50\\nQualitative causality(Acc 4/ Rev 4)0.72 / 0.12 1.00 / 0.00 0.92 / 0.05 0.99 / 0.01 0.70 / 0.24 1.00 / 0.00 0.67 / 0.33 0.36 / 0.26\\nof DAGs over nodes Vthat have the configuration C. Thus,\\nP(G |CG) = 1 /NCG, leading to:\\nP(G |J) =JCG\\nNCGand Sc(G |J) = log\\x12JCG\\nNCG\\x13\\n(16)\\nGiven that the count of edge variables (or edge constraints)\\nremains consistent across all DAGs, NCGis also consistent for\\nall DAGs. Therefore:\\nSc(G |J) = log JCG= log P(R=CG|Π) =X\\nri∈RlogP(ri)\\n(17)\\nAssuming P(ri) =Pλwhen λindicates the presence of the\\ncorresponding edge, and P(ri) = 1 −Pλwhen the edge’s\\nexistence is negated, we deduce:\\nSc(G |J) =X\\nXj→Xi∈λIXj→Xi∈E(G)logPλ+IXj→Xi̸∈E(G)log(1−Pλ)+\\nX\\nXj↛Xi∈λIXj→Xi∈E(G)log(1−Pλ) +IXj→Xi̸∈E(G)logPλ\\n(18)\\nBy integrating Equations (14), (18), and (2), we derive the\\nform of the local prior constraint-based scoring function, as\\ndepicted in Equations (6) and (7).\\nB. Parameter Estimation in Section V-B\\nThis section presents the details on the estimation of pa-\\nrameters related to the quality of LLM based causal inference,\\npe, pr, pd\\nr, pd\\nm, pc, structures of the true causal DAGs, γ1, and\\nstructures of the learned causal DAGs, γ2, z1, z2, z3, PR|E.\\na) Quality of LLM causal inference: We randomly sam-\\nple three kinds of pairwise variables from the employed eight\\ndatasets in experiments:\\n1) Direct edges: Sampling pairwise variables with direct\\nedge Xi→Xjin the ground truth.\\n2) Indirect path: Sampling pairwise variables without direct\\nedge but with a directed path, Xi↛Xj, Xi⇝Xj.\\n3) Not connected: Sampling pairwise variables without any\\npath, Xi̸⇝Xj, Xj̸⇝Xi.\\nFor each type, we sample 20 pairwise variables form each\\ndataset, if more than 20 pairwise variables satisfying the\\ncondition exist in the causal DAG. Or we use all the pairwise\\nvariables as samples.\\nSubsequently, we query GPT-4 the causality between each\\npairwise variables through the prompt in Section IV. The true\\nanswer of Types 1 and 2 is A, and that of Type 3 is C.\\nThe accuracy of GPT-4 on different datasets on these samplestogether with the ratio of reversed inference (B for Types 1\\nand 2) are reported in Table II.\\nDirect causality corresponds to direct edges, indirect causal-\\nity to indirect paths, and no causality corresponds to not\\nconnected variables. The accuracy and reversed ratio of LLM\\ninference on them is obtained by experiments. The qualitative\\ncausality corresponds the paths (including edges), whose accu-\\nracy is estimated by Acc 4= (Acc 1×|E|+Acc 2×|P|)/(|E|+\\n|P|), where |E|and|P|represents the number of edges and\\nindirect paths in the true causal DAG.\\nBy weighted sum of the accuracy and reversed ratio, we\\nobtain the estimation of them. Then the probability of the five\\nintroduced error that GPT-4 makes are presented as follows:\\n1) Extra causality: pe= 1−Acc 3= 0.56\\n2) Reversed causality: pr=Rev 4= 0.15\\n3) Reversed direct causality: pd\\nr=Rev 1= 0.03\\n4) Missing direct causality: pd\\nm= 1−Acc 1−Rev 1= 0.05\\n5) Correct existing causality: pc=Acc 4= 0.75\\nWe see that the major errors of GPT-4 inference is sourced\\nfrom the extra causality, which is because some intuitively\\ncorrelated concepts may not generate real causal relations\\nin an experiment with specific conditions. And that is why\\nwe should refer to data for causal analysis. However, GPT-\\n4 is prone to infer correct causality on pairwise variables\\nwith direct causality, which is the base of our framework to\\nefficiently improves the quality of learned causal DAGs.\\nb) Structural parameters: The structural parameters is\\nestimated by the average value of them on the eight datasets.\\nThe ones related to the causal structure learning of each dataset\\nis estimated by the average value of them on twelve segments\\nof observed data, using MINOBSx search and BDeu score.\\nSee the detailed results in Table III.\\nTABLE III: The estimated structural paramters on eight\\ndatasets.\\nDataset Alarm Asia Insurance Mildew Child Cancer Water Barley Avg.\\nγ1 0.67 0.36 0.52 0.52 0.66 0.20 0.65 0.52 0.51\\nγ2 1.22 1.01 1.44 0.79 1.09 0.55 1.34 1.27 1.09\\nz1 0.96 0.88 0.91 0.87 0.98 0.90 0.67 0.84 0.88\\nz2 0.02 0.00 0.05 0.08 0.00 0.07 0.12 0.07 0.05\\nz3 0.02 0.12 0.04 0.05 0.02 0.03 0.21 0.09 0.07\\nPR|E 0.02 0.00 0.05 0.08 0.00 0.10 0.12 0.08 0.05\\nVII. E XPERIMENTS\\nWe conduct experiments to address the research questions:\\nRQ1: Can ILS-CSL enhance data-based CSL baselines and\\noutperform the existing LLM-driven CSL method? TABLE IV: The used datasets of causal DAGs.\\nDataset Cancer Asia Child Alarm Insurance Water Mildew Barley\\nVariables 5 8 20 37 27 32 35 48\\nEdges 4 8 25 46 52 66 46 84\\nParameters 10 18 230 509 1008 10083 540150 114005\\nData size 250 / 1000 250 / 1000 500 / 2000 1000 / 4000 500 / 2000 1000 / 4000 8000 / 32000 2000 / 8000\\nTABLE V: Scaled SHD ↓comparison to data-based and LLM-driven CSL.\\nDataset Cancer Asia Child Insurance\\nN 250 1000 250 1000 500 2000 500 2000\\nMINOBSx 0.75 ±0.22 0.46 ±0.29 0.52 ±0.32 0.31 ±0.07 0.38 ±0.08 0.21 ±0.04 0.46 ±0.05 0.29 ±0.02\\n+sepLLM-hard 0.13 -83% 0.00 -100% 0.27 -48% 0.04 -87% 0.42 +11% 0.31 +48% 0.91 +98% 0.60 +107%\\n+ILS-CSL-hard 0.50 ±0.22 -33% 0.29 ±0.29 -37% 0.42 ±0.37 -19% 0.15 ±0.15 -52% 0.25 ±0.06 -34% 0.07 ±0.03 -67% 0.42 ±0.03 -9% 0.28 ±0.06 -3%\\nCaMML 0.75 ±0.00 0.62 ±0.14 0.58 ±0.29 0.27 ±0.05 0.25 ±0.03 0.09 ±0.04 0.69 ±0.04 0.61 ±0.15\\n+sepLLM-soft 0.50 -33% 0.33 -47% 0.02 -97% 0.00 -100% 0.19 -24% 0.04 -56% 1.00 +45% 0.82 +34%\\n+ILS-CSL-soft 0.75 ±0.00 +0% 0.33 ±0.20 -47% 0.23 ±0.09 -60% 0.15 ±0.18 -44% 0.17 ±0.05 -32% 0.04 ±0.00 -56% 0.47 ±0.04 -32% 0.47 ±0.11 -23%\\nDataset Alarm Mildew Water Barley\\nN 1000 4000 8000 32000 1000 4000 2000 8000\\nMINOBSx 0.21 ±0.06 0.14 ±0.04 0.50 ±0.02 0.46 ±0.05 0.77 ±0.07 0.61 ±0.04 0.56 ±0.04 0.40 ±0.03\\n+sepLLM-hard 0.27 +29% 0.19 +36% 0.88 +76% 0.47 +2% 1.01 +31% 0.84 +38% 0.62 +11% 0.65 +62%\\n+ILS-CSL-hard 0.09 ±0.03 -57% 0.08 ±0.02 -43% 0.43 ±0.00 -14% 0.33 ±0.18 -28% 0.68 ±0.05 -12% 0.56 ±0.02 -8% 0.54 ±0.02 -4% 0.38 ±0.02 -5%\\nCaMML 0.24 ±0.05 0.18 ±0.06 1.20 ±0.10 1.30 ±0.12 0.88 ±0.08 0.81 ±0.04 0.96 ±0.07 0.96 ±0.10\\n+sepLLM-soft 0.13 -46% 0.07 -61% 1.07 -11% 1.30 +0% 0.89 +1% 0.73 -10% 0.98 +2% 0.98 +2%\\n+ILS-CSL-soft 0.08 ±0.01 -67% 0.06 ±0.01 -67% 1.01 ±0.07 -16% 1.26 ±0.05 -3% 0.70 ±0.02 -20% 0.63 ±0.04 -22% 0.90 ±0.06 -6% 0.83 ±0.06 -14%\\nThe suffixes ‘-hard’ and ’-soft’ represent the approach to apply the LLM inferred prior constraints. The performances of sepLLM method are obtained from the work [15].\\nRQ2: Across diverse backbone algorithms, can ILS-CSL\\nconsistently improve the quality of causal structures? which\\nof the soft and hard constraint is better?\\nRQ3: Is ILS-CSL resistant to imperfect LLM causal infer-\\nences, and capable to derive accurate prior? Why?\\nRQ4: How does the process, where LLM supervises causal\\ndiscovery, unfold in detail?\\nAll the datasets, codes, and supplementary results can be\\naccessed in the external repository5.\\nA. Datasets and Baselines\\nTo address RQ1, we employ the eight real-world datasets of\\ncausal DAGs from the Bayesian Network Repository6as used\\nin the comparative study [15]. Dataset specifics are provided\\nin Table IV. For backbone CSL algorithms, we adopt the same\\nMINOBSx (BDeu score) [28] and CaMML (MML score) [31]\\nalgorithms, and utilize the same setting of prior probability for\\nCaMML, 0.99999. For supervision on CSL, we utilize GPT-4-\\nWEB7. For RQ2, the used baselines comprise a combination of\\npopular scoring functions, namely BIC and BDeu score [24],\\nand search algorithms, including HC [32] and MINOBSx [33].\\nB. Observed Data and Evaluation Metric\\nWe utilize a collection of observed data sourced from a\\npublic repository8. This data, generated based on the eight\\ncausal DAGs, is provided by Li and Beek [28], and used in the\\n5https://github.com/tyMadara/ILS-CSL\\n6https://www.bnlearn.com/bnrepository/\\n7https://chat.openai.com/\\n8https://github.com/andrewli77/MINOBS-anc/tree/master/data/csvcomparative work [15]. The repository offers datasets in two\\ndistinct sample sizes for each DAG, as detailed in Table IV.\\nFor every sample size, six distinct data segments are available.\\nTo assess the quality of the learned causal structures, we\\nprimarily employ the scaled Structural Hamming Distance\\n(SHD) [34]. This metric is defined as the SHD normalized\\nby the total number of edges in the true causal DAG.\\nTABLE VI: Ranking of methods in Table V.\\nData-based CSL SepLLM ILS-CSL\\nMINOBSx CaMML MINOBSx CaMML MINOBSx CaMML\\n3.6 4.8 4.0 3.9 1.9 2.9\\nC. Comparison Experiments (RQ1)\\nWe compare the performance of MINOBSx (BDeu) and\\nCaMML that are used in the separate LLM prior-driven CSL\\napproach proposed by [15], referred to as sepLLM, and our\\nproposed framework, termed ILS-CSL. This comparison is\\nconducted using all the introduced observed data across eight\\ndatasets. The results, presented in terms of scaled SHD (where\\na lower value is preferable), are detailed in Table V. The\\ndifference between scaled SHD of data-based ( ∆data) and\\nLLM-driven ( ∆LLM) CSL is also reported, by calculating\\n(∆LLM−∆data)/∆data. The Friedman ranking of the methods\\nand more is reported in Table VI.\\nKey observations from Table V are presented as follows.\\n1) ILS-CSL consistently improves the quality of data-based\\nCSL in all cases, with the sole exception observed in the\\nCancer dataset with 250 samples, where it maintains the TABLE VII: Scaled SHD ↓enhancement on data-based CSL with different scores, search algorithms and approaches to apply\\nprior constraints, by the proposed framework.\\nDataset Cancer Asia Child Insurance\\nN 250 1000 250 1000 500 2000 500 2000\\nHC-BDeu 0.58 ±0.13 0.33 ±0.26 0.56 ±0.27 0.23 ±0.17 0.57 ±0.12 0.49 ±0.18 0.69 ±0.06 0.68 ±0.09\\n+ILS-CSL-hard 0.50 ±0.22 -14% 0.29 ±0.29 -12% 0.46 ±0.33 -18% 0.15 ±0.15 -35% 0.24 ±0.07 -58% 0.10 ±0.02 -80% 0.45 ±0.06 -35% 0.34 ±0.04 -50%\\n+ILS-CSL-soft 0.50 ±0.22 -14% 0.29 ±0.29 -12% 0.44 ±0.30 -21% 0.15 ±0.15 -35% 0.26 ±0.06 -54% 0.11 ±0.03 -78% 0.50 ±0.08 -28% 0.35 ±0.04 -49%\\nMINOBSx-BDeu 0.75 ±0.22 0.46 ±0.29 0.52 ±0.32 0.31 ±0.07 0.38 ±0.08 0.21 ±0.04 0.46 ±0.05 0.29 ±0.02\\n+ILS-CSL-hard 0.50 ±0.22 -33% 0.29 ±0.29 -37% 0.42 ±0.37 -19% 0.15 ±0.15 -52% 0.25 ±0.06 -34% 0.07 ±0.03 -67% 0.42 ±0.03 -9% 0.28 ±0.06 -3%\\n+ILS-CSL-soft 0.50 ±0.22 -33% 0.29 ±0.29 -37% 0.42 ±0.37 -19% 0.15 ±0.15 -52% 0.25 ±0.04 -34% 0.08 ±0.04 -62% 0.41 ±0.03 -11% 0.26 ±0.04 -10%\\nHC-BIC 0.92 ±0.29 0.62 ±0.34 0.48 ±0.36 0.31 ±0.29 0.53 ±0.07 0.38 ±0.16 0.76 ±0.05 0.72 ±0.06\\n+ILS-CSL-hard 0.92 ±0.29 +0% 0.42 ±0.34 -32% 0.33 ±0.25 -31% 0.19 ±0.17 -39% 0.26 ±0.07 -51% 0.07 ±0.03 -82% 0.60 ±0.03 -21% 0.41 ±0.03 -43%\\n+ILS-CSL-soft 0.92 ±0.29 +0% 0.42 ±0.34 -32% 0.35 ±0.26 -27% 0.21 ±0.19 -32% 0.27 ±0.08 -49% 0.07 ±0.05 -82% 0.62 ±0.06 -18% 0.42 ±0.03 -42%\\nMINOBSx-BIC 1.00 ±0.25 0.62 ±0.21 0.46 ±0.23 0.27 ±0.05 0.34 ±0.06 0.18 ±0.04 0.62 ±0.05 0.55 ±0.05\\n+ILS-CSL-hard 0.92 ±0.29 -8% 0.38 ±0.26 -39% 0.42 ±0.40 -9% 0.12 ±0.08 -56% 0.24 ±0.08 -29% 0.06 ±0.02 -67% 0.55 ±0.03 -11% 0.39 ±0.08 -29%\\n+ILS-CSL-soft 0.92 ±0.29 -8% 0.38 ±0.26 -39% 0.35 ±0.26 -24% 0.15 ±0.12 -44% 0.25 ±0.05 -26% 0.06 ±0.02 -67% 0.55 ±0.03 -11% 0.41 ±0.09 -25%\\nDataset Alarm Mildew Water Barley\\nN 1000 4000 8000 32000 1000 4000 2000 8000\\nHC-BDeu 0.65 ±0.12 0.64 ±0.09 0.79 ±0.11 0.99 ±0.07 0.76 ±0.07 0.64 ±0.08 0.80 ±0.06 0.65 ±0.06\\n+ILS-CSL-hard 0.12 ±0.02 -82% 0.08 ±0.01 -88% 0.46 ±0.01 -42% 0.22 ±0.02 -78% 0.64 ±0.02 -16% 0.55 ±0.03 -14% 0.69 ±0.06 -14% 0.57 ±0.06 -12%\\n+ILS-CSL-soft 0.30 ±0.05 -54% 0.25 ±0.06 -61% 0.43 ±0.00 -46% 0.47 ±0.04 -53% 0.64 ±0.01 -16% 0.56 ±0.03 -12% 0.76 ±0.04 -5% 0.62 ±0.03 -5%\\nMINOBSx-BDeu 0.21 ±0.06 0.14 ±0.04 0.50 ±0.02 0.46 ±0.05 0.77 ±0.07 0.61 ±0.04 0.56 ±0.04 0.40 ±0.03\\n+ILS-CSL-hard 0.09 ±0.03 -57% 0.08 ±0.02 -43% 0.43 ±0.00 -14% 0.33 ±0.18 -28% 0.68 ±0.05 -12% 0.56 ±0.02 -8% 0.54 ±0.02 -4% 0.38 ±0.02 -5%\\n+ILS-CSL-soft 0.09 ±0.02 -57% 0.07 ±0.01 -50% 0.47 ±0.01 -6% 0.37 ±0.02 -20% 0.68 ±0.04 -12% 0.56 ±0.02 -8% 0.55 ±0.03 -2% 0.38 ±0.02 -5%\\nHC-BIC 0.68 ±0.05 0.59 ±0.10 0.90 ±0.06 0.91 ±0.13 0.76 ±0.04 0.70 ±0.03 0.87 ±0.05 0.80 ±0.08\\n+ILS-CSL-hard 0.22 ±0.04 -68% 0.12 ±0.04 -80% 0.58 ±0.01 -36% 0.46 ±0.04 -49% 0.69 ±0.02 -9% 0.61 ±0.03 -13% 0.76 ±0.02 -13% 0.69 ±0.06 -14%\\n+ILS-CSL-soft 0.41 ±0.04 -40% 0.35 ±0.11 -41% 0.71 ±0.01 -21% 0.57 ±0.02 -37% 0.69 ±0.02 -9% 0.61 ±0.03 -13% 0.82 ±0.04 -6% 0.74 ±0.09 -8%\\nMINOBSx-BIC 0.32 ±0.08 0.15 ±0.04 0.74 ±0.01 0.73 ±0.09 0.82 ±0.03 0.77 ±0.03 0.79 ±0.04 0.58 ±0.03\\n+ILS-CSL-hard 0.16 ±0.07 -50% 0.09 ±0.03 -40% 0.58 ±0.01 -22% 0.45 ±0.03 -38% 0.69 ±0.03 -16% 0.62 ±0.01 -19% 0.73 ±0.03 -8% 0.55 ±0.03 -5%\\n+ILS-CSL-soft 0.19 ±0.06 -41% 0.10 ±0.01 -33% 0.73 ±0.01 -1% 0.64 ±0.04 -12% 0.70 ±0.02 -15% 0.64 ±0.02 -17% 0.76 ±0.02 -4% 0.56 ±0.03 -3%\\nTABLE VIII: Ranking of methods in Table VII.\\nBDeu BIC\\nMIN-\\nOBSx+hard+soft HC +hard+softMIN-\\nOBSx+hard+soft HC +hard+soft\\n7.0 2.7 2.810.1 3.4 5.3 9.8 4.9 6.2 11.2 6.6 8.0\\nsame performance. In contrast, sepLLM shows consis-\\ntent improvement only in the Cancer andChild datasets,\\nwhile exhibiting partial performance degradation in oth-\\ners. This observation underscores the robust and stable\\nenhancement offered by our ILS-CSL framework.\\n2) Our framework outperforms sepLLM in datasets with\\nmore than 20 variables, albeit showing lesser perfor-\\nmance in small-scale datasets, Cancer and Asia. This\\ntrend is attributed to the relatively simple causal mecha-\\nnisms in these smaller datasets, where LLM effectively\\ninfers correct causal relationships between variables\\n(refer to Table II in Section VI-B). Despite sepLLM\\nleveraging all existing causality inferred by LLM, its\\nadvantage is pronounced only in these two datasets.\\nAs the complexity of causal mechanisms increases with\\nthe number of variables, the quality of LLM inference\\ndiminishes, highlighting the resilience of our framework\\nagainst imperfect LLM inference.\\nTable VI demonstrates that ILS-CSL consistently ranks\\nwithin the top two positions. Notably, within the sepLLM\\nframework, CaMML, which uses soft constraints, outperformsMINOBSx, which relies on hard constraints. However, this\\ntrend reverses in the ILS-CSL framework. This shift is at-\\ntributed to the ability of soft constraints to filter out some\\nincorrect prior structures that significantly conflict with the\\ndata distribution. The prior constraints in sepLLM are not as\\nhigh-quality as those in LLM-CSL, and the use of ancestral\\nconstraints in sepLLM tends to introduce erroneous edges.\\nD. ILS-CSL With Diverse Backbone Algorithms (RQ2)\\nWe experiment with varying scoring functions, BDeu and\\nBIC scores, and search algorithms, MINOBSx and HC,\\nand compare to corresponding data-based CSL performances.\\nMoreover, we experiment with both hard and soft approaches\\nto apply prior constraints, with the prior probability setting\\nPλ= 0.99999 introduced in Equation (7). The results on the\\nutilized observed data of eight datasets are reported in Table\\nVII. The Friedman ranking of the methods is reported in Table\\nVIII. Key observations include:\\n1) Nearly all scenarios showcase an enhancement, under-\\nscoring the impactful role of ILS-CSL in improving CSL\\nperformance across diverse datasets and algorithms.\\n2) ILS-CSL’s impact on causal discovery significantly sur-\\npasses the limitations imposed by scoring functions and\\nsearch algorithms. The ranking results demonstrate this\\nclearly, as HC+ILS-CSL exceeds the performance of\\nMINOBSx, even with a less robust baseline. This also\\nholds true across different scoring functions, highlight- TABLE IX: The precision along with ratio of different structures of different answers by GPT-4.\\nOverall PrecisionAnswer DatasetDirect\\nedgesReversed\\nedgesPrecisionIndirect\\npathsReversed\\nindirect pathsNot\\nreachable Qualitative Structural\\nAlarm 0.33 0.02 0.94 0.28 0.00 0.37 0.61 0.33\\nAsia 0.44 0.00 1.00 0.50 0.00 0.06 0.94 0.44\\nBarley 0.22 0.12 0.65 0.23 0.12 0.31 0.45 0.22\\nCancer 0.36 0.09 0.80 0.36 0.09 0.09 0.73 0.36\\nChild 0.46 0.02 0.96 0.26 0.04 0.22 0.72 0.46\\nInsurance 0.41 0.05 0.89 0.32 0.06 0.15 0.74 0.41\\nMildew 0.45 0.04 0.92 0.36 0.03 0.11 0.82 0.45A\\nWater 0.47 0.13 0.78 0.11 0.01 0.28 0.58 0.47\\nAlarm 0.02 0.36 0.95 0.10 0.18 0.34 0.54 0.36\\nAsia 0.00 0.50 1.00 0.00 0.36 0.14 0.86 0.50\\nBarley 0.02 0.21 0.91 0.08 0.43 0.25 0.64 0.21\\nCancer 0.00 0.60 1.00 0.00 0.00 0.40 0.60 0.60\\nChild 0.00 0.45 1.00 0.24 0.12 0.18 0.58 0.45\\nInsurance 0.02 0.59 0.97 0.02 0.10 0.27 0.68 0.59\\nMildew 0.01 0.49 0.98 0.00 0.14 0.35 0.64 0.49B\\nWater 0.03 0.51 0.94 0.29 0.03 0.14 0.54 0.51\\nAlarm 0.00 0.00 - 0.00 0.03 0.97 0.97 1.00\\nAsia 0.00 0.00 - 0.00 0.00 1.00 1.00 1.00\\nBarley - - - - - - - -\\nCancer - - - - - - - -\\nChild 0.00 0.11 - 0.00 0.11 0.79 0.79 0.89\\nInsurance 0.03 0.05 - 0.00 0.10 0.83 0.83 0.93\\nMildew 0.00 0.01 - 0.32 0.36 0.32 0.32 0.99C\\nWater 0.00 0.04 - 0.30 0.19 0.47 0.47 0.96\\nAlarm Asia Cancer Child0246810 Number of errors\\nBarley Insurance Mildew Water0102030\\nLLM reasoning\\nPrior constants\\nFig. 2: Erroneous LLM inference and erroneous specified edge\\nconstraints of MINOBSx-BDeu+ILS-CSL-hard.\\ning ILS-CSL’s broad applicability and effectiveness in\\nimproving causal discovery outcomes.\\n3) The hard approach outperforms the soft approach, at-\\ntributed to the high quality of specified constraints within\\nILS-CSL. This stands in stark contrast to the findings\\nby [15], where the soft approach fared better due to the\\nlower quality of prior constraints.\\nE. Errors in LLM Inference and Prior Constraints (RQ3)\\nThis section is dedicated to the evaluation of ILS-CSL’s\\nrobustness against the inaccuracies in LLM inference. We\\nscrutinize the erroneous causal relationships inferred by LLM\\non the edges of the learned DAG, along with the incorrect\\nprior constraints that stem from them. The results pertaining\\nto each dataset, which includes two unique sizes of observed\\ndata related to MINOBSx-BDeu with the hard approach, are\\nillustrated in Figure 2. For a more comprehensive set of results,\\nrefer to the external repository.Our observations highlight a substantial reduction in the\\nerrors of specified edge constraints compared to erroneous\\nLLM inference. This reduction stems from the strategy of only\\nimposing constraints on causality that is inconsistent with what\\nhas been learned. A more detailed analysis on the superior\\naspect of ILS-CSL to reduce erroneous constraints is made in\\nthe following experiment.\\nF . Why Resistant to Imperfect LLM Inference (RQ3)\\nThis section elucidates the ability of ILS-CSL to minimize\\nprior errors by limiting LLM supervision to edges. We present\\nthe ratio of various real structures corresponding to all pairwise\\nvariables inferred by GPT-4. Table IX displays the results\\nfor all datasets, highlighting the precision related to ILS-\\nCSL (light red cells) and full inference (light blue cells). It\\ndistinguishes between qualitative precision (correct paths) and\\nstructural precision (correct edges only).\\nIn the context of the analysis, the outcomes A, B, and C\\nfrom GPT-4 have specific meanings related to inferred causal\\nrelationships between two variables X1andX2:\\nOutcome A: GPT-4 infers that X1causes X2(X1→X2).\\nOutcome B: GPT-4 infers that X2causes X1(X2→X1).\\nOutcome C: GPT-4 infers that X1andX2are not causally\\nrelated ( X1↮X2).\\nIn the table, various columns represent the ratio of different\\ncorresponding structures in the ground truth:\\nDirect Edges: The edge ( X1→X2) exists in truth.\\nReversed Edges: An reversed edge ( X2→X1) exists in truth.\\nIndirect Paths: A path ( X1⇝X2) exists, but ( X1↛X2). Reversed Indirect Paths: (X2⇝X1), but ( X2↛X1).\\nNot Reachable: (X1̸⇝X2, X2̸⇝X1).\\nThe precision of LLM on variables that have edges (light\\nred cells of answers A and B) is notably high, significantly\\nexceeding the precision on variables that may not. Analyzing\\nprior errors in ILS-CSL reveals:\\n1) For GPT-4 outcome C, the corresponding edge forbidden\\nconstraints exhibit high precision, generating few erro-\\nneous structural constraints. This is attributed to the high\\nconfidence in the absence of causal relations inferred\\nbased on knowledge, leading to excellent precision on\\npairwise variables without structural edges, albeit with\\na lower recall.\\n2) For GPT-4 outcomes A or B, high precision is observed\\non learned edges belonging to the true skeleton, produc-\\ning few erroneous structural constraints. Given known\\ndirect causality between pairwise variables, LLM can\\neasily infer the correct causal direction, stemming from\\nthe counterintuitive nature of reversed causal statements.\\n3) Major LLM inference errors stem from outcomes A and\\nB on learned edges outside the true skeleton. However,\\nthe impact of these errors on generating incorrect struc-\\ntural constraints is mitigated by the low probability of\\nextra edges occurring in a learned structure ( z3≈0.07,\\nsee Table III) and the strategy of specifying a prior\\nconstraint only when inconsistent.\\nIn essence, the primary limitation of LLM in causal in-\\nference is the confusion between direct causal relationships,\\nindirect causality, and correlations, evidenced by the low\\noverall qualitative and structural precision. This limitation\\nhampers the performance of using LLM-derived existence on\\ncausality as ancestral (qualitative precision) or edge constraints\\n(structural precision) seperately.\\nContrarily, ILS-CSL effectively minimizes prior errors by\\nleveraging the inherent precision of LLM in inferring non-\\ncausal relations and determining causal direction on pairwise\\nvariables with direct causality. It smartly circumvents LLM’s\\nlimitation in discerning the existence of direct causal rela-\\ntionships, which are easily confused with indirect causality or\\ncorrelations, by restricting the LLM inference into the range\\nof learned structures from data, as analyzed in point 3.\\nG. Trend of DAG Quality over Iterations (RQ4)\\nThis section outlines the iterative trends of scaled SHD\\n(aiming for a decrease, denoted as SHD ↓) and True Positive\\nRate (aiming for an increase, denoted as TPR ↑) for various\\nbackbone algorithms across eight datasets. Each dataset spans\\ntwo distinct data sizes, resulting in 12 segments of observed\\ndata. It’s crucial to note the potential for significant derivation\\ndue to performance differences across varying data sizes,\\nparticularly for smaller-scale datasets like Cancer and Asia.\\nThe results of HC+BIC+ILS-CSL-hard on various datasets are\\nreported in Figure 3, with comprehensive results available in\\nthe external repository. Key observations from the iterative\\ntrends include:\\nAlarm\\n1 2 3 4 500.51Asia\\n1 2 3 400.51Barley\\n1 2 3 4 500.51\\nCancer\\n1 2 300.51PerformanceChild\\n1 2 3 4 500.51Insurance\\n1 2 3 4 500.51\\nMildew\\n1 2 3 4 500.51Water\\n1 2 3 4\\nIteration00.51\\nTPR↑\\nScaled SHD↓Fig. 3: Trend of TPR ↑(green line) and scaled SHD ↓(purple\\nline) of HC+BIC+ILS-CSL-hard on various datasets.\\n•Limited Iteration Numbers: Most cases require a lim-\\nited number of iterations. The area near the maximum\\niteration in each figure is small when exceeding 4,\\nindicating that few out of the 12 cases reach this point.\\nSome cases even have a derivation of zero at the maxi-\\nmum iteration, signifying that only one case attains this\\nmaximum value.\\n•Quality Improvement Trend: Generally, as the iteration\\nnumber increases, the scaled SHD decreases, and the\\nTPR increases. This trend underscores the enhancement\\nin the quality of the learned causal structures as ILS-CSL\\nprogresses.\\n•Significant Initial Improvement: The most substantial\\nimprovement in the quality of learned causal DAGs oc-\\ncurs in the first round of LLM supervision (from Iteration\\n1 to 2). Subsequent iterations offer diminished enhance-\\nments. This pattern is attributed to the initial presentation\\nof most inconsistent edges with LLM inference in the\\nfirst iteration. Post the integration of prior constraints,\\nthe new structures learned by CSL exhibit far fewer\\ninconsistencies with LLM inference.\\n•Potential Quality Degradation: In certain instances, the\\nquality of the causal DAG diminishes across specific\\niterations. This decline could stem from the introduction\\nof new erroneous prior constraints in a given iteration\\nor a statistical artifact. The latter scenario arises when\\ntwo consecutive iterations do not employ the same set of\\nobserved data, as some cases conclude in the preceding\\niteration.\\nThese observations provide a comprehensive insight into the\\niterative behavior of ILS-CSL, highlighting its effectiveness\\nand areas of caution to ensure consistent enhancement in\\nlearned causal structures.\\nH. Illustrative Example of DAG Evolution (RQ4)\\nWe visualize the learned causal structures in iterations to\\nunfold the details of ILS-CSL. An illustrative example by HC Iteration 0 Iteration 1\\nIteration 2\\nCorrect edge Wrong edge Correct constraint Wrong constraint\\nIteration 0 Iteration 1\\nIteration 2\\nCorrect edge Wrong edge Correct constraint Wrong constraintFig. 4: Visualized process of HC-BDeu+ILS-CSL-hard on a set of observed data of Child , 2000 samples. The SHD of iterations\\nare: 12 for Iteration 0, 3 for Iterations 1 and 2.\\n(BDeu) algorithm on Child dataset, 2000 samples, with hard\\nconstraining approach in ILS-CSL, is reported in Figure 4.\\nInitially, HC (BDue) learns a causal DAG from pure ob-\\nserved data (Iteration 0), whose edges are supervised by LLM,\\nleading to edge constraints (colored arrows) on inconsistent\\ninferred edge by LLM. The constraints could refine local\\nstructures (red arrows) or bring harm due to the erroneous\\ninference (blue arrows). The erroneous edges (dotted arrows)\\nare reduced as the iteration goes. Details of further observa-\\ntions are presented as follows:\\n•The SHD of the learned causal DAG is greatly reduced\\nfrom 12 to 3 by employing the ILS-CSL framework,\\nshowcasing the significant capability of our framework\\nto enhance the quality of learned causality.\\n•The first round of LLM-based supervision refines the\\nlearned DAG to a much greater extent than the following\\nrounds. This addresses the acceptable efficiency loss of\\nILS-CSL, which usually does not require many iterations.\\n•There are 7correct constraints (red arrow) and 2erro-\\nneous ones (blue arrow) in total. The number of directly\\ncorrected edges by these priors is 7−2 = 5 , while the\\nreduced SHD is 8, meaning that 3 edges that are distinct\\nfrom those in constraints are corrected without any prior\\nknowledge on them. It underscores the capability ofdiscovering structures unrelated to prior constraints by\\nintegrating them. This phenomenon could be interpreted\\nas the capability of aiding discovery of unknown causal\\nmechanisms by the known causal knowledge.\\nVIII. C ONCLUSIONS\\nThis paper presents ILS-CSL, a framework that enhances\\ncausal discovery from data using Large Language Models\\n(LLMs). ILS-CSL seamlessly incorporates LLM inference on\\nthe edges of the learned causal Directed Acyclic Graph (DAG),\\nconverting qualitative causal statements into precise edge-level\\nprior constraints while effectively mitigating constraint errors\\nstemming from imperfect prior knowledge. Comprehensive\\nexperiments across eight real-world datasets demonstrate the\\nsubstantial and consistent improvement ILS-CSL brings to the\\nquality of causal structure learning (CSL) outputs. Notably,\\nILS-CSL surpasses the existing separate way to guide CSL by\\napplying LLM inferred causality as ancestral constraints, with\\na marked performance increase as the number of variables\\ngrows. This advancement underscores the promising applica-\\ntion of the ILS-CSL framework in assistance of complex, real-\\nworld causal discovery tasks. REFERENCES\\n[1] J. Pearl, Causality . Cambridge university press, 2009.\\n[2] B. Ellis and W. H. Wong, “Learning causal bayesian network structures\\nfrom experimental data,” Journal of the American Statistical Association ,\\nvol. 103, no. 482, pp. 778–789, 2008.\\n[3] D. M. Chickering, “Learning bayesian networks is np-complete,” Learn-\\ning from data: Artificial intelligence and statistics V , pp. 121–130, 1996.\\n[4] N. K. Kitson, A. C. Constantinou, Z. Guo, Y . Liu, and K. Chobtham,\\n“A survey of bayesian network structure learning,” Artificial Intelligence\\nReview , pp. 1–94, 2023.\\n[5] S. L. Morgan and C. Winship, Counterfactuals and causal inference .\\nCambridge University Press, 2015.\\n[6] D. M. Chickering, “Optimal structure identification with greedy search,”\\nJournal of machine learning research , vol. 3, no. Nov, pp. 507–554,\\n2002.\\n[7] E. Y .-J. Chen, Y . Shen, A. Choi, and A. Darwiche, “Learning bayesian\\nnetworks with ancestral constraints,” Advances in Neural Information\\nProcessing Systems , vol. 29, 2016.\\n[8] H. Amirkhani, M. Rahmati, P. J. Lucas, and A. Hommersom, “Exploiting\\nexperts’ knowledge for structure learning of bayesian networks,” IEEE\\ntransactions on pattern analysis and machine intelligence , vol. 39,\\nno. 11, pp. 2154–2170, 2016.\\n[9] A. C. Constantinou, Z. Guo, and N. K. Kitson, “The impact of prior\\nknowledge on causal structure learning,” Knowledge and Information\\nSystems , pp. 1–50, 2023.\\n[10] E. Kıcıman, R. Ness, A. Sharma, and C. Tan, “Causal reasoning and\\nlarge language models: Opening a new frontier for causality,” arXiv\\npreprint arXiv:2305.00050 , 2023.\\n[11] H. Nori, N. King, S. M. McKinney, D. Carignan, and E. Horvitz,\\n“Capabilities of gpt-4 on medical challenge problems,” arXiv preprint\\narXiv:2303.13375 , 2023.\\n[12] L. Chen, T. Ban, X. Wang, D. Lyu, and H. Chen, “Mitigating prior errors\\nin causal structure learning: Towards llm driven prior knowledge,” arXiv\\npreprint arXiv:2306.07032 , 2023.\\n[13] R. Tu, C. Ma, and C. Zhang, “Causal-discovery performance of\\nchatgpt in the context of neuropathic pain diagnosis,” arXiv preprint\\narXiv:2301.13819 , 2023.\\n[14] S. Long, T. Schuster, A. Pich ´e, S. Research et al. , “Can large language\\nmodels build causal graphs?” arXiv preprint arXiv:2303.05279 , 2023.\\n[15] T. Ban, L. Chen, X. Wang, and H. Chen, “From query tools to\\ncausal architects: Harnessing large language models for advanced causal\\ndiscovery from data,” arXiv preprint arXiv:2306.16902 , 2023.\\n[16] A. Vashishtha, A. G. Reddy, A. Kumar, S. Bachu, V . N. Balasubrama-\\nnian, and A. Sharma, “Causal inference using llm-guided discovery,”\\narXiv preprint arXiv:2310.15117 , 2023.\\n[17] M. Willig, M. Ze ˇcevi´c, D. S. Dhami, and K. Kersting, “Can foundation\\nmodels talk causality?” arXiv preprint arXiv:2206.10591 , 2022.\\n[18] H. Liu, R. Ning, Z. Teng, J. Liu, Q. Zhou, and Y . Zhang, “Evaluating\\nthe logical reasoning ability of chatgpt and gpt-4,” arXiv preprint\\narXiv:2304.03439 , 2023.\\n[19] P. Hoyer, D. Janzing, J. M. Mooij, J. Peters, and B. Sch ¨olkopf,\\n“Nonlinear causal discovery with additive noise models,” Advances in\\nneural information processing systems , vol. 21, 2008.\\n[20] J. Frohberg and F. Binder, “Crass: A novel data set and benchmark to\\ntest counterfactual reasoning of large language models,” arXiv preprint\\narXiv:2112.11941 , 2021.\\n[21] D. Heckerman, “A bayesian approach to learning causal networks,” arXiv\\npreprint arXiv:1302.4958 , 2013.\\n[22] P. Spirtes and C. Glymour, “An algorithm for fast recovery of sparse\\ncausal graphs,” Social science computer review , vol. 9, no. 1, pp. 62–\\n72, 1991.\\n[23] E. V . Strobl, S. Visweswaran, and P. L. Spirtes, “Fast causal inference\\nwith non-random missingness by test-wise deletion,” International jour-\\nnal of data science and analytics , vol. 6, pp. 47–62, 2018.\\n[24] D. Heckerman and D. Geiger, “Learning bayesian networks: a unifi-\\ncation for discrete and gaussian domains,” in Proceedings of the 11th\\nConference on Uncertainty in Artificial Intelligence , 1995, pp. 274–284.\\n[25] A. A. Neath and J. E. Cavanaugh, “The bayesian information crite-\\nrion: background, derivation, and applications,” Wiley Interdisciplinary\\nReviews: Computational Statistics , vol. 4, no. 2, pp. 199–203, 2012.\\n[26] C. Yuan, B. Malone, and X. Wu, “Learning optimal bayesian networks\\nusing a* search,” in Twenty-second international joint conference on\\nartificial intelligence , 2011.[27] F. Tr ¨osser, S. de Givry, and G. Katsirelos, “Improved acyclicity rea-\\nsoning for bayesian network structure learning with constraint program-\\nming,” arXiv preprint arXiv:2106.12269 , 2021.\\n[28] A. Li and P. Beek, “Bayesian network structure learning with side\\nconstraints,” in International Conference on Probabilistic Graphical\\nModels . PMLR, 2018, pp. 225–236.\\n[29] L. M. de Campos and J. G. Castellano, “Bayesian network learning\\nalgorithms using structural restrictions,” International Journal of Ap-\\nproximate Reasoning , vol. 45, no. 2, pp. 233–254, 2007.\\n[30] G. F. Cooper and E. Herskovits, “A bayesian method for the induction\\nof probabilistic networks from data,” Machine learning , vol. 9, pp. 309–\\n347, 1992.\\n[31] R. T. O’Donnell, A. E. Nicholson, B. Han, K. B. Korb, M. J. Alam,\\nand L. R. Hope, “Causal discovery with prior information,” in AI 2006:\\nAdvances in Artificial Intelligence: 19th Australian Joint Conference on\\nArtificial Intelligence, Hobart, Australia, December 4-8, 2006. Proceed-\\nings 19 . Springer, 2006, pp. 1162–1167.\\n[32] J. A. G ´amez, J. L. Mateo, and J. M. Puerta, “Learning bayesian networks\\nby hill climbing: efficient methods based on progressive restriction of\\nthe neighborhood,” Data Mining and Knowledge Discovery , vol. 22, pp.\\n106–148, 2011.\\n[33] C. Lee and P. van Beek, “Metaheuristics for score-and-search bayesian\\nnetwork structure learning,” in Advances in Artificial Intelligence: 30th\\nCanadian Conference on Artificial Intelligence, Canadian AI 2017,\\nEdmonton, AB, Canada, May 16-19, 2017, Proceedings 30 . Springer,\\n2017, pp. 129–141.\\n[34] M. Scutari, C. E. Graafland, and J. M. Guti ´errez, “Who learns better\\nbayesian network structures: Accuracy and speed of structure learning\\nalgorithms,” International Journal of Approximate Reasoning , vol. 115,\\npp. 235–253, 2019. '},\n",
       " {'url': 'http://arxiv.org/pdf/2311.14030v1',\n",
       "  'title': 'PrivateLoRA For Efficient Privacy Preserving LLM',\n",
       "  'text': 'PRIVATE LORA F OREFFICIENT PRIVACY PRESERVING LLM∗\\nYiming Wang, Yu Lin, Xiaodong Zeng, Guannan Zhang\\nAnt Group\\nShanghai, China\\nABSTRACT\\nEnd users face a choice between privacy and efficiency in current Large Language Model (LLM)\\nservice paradigms. In cloud-based paradigms, users are forced to compromise data locality for\\ngeneration quality and processing speed. Conversely, edge device paradigms maintain data locality\\nbut fail to deliver satisfactory performance. In this work, we propose a novel LLM service paradigm\\nthat distributes privacy-sensitive computation on edge devices and shared computation in the cloud.\\nOnly activations are transmitted between the central cloud and edge devices to ensure data locality.\\nOur core innovation, PrivateLoRA, addresses the challenging communication overhead by exploiting\\nthe low rank of residual activations, achieving over 95% communication reduction. Consequently,\\nPrivateLoRA effectively maintains data locality and is extremely resource efficient. Under standard\\n5G networks, PrivateLoRA achieves throughput over 300% of device-only solutions for 7B models\\nand over 80% of an A100 GPU for 33B models. PrivateLoRA also provides tuning performance\\ncomparable to LoRA for advanced personalization. Our approach democratizes access to state-of-the-\\nart generative AI for edge devices, paving the way for more tailored LLM experiences for the general\\npublic. To our knowledge, our proposed framework is the first efficient and privacy-preserving LLM\\nsolution in the literature2.\\n1 Introduction\\nIn the rapidly evolving field of artificial intelligence, Large Language Models (LLMs) have emerged as a powerhouse[ 1,\\n2,3,4]. Previously unsolvable long-tail problems are effectively tackled by LLMs, such as programming[ 5,6],\\ninstruction following[ 3,7] and real world interaction[ 8,9,10,11]. To fully harness the potential of large language\\nmodels, it is crucial to concentrate on the privacy-preserving aspect of LLMs. A critical dimension of this focus is data\\nlocality, which implies that user data are not stored on remote cloud servers but are instead kept closer to the user’s own\\nenvironment. This approach not only reduces the risks associated with data breaches and unauthorized access but also\\naligns with growing global concerns over data sovereignty and user privacy.\\nHowever, like all other privacy preserving efforts, practicality of data locality is severely challenged by efficiency in\\nthe context of LLMs. For current cloud-only and device-only LLM service solutions, end users are forced to choose\\nbetween data locality and model scale. Cloud-only centralized solution offers quick and quality generations from large\\nscale models with its sufficient computing power. But the data locality of personal data and personalized parameters\\nare compromised. Alternative decentralized solutions[ 12] like deploying smaller quantized[ 13] LLMs ( ≤7B) on edge\\ndevices offer superior privacy but at the cost of performance. Firstly, smaller models often fall short in delivering the\\nemergent abilities exhibited by their larger counterparts[ 14,4]. Secondly, on edge devices like smartphones, pure device\\nsolutions offer markedly low throughput. For instance, a 3-bit quantized 7B model on a flagship smartphone achieves\\nonly 58.1 and 8.1 tokens per second for prefill and decode, respectively. The prefill throughout is only 0.5% of single\\nA100 80G GPU, let alone comparing to clusters of high-end GPUs on Cloud. Other efforts such as Federated Learning\\nand Split Learning also face the challenge in efficiency, and do not solve the problem of inference.\\nAfter comparing Cloud-based and Device-based solutions, we confront a fundamental question: Can their benefits be\\nsynergistically combined to simultaneously achieve data locality and model scale? Our solution is a heterogeneous\\n∗Preprint. Work in progress.\\n2Demo and code is coming to github soon.arXiv:2311.14030v1  [cs.AI]  23 Nov 2023 PRIME AI paper\\ndistributed system[ 15] that leverages edge devices’ storage for private data and personalized parameters, while utilizes\\nthe Cloud for computational enhancement. Model parameters are split across the cloud and edge devices and only\\nunreadable activations and gradients are transmitted to meet the requirement of data locality. The largest challenge\\nto build such heterogeneous systems is to use much smaller connection bandwidth to transmit equal amount of\\nactivations in homogeneous setups[ 16,17]. The central cloud and edge devices are presumably connected via Internet,\\nthe bandwidth of which is significantly smaller than dedicated connection found in homogeneous setup ( 1Gbps vs\\n100Gbps ). The low connection bandwidth being the largest challenge, other challenges involve balanced workload\\ndistribution to tolerate significant internal performance gap. For example, A100 GPU and flagship chipsets from\\nsmartphones can have over 20 times FLOPS difference and 40 times memory bandwidth difference. The hardware\\nperformance gap can result in reduced throughput caused by blocking in pipeline.\\nTo tackle the above challenges, we propose PrivateLoRA, a novel Parameter Efficient Fine-Tuning (PEFT)[ 18,19,20]\\nmethod that exploits low rank of residual activations for communication and workload distribution. Given the fact\\nthat low rank transforms on residual activation are enough to adapt transformer models, we make the hypothesis\\nthat decomposing one integral low rank transform into three sequential transforms yields comparable adaptation\\nperformances. Therefore, PrivateLoRA integrates three sequential low-rank matrices ( A, M, B ) for weight adaptation\\n(in Figure 1). Non-trainable AandB, deployed on the Cloud, serve as an encoder-decoder duo that condenses residual\\nactivations to reduce communication overhead. We term this practice to reduce communication Low Rank Residual\\nTransmission. Trainable Mon edge devices transforms residual activations to steer the transformer for personalized\\noutputs. With less than 0.1% of total parameters on edge devices, FLOPS and memory requirement are also largely\\nreduced, yielding closer workload to processing power ratio between the cloud and edge devices.\\nOn the basis of PrivateLoRA, we propose a data locality preserving paradigm for LLM on a heterogeneous dis-\\ntributed system built on central cloud and edge devices. Raw data and personalized parameters Mare kept on edge\\ndevices throughout training and inference. Only unreadable activations and gradients are transmitted. Thanks to Low\\nRank Residual Transmission, the communication overhead of activations are reduced by over 95% percent, yielding\\ncomparable throughput to cloud-only solutions.\\nAs a result, while totally respecting data locality, PrivateLoRA effectively harnesses large-scale LLMs on the Cloud\\nwith limited resources on edge devices and provides good adaptation performance for personalization. PrivateLoRA has\\nbeen tested across various scales and benchmarks, including GSM8K, MMLU, BoolQ, and HellaSwag. Despite\\nthe untrainable A, B and the triplet structure, PrivateLoRA maintains tuning performance on par with the original\\nLoRA. Our throughput estimations reveal that PrivateLoRA surpasses device-only solutions in both inference and\\ntraining. Utilizing average consumer-accessible network bandwidth and smartphones, PrivateLoRA achieves 175.5 and\\n26.5 tokens per second on 7B model for generation prefill and decoding, respectively, which are both over 300% of\\ndevice-only solutions. Additionally, as model scales increase, PrivateLoRA’s advantages become more pronounced,\\nachieving over 74% of the throughput of an A100 80G GPU with a 33B model backend. Our work is also orthogonal to\\nprevious efforts on efficient transformer inference, thus can be employed together to further boost efficiency.\\nIn summary, the contributions of our paper can be summarized as,\\n•We propose PrivateLoRA, a novel PEFT method that achieves communication reduction and balanced workload\\ndistribution in distributed scenario.\\n•On the basis of PrivateLoRA, we propose a new LLM service paradigm that heterogeneously distributes LLMs\\nto protect data locality.\\n•Extensive empirical experiments are carried out to present a comprehensive study on the tuning performance,\\nintegrity and scalability of PrivateLoRA.\\n•Numerical estimations show that with PrivateLoRA, edge devices can achieve a throughput over 3 times of\\ndevice-only solutions and comparable to running on GPU.\\n2 Related Work\\n2.1 PEFT\\nPEFT methods lowers hardware requirement of model fine-tuning by significantly reducing trainable parameters\\nand consequently optimizer states cached in VRAM. By exploiting the local optimum of a pretrained model, a\\nmuch smaller solution space brought by reduce trainable parameters helps PEFT methods achieve comparable tuning\\nperformance[ 19,20]. PEFT can be classified into two categories: 1) reparameterization-based methods[ 21,22] that\\nretrain a portion of the parameters and 2) addition-based methods that train additional parameters[ 23,24,25]. Recent\\n2 PRIME AI paper\\nworks in PEFT focus on resource efficiency[ 25,26,23,24]. LoRA[ 23] fits incremental weights by decomposing them\\ninto low-rank matrices. (IA)3tunes hidden states with learned multipliers.\\n2.2 Distributed Learning For Data Privacy\\nFederated Learning (FL) and Split Learning[ 15] are proposed to tackle the problem of data privacy in a distributed\\nmanner. FL allows multiple nodes to locally train a complete neural networks without explicit exchange of local data via\\nspecialized optimization algorithms and transmission protocols. FL has been widely applied in various domain such as\\ncomputer vision, text typing (Google’s G-board) and intrusion detection[ 27,28]. Split Learning splits model vertically\\namong different nodes so that only activations and gradients are transmitted to protect data locality. In the context\\nof LLMs, efficiency of these methods are highly questionable as both compute and communication are astrological\\ncompared to conventional deep neural networks.\\n2.3 Running LLMs on Edge Devices\\nEfforts have also been made to run LLMs on edge devices. llama.cpp3ports LLaMA model in C/C++ so that models\\ncan be efficiently executed on limited hardwares such as laptops and Raspberry Pi. MLC Chat[ 12] showcases a model\\ncompilation solution for deploying language models on diverse hardware backends and applications. With MLC Chat,\\nquantized 7B model can be run on smart phones. As for training LLMs on edge device, PockEngine[ 29] introduces\\nsparse back-propagation, pruning the backward graph, and updating the model sparsely to save memory and reduce\\nlatency while maintaining model quality. PockEngine has demonstrated the capability to fine-tune LLaMav2-7B on\\nNVIDIA Jetson AGX Orin with significant speed and memory efficiency compared to standard TensorFlow.\\n3 Method\\n3.1 Problem Formulations\\n3.1.1 Notations\\nTwo heterogeneous runtimes with huge performance gap are in the scope, namely the cloud ( C) and the edge device\\n(D). Cloud has powerful hardware and a large-scale shared autoregressive transformer model PΦ(y|x)that produces\\nnon-personalized outputs. Edge device has limited hardware and stores private data Z={(xi, yi)}, where both xiand\\nyiare sequences of tokens. For example, Zcould be rounds of chat and xiandyidenotes messages of senders and\\nreceivers. Hardwares are parameterized with FLOPS Fand memory bandwidth MB . Two runtimes are connected with\\nInternet with asymmetric network bandwidths denoted as BD2CandBC2D. And we note such hybrid runtime D&C.\\nWe obviously have FC≫FDandMB C≫MB D. For a paired connection, BD2CandBC2Dare usually bound by\\nedge device’s Internet Service Provider, which result in BC2D> BD2CbutBC2DandBD2Care in the same order of\\nmagnitude.\\nModel PΦ(y|x), parameterized by Φ, is composed of an embedding layer, Ntransformer layers and an LM Head for\\ntoken prediction. The dimension of hidden states his noted as d. For simplicity of discussion, we assume Mis\\ndecoder-only transformer but obviously PrivateLoRA also works with encoder architecture.\\n3.1.2 Objectives of PrivateLoRA\\nLow Rank Residual Transmission aims to solve the impossible triangle of performance, parameter-based personalization\\nand data locality becomes for centralized or decentralized paradigm. We then give formal definition on the three\\nobjectives.\\nPerformance indicates both task solving capability and processing speed. Since cloud-based solution fails in data\\nlocality, the performance to beat is benchmarked from device-only solutions. PrivateLoRA has to outperform device-\\nonly solutions from both aspects. PrivateLoRA is capable of leveraging large scale model on Cloud, therefore guarantees\\nto outperform smaller models on edge devices in task solving capabilities.\\nTask solving capability is measured with conventional benchmark scores. Processing speed is measured by number of\\nprocessed tokens per second, or tokens per second (TPS) for short. Transformer throughput largely depends on the\\nsequence length and batch size, therefore throughput of generation prefill, generation decoding and batched training\\nwill all be considered.\\n3https://github.com/ggerganov/llama.cpp\\n3 PRIME AI paper\\nParameter-based Personalization achieves personalization by tuning knowledge or preferences into the models. For\\nexample, in order to make model mimic receiver’s tone, we can tune additional parameters ∆Φwith chat records Z\\nwhile freezing Φ. The objective can be written as,\\narg max\\n∆ΦX\\n(x,y)∈Z|y|X\\nt=1log(PΦ+∆Φ (yt|x, y<t)) (1)\\nCompared to In-Context Learning (ICL) based personalization, the major advantage of parameter-based personalization\\nis the capability to learn unseen concepts. ICL-based personalization also increases sequence length and consequently\\ncomputation and communication overhead, which may be critical to throughput on edge devices.\\nData Locality requires that persistent storage of both raw data Zand its derivative is only allowed on edge devices. Raw\\ndataZis used as input for generation and input-label pair for autoregressive training. Derivatives of data, especially\\npersonalized parameters, are necessary in training and inference. Between the two, persistent storage of personalized\\nparameters is more critical for centralized cloud-based solutions. From the perspective of Cloud, storage of raw data is\\nno longer required as long as the personalized parameters are obtained. For PrivateLoRA, this restriction also poses\\nchallenges on processing speed as it forces communication between edge devices and the cloud.\\n3.2 Low Rank Residual Transmission\\nGiven the fact that low rank transform is enough for effective LLM adaptation, PrivateLoRA exploits such low rank for\\ncommunication and workload distribution. Drawing inspiration from LoRA, PrivateLoRA adapts model weights by\\nadding three sequential low rank matrices parallel to target weight (see Figure 1). Given weight matrix W∈Rd×kin\\ntarget linear module and activations xof dimension d, adaptation of PrivateLoRA can be written as,\\nxW+x∆W=xW+xAMB (2)\\nwhere A∈Rd×rC2D,M∈RrC2D×rD2CandB∈RrD2C×d.rC2D, rD2C≪d, dimensions of M, are separately\\nnoted to account for the asymmetric bandwidth BC2D, BD2C.\\n+CloudEdge\\nDevice\\nFigure 1: Weight adapted with PrivateLoRA.\\nAdaption matrices AandBare kept frozen af-\\nter random initialization, thus contain no user\\ninformation and can be stored and deployed on\\nCloud. Weight Mis trained with user data thus\\ndeployed on Phone. Thanks to PrivateLoRA, the\\ncommunication base multiplier is significantly\\nsmaller than model’s hidden size and irrelevant\\nto model architecture and scales.Like all PEFT methods, the decoder stack of the model is frozen\\nand only additional parameters are trainable. In the adapted model\\nforward, residual activations are firstly down-projected by Ato low\\nrank for fast download, then transformed by Mwhile maintaining its\\nlow dimension for upload transmission, and finally up-projected by\\nBto merge residual activations into the base activation. By dividing\\none integral low rank transform ∆Winto three sequential transforms,\\nwe could explicitly exploit the low rank for communication. Matrices\\nA, B serve as encoder-decoder pair to condense residual activations\\nfor transmission.\\n3.2.1 Communication Overhead\\nPrivateLoRA cuts down communication base from dtorD2CorrC2D\\nand make the transmission base number irrelevant to model scale.\\nAssuming the original LoRA is similarly deployed, transmission over-\\nhead of decoder stack would be proportional to the dimension of\\nhidden states d.dis too big as transmission base and also scales\\nup with model scale. For example, dequals 4096, 5120 and 6656\\nfor LLaMA 2-7B, LLaMA 2-13B and LLaMA-30B, respectively.\\nTransmitting hidden states of one token of 16-bit precision on all de-\\ncoder layers of LLaMA 2-7B produces over 2Mb overhead, let alone\\nmultiple back-and-forth transmissions and much longer sequences in\\nactual training and inference. Whereas for PrivateLoRA, assuming\\nrC2D=rD2C= 128 , the base multiplier is cut down from 4096\\nto 128, a reduction of 96.88%. Another thing worth mentioning is\\nthatrC2D, rD2Cis invariant to model architecture, thus don’t scale\\nup with model scale. The implication is that when leveraging larger\\nmodels as personalization backend, the impact of communication overhead is more negligible.\\n4 PRIME AI paper\\nShared Model\\nPowerful Hardware\\nCloud\\nQuery\\n❌Non-Personalized  \\nGeneration\"What would\\nmy\\xa0Father\\xa0say ?\"\\nQuery\"What would\\nmy\\xa0Father\\xa0say ?\"\\n🔥Private  \\nData Leakage\"Without knowing about\\nyour father , it is difﬁcult\\nto determine what your\\nfather would say \"\"Without knowing about\\nyour father , it is difﬁcult\\nto determine what your\\nfather would say \"\\nEdge Device\\n(a) Centralized cloud-based paradigm\\nLimited Hardware\\nPrivate Personal  \\nParameter\\nLow Rank Residual T ransmissionShared Model &  \\nPublic Personal Params\\n Powerful Hardware\\nCloud\\n Edge Device\\nQuery\\n✅Personalized Generation\"What would\\nmy\\xa0Father\\xa0say ?\"\\n\"Listen to\\nyour Mom!\"\\n✅No Personal Data (b) PrivateLoRA-powered paradigm\\nFigure 2: Inference pipeline comparison between (a) centralized Cloud-based paradigm and (b) PrivateLoRA-powered\\nparadigm. PrivateLoRA has the advantages of data locality and personalized generation compared to Cloud-based\\nparadigm. PrivateLoRA leverages powerful Cloud hardware to achieve parameter-based personalization with limited\\nlocal resource on edge devices. Parameters containing personal data are stored on edge devices and remain on device\\nduring training and inference.\\n3.2.2 Workload Distribution\\nIn PrivateLoRA, workload distribution is largely balanced so that a closer ratio of compute to processing power is\\nachieved for Cloud and Device. On Cloud, apart from the original decoder stack, A, B of PrivateLoRA adds very\\nmarginally additional compute. For LLaMA 2-7B, assuming rC2D=rD2C= 64 , additional parameters take up\\n1.4% of original model. Also, A, B can be computed on parallel with W, thus no additional latency is introduced.\\nOn edge devices, during forward computation of the decoder stack, the computation workload only involves matrix\\nmultiplication of small dimensions. For LLaMA 2-7B, the parameter count of Mon edge devices take up less than\\n0.1% of the original model, thus significantly reducing compute and memory pressure. Moreover, the pure linearity of\\ncomputation can yield higher utilization of the hardware. Softmax operation in self attention is infamously slow due to\\nlow hardware utilization[ 30,31] and reduces overall throughput in device-only solutions. In our proposed PrivateLoRA,\\nonly linear projections are performed on edge devices which are highly optimized from the perspective of both software\\nand hardware.\\n3.2.3 Data Locality\\nDuring training, AandBare non-trainable and only Mis optimized with user private data. Mis kept on Device during\\nboth training and inference. Since AandBare randomly initialized and kept random, they contain no user data can\\nalso be stored on Cloud. Therefore, PrivateLoRA meets the requirement that no private data or its derivative are kept on\\nCloud. Moreover, A, B andBform a tight pairing similar to public-private key used in encryption. Possessing either\\ncomponent do not result in correct model output.\\n3.2.4 Target Modules of PrivateLoRA\\nSeveral researches have pointed out that a thorough adaptation to every linear projection in transformers, including\\nself attention and MLP, yields better overall performances[ 32] We only target query, key and value projections in\\nself-attention for adaptation to achieve minimized communication. Target modules don’t affect time complexity in\\nsingle forward scenarios such as generation prefill and training, but can affect generation decoding. During iterative\\nsampling in generation decoding, query projection is only calculated with the newly generated token. Key and Value\\nprojections can reutilize KV cache to make marginal cost constant. Whereas computation complexity of other linear\\nprojections are proportional to sequence lengths including both prompt and newly generated tokens. Adapting these\\nmodules make decoding throughout decay due to increasing communication overhead. Therefore, only adapting query,\\nkey and value modules produces consistent communication overhead in all scenarios.\\n3.3 Paradigm Shift Powered by PrivateLoRA\\nBased on PrivateLoRA, we propose new paradigm that solves the impossible triangle for efficient LLM personalization.\\nParameter-based Personalization Private personal parameters Mintroduced by PrivateLoRA is optimized with\\n5 PRIME AI paper\\npersonal data Z. Despite random and static A, B , residual activations can be effectively transformed by Mto output\\npersonalized generations.\\nModel Performance PrivateLoRA leverages models in cloud-based solutions as backend to guarantee better task solving\\ncapability compared to device-only solutions. Compared to cloud-based solutions, PrivateLoRA excels in providing\\ntailored generations to better suit user demands. In terms of throughput, thanks to PrivateLoRA, communication\\noverhead is significantly reduced. Leveraging large scale LLM with PrivateLoRA can yield higher overall throughput\\ncompared to small models with device-only solutions.\\nData Locality Private personal parameters M, derivatives of raw data Z, are optimized with personal data but kept\\non Device in both training and inference. Public personal parameters AandBare randomly initialized and kept\\nfrozen, thus not treated as derivatives of raw data. For the locality of raw data, we deploy word embedding and LM\\nHead on Device and only the decoder stack on Cloud (see Figure 3) so that only human-unreadable activations are\\ntransmitted (more discussion in Section 6.2). Therefore, data locality of both raw data and its derivative are protected\\nwith PrivateLoRA.\\n3.3.1 Inference and Training Cycle Powered by PrivateLoRA\\nCloud\\nShared Decoder StackWord Emb\\'\\nLM Head\\'q,k,v  \\nprojectionsWhat would my\\nfather say?\\n9e-73e-6...1e-7\\n8e-64e-5...3e-7Edge Device\\nListen to your mother .  Listen9e-73e-6...1e-7\\n8e-64e-5...3e-7\\nFigure 3: Decode cycle of decoder-only LLM genera-\\ntion with PrivateLoRA. Raw texts are kept on device\\nand only unreadable activations are transmitted. Ran-\\ndom and static matrices A, B on Cloud serve as acti-\\nvation encoder-decoder to cut down communication\\noverhead. Mon Device steers the residual activations\\nto produce personalized generation.To fully understand the paradigm powered by PrivateLoRA,\\nwe elaborate on the training and inference cycle of a heteroge-\\nneously distributed.\\nInference LLMs predict new tokens iteratively. Figure 3 plots\\nthe iterative sampling with PrivateLoRA. Starting with a query\\ninput by end user, the query is firstly tokenized and go through\\nword embedding to get initial token embeddings. These token\\nembeddings are usable to central decoder stack but cannot be\\nmapped back to token ids with embeddings on Cloud. Token\\nembeddings then go through series of decoder layers. In query,\\nkey and value projections in each self attention module, Pri-\\nvateLoRA is applied to get residual activations from Device\\nthat ultimately produce personalized generation. At the end\\nof the decoder stack, activations of the last token are trans-\\nmitted to Device and decoded by LM Head on Device. The\\nnewly obtained token are then tokenized and fed through word\\nembedding for another round of sampling.\\nTraining In training cycles, Mis tuned with private user data\\nfor parameter-based personalization. A, B and the decoder\\nstack is frozen. Training cycle starts with forward computation\\nalmost identical to inference except that activations of the entire\\nof the last decoder layer are transmitted to Device. On Device,\\nloss is calculated after feeding received activations to local LM\\nHead. After that, the loss is back propagated through the entire\\ndecoder stack. Therefore, throughout training, personal data is\\nalways kept on Device and invisible to Cloud.\\n3.3.2 Memory and FLOPs Analysis\\nPrivateLoRA is extremely friendly to edge devices with lim-\\nited computation resources. We numerically estimates required\\nresource and compute to compare deploying complete trans-\\nformer model and PrivateLoRA counterpart. For memory re-\\nquirement, we calculate the memory necessary on edge devices\\nto load model parameters into RAM. Total FLOPs on edge\\ndevices to complete one forward computation on one token are estimated. The FLOPs are roughly estimated as 2\\ntimes of parameter count. Various datatype precisions are considered. And the results are listed in Table 1. With\\nPrivateLoRA, leveraging 7B models of 16-bit datatype only needs 10.6% of memory and 2.0% of FLOPs of 3-bit\\ndevice-only solutions. Furthermore, PrivateLoRA’s efficiency allows it to leverage models up to 30B in size with less\\nmemory and total FLOPs than 1B quantized models, underscoring its high resource efficiency.\\n6 PRIME AI paper\\n3.3.3 Throughput Analysis\\nIn this section, we give detailed analysis on throughput in various scenarios. We will decompose the elapsed time of\\nforward computation into several well-known quantity in an end-to-end manner, so that the overall throughput of our\\nproposed heterogeneous distributed system can be numerically estimated in Section 4.1.2.\\nWe firstly start with forward computation and then extend our results to training. For our proposed architecture, time of\\none forward computation Tcan be decomposed into time of decoder stack on Cloud TDecoder\\nC , time of LM Head on\\nDevice TLMHead\\nDevice and overhead introduced by PrivateLoRA TPrivateLoRA.\\nT=TDecoder\\nC +TPrivateLoRA+TLMHead\\nD . (3)\\nOverhead of PrivateLoRA TPrivateLoRAcan be decomposed into network transmission tand local execution time on\\nCloud TLRRT\\nC and Device TLRRT\\nD .\\nTPrivateLoRA=TPrivateLoRA\\nNetwork +TLRRT\\nD +TLRRT\\nC . (4)\\nSince network communication is more critical in our heterogeneous distributed system, we then focus on dissecting net-\\nwork overhead TPrivateLoRA\\nNetwork . Assuming the number of adapted layer is noted as N′≤N, the network communication\\ncan be decomposed into the initial and final embedding and decoder stack communication.\\nTPrivateLoRA\\nNetwork =TTokenEmb\\nNetwork +TPrivateLoRA Activation\\nNetwork ×N′, (5)\\nwhere TTokenEmb\\nNetwork are transmission latency of input and output of decoder stack on Cloud, TPrivateLoRA Activation\\nNetwork\\ndenotes transmission latency of PrivateLoRA’s activation.\\nStarting from Equation 5, we can then derive a unitary communication time tcomplete one forward computation on\\none token:\\nt=Size DType ×(N′×(rD2C×ND2C\\nBD2C+rC2D×NC2D\\nBC2D)) +TTokenEmb\\nNetwork , (6)\\nwhere ND2C(NC2D)are number of D2C(C2D) transmissions per decoder layer and Size DType are number of bits of\\nthe used datatype. ND2C(NC2D)are determined by variable dependency during forward computation. For example,\\nquery, key and value projections share common inputs, thus NC2Dis 1. They produce three activations of the same\\ndimension, thus ND2Cis 3.\\n# Param # Bit Memory(MB) FLOPs (G)\\n1B3 491.9 2.6\\n4 655.8 2.6\\n16 2623.3 2.6\\n3B3 999.0 5.3\\n4 1333.2 5.3\\n7B3 2477.7 13.2\\n4 3303.5 13.2\\n16 (PL) 265.3 0.27\\n13B3 4819.4 25.7\\n4 6425.8 25.7\\n16 (PL) 331.6 0.33\\n30B3 12118.2 64.6\\n4 16157.7 64.6\\n16 (PL) 431.9 0.43\\nTable 1: Memory and FLOPs comparison between\\nPrivateLoRA and full model deployment. Memory\\nrequirement are calculated to load all parameters\\nand total FLOPs to compute a single token. Pri-\\nvateLoRA only needs % of. FLOPs are calculated\\nassuming batch size is 1 and sequence length is 1.Therefore, the elapsed time in Equation 3 for single forward\\ncomputation writes as,\\nT=TDecoder\\nC +TLRRT\\nD +TLRRT\\nC +TLMHead\\nD +bs×l×t,(7)\\nwhere bsis the batch size and lis the sequence length of mini-\\nbatch.\\nInference Throughput With Equation 7, we can compute\\nthe throughput measured in tokens per second. Firstly, we\\nnote combined throughput for local executing A, B andMas\\nTPSLRRT\\n0 .\\nTPSLRRT\\n0 =bs×l\\nTLRRT\\nD +TLRRT\\nC. (8)\\nBased on Equation 7, the TPS of PrivateLoRA can be written\\nas\\nTPS =1\\n1\\nTPSDecoder\\nC+1\\nTPSLRRT\\n0+1\\nTPSLMHead\\nD+t,(9)\\nwhere TPSDecoder\\nC , TPSLMHead\\nD indicates the throughput of\\ndecoder stack on cloud and LM Head on Device, respectively.\\nEquation 9 holds for both generation prefill and generation de-\\ncoding, so we can effectively estimate the inference throughput\\nof generation prefill, single query decoding and batched query\\ndecoding.\\n7 PRIME AI paper\\nTraining Throughput Training is much more complicated and many aspects can vary the time cost, such as the\\noptimizer, implementation of computation graph. Following analysis in inference, we decompose the time to complete\\none training step in an end-end manner. During back-propagation, gradients of the same dimension as activations are\\ntransmitted between Cloud and Device in reversed direction.\\nT=TDecoder\\nC +TLRRT\\nD +TLRRT\\nC +TLMHead\\nD +bs×l×(t+t′), (10)\\nwhere t′represents transmission time of gradients which is different from tdue to asymmetric network bandwidth and\\nother terms include both forward and backward computations. The TPS of PrivateLoRA during training is then,\\nTPS =1\\n1\\nTPSDecoder\\nC+1\\nTPSLRRT\\n0+1\\nTPSLMHead\\nD+t+t′, (11)\\nTherefore, with Equation 9 and 11, we can derive the performance boundary between PrivateLoRA and pure device\\nsolutions in both generation and training scenarios.\\n4 Experiments\\nThis part essentially answers the following two questions (1) Whether PrivateLoRA provides good tuning performances\\nwith randomly initialized and non-trainable AandB(2) how fast is PrivateLoRA compared to pure device solution or\\neven running on GPUs? Other properties of PrivateLoRA, such as scalability and integrity, are also studied to present a\\nmore comprehensive understanding.\\n4.1 Experiment Setup\\n4.1.1 Benchmarks\\nWe use tuning performance on various benchmarks to prove that PrivateLoRA offers good data fitting capability for\\neffective personalization. Our benchmarks involve most commonly used benchmarks including natural language\\nunderstanding, common sense reasoning and logic arithmetic. For common sense reasoning, we use BoolQ[ 33]. We\\nevaluate involved methods with LM-Eval Harness[ 34]. We report zero-shot accuracy except that 5-shot evaluation\\nis adopted for MMLU. Since we also value generation speed, we introduce a custom metric MSthat awards high\\ngeneration speed and performance improvement via tuning.\\nMS=TPS\\nTPS C(M−MNT), (12)\\nwhere TPS represents generation speed of tested method, TPS Cdenotes generation speed on GPU, Mdenotes\\naverage benchmark score of tested method and MNTdenotes task performance of original model without tuning.\\nLarger MSrepresents better overall performance.\\n4.1.2 Generation Speed Benchmark and Estimation\\nSpecifications\\nDeviceMB D= 42.7GBps\\nFLOPS D= 15.8T\\nCloudMB C= 1935 GBps\\nFLOPS C= 312 T\\nNetworkBd2c= 60Mbps\\nBc2d= 100 Mbps\\nTable 2: Assumptions for numerical estimation\\nof inference latency of PrivateLoRA. Edge De-\\nvice specifications are equivalent to flagship\\nchipsets of smart phones. Cloud specifications\\nare at the level of single A100 80G GPU. Net-\\nwork bandwidths are at the level of 5G used by\\naverage consumers.We use decoding speed of single query generation, measured in to-\\nkens per second (TPS), as the throughput metric. Throughput are\\nmeasured on GPU and smart phones representing Cloud and Device\\nruntimes, respectively. Inference speed of PrivateLoRA is numer-\\nically estimated according to Equation 9 and modest assumptions\\non the hardware and network listed in Table 2. Edge Device speci-\\nfications are equivalent to flagship chipsets of smart phones. GPU\\nspecifications are at the level of A100 80G GPU. Network band-\\nwidths are at the level of 5G accessible to average consumers[ 35,36].\\nDetailed calculation is discussed in Section 5.\\n4.1.3 Baselines\\nVarious device-only solutions are available. To demonstrate the ex-\\ntreme efficiency of PrivateLoRA, we choose solutions that allow\\nLLMs to run on smart phones as baselines, including 1) small models,\\n2) quantized models and 3) heterogeneously distributed LoRA on\\nD&C.\\n8 PRIME AI paper\\nSmall Models We refer to models with parameters fewer than 3B\\nas small models. Models under this scale can easily fit into smart\\nphone’s memory and can run even without parameter quantization. For this category, we include OPT-1B3[ 37],\\nFalcon-rw-1.3B[38] and StableLM-3B-4e1t[39].\\nQuantized Models Quantization technique is commonly used to run LLMs on low computation power devices.\\nQuantization reduces parameter precision, thus significantly reduce memory workload and can utilize processor’s low\\nprecision computation. For this category, we include 4-Bit quantized models of Falcon-rw-1.3B[ 38],OPT-1B3[ 37],\\nLLaMA 2-7B and LLaMA 2-13B. We primarily use GPTQ[ 13] to quantize our model. Aforementioned models except\\nLLaMA 2-13B can be tested on MLC Chat on mobile phones. Despite the availability on phones, we still include\\n4-Bit quantized LLaMA 2-13B to offer a more comprehensive understanding of impacts of quantization on models.\\nQuantized models weights are either publicly downloadable resources or quantized with open-source software4.\\nLoRA on D&C To better demonstrate the communication advantages of PrivateLoRA, we adopt LoRA on D&C run-\\ntime. Cloud only holds decoder stack parameters and Device only holds LoRA parameters. Activations are transmitted\\nto complete forward computation but the communication base is dimension of hidden states. We include two LoRA con-\\nfigurations, one optimized for generation speed and another optimized for tuning performance.\\n•Speed Oriented We apply the same latency analysis and get communication budget to make LoRA on\\nD&C reaches comparable speed to PrivateLoRA. With obtained communication budget, we explore among\\nseveral allocation strategies. We report score from the best performant configuration. We note this configuration\\nasLoRAS. Details about the adaptation configurations and budget allocation are listed in Appendix C.\\n•Performance Oriented The equivalent structure of adapting query, key and value projections in every decoder\\nlayer in LLaMA. We note this configuration as LoRAP\\n4.1.4 PrivateLoRA Configurations\\nWe adapt only query, key and value projections found in every decoder layer of LLaMA as mentioned in Section 3.2.4.\\nWe set rD2C=rC2D= 128 to achieve the balance between benchmark performance and generation speed. Under\\nour predefined conditions, PrivateLoRAachieves 30.1 tokens/s for LLaMA 2-7B, 20.1 token/s for LLaMA 2-13B and\\n10.1 token/s for LLaMA-30B.\\n4.2 Experimental Results\\nThe following results are obtained with Bfloat16 if not mentioned.\\nFrom Table 3, we can draw the following conclusions.\\nPrivateLoRA outperforms device-only solutions in terms of both benchmark and processing speed. Compared\\nto small models used in device-only solutions, PrivateLoRA is capable of leveraging large scale model as the tuning\\nbackend, thus guarantees better task performances. For example, with the target generation speed of around 20 tokens\\nper second, device-only solutions can only leverage 1B model while PrivateLoRA is capable of leverage 7B model.\\nAlso, PrivateLoRA can achieve higher generation throughput with backend of 33B model compared to 7B model of\\ndevice-only solutions, with a huge gap of 23.1 in average benchmark scores, let alone smaller scale models.\\nSignificantly reduced communication overhead allows better adaptation for PrivateLoRA. Combining results\\nof LoRASand LoRAP, we find extreme disparity in tuning performance and throughput. For LLaMA 2-13B, LoRAS\\nfails to improve the benchmark score of every tested task while LoRAPnotably boosts task performance of GSM8K\\nand MMLU. On the other hand, the throughput of LoRAPis even lower than device-only solutions. Such observed\\ndisparity indicates the importance of number of adapted layers. To achieve comparable generation speed, LoRASonly\\nadapts two layers due to large communication base number of d. But in the case of PrivateLoRA, thanks to significantly\\nreduced communication base, all layers can be adapted and yield good tuning performances.\\nTuning performance of PrivateLoRA is comparable to LoRA, thus offering good foundation for personalization.\\nOn tested tasks, we find very close average benchmark scores between PrivateLoRA and LoRAP. The gap between\\nPrivateLoRAand LoRAPare 3.3, 2.1 and 2.0 for LLaMA 2-7B, 13B and 33B model, respectively. In the following\\nablation study of scaling up ranks, we find the performance gap is even smaller, indicating PrivateLoRA offers reliable\\ntuning performance which can be good foundation for personalization on private data.\\n4.3 Ablation Study: Integrity of PrivateLoRA\\n4Implemented with AutoGPTQ https://github.com/PanQiWei/AutoGPTQ\\n9 PRIME AI paper\\nModel Method Runtime TPS GSM8K HSwag BoolQ PIQA MMLU A VG. MS\\nFalcon-rw-1.3BNo-tuning Cloud 49.5 0.8 61.6 62.4 74.7 25.9 45.1 0.0\\n4Bit Device 20.1 0.0 45.6 59.5 74.8 24.6 40.9 -1.7\\nOPT-1B3No-tuning Cloud 49.5 1.0 54.0 59.6 69.0 25.0 41.7 0.0\\n4Bit Device 20.1 0.0 26.5 39.1 55.3 23.4 28.9 -5.2\\nStableLM-3B No-tuning Cloud 37.3 7.7 73.9 75.3 79.2 41.8 55.6 0.0\\nLLaMA 2-7BNo-tuning Cloud 37.2 14.6 75.9 77.7 77.8 45.3 58.3 0.0\\n4Bit Device 8.1 3.6 54.8 73.9 77.2 36.2 49.1 -2.0\\nLoRASD&C 25.6 14.7 75.9* 81.7 77.8* 50.4 60.1 1.3\\nLoRAPD&C 2.0 35.7 78.2 88.5 79.2 57.9 67.9 0.5\\nPL (Ours) D&C 27.1 25.1 77.0 88.1 78.5 54.3 64.6 4.6\\nLLaMA 2-13BNo-tuning Cloud 27.8 23.5 79.4 80.5 79.1 54.8 63.5 0.0\\n4Bit Device - 6.1 78.6 80.8 78.2 51.4 59.0 -\\nLoRASD&C 19.5 23.5* 79.4* 80.5* 79.1* 54.8* 63.5 0.0\\nLoRAPD&C 1.3 42.7 80.1 89.7 79.7 61.4 70.7 0.3\\nPL (Ours) D&C 20.5 36.5 80.0 88.9 79.3 58.4 68.6 3.8\\nLLaMA-30BNo-tuning Cloud 16.7 34.8 82.6 83.1 82.3 57.8 68.1 0.0\\nLoRASD&C 11.7 34.8* 82.6* 83.1* 82.3* 57.8* 68.1 0.0\\nLoRAPD&C 0.7 51.9 82.8 90.5 82.6 63.3 74.2 0.3\\nPL (Ours) D&C 12.9 46.7 83.3 88.4 82.6 59.8 72.2 3.1\\nTable 3: PrivateLoRA offers good tuning performance and high throughput comparable to cloud-based solutions. Token\\ngeneration speed denotes specifically throughput at decoding stage. Cloud speed is measured on single A100 80G GPU\\nand Device speed is measured on flagship smart phones. D&C speed is numerically estimated with modest assumptions\\nlisted in Table 2. Starred (*) scores indicate worse benchmark performance after tuning. MSis a custom metric based\\non the product of tuning improvement and generation speed. Large MSrepresents both good tuning performance and\\nhigh generation throughput.\\n32 64 128 256 512\\nRank rD2C=rC2D01020304050Acc\\nGSM8K PL\\nGSM8K A,B Reinit\\nGSM8K No-tuningMMLU PL\\nMMLU A,B Reinit\\nMMLU No-tuningPL Infer Overhead0.02.55.07.510.012.515.017.520.0\\nPL Overhead (ms)\\nFigure 4: Benchmark scores, PrivateLoRA integrity and\\ninference overhead of PrivateLoRA with scaled up ranks\\nrD2C, rC2D. Unlike LoRA, scaling up ranks benefits tuning\\nperformance.A, B are kept random after initialization and serve as\\nactivation encoder and decoder. Despite the static nature\\nofA, B ,A, B andMform a matched integral pair dur-\\ning optimization of M. We make the hypothesis about\\nintegrity of PrivateLoRA that with either of the pair, the\\nmodel performance with degrade as the residual activa-\\ntions are transformed into noise. In order to prove the\\nintegrity of A, B andMpair, we conduct the follow-\\ning ablation study. For a tuned pair of A, B andM, we\\nreinitialize A, B and keep Mintact. We then run the\\nbenchmark to see impact of our perturbation on perfor-\\nmance. We test on two benchmarks, namely MMLU and\\nGSM8K, with LLaMA 2-7B. For each setup, we sample\\nscores for 20 rounds.\\nFigure 4 plots the benchmark scores of PrivateLoRA-\\ntuned (lighter color), reinitialized A, B (dark color) and\\noriginal performance without tuning. MMLU of reini-\\ntialized A, B are around 40.8 and GSM8K around 10.7,\\nwhich is inferior to original performance. Overall, the\\npoor performance after reinitializing A, B verifies the\\nintegrity of A, B andM.\\n4.4 Ablation Study: Ranks of PrivateLoRA\\nWe empirically choose rD2C=rC2D= 128 in our main\\nexperiment. To study the effect of ranks rD2C, rC2D, we train LLaMA 2-7B on MMLU and GSM8K with gradually\\nincreased rC2D, rD2Cfrom 32 to 512. For simplicity, we set rC2D=rD2C. The benchmark scores and resulting\\ninference overhead are plotted in Figure 4.\\n10 PRIME AI paper\\n1 16 32 64 128 256 512 1024 2048\\nTokens in mini-batch50100150200250300350400450Tokens per second\\nrC2D=rD2C=16\\nrC2D=rD2C=32\\nrC2D=rD2C=64\\nrC2D=rD2C=128\\nrC2D=rD2C=256\\nrC2D=rD2C=512\\nTPSDevice\\n(a) Prefill.\\n1 16 32 64 128 256 512 1024 2048\\nBacth Size101520253035Tokens per second\\nrC2D=rD2C=16\\nrC2D=rD2C=32\\nrC2D=rD2C=64\\nrC2D=rD2C=128\\nrC2D=rD2C=256\\nrC2D=rD2C=512\\nTPSDevice (b) Decoding.\\nFigure 5: Inference throughput of PrivateLoRA with backend of LLaMA 2-7B. Prefill and decoding speed for rank of\\n128 reaches 175.5 and 26.5 tokens per second,respectively. Throughput of both stages is almost 300% of pure device\\nsolutions.\\nUnlike LoRA, increasing ranks rD2C, rC2Dof PrivateLoRA increases tuning performances. When scaling ranks\\nfrom 32 to 128, we see obvious improvement on benchmark scores of MMLU and GSM8K. The benefit in tuning\\nperformance decreases as ranks increase and we do not see much of a difference between ranks of 256 and ranks of 512.\\nWith this in mind, we can effective adjust the tuning capability on demand. Also, we found that with an equal rank of\\n32, PrivateLoRA’s benchmark score is way lower than LoRA, indicating the impact of untrainable A, B on data fitting\\ncapability. However, with increased ranks, as the residual activation becomes of higher rank, the tuning performance is\\ncompensated.\\nMethod Config GSM8K MMLU\\nPLA×B× 25.1 54.3\\nA×B√26.7 54.5\\nA√B× 27.4 55.4\\nA√B√28.8 56.6\\nLoRAPq,k,v 31.3 57.3\\nTable 4: Ablation study on structure of Pri-\\nvateLoRA. Various A, B configurations are\\ntested. Trainable A, B of PrivateLoRA bene-\\nfits tuning performances. A×, B√means Ais\\nnon-trainable and Bis trainable.Another thing to account for when setting the ranks of PrivateLoRA is\\nthe communication overhead. The inference overhead should increase\\nlinearly with ranks and Figure 4 verifies it.\\n4.5 Ablation Study: Comparison with LoRA\\nAlthough PrivateLoRA is equally low rank as LoRA, PrivateLoRA ex-\\nhibits weaker tuning performance compared to LoRAP. To find out\\nwhether the performance difference comes from triplet structure or\\nuntrainable A, B , we then carry out an ablation study.\\nWe run all four configurations of A, B with different trainable setting\\nfor PrivateLoRA. For LoRA, we run the equivalent structure that only\\ntargets query, key and value projections. We benchmark on MMLU\\nand GSM8K as PrivateLoRA falls behind compared to LoRA.\\nTable 4 lists the benchmark scores of aforementioned methods. We\\nsee largest improvements brought by simultaneously trainable A, B .\\nAnother interesting thing is that trainable Aboosts overall performances more than trainable B, meaning a trainable\\nupstream is more preferable.\\n5 Throughput Estimation\\nAccording to TPS derived in Section 3.3.3, assumptions in Table 2 and measured base throughput (Table 8), we can\\nmake numerical estimations on throughput of PrivateLoRA. We discuss throughput of typical scenarios of transformer\\nusage, including generation prefill, generation decoding and training. To simplify the discussion and better present\\nthe idea, we use a single A100 80G GPU as Cloud. Therefore, we do not include models larger than 33B. Detailed\\ncalculation is discussed in Appendix D.\\n11 PRIME AI paper\\n5.1 Inference Throughput\\nFigure 5 plots estimated inference throughput of prefill stage and decoding stage of PrivateLoRA with backend of\\nLLaMA 2-7B (more at Appendix D.1). In general, TPS of PrivateLoRA increases with total tokens processed and\\nconverges when total tokens exceed certain thresholds. With ranks set to 128, PrivateLoRA can achieve a prefill speed of\\n175.5 token per second, almost 300% of pure device solutions. As for decoding speed, PrivateLoRA allows a speed of\\n26.5 TPS, 327% of pure device solutions. If ranks are reduced to 64 or 32, the prefill TPS will be 268 or 364, respectively.\\nTherefore, the tradeoff between tuning performance and throughput is important in PrivateLoRA application.\\n1 16 32 64 128 256 512 1024 2048\\nTokens in mini-batch20406080100120Tokens per second\\nrC2D=rD2C=16\\nrC2D=rD2C=32\\nrC2D=rD2C=64\\nrC2D=rD2C=128\\nrC2D=rD2C=256\\nrC2D=rD2C=512\\nFigure 6: Estimated training throughput of tuning LLaMA\\n2-7B with PrivateLoRA with different ranks and total tokens\\nin mini-batch. Word embedding, adaptation matrices Mand\\nLM Head are set trainable.On larger scale models, execution time of the decoder\\nlayer is larger, therefore the impact of PrivateLoRA on\\nthroughput becomes smaller. 30B models are impossi-\\nble to run on current smart phones. However, it can be\\nutilized as personalization backend of PrivateLoRA, and\\nthroughput can reach 77.2% of A100 80G GPU speed.\\nAlthough we do not offer numerical estimations on larger\\nscale model, e.g.65B and 160B, due to the single A100\\n80G limit, we can still conclude that PrivateLoRA be-\\ncomes even more competitive on models of these scales.\\n5.2 Training Throughput\\nTraining step is much more complex than pure inference.\\nCompared to inference, model training not only adds\\nback-propagation but also involves computations from\\noptimizer, computation graph, etc, thus adding highly\\nvariable workload to processor and memory. All these\\naspects make estimation of time cost of training step diffi-\\ncult. Moreover, since there are no publicly available deep\\nlearning frameworks that allow tuning LLMs on smart\\nphones, we could not offer empirical training through-\\nput on Device. However, with our estimations, we still\\nfind that PrivateLoRA is highly competitive in training\\nefficiency.\\nEstimated throughput of tuning LLaMA 2-7B with PrivateLoRA of different ranks is plotted in Figure 6. With rank\\nset to 128, the peak training throughput of PrivateLoRA on LLaMA 2-7B is around 53.8 tokens per second, which is\\nalmost as fast as prefill speed on Device. On GPU, training throughput is way smaller than prefill throughput and the\\nratio sits around 10% according to our benchmark. Despite that we can not numerically estimate training throughput on\\npure Device, we can still conclude that tuning with PrivateLoRA outperforms pure device solutions in terms of speed.\\n5.3 PrivateLoRA Overhead\\nIn this section, we analyze the overhead introduced by PrivateLoRA, so that throughput under other conditions can be\\nbetter predicted. In general, despite our modest assumptions, PrivateLoRA introduces very marginal overhead and can\\nproduce higher overall throughput if accompanied with SOTA LLM serving technology.\\nFigure 7 plots detailed inference time decomposition. Under 5G accessible to average consumers, PrivateLoRA adds\\n5.1ms, 6.4ms and 9.3ms of overhead for 7B, 13B and 30B models, respectively Inference latency overhead introduced\\nby PrivateLoRA is quite marginal and is only proportional to number of adapted layers. Architecture of PrivateLoRA re-\\nduces transmission base from hidden dimension to ranks rD2C, rC2D, so that transmission amount is model invariant.\\nForwarding one token through one decoder layer only yield 8.2 Kb of transmission, while 262.1 Kb is needed in\\nLoRA equivalent architecture and the amount is also relevant to hidden dimension.\\nTherefore, the marginal latency brought by PrivateLoRA allows for higher throughput if accompanied with dedicated\\nLLM serving technology. Throughput on GPU of this work is measured with Huggingface transformers[ 40] imple-\\nmentation, which is not dedicated for LLM serving. SOTA LLM serving technology[ 41,12] can produce much higher\\nthroughput with enhanced FLOPS utilization and multi-GPU support. For instance, running LLaMA 2-70B with MLC\\nLLM[12] on 8 A100 80G GPU yields a decoding speed of 38.8 TPS.\\n12 PRIME AI paper\\n6 Discussion\\nIn previous sections, we discussed the architecture of PrivateLoRA, its impact on LLM service mode and its sheer\\ntuning performance. In this section, we discuss other challenges of PrivateLoRA throughout the entire lifecycle and\\npotential solutions. Solutions are not limited at the level of models since many problems can be better handled in a\\nmore systematic manner.\\n6.1 Further Improve PrivateLoRA Throughput\\n7B(3bit)\\nPhone3B(4bit)\\nPhone7B(16bit)\\nPL13B(16bit)\\nPL30B(16bit)\\nPL\\nModel Scale020406080100120Single Decode Elapsed Time/msLM Head\\nDecoder Stack\\nPrivateLoRA Overhead\\nFigure 7: Inference latency decomposition shows that Pri-\\nvateLoRA adds very marginal overhead. Under 5G accessi-\\nble to average consumers, PrivateLoRA adds 5.1ms, 6.4ms\\nand 9.3ms of overhead for 7B, 13B and 30B models, respec-\\ntively. PrivateLoRA allows personalized generation with a\\nbackend of 30B models at the speed of 4bit quantized 3B\\nmodels.Various methods can be adopted to improve Pri-\\nvateLoRA throughput.\\nWith current configuration, the transmission amount per\\ntoken for LLaMA 2-7B is 4.2Mb. Following the overhead\\ndecomposition in Section 3.3.3, we can easily propose\\nseveral approaches to further reduce the transmission\\noverhead. Firstly, a lower precision data type can be used.\\nVarious researches have pointed out that 8bit, even 4bit\\nquantization does not influence much model performance\\nand the impact reduces as model scales up. Therefore,\\nit’s reasonable to use PrivateLoRA on a lower precision\\nmodel and the transmission overhead can be reduced.\\nSecondly, exploiting the redundancy in adaptation con-\\nfigurations. Adapting all modules at all layers may not\\nbe optimal and reduction in adapted modules can provide\\nsimilar performances. Therefore, we can trade fractions\\nof tuning performance with fewer adapted modules. With\\nfewer adapted modules, the activation transmission is also\\ncut down. Thirdly, we can deploy some of the decoder\\nlayers on Device. For devices with reasonable hardware,\\ndeploy some decoder layers on Device can better utilize\\nthe hardware and reduce the transmission amount. As\\nmentioned earlier, this practice also enhances the activa-\\ntion security.\\n6.2 Activation Privacy\\nIn our work, we protect privacy by ensuring data locality,\\nwhich refers to the prohibition of persistent storage of\\nraw data and its derivative on Cloud. With PrivateLoRA, raw text and personalized parameters remain on edge\\ndevices no matter in training or inference. This protects the user from being identified by visual-based or norm-based\\ncomparison[ 42]. In our proposed paradigm, activations and gradients are transmitted to prevent raw data is uploaded to\\nCloud. This contrasts with existing literature that focuses on extracting bag of words from general-purpose language\\nmodel embeddings[43]. However, such method targets final output embeddings and effect on intermediate activations\\nof decoder-only generative LLMs is unknown. To further enhance privacy, we can deploy critical decoder layers on\\nedge devices to reduce the exposure of vulnerable activations[ 44]. Moreover, our proposed heterogeneous architecture\\ndesign complicates potential attacks. Looking ahead, our future work will concentrate on bolstering activation privacy,\\nexploring more robust methods to protect against sophisticated data breaches.\\n6.3 PrivateLoRA Integrity\\nIn Section 4.3, we confirm the integrity of the model components A, B, and M, showing that only matched pairs produce\\ncorrect outputs. Two critical observations emerge from our analysis. First, we observed an intriguing phenomenon:\\nthe perturbed benchmark scores of GSM8K and MMLU both decline by approximately 5% in accuracy compared\\nto the original model, regardless of the noise activation rank. This finding indicates that increasing the rank of noise\\nactivations does not further degrade performance. Intriguingly, higher ranks improve tuning performance. Starting from\\nthis observation, we may gain valuable insights into the mechanisms of low-rank adaptation of LLM.\\nSecond, we identify a potential vulnerability (e.g., backdoor attack on edge[ 45]) in the model’s design: the simple linear\\nnature of components A, B , andMcould allow for ’hacking’ of M, assuming AandBare known. By tracking several\\n13 PRIME AI paper\\nrounds of forward computation, one might deduce M, leading to privacy concerns. However, it’s crucial to note that the\\nparameters on Device are essentially a black box to the Cloud, and their internal architecture could be more complex\\nthan a single linear projection. This realization directs our future work towards enhancing the security and integrity of\\nthese model components to achieve responsible[46], secure and ethical use of LLM.\\n6.4 Low power consumption is the core advantage of PrivateLoRA.\\nAs detailed in Section 3.3.2, PrivateLoRA necessitates a mere 2% of the FLOPs required by a device-only solution,\\npresenting a significant computational advantage. This reduction in computation not only enhances processing speed but\\nalso leads to a substantial decrease in power consumption. The key benefit of PrivateLoRA lies in its ability to minimize\\npower usage on edge devices. While advancements in hardware architecture and manufacturing processes may enable\\nlarger scale transformers to operate on edge devices, PrivateLoRA’s ability to drastically reduce local compute demands\\nremains its standout feature. This is particularly crucial for most edge devices, which aren’t typically built for sustained\\npeak performance and where power efficiency is paramount, especially in battery-dependent scenarios. Comparing\\nPrivateLoRA with device-only solutions, there is a notable shift in power consumption from edge devices to the broader\\nnetwork and Cloud infrastructure. Expanding on this, the adoption of PrivateLoRA could have profound implications\\nfor the future design and functionality of edge devices.\\n7 Conclusion and Future Work\\nIn this paper, we propose PrivateLoRA, a Parameter Efficient Fine-Tuning (PEFT) method for heterogeneously\\ndistributing LLMs. This novel approach, centered around the concept of Low Rank Residual Transmission, significantly\\ndiminishes communication overheads, thus offering a more efficient and privacy-conscious alternative to traditional\\ncloud-based solutions. Our proposed method democratizes access to advanced LLM capabilities and could spur a wave\\nof innovation and new applications across various sectors.\\nThe following directions can be our future work.\\n•Communication Budget Allocation In our experiment, we adapt all decoder layers, but redundancy in\\nadaptation can be exploited to further reduce communication overhead.\\n•Activation Privacy Activation privacy is the next challenge towards a more comprehensive privacy protection\\nin LLM service.\\n•Integrity of A, B, M Unmatched A, B andMonly slightly reduces performance but it should work like\\npublic private key pair that unmatched pair results in block of access. A method should be found to completely\\npoison the activations so that even semantics can not be deduced.\\nReferences\\n[1]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Nee-\\nlakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger,\\nTom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher\\nHesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner,\\nSam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In\\nAdvances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing\\nSystems 2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020.\\n[2]Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,\\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave,\\nand Guillaume Lample. Llama: Open and efficient foundation language models. ArXiv , abs/2302.13971, 2023.\\n[3]Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang,\\nSandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie\\nSimens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models\\nto follow instructions with human feedback. In NeurIPS , 2022.\\n[4]Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten\\nBosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean,\\nand William Fedus. Emergent abilities of large language models. Trans. Mach. Learn. Res. , 2022, 2022.\\n[5]Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei Shen, Zihan Wang, Andi Wang, Yang\\nLi, Teng Su, Zhilin Yang, and Jie Tang. Codegeex: A pre-trained model for code generation with multilingual\\n14 PRIME AI paper\\nbenchmarking on humaneval-x. In Ambuj K. Singh, Yizhou Sun, Leman Akoglu, Dimitrios Gunopulos, Xifeng\\nYan, Ravi Kumar, Fatma Ozcan, and Jieping Ye, editors, Proceedings of the 29th ACM SIGKDD Conference on\\nKnowledge Discovery and Data Mining, KDD 2023, Long Beach, CA, USA, August 6-10, 2023 , pages 5673–5684.\\nACM, 2023.\\n[6]Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu\\nLiu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian\\nCanton-Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron,\\nLouis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for\\ncode. CoRR , abs/2308.12950, 2023.\\n[7]Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh\\nHajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Anna Rogers, Jordan L.\\nBoyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023 , pages\\n13484–13508. Association for Computational Linguistics, 2023.\\n[8]Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu\\nJain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button,\\nMatthew Knight, Benjamin Chess, and John Schulman. Webgpt: Browser-assisted question-answering with\\nhuman feedback. CoRR , abs/2112.09332, 2021.\\n[9]Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,\\nJonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel\\nDuckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor\\nMordatch, and Pete Florence. Palm-e: An embodied multimodal language model. In Andreas Krause, Emma\\nBrunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International\\nConference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA , volume 202 of\\nProceedings of Machine Learning Research , pages 8469–8488. PMLR, 2023.\\n[10] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy P. Lillicrap. Android in the wild: A\\nlarge-scale dataset for android device control. CoRR , abs/2307.10088, 2023.\\n[11] Zhuosheng Zhang and Aston Zhang. You only look at screens: Multimodal chain-of-action agents. CoRR ,\\nabs/2309.11436, 2023.\\n[12] MLC team. MLC-LLM, 2023.\\n[13] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: accurate post-training quantization for\\ngenerative pre-trained transformers. CoRR , abs/2210.17323, 2022.\\n[14] Peiyu Liu, Zikang Liu, Ze-Feng Gao, Dawei Gao, Wayne Xin Zhao, Yaliang Li, Bolin Ding, and Ji-Rong Wen. Do\\nemergent abilities exist in quantized large language models: An empirical study. CoRR , abs/2307.08072, 2023.\\n[15] Otkrist Gupta and Ramesh Raskar. Distributed learning of deep neural network over multiple agents. J. Netw.\\nComput. Appl. , 116:1–8, 2018.\\n[16] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: memory optimizations toward training\\ntrillion parameter models. In Proceedings of the International Conference for High Performance Computing,\\nNetworking, Storage and Analysis, SC 2020, Virtual Event / Atlanta, Georgia, USA, November 9-19, 2020 , page 20.\\nIEEE/ACM, 2020.\\n[17] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri,\\nMyle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Pritam Damania, Bernard Nguyen, Geeta Chauhan,\\nYuchen Hao, Ajit Mathews, and Shen Li. Pytorch FSDP: experiences on scaling fully sharded data parallel. Proc.\\nVLDB Endow. , 16(12):3848–3860, 2023.\\n[18] Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan. Peft:\\nState-of-the-art parameter-efficient fine-tuning methods. https://github.com/huggingface/peft , 2022.\\n[19] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min\\nChan, Weize Chen, et al. Delta tuning: A comprehensive study of parameter efficient methods for pre-trained\\nlanguage models. arXiv preprint arXiv:2203.06904 , 2022.\\n[20] Ruidan He, Linlin Liu, Hai Ye, Qingyu Tan, Bosheng Ding, Liying Cheng, Jia-Wei Low, Lidong Bing, and Luo Si.\\nOn the effectiveness of adapter-based tuning for pretrained language model adaptation. In Proceedings of the 59th\\nAnnual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on\\nNatural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021 ,\\npages 2208–2222. Association for Computational Linguistics, 2021.\\n15 PRIME AI paper\\n[21] Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. Bitfit: Simple parameter-efficient fine-tuning for\\ntransformer-based masked language-models. In Proceedings of the 60th Annual Meeting of the Association\\nfor Computational Linguistics (Volume 2: Short Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022 , pages 1–9.\\nAssociation for Computational Linguistics, 2022.\\n[22] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in\\nGPT. In NeurIPS , 2022.\\n[23] Edward Hu, Yelong Shen, Phil Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Lu Wang, and Weizhu Chen. Lora:\\nLow-rank adaptation of large language models, 2021.\\n[24] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao.\\nAdaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh International Conference on\\nLearning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 . OpenReview.net, 2023.\\n[25] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo,\\nMona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. CoRR , abs/1902.00751, 2019.\\n[26] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel.\\nFew-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. In NeurIPS , 2022.\\n[27] Tian Dong, Song Li, Han Qiu, and Jialiang Lu. An interpretable federated learning-based network intrusion\\ndetection framework. arXiv preprint arXiv:2201.03134 , 2022.\\n[28] Tian Dong, Han Qiu, Jialiang Lu, Meikang Qiu, and Chun Fan. Towards fast network intrusion detection based on\\nefficiency-preserving federated learning. In 2021 IEEE Intl Conf on Parallel and Distributed Processing with\\nApplications, Big Data and Cloud Computing, Sustainable Computing and Communications, Social Computing\\nand Networking (ISPA/BDCloud/SocialCom/SustainCom) , pages 468–475, 2021.\\n[29] Ligeng Zhu, Lanxiang Hu, Ji Lin, Wei-Chen Wang, Wei-Ming Chen, Chuang Gan, and Song Han. Pockengine:\\nSparse and efficient fine-tuning in a pocket. CoRR , abs/2310.17752, 2023.\\n[30] Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast and memory-efficient\\nexact attention with IO-awareness. In Advances in Neural Information Processing Systems , 2022.\\n[31] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya,\\nJonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference. CoRR ,\\nabs/2211.05102, 2022.\\n[32] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized\\nllms. CoRR , abs/2305.14314, 2023.\\n[33] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.\\nBoolq: Exploring the surprising difficulty of natural yes/no questions. In Jill Burstein, Christy Doran, and Thamar\\nSolorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for\\nComputational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7,\\n2019, Volume 1 (Long and Short Papers) , pages 2924–2936. Association for Computational Linguistics, 2019.\\n[34] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey\\nHsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang,\\nKevin Wang, and Andy Zou. A framework for few-shot language model evaluation, September 2021.\\n[35] Godfrey Anuga Akpakwu, Bruno J. Silva, Gerhard P. Hancke, and Adnan M. Abu-Mahfouz. A survey on 5g\\nnetworks for the internet of things: Communication technologies and challenges. IEEE Access , 6:3619–3647,\\n2018.\\n[36] Arvind Narayanan, Eman Ramadan, Jason Carpenter, Qingxu Liu, Yu Liu, Feng Qian, and Zhi-Li Zhang. A first\\nlook at commercial 5g performance on smartphones. In Yennun Huang, Irwin King, Tie-Yan Liu, and Maarten\\nvan Steen, editors, WWW ’20: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020 , pages 894–905.\\nACM / IW3C2, 2020.\\n[37] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan,\\nMona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig,\\nPunit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. OPT: open pre-trained transformer\\nlanguage models. CoRR , abs/2205.01068, 2022.\\n[38] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobei-\\ndli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The RefinedWeb dataset for Falcon LLM:\\noutperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116 , 2023.\\n[39] Jonathan Tow, Marco Bellagente, Dakota Mahan, and Carlos Riquelme. Stablelm 3b 4e1t.\\n16 PRIME AI paper\\n[40] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,\\nTim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite,\\nJulien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M.\\nRush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on\\nEmpirical Methods in Natural Language Processing: System Demonstrations , pages 38–45, Online, October 2020.\\nAssociation for Computational Linguistics.\\n[41] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez,\\nHao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention.\\nInProceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles , 2023.\\n[42] Tian Dong, Bo Zhao, and Lingjuan Lyu. Privacy for free: How does dataset condensation help privacy?\\nIn Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato, editors,\\nInternational Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA , volume\\n162 of Proceedings of Machine Learning Research , pages 5378–5396. PMLR, 2022.\\n[43] Xudong Pan, Mi Zhang, Shouling Ji, and Min Yang. Privacy risks of general-purpose language models. In 2020\\nIEEE Symposium on Security and Privacy, SP 2020, San Francisco, CA, USA, May 18-21, 2020 , pages 1314–1331.\\nIEEE, 2020.\\n[44] Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James R. Glass, and Pengcheng He. Dola: Decoding by\\ncontrasting layers improves factuality in large language models. CoRR , abs/2309.03883, 2023.\\n[45] Tian Dong, Ziyuan Zhang, Han Qiu, Tianwei Zhang, Hewu Li, and Terry Wang. Mind your heart: Stealthy\\nbackdoor attack on dynamic deep neural network in edge computing. In IEEE INFOCOM 2023 - IEEE Conference\\non Computer Communications , pages 1–10, 2023.\\n[46] Tian Dong, Shaofeng Li, Guoxing Chen, Minhui Xue, Haojin Zhu, and Zhen Liu. Rai2: Responsible identity\\naudit governing the artificial intelligence. In 30th Annual Network and Distributed System Security Symposium,\\nNDSS 2023 . The Internet Society, 2023.\\n[47] Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D. Lee, Danqi Chen, and Sanjeev Arora.\\nFine-tuning language models with just forward passes. CoRR , abs/2305.17333, 2023.\\n17 PRIME AI paper\\nA Experiment Setup\\nA.1 Hyperparameters\\nDetailed hyperparameters are listed in Table 5. All experiments are carried out in almost identical hyperparameter\\nconfiguration except that learning rates may vary depending on the task. Learning rates for PrivateLoRA is generally\\nlarge. Additionally, PrivateLoRA is more sensitive to learning rate. For example, HellaSwag performance vary for\\ndifferent.\\nExperiment Method Hyperparameters Values\\nSharedOptimizer AdamW\\nWeight Decay 0\\nWarmup Ratio 0.1\\nLR Scheduler Linear\\nBatch Size ×Num GPU 256\\nBoolQLoRA Learning Rate 5e-4\\nPrivateLoRA Learning Rate {5e-3,1e-3}\\nMMLULoRA Learning Rate 5e-4\\nPrivateLoRA Learning Rate {5e-3,1e-3}\\nHellaSwagLoRA Learning Rate 5e-4\\nPrivateLoRA Learning Rate {5e-3,1e-3}\\nPIQALoRA Learning Rate 5e-4\\nPrivateLoRA Learning Rate {5e-3,1e-3}\\nGSM8KLoRA Learning Rate 5e-4\\nPrivateLoRA Learning Rate {5e-3,1e-3}\\nTable 5: Detailed hyperparameters used in our experiments. We vary learning rates depending on the dataset no matter\\nthe model scale. PrivateLoRA generally use larger learning rates. Gradient accumulation step is set to ensure the\\nequality of total train steps for different models.\\nA.2 Prompt Templates\\nWe follow previous works to build prompts for tuning. More specifically, we follow QLoRA[ 32] for MMLU, MeZO[ 47]\\nfor BoolQ and LM-Eval Harness[ 34] for the rest. Despite improvement on benchmark scores in most of our experiments,\\nthe following prompts do not guarantee improvement on every model.\\nDataset Task Type Prompt & Target\\nMMLU Multiple Choice<question>\\nA.<samples>[0]\\nB.<samples>[1]\\nC.<samples>[2]\\nD.<samples>[3]\\nA/B/C/D\\nBoolQ Classification<passage><question>?\\nYes/No\\nGSM8K GenerationQuestion: <question>\\nAnswer: <answer>\\nHellaSwag Multiple Choice<activity_label>: <ctx> ____\\nA.<endings>[0]\\nB.<endings>[1]\\nC.<endings>[2]\\nD.<endings>[3]\\nAnswer: A/B/C/D. <endings>\\nTable 6: Prompt templates used for tuning. Prompts are adapted from QLoRA[ 32], MeZO[ 47] and LM-Eval Harness[ 34].\\n<question> refers to fields of datasets and Adenotes target. During training, only loss on target part is calculated for\\nback-propagation. For evaluation, we use standard prompts provided in LM-Eval Harness and make no modification.\\n18 PRIME AI paper\\nB Throughput Measurement\\nB.1 Inference Throughput Measurement\\nB.1.1 Quantized Models On Mobile Phones\\nWe tested generation speed on mobile phones with MLC Chat[ 12], currently only solution to running LLMs on mobile\\ndevices. We use LLaMA 2-7B (3bit) and RedPajama-3B (4bit) provided in the APP by default for benchmark. We\\nsample on each device for 20 times with inputs of 1024 tokens and report the generation speed from built-in benchmarker.\\nWe use long sequences so that prefill stage is guaranteed to be FLOPS bound.\\nDevice ModelLLaMA 2-7B 3bit RedPajama-3B 4bit\\nPrefill Decode Prefill Decode\\niPhone 13 Pro Max 54.3 5.7 81.8 14.3\\nXiaomi 13 58.9 8.1 88.1 16.3\\nTable 7: Token generation speed tested on mobile phones with MLC Chat. Both prefill speed and decoding speed are\\nreported with input sequences of 1024 tokens. Decoding speed is around 10% of prefill speed due to memory boundness\\ncaused by non-batched computation.\\nTable 7 lists prefill and decoding speed of . Decoding speed is much slower than prefill speed because the computation\\nis not batched, thus memory bound.\\nB.1.2 BF16 Models on a Single A100 80G GPU\\nFalcon-rw-1B\\nStableLM-3B-4e1tLLaMA 2-1B*\\nLLaMA 2-3B*\\nLLaMA 2-7BLLaMA 2-13BLLaMA-30B\\nRedPajama-3B (4bit)LLaMA 2-7B (3bit)0102030405060Tokens per secondRuntime\\nGPU\\nSmart Phone\\n(a) Token generation speed of decoding stage.\\nFalcon-rw-1B\\nStableLM-3B-4e1tLLaMA 2-1B*\\nLLaMA 2-3B*\\nLLaMA 2-7BLLaMA 2-13BLLaMA-30B\\nRedPajama-3B (4bit)LLaMA 2-7B (3bit)102103104Tokens per secondRuntime\\nGPU\\nSmart Phone (b) Throughput rate of prefill stage.\\nFigure 8: Decoding speed of models of scale between 1B and 30B parameters tested on 1×A100 80G GPU. Models\\nwith∗are not official releases and are created by scaling transformer dimensions.\\nWe use Huggingface Transformers[ 40] implementations to benchmark generation speed. As for benchmark protocol,\\nwe follow vLLM[ 41] to sample elapsed time of prefilling 1024-token input and decoding 1024 new tokens. Due to\\nimplementation, models can produce very results of large variance, e.g.Falcon-rw-1.3B. To rule out implementation\\ndiscrepancy, we scale LLaMA to 1B and 3B according to dimensions of Falcon-rw-1.3B and StableLM-3B-4e1t.\\nObtained statistics are plotted on Figure 8. In decoding phase, 4bit quantized 3B model on smart phone is as fast as\\n30B model on a single A100 80 GPU. For prefill stage, the gap between GPU and smart phones are significantly wider.\\nThroughput of 30B model on GPU is almost 3000% of quantized 3B model on smart phones. The low throughput of\\nprefill on smart phones hinders training on personal data.\\nModel Batch Size TPS\\nLLaMA 2-7B 16 1088.6\\nLLaMA 2-13B 8 652.8\\nLLaMA-30B 2 296.0\\nTable 8: Training throughput of llama series on a single A100 80G GPU.\\n19 PRIME AI paper\\nC Transmission Budget Allocation of LoRAS\\nLoRASdenotes series of LoRA configurations that allows LoRA to reach predefined inference speed under the\\ndistributed architecture of PrivateLoRA. Used as baselines in our experiments, we set LoRASto reach 70% of Cloud\\nspeed and derive the transmission budget per token tfor LoRA. We restrict adaptation modules to query, key and\\nvalue for the same reason as PrivateLoRA. Therefore, the only variable to transmission amount is the number of\\nadapted layers. For LLaMA 2-7B, to reach 70% of Cloud speed, only 2 layers can be adapted. For LLaMA 2-13B and\\nLLaMA-30B, the number of layers are 2, 3, respectively. Therefore, we obtain the following potential configurations.\\nWe test obtained configurations by tuning LLaMA 2-7B on MMLU. Ranks of LoRA are set to 32 and αset to 32.\\nLearning rates are 5e-4 and identical for all experiments.\\nTarget TPS % Cloud TPS Modules Layers MMLU\\n25.6 ∼70%q,k,v 0:2 47.4\\nq,k,v 30:32 45.3*\\nq,k,v 15, 31 50.4\\nq,k,v 0, 15 48.7\\nq,k,v 0, 31 45.9\\n11.7 ∼30%q,k,v 0:16 55.5\\nq,k,v 16:32 51.3\\nq,k,v 0:32:2 55.8\\n- - No-tuning 45.3\\nTable 9: MMLU scores of LLaMA 2-7B tuned with different LoRASconfigurations. Notations of layers follow python\\nlist slicing. 0:16 in layers represents the first 16 decoder layers are adapted. Starred (*) scores indicate worse benchmark\\nperformance after tuning.\\nTable 9 list MMLU scores of various LoRASconfigurations. With a target TPS of 25, LoRASprovides terrible tuning\\nperformances. Adaptation on the 31 st,32ndlayers and 1 st,32ndyields worse benchmark performances than no-tuning.\\nIncreasing the number of adapted layers also increase the performances. Adapting only the first 16 layers yield similar\\ntuning performance to LoRAP. Alongside performances of 30% of Cloud TPS, adapting the first few layers gives better\\ntuning performances. Therefore, the general adaptation budget allocation strategy is to adapt shallow layers and final\\nLoRASconfigurations are listed in Table 10\\nModel Modules Layers TPS % Cloud TPS\\nLLaMA 2-7B q,k,v 15,31 25.6 68.9\\nLLaMA 2-13B q,k,v 19,39 19.5 70.0\\nLLaMA-30B q,k,v 29,59 12.4 74.6\\nTable 10: Detailed adaptation configurations of LoRASfor LLaMA 2-7B, LLaMA 2-13B and LLaMA-30B used in\\nSection 4.2. Adapted modules are restricted to query,key and value to reduce transmission. The adapted layers are\\nchosen with the insight that adapting shallow layers yield more performance gain.\\nD PrivateLoRA Throughput Estimation\\nD.1 Inference Throughput\\nD.2 Training Throughput\\n20 PRIME AI paper\\n1 16 32 64 128 256 512 1024 2048\\nTokens in mini-batch50100150200250300350Tokens per second\\nrC2D=rD2C=16\\nrC2D=rD2C=32\\nrC2D=rD2C=64\\nrC2D=rD2C=128\\nrC2D=rD2C=256\\nrC2D=rD2C=512\\n(a) LLaMA 2-13B Prefill.\\n1 16 32 64 128 256 512 1024 2048\\nBacth Size161820222426Tokens per second\\nrC2D=rD2C=16\\nrC2D=rD2C=32\\nrC2D=rD2C=64\\nrC2D=rD2C=128\\nrC2D=rD2C=256\\nrC2D=rD2C=512 (b) LLaMA 2-13B Decoding.\\n1 16 32 64 128 256 512 1024 2048\\nTokens in mini-batch50100150200250Tokens per second\\nrC2D=rD2C=16\\nrC2D=rD2C=32\\nrC2D=rD2C=64\\nrC2D=rD2C=128\\nrC2D=rD2C=256\\nrC2D=rD2C=512\\n(c) LLaMA 2-30B Prefill.\\n1 16 32 64 128 256 512 1024 2048\\nBacth Size10111213141516Tokens per second\\nrC2D=rD2C=16\\nrC2D=rD2C=32\\nrC2D=rD2C=64\\nrC2D=rD2C=128\\nrC2D=rD2C=256\\nrC2D=rD2C=512 (d) LLaMA 2-30B Decoding.\\nFigure 9: Estimated inference throughput of PrivateLoRA on different base models.\\n1 16 32 64 128 256 512 1024 2048\\nTokens in mini-batch20406080100120140Tokens per second\\nrC2D=rD2C=16\\nrC2D=rD2C=32\\nrC2D=rD2C=64\\nrC2D=rD2C=128\\nrC2D=rD2C=256\\nrC2D=rD2C=512\\n(a) Training throughput (LLaMA 2-13B).\\n1 16 32 64 128 256 512 1024 2048\\nTokens in mini-batch20406080100Tokens per second\\nrC2D=rD2C=16\\nrC2D=rD2C=32\\nrC2D=rD2C=64\\nrC2D=rD2C=128\\nrC2D=rD2C=256\\nrC2D=rD2C=512 (b) Training throughput (LLaMA-30B).\\nFigure 10: Estimated training throughput of PrivateLoRA on different base models.\\n21 '},\n",
       " {'url': 'http://arxiv.org/pdf/2306.01941v2',\n",
       "  'title': 'AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap',\n",
       "  'text': 'AI Transparency in the Age of LLMs: A Human-Centered\\nResearch Roadmap\\nQ. Vera Liao and Jennifer Wortman Vaughan∗\\nMicrosoft Research\\nAugust 9, 2023\\nAbstract\\nThe rise of powerful large language models (LLMs) brings about tremendous opportunities\\nfor innovation but also looming risks for individuals and society at large. We have reached a\\npivotal moment for ensuring that LLMs and LLM-infused applications are developed and deployed\\nresponsibly. However, a central pillar of responsible AI—transparency—is largely missing from the\\ncurrent discourse around LLMs. It is paramount to pursue new approaches to provide transparency\\nfor LLMs, and years of research at the intersection of AI and human-computer interaction (HCI)\\nhighlight that we must do so with a human-centered perspective: Transparency is fundamentally\\nabout supporting appropriate human understanding, and this understanding is sought by different\\nstakeholders with different goals in different contexts. In this new era of LLMs, we must develop\\nand design approaches to transparency by considering the needs of stakeholders in the emerging\\nLLM ecosystem, the novel types of LLM-infused applications being built, and the new usage\\npatterns and challenges around LLMs, all while building on lessons learned about how people\\nprocess, interact with, and make use of information. We reflect on the unique challenges that\\narise in providing transparency for LLMs, along with lessons learned from HCI and responsible AI\\nresearch that has taken a human-centered perspective on AI transparency. We then lay out four\\ncommon approaches that the community has taken to achieve transparency—model reporting,\\npublishing evaluation results, providing explanations, and communicating uncertainty—and call\\nout open questions around how these approaches may or may not be applied to LLMs. We hope\\nthis provides a starting point for discussion and a useful roadmap for future research.\\n1 Introduction\\nHugely powerful large language models (LLMs) like GPT-4, LaMDA, and LLaMA are now being\\ndeployed in applications from search engines to code generation tools to productivity suites. These\\ngenerative models are widely expected to have impact across industries, changing the way we engage in\\ntasks like writing, programming, and design, and reshaping occupations in medicine, law, marketing,\\neducation, and beyond [3, 22, 40, 41, 52, 113]. As the chair of the U.S. Federal Trade Commission put\\nit in a recent op-ed, “the full extent of generative AI’s potential is still up for debate, but there’s little\\ndoubt it will be highly disruptive” [95].\\nWhile the capabilities of LLMs are impressive, they also raise new risks [16, 106, 151, 196]. Language\\nmodels are found to encode biases [2, 157], which risks propagating harmful discrimination, stereotypes,\\nand exclusion at scale. They are widely known to “hallucinate” information [16, 87, 124, 138], produc-\\ning outputs that are plausible—even convincing—but incorrect. They may project confidence about\\nthese hallucinated outputs, potentially contributing to automation bias, overreliance, or automation-\\ninduced complacency [153, 197]. LLMs can generate harmful, sometimes toxic content, including\\nhate speech and offensive language [16, 195], or reveal sensitive information that threatens privacy or\\nsecurity. They can contribute—both intentionally and unintentionally—to the spread of misinforma-\\ntion [28, 103, 209]. And in the longer term, LLMs may lead to environmental harms [16] as well as\\nsocioeconomic harms, including the displacement and deskilling of workers across industries [195].\\nGiven the anticipated impact that LLMs will have on both our day-to-day lives and society at large,\\nit is critical that LLMs and LLM-infused applications be developed and deployed responsibly. One\\n∗equal contribution\\n1arXiv:2306.01941v2  [cs.HC]  8 Aug 2023 central component of responsible AI development and deployment is transparency : enabling relevant\\nstakeholders to form an appropriate understanding of a model or system’s capabilities, limitations,\\nhow it works, and how to use or control its outputs. Developers of LLMs cannot debug their models,\\nresponsibly assess whether they are ready to launch, and enforce responsible and safe usage policies\\nfor their models without some understanding of their behavior and performance on different tasks.\\nBusiness decision-makers, designers, and developers building LLM-infused applications must be able\\nto understand the LLM’s capabilities and limitations in order to ideate and make decisions about\\nwhether, where, and how to use the model—potentially including how to fine-tune, prompt, or other-\\nwise adapt the model to better fit their use case. End-users must be able to form a sufficiently accurate\\nunderstanding of LLM-infused applications to control the application’s behavior and achieve appro-\\npriate levels of trust and reliance. People impacted by LLMs or LLM-infused applications may require\\ntransparency in order to understand their options for recourse. Additionally, given the speed at which\\npowerful new LLMs and their applications are being released and the growing concerns over potential\\nharms, we should expect to see an increased demand for transparency around their development and\\ninner workings from policymakers and third-party auditors aiming to regulate and oversee their use.\\nIn recent years, we have witnessed the creation of a whole research field at the intersection of AI and\\nhuman-computer interaction (HCI) that is focused on developing and evaluating different approaches\\nto achieve transparency. These approaches range from frameworks for documenting models and the\\ndatasets they are trained on [e.g., 6, 15, 38, 58, 76, 143] to techniques for producing explanations\\nof individual model outputs [e.g., 102, 130, 161, 163, 184] to approaches for communicating uncer-\\ntainty [e.g., 19, 43, 191] and beyond. There is no one-size-fits-all solution. In the case of LLMs, the\\nneeds of an application developer engaging in ideation are probably different from those of a writer\\nwho is using an LLM-infused application to edit a novel or a public figure who is concerned about how\\ntheir life is presented by an LLM-infused search engine. In our own work [118, 188], we have argued\\nfor the importance of taking a human-centered perspective on transparency—designing and evaluating\\ntransparency approaches with stakeholders and their goals in mind. We believe that this is even more\\nimportant in the era of LLMs, when the diversity of stakeholders and their experience levels, contexts,\\ngoals, and transparency needs, is greater than ever.\\nIn this paper, we map out a human-centered research roadmap for transparency in this new era.\\nWe first reflect on the unique challenges that arise in providing transparency for LLMs compared\\nwith smaller-scale, more specialized models that have traditionally been the focus of AI transparency\\nresearch. We reflect on lessons learned from HCI and Responsible AI/FATE (fairness, accountability,\\ntransparency, and ethics) research that centers on human needs of, interactions with, and impact from\\nAI transparency. We then lay out common approaches, including techniques and artifacts, that the\\ncommunity has taken to achieve transparency and call out open questions around how they may or\\nmay not be applied to LLMs.\\nWe note that there is no agreed-upon definition of transparency, and indeed, transparency has been\\nrecognized as a multi-faceted concept. In this paper, we adopt a focus on informational transparency—\\nessentially, what information about a model (or system building on that model) should be disclosed\\nto enable appropriate understanding—which has been emphasized within the machine learning (ML)\\nresearch community and in industry practice, though we note that there are other perspectives, such as\\nthe normative, relational, and social dimensions of transparency, that have been studied in the broader\\nliterature [53, 139]. Some of the approaches we cover, such as model reporting, are primarily aimed\\nat supporting a functional understanding of what the model (or system) can do, often by exposing\\nthe goals, functions, overall capabilities, and limitations. Others, like the explanations frequently\\nexplored in the explainable AI (XAI) and interpretable ML communities, are primarily aimed at\\nsupporting a mechanistic understanding of how the model (or system) works, by disclosing the parts\\nand processes [128]. We believe that both understandings play important roles and the appropriate\\nform of transparency in any given context will depend on the stakeholder and the goal that they wish\\nto achieve.\\nFinally, we note that many of the challenges, lessons learned, potential approaches, and open\\nproblems that we call out in this paper apply not only to LLMs but to other large-scale generative\\nmodels, including multimodal models that allow for both textual and visual input or output. While\\nwe adopt the narrower focus on LLMs for simplicity, we encourage additional research on transparency\\nfor these other models.\\n2 2 What Makes Transparency for LLMs Challenging?\\nTo ground the discussion in the remainder of the paper, we first explore the unique characteristics of\\nLLMs and the emerging patterns of their usage that are likely to make it more challenging to achieve\\ntransparency compared with the smaller-scale, specialized models that AI transparency research has\\ntraditionally dealt with. We start by providing some brief background on LLMs and establishing some\\nterminology that we will use in the rest of the paper.\\n2.1 Background on LLMs\\nAn LLM, like any language model, predicts the conditional probability of a token—which might be a\\ncharacter, word, or other string—given its preceding context and, in the case of bidirectional models,\\nits surrounding context [17, 156]. Present-day LLMs are based on modern neural network self-attention\\narchitectures like the transformer [187] with hundreds of billions or even more than a trillion parame-\\nters [56]. While earlier models were trained on datasets of moderate size, LLMs are trained on datasets\\nof massive scale, with hundreds of billions or even more than a trillion tokens [23, 75], requiring many\\norders of magnitude more compute time. This makes LLMs vastly more sophisticated and expressive\\nthan their predecessors.\\nWhile a basic pre-trained LLM model can be viewed as a “general-purpose” next-word predictor,\\nLLMs can be adapted to exhibit or suppress specific behaviors or to perform better on specific tasks like\\ntext summarization, question answering, or code generation. One common approach is fine-tuning, in\\nwhich the model’s parameters are updated based on additional, specialized data [e.g., 42, 79, 111, 155].\\nA popular technique for fine-tuning is reinforcement learning from human feedback (RLHF) in which\\nhuman preferences are used as a reward signal [36, 152]. Another approach is prompting or prompt\\nengineering, in which natural-language prompts—often containing examples of tasks (for few-shot\\nprompting/in-context learning) or demonstrations of reasoning (for chain-of-thought prompting)—\\nare provided to the model to alter its behavior without making any changes to the model’s internal\\nparameters [e.g., 26, 126, 169, 194]. The adapted model can be incorporated into applications such as\\nchatbots, search engines, or productivity tools. Models can also be augmented with the ability to call\\non external models, tools, or plugins [141], for example, querying an information retrieval system to\\nground their output or controlling and receiving feedback from a physical robot.\\nIt is important to note that the party adapting the model or building the application is frequently\\nnot the same party who built the underlying pre-trained LLM, and may only be able to access the LLM\\nthrough an API. A model may also be adapted more than once by different parties; for instance, a base\\nmodel may be fine-tuned using RLHF by its creators, fine-tuned on domain-specific data by application\\ndevelopers, and then adapted via in-context learning by end-users. When we talk about transparency,\\nwe must keep in mind whether we are referring to transparency about the pre-trained LLM, an adapted\\nLLM, or the application using the pre-trained or adapted model (LLM-infused application). We aim\\nto call out which of these we are referring to when it is not clear from the context.\\n2.2 Challenges for Achieving Transparency\\nThere are several characteristics of LLMs and their usage that pose challenges for transparency. The\\nlist we lay out here is not meant to be exhaustive, but to provide context for later discussion.\\nComplex and Uncertain Model Capabilities and Behaviors. LLMs can perform an astonish-\\ningly wide variety of tasks in different contexts [22]. Unlike classical machine learning models where\\nthere is typically a well-defined structure of inputs and outputs, LLMs are more flexible. The capabil-\\nities of LLMs—sometimes also referred to as use cases [152] or tasks [116] in the literature—include\\nquestion answering, dialogue generation, sentence completion, summarization, paraphrasing, elabo-\\nration, rewriting, classification, and more. Researchers are now additionally identifying “emergent\\ncapabilities” of LLMs—like performing arithmetic or chain-of-thought reasoning—that are not present\\nin smaller-scale models but emerge at scale [193]. Furthermore, as described above, the precise behav-\\nior and capabilities of an LLM can be steered through approaches like fine-tuning and prompting. All\\nof this contributes to “capability unpredictability” [56], the idea that an LLM’s capabilities cannot be\\nfully anticipated, even by the model’s creators, until its behavior on certain input is observed.\\n3 Additionally, present-day LLMs exhibit unreliable behaviors. Their responses can change with\\nupdates, the details of which are often not made transparent by LLM providers. Depending on the\\nsampling strategy used [77], outputs can be non-deterministic in the sense that the same prompt leads\\nto a different response when input to the model again. They can misinterpret a prompt in unpredictable\\nways and respond inconsistently to a type of prompt, making the behavior of adapted models difficult\\nto predict. These unreliable behaviors can make it challenging, if not impossible, to gain a generalized\\nunderstanding of the model’s behavior.\\nMassive and Opaque Architectures. Given the complexity and massive scale of present-day\\nLLMs, there are currently no techniques that would provide us with a complete picture of the knowledge\\nreflected in a model or the reasoning that is used to produce its output [24]. The mechanism of the\\ntransformer architecture underpinning LLMs is yet to be fully understood, even among experts, and\\nsome techniques that initially appear promising for interpreting the behavior of LLMs, such as looking\\nat attention weights or perturbing inputs, can be misleading [21, 54, 84]. A more unique challenge with\\nLLMs is the massive scale of the training data and diverse sources from which it is pulled—for example,\\nCommon Crawl and Wikipedia, often with no specific topics or formats targeted, and no thorough\\ndocumentation about how the dataset was developed [96]. This makes it challenging, if not impossible,\\nto understand what went into an LLM’s training. There are currently no established answers, even in\\nthe research community, to questions such as precisely how and why these models work as well as they\\ndo, why they can or cannot perform certain tasks, and how characteristics of the training data impact\\nmodel capabilities. Furthermore, the sheer size can make it challenging to develop and operationalize\\ntransparency approaches (e.g., due to contraints of method scalability or computing resource).\\nProprietary Technology. An elephant in the room that will inevitably inhibit attempts at trans-\\nparency for LLMs is the proprietary nature of the models. Currently, while the efforts for developing\\nopen-source LLMs are growing [166, 181, 182], most of the powerful LLMs were developed at large\\ntechnology companies or other non-academic organizations. They are either released through APIs or\\ncompletely proprietary, making it impossible to access their inner workings (e.g., weights and parame-\\nters). In many cases, details such as the size, make-up and provenance of the training data, the number\\nof parameters, and the resources required to train the model are also not shared publicly. In essence,\\nthen, such models can only be probed in a black-box manner, which may not be sufficient to meet\\nthe transparency requirements for stakeholders, and poses challenges for the research community to\\ndevelop transparency approaches. Addressing this fundamental challenge may not be possible without\\npolicy and regulatory efforts that enforce transparency requirements on LLM creators and providers.\\nNew and Complex Applications. End-users may not interact with LLMs directly, but rather\\nthrough LLM-infused applications. Emerging applications include general and specialized chatbots,\\nweb search, programming assistants, productivity tools such as for writing support or presentation\\ngeneration, and text analysis tools such as for customer insights discovery. As LLMs’ capabilities\\ncontinue to be discovered, we can only expect the number and variety of LLM-infused applications\\nto grow. While any opacity of the model will likely trickle down to hinder the transparency of the\\napplications built on them, as mentioned above, the transparency requirements for LLM-infused ap-\\nplications will be different from the model as they serve a different set of stakeholders. Furthermore,\\njust as the models themselves are flexible, the use cases for LLM-infused applications can be flexible\\nand open-ended. For example, an LLM-infused search engine may be used to plan a trip, research\\na report, or write a poem—use cases that reflect different needs in terms of accuracy, verifiability of\\noutput, and likely the required transparency approaches.\\nTo further complicate transparency around LLM-infused applications, such applications may not\\nbe built on a single LLM, but may involve many interacting models and tools. For example, auxiliary\\nLLMs can be used to augment the output or expand the capabilities of a primary LLM. LLMs can\\nbe embedded in a complex system to operate other models or external services, for example through\\nplugins, allowing them to perform tasks like ordering groceries or booking flights with no human in the\\nloop. An application may also include other components like input or output filters. For example, an\\nLLM-infused search engine may rely on results obtained from a traditional search engine to “ground”\\nits responses. Changes to any component can change the behavior of the application, making it more\\n4 difficult to understand its behavior. Approaches to transparency must therefore take into account all\\ncomponents and how they fit together rather than focusing on an LLM in isolation.\\nExpanded and Diverse Stakeholders. As the number of LLM-infused applications grows and\\npopular applications such as LLM-infused search engines expand their user bases, a larger number\\nof people—diverse along many dimensions—will interact with or be impacted by LLMs. Research\\nin AI transparency typically considers stakeholder groups like data scientists and model developers,\\nbusiness decision-makers, regulators and auditors, end-users, and impacted groups (i.e., the people\\nwho are directly or indirectly affected by a model or application) [78, 118, 188]. The use of LLMs\\nmay introduce new stakeholder groups with unique transparency needs. For example, it is increasingly\\ncommon for product teams to have dedicated prompt engineers—a role that, until recently, did not\\nexist—to streamline tasks, evaluate models, or contribute to model adaptation. As another example,\\nas LLMs are increasingly used for productivity support to augment people’s writing, we must consider\\nboth the creators of LLM-assisted articles and the consumers of these articles as “users” of the LLM’s\\noutputs and support both groups’ transparency needs. Meanwhile, we must support any subjects\\nreferred to in the articles as “impacted groups.”\\nAs the pre-trained nature of LLMs lowers the barrier to using and building on AI capabilities, we\\nbelieve application developers—including those working on model adaptation—will become a signif-\\nicant group and diverse in itself, potentially including developers, entrepreneurs, product managers,\\ndesigners, or essentially anyone. In some cases, the line between application developers and end-users\\nmay be blurred. Consider, for example, a writer who experiments with using an LLM for writing sup-\\nport. This writer might benefit from model transparency to assess the LLM’s suitability for different\\nwriting tasks and identify effective ways to adapt the model for each task.\\nRecent research has begun to inquire about the ecosystem of LLMs and the roles in it [22], from\\ndata creation, curation, model training, and model adaptation through to deployment. Identifying\\nthese LLM stakeholder roles and supporting their role-, task- and context-specific transparency needs\\nwill be of primary importance for the AI transparency research community.\\nRapidly Evolving and Often Flawed Public Perception. Effective approaches to transparency\\nshould take into account the receivers’ existing perception of what the model or system can or cannot\\ndo, and how it works—often referred to as their mental model [60, 89, 149]. This is especially challeng-\\ning for LLMs as their public perception is still evolving and shaped by complex mechanisms including\\nmass media, marketing campaigns, ongoing events, and design choices of popular LLM-infused appli-\\ncations. The natural language modality also contributes to a unique set of challenges: people may be\\nmore likely to assign human-like attributes to the model and have corresponding expectations [148],\\nand even subtle language and communication cues can have profound impact on people’s mental\\nmodel [1]. Recent studies show that people already have flawed mental models about LLMs, such\\nas incorrect perceptions of how their output differs from human-written texts [85]. Interacting with\\nLLMs with a flawed mental model can lead to misuse, unsafe use, over- and under-reliance, deception,\\nprivacy and security threats, and other interaction-based harms [195]. Flawed public perceptions can\\nbe attributed to a lack of accurate, comprehensive, and responsible information. In addition to incor-\\nporating transparency approaches, the organizations creating LLMs and LLM-infused applications and\\nthe research community more broadly should reflect on the implications of the way they communicate\\nwith the public. For example, the use of ill-defined, catch-all phrases such as “general-purpose model”\\nor inappropriate anthropomorphizing may hinder accurate public perception of LLMs.\\nOrganizational Pressure to Move Fast and Deploy at Scale. Lastly, we note that there are\\norganizational challenges that may hinder the development and adoption of transparency approaches\\nbeyond the proprietary nature of LLMs. Responsible AI efforts are often in tension with pressures to\\nrelease products quickly and to scale up across geographies, use cases, and user bases [133, 134, 159],\\na kind of “scale thinking” [72]. Given the speed at which research and product breakthroughs are\\noccurring and the vast financial stakes, companies are incentivized to move at a pace that is unusual to\\nwitness even in the technology industry to be first to market—what some media outlets have dubbed an\\n“AI race” or “AI arms race” [35, 65]. The organizations building LLMs and LLM-infused applications\\nwill need to take extra steps to ensure that transparency and other responsible AI considerations\\n5 are not lost in the process, which may require enhanced internal governance or external regulatory\\nrequirements in addition to organizational incentives for the individuals working in this space.\\n3 What Lessons Can We Learn from Prior Research?\\nIn this section, we reflect on lessons from the HCI and Responsible AI/FATE research communities,\\nwhich tend to take a human-centered perspective on transparency. While many technical transparency\\napproaches (to be discussed in the next section) have been developed to deal with what information\\nabout the model can be disclosed, the human-centered perspective focuses on how people useand\\ncognitively process transparency information. Knowledge from this human-centered perspective should\\ndrive the development of transparency features, which concern not only the model-centered techniques\\nbut also the interfaces.\\n3.1 Transparency as a Means to Many Ends: A Goal-Oriented Perspective\\nWithin the HCI community, researchers have attempted to guide the development and evaluation of\\ntransparency approaches by digging into the reasons why people seek information [109, 176]. This goal-\\noriented perspective resonates with studies of human explanations from the social sciences [129, 142],\\nwhere it is recognized that seeking explanations and achieving understanding is often a means to an\\nend for downstream cognitive tasks like learning, decision-making, trust development, and diagnosis.\\nThis goal-oriented perspective has led to works developing taxonomies of common goals that people\\nseek explanations for [119, 120, 176] and empirical studies to delineate common transparency goals of\\nstakeholders groups such as data scientists [18, 78] and designers [121]. By focusing on the goals (as\\nopposed to the low-level application or interface types), this perspective provides a useful level of ab-\\nstraction to consider people’s different transparency needs according to their usage of the information.\\nFor example, Suresh et al. [176] lay out a set of common goals that people seek AI explanations for,\\nincluding improving a model, ensuring regulatory compliance, taking actions based on model output,\\njustifying actions influenced by the model, understanding data usage, learning about a domain, and\\ncontesting model decisions.\\nThe goal-oriented perspective also has several practical implications for developing human-centered\\ntransparency approaches. First, whether a transparency approach is effective should be evaluated by\\nwhether it successfully facilitates a stakeholder’s end goal. This means that not all situations require the\\nsame level of transparency (e.g., a low-stakes application such as generating poetry for fun may require\\nlittle transparency). This also requires articulating end goals up front in order to choose criteria for\\nevaluating transparency approaches. As an example, in our own work with collaborators, we focused on\\ndata scientists’ goal of model debugging and evaluated two common techniques from the interpretable\\nmachine learning literature in terms of how well they help data scientists identify common problems\\nwith training datasets and the resulting models, finding evidence that the techniques may hamper the\\ndebugging goal by leading to over-trust or over-confidence about the model [91]. Second, achieving an\\nend goal may require information beyond details of the model, such as information about the domain\\nand the social-organizational context the model is situated in [44], and hence require holistic support\\nwith information tailored to the task at hand and integrated into the application interface.\\nWhat are the new transparency goals for LLMs? The new ecosystem and novel applications\\nof LLMs call for investigations into what are the new types of common stakeholder goals that require\\ntransparency. For example, there may be heightened needs for supporting ideation, model adaptation,\\nprompting, and discovering risky model behaviors. New transparency approaches for LLMs should be\\ndeveloped and evaluated in terms of how well they help achieve these goals.\\n3.2 Transparency to Support Appropriate Levels of Trust\\nAlthough transparency has often been embraced within the tech industry as a mechanism to build\\ntrust, recent HCI research has taken the position that transparency should instead aim to help people\\ngain an appropriate level of trust [11]—enhancing trust when a model or application is trustworthy,\\nand reducing trust when it is not. While relevant to many use cases of transparency, achieving an\\n6 appropriate level of trust is especially critical for end-users to harness the benefits of AI systems\\nwithout over-relying on flawed AI outputs.\\nEmpirical studies on the relationship between transparency and user trust have painted a complex\\npicture. In particular, a wave of HCI studies repeatedly showed that AI explanations can lead to\\noverreliance—increasing people’s tendency to mistakenly follow the AI outputs even when they are\\nwrong [10, 34, 154, 192, 208]. Understanding this pitfall of AI explanations requires paying attention\\nto people’s cognitive processes. Researchers have attributed this difficulty to detect model errors\\nfrom popular forms of AI explanations to their complexity and incompatibility with people’s reasoning\\nprocess [34], as well as to the heuristics and biases that people bring into their cognitive processes, such\\nas an inclination to superficially associate an AI system being explainable with it being trustworthy [45,\\n117]. Studies of other transparency approaches have also reported nuanced results [160, 167, 204, 208].\\nFor example, while one study demonstrates that communicating uncertainty is more effective than\\nproviding explanations in supporting appropriate trust [208], another study suggests that people’s\\ntrust level is more likely to be dominated by aggregate evaluation metrics such as accuracy [160].\\nWhich approaches to transparency can best support appropriate trust of LLMs and how?\\nThere is a need to disentangle the relationship between trust and transparency for LLMs through both\\nbetter conceptualization and careful empirical investigations. For the former, recent FATE literature\\nhas begun to unpack trust as a multi-faceted and multi-loci concept [83, 117]. For LLMs, people’s locus\\nof trust can be at the base model, the LLM-infused application, the application provider (e.g., based\\non brand), or specific application functions or types of outputs, each of which may require different\\nkinds of transparency support but also be intertwined with other loci. For example, people need to\\nunderstand and reconcile that LLMs are powerful technologies but may not be used reliably for a\\ncertain application function. For empirical investigations, there is extensive literature on measuring\\ntrust on which to build [189], though it remains a challenge in practice [11], and even more so with the\\ncomplex, dynamic, and multi-loci nature of trust around LLMs. Furthermore, evaluating appropriate\\ntrust requires further unpacking the actual “trustworthiness” of a model or system and what counts\\nas “appropriate,” both of which remain open questions for LLMs.\\n3.3 Transparency and Control Often Go Hand-in-Hand\\nMany of the end goals we discussed in Section 3.1, such as improving or contesting the model and\\nadapting data usage, can only be achieved by having both a good understanding of the model and\\nappropriate control mechanisms through which to take action. Indeed, transparency and control have\\nlong been studied together in HCI as intertwined design goals for effective user experience [112, 201].\\nThis is well reflected in the interdisciplinary area of interactive machine learning (iML) [4]—learning\\ninteractively through feedback from end-users—and related areas such as machine teaching [31, 170].\\nThese paradigms simultaneously ask what information about a model should be presented to users and\\nwhat forms of input or feedback users should be able to give in order to steer the model. We believe\\ncurrent work on training, adapting, and building applications around LLMs can take valuable lessons\\nfrom thes lines of research. More recent HCI studies on algorithmic transparency also highlight that\\nproviding transparency without supporting control leaves users frustrated, while effective, efficient,\\nand satisfying control cannot be achieved without transparency [171, 174]. More critically, scholars\\nhave called out the risk of algorithmic transparency without paths for actionability and contestability\\nas creating a false sense of responsibility and user agency [5, 100, 131].\\nHow can different approaches to transparency contribute to better control mechanisms\\nfor LLMs? While safety and control have become central topics in research and practices around\\nLLMs [94, 115], the role of transparency is less emphasized. We encourage the community to consider\\nthe role of transparency in establishing better mechanisms for control and enabling more participatory\\nand inclusive approaches that allow stakeholders to understand and then steer LLM behavior.\\n3.4 The Importance of Mental Models\\nPeople’s existing understanding of a system impacts what information they seek for transparency\\nand how they process the information. This is often studied in HCI work through the concept of a\\n7 mental model—one’s internal representation of a system based on their experiences, whether direct\\nor indirect, with the system. A good mental model should be both accurate and complete, as it is\\nthe foundation for effective, efficient, and satisfying interactions with a system [150]. HCI research\\nalso differentiates between a functional (shallow) mental model—knowing what a system can be used\\nfor and how to use it—and a structural (deep) mental model—knowing how and why the system\\nworks [105]. Transparency approaches for functional and mechanistic understandings can be seen\\nas supporting these two aspects of mental models, respectively. However, since mental models are\\nshaped by continuous interactions with a system, some researchers have argued that notions like the\\n“interpretability” of an AI system need to be considered as evolving through dynamic and situated\\nsystem interactions rather than considered in the context of a single intervention like the introduction\\nof documentation or explanations [180].\\nWe highlight several ways that transparency approaches should consider people’s mental models.\\nFirst, transparency approaches should be designed to support different stakeholders in building a\\ngood mental model. It may therefore be appropriate for evaluations of transparency approaches to\\nincorporate assessments of mental model accuracy and completeness, for example by analyzing people’s\\ncomments or answers to questions about their beliefs about a model or application’s function and\\nstructure [48, 61, 67, 105]. Second, transparency approaches should account for people’s existing mental\\nmodels, and focus on closing the necessary gaps to allow them to achieve their end goal [46]. This means\\nthat approaches to transparency should avoid conveying redundant information that people already\\nhave in their mental models, but more importantly, aim to correct flawed mental models. However,\\nit is known that a mental model, once built, is often difficult to shift even if people are aware of\\ncontradictory evidence [199], which may present a significant challenge for transparency approaches to\\nbe effective. This highlights the importance of responsible communication (e.g., in marketing material\\nand media coverage) to accurately shape the public perception around new technologies like LLMs.\\nIn addition, Norman [150] noted that people’s mental models are often incomplete, unstable, have\\nunclear boundaries (e.g., mixing up different parts of the systems), and favor simple rules, all of which\\nmay pose challenges for transparency approaches to help people build an appropriate understanding.\\nHow can we unpack people’s mental models of LLMs and support forming better mental\\nmodels? Just as it is difficult to characterize the capabilities and limitations of LLMs given\\ntheir scope and capability unpredictability, it is difficult to characterize people’s mental models of\\nthem. More research is also needed to understand the general mental models that people already\\nhave of LLMs, especially in response to their unique characteristics such as human-like language\\ncapabilities and unreliable behaviors (e.g., hallucinating and non-deterministic output). Moreover, HCI\\nresearch has traditionally dealt with mental models at the system level, while people’s mental models\\nof an LLM-infused application could be muddled by the blurred boundaries between the pre-trained\\nmodel, the adapted model(s) used in the application, and the application itself. While it remains\\ncritical for transparency approaches to aim to correct flawed mental models and build accurate and\\ncomplete mental models, the field may need foundational work on how to characterize, assess, and\\noffer opportunities to build and shift mental models of LLMs.\\n3.5 How Information is Communicated Matters\\nHCI research on AI transparency is often concerned with not only what information to communicate\\nabout a model, but how to communicate it. Work has explored ways of communicating performance\\nmetrics [63], explanations of model outputs [71, 107, 177], and uncertainty estimates [92], as well\\nas how to frame the model’s output itself in order to appropriately shape people’s mental model\\n(e.g., whether to use certain terms like “risk” [66]). Such information can be communicated through\\ndifferent modalities (e.g., by a visualization or in natural language), at different levels of precision\\nor abstraction, framed using different language, supplemented with different information to close any\\ngaps in understanding, and through various other visual and interactive interface designs. These\\nchoices of communication design can significantly impact how people perceive, interpret, and act on\\nthe information provided.\\nAn effective design should be guided by the ways that people process information cognitively and\\nsocially. For example, a line of HCI research explored more user-friendly visualization designs to\\novercome the trouble that people often have understanding statistical uncertainty and the cognitive\\n8 biases they bring [55, 92]. In light of the difficulty of reasoning about the complex explanations\\nproduced by some AI explainability techniques, HCI research has explored how to present explanations\\nin more human-compatible ways [71, 177]. In our recent work with collaborators [107], we argue that\\npeople engage in two processes to produce explanations [137]: an information-gathering process in\\nwhich they come up with a set of reasons, and a communication process to present reasons, often\\nselectively tailored to the recipient. Explainability techniques that focus on revealing the inner workings\\nof a model are typically only concerned with the former. We then propose a framework to tailor\\nthese explanations by learning the recipient’s preferences as a selective communication strategy, and\\nempirically demonstrate that these selected explanations are easier to process and better at helping\\npeople detect model errors in an AI-assisted decision-making task.\\nWhat are the new opportunities and challenges for communicating information during\\ninteractions with LLMs? The natural language modality of LLMs has significant implications\\nfor the communication aspect of transparency. For example, instead of presenting a numerical score\\nfor uncertainty, LLM-infused applications like chatbots can express uncertainty by using hedging lan-\\nguage or refusing to answer a question. This behavior can now be built into the adapted model directly\\nthrough fine-tuning or prompting [125], making it potentially harder to precisely control and interpret\\nthe communication. Meanwhile, as decades of HCI research on chatbots and conversational interfaces\\nsuggest, people’s perceived utility of these technologies can be shaped by a wide range of communi-\\ncation, social, and linguistic behaviors such as how the agents introduce and clarify their capabilities,\\ntake initiatives, repair errors, and respond to chit-chat requests, and even their language style [e.g.,\\n7, 8, 110]. We believe more research is needed to distill principles to effectively communicate neces-\\nsary information about the model’s capabilities, limitations, and mechanisms during natural language\\ninteractions, as well as to establish reliable approaches for LLMs to follow these principles.\\n3.6 Limits of Transparency\\nLast but not least, we call attention to some critiques on the limits of transparency offered by FATE\\nand STS scholars [5, 101, 122]. First, related to several arguments throughout the paper, model-\\ncentric transparency without ensuring human understanding or meaningful effects on people’s end-\\ngoals (“seeing without knowing” [5]) loses its purpose, and worse, can create a false sense of power and\\nagency. Second, transparency can be misused to shift accountability and place burdens on users, and\\ncan even be used to intentionally occlude information. Those users without the necessary technical\\nbackground and training to make sense of the provided information may face higher burdens. This is\\na warning to the field to pay attention to the consumability of transparency approaches and to seek\\nalternative paths to ensure accountability. Lastly, transparency approaches can lead to harms if used\\nmaliciously or inappropriately. In addition to the risk of exploiting user trust and reliance, they can\\nalso threaten privacy and security.\\nWhen is transparency not enough, and what else do we need? More research is needed to\\nunderstand the limits of transparency for LLMs and how to properly hold the organizations building\\nand deploying LLMs and LLM-fused applications accountable. The latter may require policy and\\nregulatory changes, in addition to new approaches for external auditing [144].\\n4 What Existing Approaches Can We Draw On?\\nThe ML and HCI research communities have explored a variety of approaches to achieving trans-\\nparency, including model and data reporting, publishing the results of evaluations, generating ex-\\nplanations, and communicating uncertainty. In this section, we briefly review these approaches and\\nexplore the extent to which they may or may not be applicable in the context of LLMs, while calling\\nout needs specific to stakeholders of LLMs and open questions that arise. We note that, while we focus\\non these four categories by building on existing approaches, we also encourage the research commu-\\nnity to explore new categories of approaches that can help people achieve functional and mechanistic\\nunderstandings of LLMs. For example, along the way, we suggest areas such as tools to support model\\ninterrogation and communicating output-specific risk and safety concerns.\\n9 4.1 Model Reporting\\nDocumentation has become a building block for responsible AI in industry practice. Standardized\\ndocumentation frameworks have been proposed to encourage both reflection and transparency around\\nmodels [38, 143], AI services [6], and training and evaluation datasets [15, 58, 76]. For example, the\\nmodel cards framework [143], a popular framework for model reporting that has been adopted by\\ncompanies like Google and HuggingFace, specifies comprehensive information that should be reported\\nabout a model, including a description of its inputs and outputs, the algorithm used to train it, the\\ntraining data, additional development background, the model’s intended uses, and ethical considera-\\ntions. The framework emphasizes the inclusion of quantitative model evaluation results (more on that\\nin the next section), including disaggregated evaluations [12], in which results are broken down by\\nindividual, cultural, demographic, or phenotypic groups, domain-relevant conditions, and intersections\\nof multiple groups or conditions. Disaggregated evaluation can help identify fairness issues, and also\\nassist stakeholders in identifying when or where the model is suitable or reliable to use. In short, good\\ndocumentation can help stakeholders who are building on a model or dataset assess its suitability for\\ntheir purpose and avoid misuse. It can also provide the necessary context for end-users, impacted\\ngroups, regulators, and auditors to understand how models and systems are being built and deployed.\\nWhile celebrated as an approach to providing transparency, creating good documentation remains\\nchallenging in practice. In our prior work with collaborators, we found that practitioners tasked with\\ndocumenting a dataset they worked with struggled to make the connection between the information\\nthat they were asked to include and its implications for responsible AI, were unsure of the appropriate\\nlevel of detail to include and who the target audience was, and in some cases were uncertain about\\nwhat even counts as a dataset [73]. Some stakeholders also struggle to consume existing forms of\\ndocumentation. For example, designers or analysts without formal training in machine learning can find\\nstandard documentation to be too technical, and the lengthy textual format to be cumbersome [38, 117].\\nWhat information is needed to characterize the functional behavior of an LLM? In prin-\\nciple, existing model reporting frameworks could be applied as-is to LLMs. However, some of the\\ninformation categories in a standard model card would be difficult to pin down due to the “general-\\npurpose” positioning of LLMs and the uncertainty surrounding their capabilities. Even providing\\nbasic details such as what the input and output spaces of an LLM or LLM-infused application are, and\\nthe mapping between inputs and outputs, can be an elusive task. Currently, it is common for LLM\\nproviders to instead provide a description of intended use cases (like “summarization” or “creative and\\ncollaborative writing”) or demonstrations of example prompts and responses. While this information\\ncan be a useful component of model reporting, it can also be misleading or, in some cases, even decep-\\ntive, since cherry-picked examples can shape user and public perception in a skewed way. This raises\\nquestions about how these examples should be selected and who should select them.\\nWhile we elaborate on the issues with performance reporting in the next section, we call out two\\nother important categories in the model cards framework that are currently missing or incomplete\\nfor most LLMs: training data and development background. Besides the incentive for organizations\\nto keep this information proprietary, we must recognize that there are open questions about how to\\nprovide such information given the complexity of LLMs and unique aspects of their training processes.\\nFor data transparency, as discussed in Section 2, the datasets used to pre-train base models are\\nunprecedentedly massive in scale and pulled from diverse sources. Conveying their full scope and\\nmake-up is impossible, but there may be ways of distilling the most critical characteristics of these\\ndatasets to provide a basic understanding of what goes into the models. Different issues arise when\\nconsidering the datasets used for model adaptation. For example, as companies engage in user data\\ncollection for the purpose of fine-tuning models, they must pay due diligence to the transparency of\\ntheir user data handling, including privacy.\\nFor development background, besides standard information such as the choice of algorithms, ar-\\nchitecture, and parameters, LLM providers should include additional details on the training process.\\nFor example, an emerging practice is for LLM development to include some sort of “alignment” effort\\nto make the model more usable or safe (e.g., producing less toxic or harmful content). This can be\\ndone using human feedback through RLHF [152] or by having the model critique itself based human-\\nspecified rules or principles [9]. Given that LLM’s behaviors can be governed by these alignment\\nefforts, it is especially important to make them transparent to allow the public and regulatory bodies\\nto understand, scrutinize, and iterate on them.\\n10 What do different (and new) types of stakeholders need from model reporting frame-\\nworks? In light of the lessons discussed in Section 3, we recommend more research on the funda-\\nmental question of what different stakeholders want to know—and what they should know—about the\\nmodel, along with a careful examination of how different forms of information shape their perception\\nand usage of LLMs. As LLMs change the ML product development and deployment lifecycles, we may\\nneed to revisit the positioning of model reporting and consider new types of frameworks that address\\nthe specific needs of new stakeholder groups. For example, as discussed above, the LLM ecosystem\\nintroduces a new stage of model adaptation through fine-tuning, prompting, or other techniques. This\\nadaptation may be performed by the original model builder, an application developer, or in some cases,\\ndirectly by end-users. To date, there has been little or no research on these stakeholders’ transparency\\nneeds when adapting the model, or about how they should transparently convey information about\\nmodel adaptations to other parties.\\nWhat is needed beyond static documentation? Lastly, we call out that model reporting should\\nnot be limited to static, textual documentation or a basic “card” format. Any formats or features that\\nprovide functional information about the model and shape people’s understanding can contribute\\nto model reporting. These may include FAQ pages, landing or onboarding pages, or even media\\ncommunication describing the model. All such features can benefit from standardization and, where\\nappropriate, regulation.\\nFollowing recent HCI and FATE studies investigating how to design effective documentation inter-\\nfaces [38, 121], we suggest that those designing model reports for LLMs should explore more interactive\\nfeatures. For example, prior works have explored interfaces for uploading, customizing and slicing in-\\nput data to generate customized reports and visualize input-output spaces. Interactive interfaces are\\nparticularly suitable for LLMs for several reasons. First, interactive features can better support in-\\nformation navigation and consumption to accommodate LLM stakeholders from diverse backgrounds.\\nSecond, interaction allows for experienced affordance and interrogation to understand LLMs’ complex\\ncapabilities and behaviors that could be difficult to capture with textual descriptions. Lastly, as our\\nstudy with collaborators on designers’ use of model documentation suggests [121], static documenta-\\ntion presents significant gaps for contextualizing the model capabilities and limitations for one’s own\\nsetting. It will be impossible for documentation creators to anticipate every downstream use case of\\nLLMs. Instead, stakeholders should be provided with opportunities to interrogate the model with their\\nown input data, capabilities of interest, hypotheses, and questions.\\n4.2 Publishing Evaluation Results\\nWhile evaluation results are often included as one component of a model report, we believe that\\npublishing evaluation results is an important and complex enough topic that it deserves a separate\\ndiscussion. Beyond model reports, evaluation results may also be published by third-party auditors\\nor researchers for the purpose of ensuring compliance with regulations or standards, benchmarking, or\\nexposing model limitations or potential harms. As discussed, evaluation can happen at the aggregate\\nor disaggregated level by groups or conditions of interest. Evaluations may also be performed on a\\nmodel or on the full system into which it is incorporated. While performance quality (e.g., some\\nnotion of accuracy) is often the primary focus of an evaluation, evaluations may also consider fairness\\n(through disaggregated evaluations or using specific fairness metrics), robustness, efficiency, or other\\ncharacteristics of a model or system’s behavior, including how they impact end-users.\\nWe note that the ML and natural language processing (NLP) communities have long dealt with\\nthe challenges of evaluating the performance of generative models [164]. Until recently, natural lan-\\nguage generation (NLG) evaluations have focused on tasks that specialized NLG models commonly\\nperform, such as machine translation, abstractive summarization, question answering, and dialogue\\ngeneration. For tasks that involve classification, standard performance metrics relying on exact match-\\ning with ground-truth labels like accuracy, precision, and recall can be used. In contrast, when the\\noutput space is open-ended and complex, as it often is for generative models, it becomes necessary to\\nrely on more sophisticated performance metrics for word-based or embedding-based matching (e.g.,\\nROUGE score [123] or BERTscore [207]) and more complicated (but often flawed) ways to obtain a\\n“ground-truth” reference to compare against. In practice, ground-truth data are often either chosen\\nbecause they are conveniently available (e.g., using the “highlights” of news articles as the ground\\n11 truth for summarization [168]) or generated by crowd workers. Recently there has been a wave of\\ndata auditing work questioning the assumptions behind and quality of some widely used evaluation\\nbenchmarks and datasets [20, 50, 158]. Furthermore, even if high-quality, such ground truth may be\\ninsufficient to capture all the “goodness” criteria of generated outputs, which can be multi-faceted and\\ncontext-dependent [59]. Because of these challenges, automated evaluations are often complemented\\nby some form of human evaluation, which may involve asking people to rate the quality, fluency, co-\\nherence, relevance, adequacy, or informativeness of an output. However, human evaluation is costly\\nand also lacks established practices about what and how to evaluate, leading to critiques about lack\\nof standardization, reproducibility, validity, and generalizability to real-world settings [14, 37, 59, 80].\\nWhat should LLMs be evaluated for? Compared to specialized NLG models, the extensive and\\ncurrently under-defined space of LLMs’ capabilities make it challenging to answer even the most basic\\nquestion about evaluation: What should LLMs be evaluated for? In the NLP community, initial\\nefforts have emerged to create meta-benchmarks, in which LLMs are evaluated across a large suite\\nof specialized tasks [116, 173]. For example, BIG-bench [173] consists of more than 200 language\\ntasks collaboratively created by more than 400 researchers that are “intended to probe large language\\nmodels.” However, the sheer size could make it challenging for stakeholders to make sense of the\\nevaluation results. Another recent meta-benchmark called HELM (Holistic Evaluation of Language\\nModels) [116] introduces the concept of a “scenario” (e.g., question-answering for English news). This\\nprovides more structure, since different models can be compared by scenario.\\nAnother line of work seeks to be task-agnostic and instead evaluate LLMs’ intrinsic capabilities [22].\\nThis has attracted broad attention from different academic disciplines. For example, researchers have\\napplied human cognitive and linguistic competencies to evaluate LLMs [27, 49, 136, 145], in some\\ncases distinguishing between LLMs’ “formal linguistic competence” — how well they can mimic the\\nrules and patterns of a given language, which LLMs typically do well — and their “functional linguistic\\ncompetence” — how well they can apply cognitive abilities such as planning or causal inference, which\\nis typically more difficult for present-day LLMs [136, 145]. There have also been various attempts to\\nbenchmark LLMs by evaluating their performance on human tests like the SAT or the bar exam [151].\\nWhile these efforts can be useful for exploring LLMs’ capability spaces, they should not be taken\\nas comprehensive evaluation, and their validity (e.g., what are they a valid proxy for), underlying\\nassumptions, statistical robustness, and possible implications (e.g., anthropomorphizing LLMs by using\\nhuman tasks) need to be carefully examined.\\nDespite this surging interest in benchmarking LLMs, we believe a human-centered question is\\nmissing: Who is the evaluation targeted at and for what purpose? For example, the evaluation metrics\\nthat a practitioner cares about when ideating on how to use LLMs for their application are likely\\ndifferent from those that NLP researchers would be interested in to track research progress. For some\\nstakeholders, neither meta-benchmarks or evaluation by human-like cognitive competence may satisfy\\ntheir needs. By better articulating different goals for model evaluation and the resulting needs that\\narise, the community will be able to develop better evaluation techniques that serve these goals, and\\nalso allow many different evaluation techniques to co-exist.\\nFurthermore, transparently communicating the evaluation details and the motivation behind the\\nevaluation choices is all the more important for LLMs. This is not only because of the diverse evaluation\\ntechniques being explored, but also because LLMs are by nature adaptable (e.g., through fine-tuning\\nand prompting) and stochastic (output can vary for similar or even the same input). Care must\\nalso be taken to ensure the evaluation material has not been included when training the model to\\ncontaminate the results. However, providing transparency to allow checking for such contamination\\ncan be a challenge in itself given the opacity and scale of the datasets LLMs are trained on. All of this\\ncalls for the development of new evaluation techniques and communication patterns that account for\\nthese new challenges [22].\\nAt what level should the evaluation take place? To provide transparency at the level of a\\npre-trained model, an adapted model, or an LLM-infused application, evaluations can take place at\\neach of these points. Performance metrics may shift dramatically when moving from a pre-trained\\nmodel to an adapted model, but neither may be reflective of how end-users will react to a model’s use\\nin the context of a real application. Consider an LLM-infused search engine. The developers of the\\nsearch engine may require transparency about how the pre-trained model was evaluated in order to\\n12 ideate on its usage, but this information might not tell them everything they need to know because\\nthey have the ability to adapt the model further themselves. Furthermore, an evaluation of the pre-\\ntrained model may be irrelevant for an auditor who wants to understand whether the deployed search\\nengine application, built on an adapted model, meets certain standards. Some forms of evaluation are\\nonly possible at certain levels. If we want to evaluate the value of the LLM-infused search engine to\\nend-users, we cannot evaluate the (pre-trained or adapted) model in isolation but need to perform a\\nhuman evaluation in the context of the application itself.\\nHow should LLM limitations and risks be evaluated? Given the potential for immense down-\\nstream harms, it is not enough to evaluate LLMs by their capabilities, but also their limitations and\\nrisks. Recent work has begun to delineate the risks of LLMs [16, 22, 195]. For example, Weidinger et al.\\n[195] developed a taxonomy of risks posed by LLMs considering six areas: discrimination, exclusion,\\nand hate speech as encoded in the generated language; information hazards threatening privacy and\\nsecurity by leaking sensitive information; misinformation harms arising when false, poor, and otherwise\\nmisleading information is disseminated; harms from malicious uses of LLMs such as facilitating disin-\\nformation (e.g., fraud), cybersecurity attacks, and censorship; harms from (human-like) interactions\\nsuch as unsafe use and exploitation of user trust; and lastly, environmental and other socio-economic\\nharms such as increasing inequality and negative impact on the labor market.\\nDespite best intentions, these taxonomies may not provide enough coverage or granularity of risks\\nfor specific use cases. And not all risks can nor should be quantified in an abstract manner without\\ntaking into account the deployment context, stakeholders, and kinds of harm they may experience.\\nTo discover and assess model limitations, practitioners frequently rely on behavioral evaluation [30].\\nThis requires hypothesizing and then testing what limitations the model may have in the application\\ncontext, and ideally should be done in a participatory and iterative fashion with stakeholders. While\\nthere has been emerging HCI work developing tools for behavioral evaluation of models [29, 200], how\\nto extend this work to LLMs is a non-trivial question. Meanwhile, we note that developers of LLMs or\\nLLM-infused applications are engaging in substantial “red teaming” practices to discover, measure and\\nmitigate risks of LLMs. However, given that there have been only a few published works [57, 151, 182],\\nthere is currently insufficient transparency around how red teaming work is done to allow us to fully\\nunderstand the risks of LLMs. We believe that the community should work towards shared best\\npractices to perform—and communicate the results of—red teaming.\\n4.3 Providing Explanations\\nTo support mechanistic understanding, there has been a wave of research on approaches to produce\\nexplanations of a model’s internal processes and outputs, a line of research referred to as explainable AI\\n(XAI) or interpretable ML, depending on the community. At the highest level, there are two common\\napproaches. One is to provide “intrinsic explanations” by exposing the model’s inner workings directly.\\nThe other is to generate post-hoc explanations as approximations for how the model works.\\nFor the former, the traditional approach is to train a relatively simple model that is deemed\\n“directly interpretable” such as a rule-based model, decision tree, or linear regression. More recent\\nresearch aims to develop “explainable architectures” with representations meaningful to people [e.g.,\\n69, 203]. For modern neural NLP models, various analyses and visualization techniques of activation\\npatterns have been explored to help people make sense of the model’s internal structures (e.g., neurons,\\nlayers, and specific architectural mechanisms). For example, for models like transformers that utilize\\nattention mechanisms, a popular approach is to leverage the attention weights in the intermediate\\nrepresentation to explain how much the model “attends to” each input feature or token. However,\\nthere has been a long debate on whether attention weights provide faithful explanations for how the\\nmodel actually produces its outputs [13, 84, 198]. This highlights the challenge of understanding\\nmodel behavior under highly complex and massive architectures, even when internals are accessible.\\nWe additionally emphasize that direct interpretability, while desirable [162], should not be taken at\\nface value unless shown to help stakeholders achieve their desired understanding. In our own prior\\nwork with collaborators, we have observed cases in which exposing the internals of even a simple linear\\nregression model made people less able to detect and correct for the model’s mistakes [154], with\\nevidence suggesting that this was due to information overload.\\nPost-hoc explanations can be used for complex models as well as “black box” models for which\\nmodel internals cannot be accessed, for example, when the models are proprietary. Explanations can\\n13 be global, providing an overview of the model’s overall logic, or local, providing the reasoning behind a\\nparticular model output. Local explanations can take several forms. The most common form is feature\\nattribution scores, which capture different notions of how “important” each input feature is to the\\nmodel’s output—sometimes referred to as saliency methods for vision and language models. There are\\nmany types of techniques to generate feature attribution scores for neural NLP models, as summarized\\nin several recent survey papers on explainability for NLP [39, 132, 135]. Some techniques, like gradient-\\nbased or propagation-based methods, require access to the model architecture. Other techniques are\\ninstead based on surrogate models, i.e., directly interpretable models that are trained using the original\\nmodel’s inputs and outputs and are meant to serve as a local approximation to explain a target output.\\nThe most popular examples include Local Interpretable Model-Agnostic Explanations (LIME) [161]\\nand SHapley Additive exPlanations (SHAP) [130]. Inspired by the often contrastive nature of human\\nexplanation, other local explanations take the form of counterfactuals, showing how an input could be\\nmodified in order to obtain a different output [163, 184]. Lastly, explanations can be in the form of\\nexamples, intended to support case-based reasoning. These examples may be prototypes of a certain\\nprediction class [97], influential examples in the training data [102], or similar examples that would\\nlead the model to produce the same or alternate outputs [146].\\nThe language modality of NLP models poses some unique requirements for explanations. We call\\nout two intertwined pursuits that will remain important for LLMs. One is to explain using human-\\ncompatible concepts, which often means using more abstract features (e.g., a more general notion,\\nsemantics) as opposed to raw input features at the token level. Some have argued that example-based\\nexplanations allow for more abstraction without fixating on individual tokens [34, 135]. Others explored\\ntechniques that map raw tokens to more abstract and meaningful concepts [98, 190]. The second pursuit\\nis to explain through natural language. For example, prior research explored techniques that directly\\noutput rationales together with the model prediction [70]. A common endeavor is to develop “self-\\nexplanatory” rationale-based models that engage in rationalization (e.g., extracting rules or judging\\na set of premises from the input [114, 178]) as part of the process for arriving at a prediction. Aside\\nfrom the explainability benefits—these rationales are faithful to the model’s behavior by design—one\\nmight argue that these more “principled” models could be expected to be more robust.\\nDespite the proliferation of approaches for providing explanations, the community has long debated\\nwhat it is that makes an explanation “good.” For a long list of goodness criteria, we point interested\\nreaders to Sokol and Flach [172] and Carvalho et al. [32]. For our purposes, we note that at a minimum,\\na good explanation should be relatively faithful to how the model actually works, understandable to\\nthe receiver, and useful for the receiver’s end-goals—indeed, we contend that these criteria should be\\nbroadly considered for all transparency approaches.\\nHow can we provide faithful explanations for the ultimate black box? Given their complex\\narchitecture, unprecedented scale, and often proprietary nature, LLMs are unarguably black box in\\nnature, but there is a sense in which they naturally “explain.” While explanation is a contested concept,\\none common definition is “an answer to a why question.” Indeed, people have already been asking\\nLLMs why they generate certain outputs directly and taking the answer as the model’s explanation.\\nHowever, explanations generated in this manner are not guaranteed to be faithful to the internal\\nprocess of the model, especially given that LLMs are trained to generate plausible texts without\\ngrounding in facts, and this carries over to their explanations too [22]. One recent study [27] shows\\nthat GPT-4’s explanations lack this “process consistency”; see Figure 1, which is taken from Bubeck\\net al. [27].1Specifically, GPT-4 can provide contradicting explanations for the same tasks depending\\non the precise inputs, often as a way to justify its different outputs. The authors’ analysis also suggests\\nthat, in some cases, GPT-4’s explanations are implausible or even inconsistent with the output itself.\\nIn fact, experimenting with asking why questions in different tasks, we found that ChatGPT often\\nprovides a justification that has little to do with its internal process, such as stating what function\\nits recommendation serves. Similarly, while it is tempting to deem output that appears to include\\nchain-of-thought reasoning as reflecting the reasoning of the LLM, a recent study [183] shows that\\nit does not reflect the true reasons why a model arrives at its output, evidenced by the fact that\\nthe model can be heavily influenced by introducing biases in the prompt but systematically fail to\\nmention the influence in its reasoning. Unfaithful explanations can do more harm than good if their\\n1The examples were obtained from an older version of GPT-4 from before their paper was published on March 22,\\n2023. The authors noted that “GPT4 development continued after [their] experiments” so the outputs may not replicate.\\n14 Figure 1: Example taken from Bubeck et al. [27] showing that explanations from GPT-4 lack process\\nconsistency—providing contradicting explanations for the same tasks depending on the inputs\\nreceivers accept them without proper scrutiny. This is especially worrisome as prior work has shown\\nthat people can be influenced by the presence of explanations even when those explanations are not\\nmeaningful [47, 91, 108], for example, trusting a model more because of the mere presence of an\\nexplanation rather than its contents. This tendency to overtrust based on the LLM’s own explanations\\nmay be further amplified by the common anthropomorphization [62] and presentation of LLMs as\\n“intelligent” systems.\\nThe community must seek ways to improve the faithfulness of LLM explanations, whether through\\ndirect generation or other approaches, as well as principled ways of auditing explanation faithfulness.\\nWe must note that there is currently no agreed-upon metric or formal technique for evaluating expla-\\nnation faithfulness [82, 132]. Common approaches rely on evaluating necessary conditions to disprove\\nfaithfulness via counter-examples, such as if two functionally equivalent models have different expla-\\nnations, if the explanations vary for similar inputs and outputs, or if the explanations would suggest\\nthe model behave differently than it does on new inputs. When outlining guidelines for developing\\nevaluation methods for faithfulness, Jacovi and Goldberg [82] argue that this focus on disproof is\\nunproductive, as post-hoc explanations are by definition approximations and always involve a loss of\\ninformation. Instead, the community should aim to develop a formal understanding and approach to\\nevaluation that allow us “the freedom to say when a method is sufficiently faithful to be useful in\\npractice.” We believe this requires formalizing different types of “faithfulness gaps” and empirically\\ninvestigating the impact on stakeholders in different contexts with different use cases. For example, a\\nhigher level of faithfulness may be required for debugging or adapting an LLM than is required for an\\nend-user who is interacting with an LLM in a low-stakes application.\\nHow should we rethink explanations for LLMs? We encourage the community to rethink the\\nspace of what explanations might look like and how they might be derived for LLMs. This is necessary\\nfor several reasons. First, most current XAI techniques cannot be easily applied to LLMs. As discussed\\nabove, their complex and massive scale makes them far from directly interpretable and also renders\\nsome post-hoc explanation techniques infeasible. Their often inaccessible internals and training data\\nmake it impossible to use some saliency methods or provide influential training examples. And the\\ncomplexity of their input and output spaces makes it difficult to build surrogate models to provide\\npost-hoc explanations.\\n15 Second, the diverse model capabilities of LLMs may require different types of explanations. For ex-\\nample, while text classification tasks could be adequately explained via feature attributions, explaining\\nmore complex tasks such as question-answering and reading comprehension is likely to require more\\ncomplex rationales and abstraction. More fundamentally, researchers have wrestled with the question\\nof “one model versus many models” [22]—that is, the extent to which the mechanism by which a model\\nproduces an answer for a single task can be generalized to understand its behavior on other tasks. If\\nan LLM uses different internal processes for different tasks (“many models”), independent studies of\\ntheir mechanisms and different explanation methods may need to be developed for each.\\nLastly, explanations for LLM tasks are often sought through natural language interactions and\\nin the context of evolving multi-turn dialogues. This requires the community to not only continue\\npursuing natural-language explanations but also explanations that are more compatible with how\\npeople seek explanations in social interactions. Miller [142] reviewed the social science literature on\\nhow people produce explanations and summarized a few fundamental properties of human explanations,\\nincluding being contrastive, selected (that is, containing only the most relevant causes), interactive (for\\nexample, through a conversation), and tailored to the recipient, many of which are missing from current\\nXAI techniques. We believe that with LLMs it is even more important to explore how to provide\\nexplanations that are interactive and tailored, including accounting for the history of interaction and\\nother contexts.\\nOur view is not that the community should take a monolithic standard on what constitutes LLM\\nexplanations, but rather must articulate what different types of explanations are, along with their\\nsuitable contexts, limitations, and pitfalls. For example, justifications, when provided truthfully, can\\nsupply useful additional information for information seekers [202]. In philosophy, the social sciences,\\nand HCI, there is a long tradition of breaking down different types of explanations by their mechanism,\\nstance, and the questions that they answer (e.g., what, how, why, why not, what if) [64, 74, 93, 119,\\n128, 129]. This literature may offer a useful basis for considering different types of LLM explanations.\\nWhat explanations are appropriate for LLM-infused applications? As we have emphasized\\nthroughout this paper, providing transparency for LLM-infused applications may require different ap-\\nproaches compared with transparency for the underlying models. For some applications, explanations\\nmay need to take into account the workings of the broader system rather than the LLM alone. For\\nexample, current search engines based on LLMs use traditional web search results to ground the LLM’s\\noutput. In such cases, providing links to the search results that were used can be viewed as a form of\\nexplanation. Of course, issues with faithfulness arise here as well, and indeed, a recent study showed\\nthat results returned by generative search engines often contain unsupported statements and inaccu-\\nrate citations [127]. As another example, explaining why a purchase was made by an LLM that makes\\ncalls to a shopping service through a plugin may require explaining not only the behavior of the LLM,\\nbut also the behavior of the shopping service (e.g., what products were available at what price), and\\ntheir interaction (e.g., how did the LLM choose to request a specific product).\\nFollowing a human-centered perspective, a path to develop useful and new types of explanations is to\\ninvestigate the reasons why people seek explanations in common contexts of LLM-infused applications.\\nFor example, in a recent HCI study with collaborators [175], we explored what explanations people\\nseek from code generation applications and why they seek them. The results suggest that people\\nprimarily want explanations to improve the way they prompt. This includes gaining both a better\\nglobal understanding of what prompts can or cannot generate certain outputs and a better local\\nunderstanding of how to improve their prompts to produce more desirable outputs. Therefore, rather\\nthan the why explanations about the model process for a specific output, global explanations about\\nthe model logic, input and output spaces, as well as counterfactual explanations about how to improve\\nthe input appear to be more useful for this kind of application.\\n4.4 Communicating Uncertainty\\nBeyond explanations, another approach that can be used to help stakeholders assess how much to rely\\non a model’s output is to convey some notion of the model’s uncertainty. Uncertainty is typically\\nmodeled in terms of probabilities, though different ways of measuring and communicating uncertainty\\nmake sense for different types of model outputs. For classification models, uncertainty is often pre-\\nsented as the probability that the model is correct, sometimes referred to as the model’s “confidence.”\\n16 For regression models, uncertainty may be expressed as a distribution over possible outcomes or a\\nconfidence interval around a specific prediction.\\nUncertainty arises from different sources [81]. Aleatoric uncertainty refers to inherent randomness\\nin the quantity that is being predicted; this would capture the uncertainty in the outcome of a coin flip.\\nOn the other hand, epistemic uncertainty refers to uncertainty that stems from a lack of knowledge\\nabout the best possible model. In the context of machine learning, if uncertainty could be reduced\\nby collecting more training data, it is epistemic. While this distinction is conceptually useful, the\\nline between aleatoric and epistemic uncertainty can be hard to draw. They are context-dependent\\n(whether or not more data reduces uncertainty depends on the class of models used) and cannot always\\nbe easily distinguished, let alone measured.\\nWhile some ML models yield a natural way of estimating uncertainty directly, others do not.\\nResearch has explored post-hoc techniques to estimate uncertainty from the model’s errors [33]. In\\norder to be useful, an estimate of uncertainty should be well calibrated, reliably reflecting the model’s\\nlikelihood of making a mistake on a particular input. Common metrics to assess calibration include\\nproper scoring rules like the Brier score [25] and expected calibration error [147]. Deep neural networks\\nare known to generate uncalibrated uncertainty, leading to recent research looking into re-calibration\\ntechniques [68, 88].\\nOnce uncertainty estimates can be obtained, there are design decisions that must be made regarding\\nhow to communicate these estimates. While more complex designs can be created, two decision\\ndimensions are commonly explored. One dimension is communication precision. For classification,\\na more precise option might be to present a probability, while a less precise option might be to\\npresent the confidence level as low, medium, or high. For regression, it is less precise to present a\\nconfidence interval compared with a detailed distribution. With some loss of information, less precise\\ncommunication is easier to process and often preferred by lay people or in cognitively constrained\\nsettings. The second dimension concerns the modality in which uncertainty is communicated, which\\ncould be verbal, numerical, or visual. For a detailed discussion of quantifying and communicating\\nuncertainty, we point interested readers to Bhatt et al. [19].\\nWe remark that uncertainty is just one way of quantifying the limitations of a particular output,\\nand that communicating other output limitations (e.g., potential safety concerns) may be useful in\\nsome contexts. While we do not discuss such approaches, similar lessons likely apply.\\nWhat is a useful notion of uncertainty for LLMs? While LLMs have a notion of uncertainty\\nbaked in them—the likelihood that the model would generate a specific token given its preceding or\\nsurrounding context [17], what we have referred to in past work as the generation probability [186]—\\nwhether this notion would be useful to different stakeholders is questionable. In particular, this notion\\nmay not line up with people’s intuition about what it means for the model to be uncertain. For\\nexample, in a question-answering context, a correct answer may have many synonyms, and the model\\nmay appear “uncertain” simply because there are many correct options. As Kuhn et al. [104] put\\nit, the likelihoods output by LLMs represent “lexical confidence,” while “for almost all applications\\nwe care about meanings.” For example, if an end-user asks a question to an LLM-infused chatbot\\nor search engine, they would presumably expect a notion of uncertainty to reflect how likely it is\\nthat the answer they receive is factually correct, which may be quite different from the likelihood it\\nis generated by the model. Recent work has begun to explore techniques for generating uncertainty\\nestimates that more accurately capture correctness, including using probabilistic methods [104], fine-\\ntuning the LLM to describe its own confidence [125], and sampling multiple outputs and having the\\nLLM evaluate them [90]. However, we note that even whether or not an answer is correct can be\\nambiguous. Generative models do not have a single notion of ground truth to compare against. A\\ncomplex response to a query may be generally correct but contain inaccurate details or justifications.\\nAnd some questions are fundamentally subjective.\\nCarefully selecting a notion of uncertainty to convey to stakeholders matters because the particular\\nnotion used impacts their behavior and trust. In our recent work with collaborators [186], we explored\\nthe effectiveness of displaying two alternative notions of uncertainty to programmers interacting with\\nan LLM-powered code completion tool. In a mixed-methods study with 30 programmers, we compared\\nthree conditions: providing a code completion alone, highlighting those tokens with the lowest likeli-\\nhood of being generated by the underlying LLM (i.e., lowest generation probability), and highlighting\\ntokens with the highest predicted likelihood of being edited by a programmer according to a separate\\n17 Base LLMAdapted LLMLLM-Powered ApplicationsStakeholders(develop, deploy, audit, use, be impacted by LLMs)Organizations developing LLMsOrganizations developing applicationsSocio-organizational contexts of stakeholders-Complex and uncertain model capabilities and behaviors-Massive and opaque architectures-Proprietary technology-New and complex applications-Organizational pressure to move fast and deploy at scale-Expanded and diverse stakeholders-Rapidly evolving and often flawed public perceptions•How can we characterize LLMs’ functionals (capabilities and limitations), mechanisms, performance, and risks?•At what level should transparency approaches take place?•What transparency approaches are useful for (popular) LLM-infused applications?•How can we effectively communicate or present transparency approaches in natural language and interactive interfaces?•What are the different (and new) types of stakeholders and their transparency goals?•How can transparency support appropriate trust of LLMs?•How can transparency contribute to better control mechanisms forLLMs?•How can we characterize and improve people’s mental models of LLMs?Figure 2: Summary of general open questions for transparency approaches in the age of LLMs and unique\\nchallenges (in purple italics) that arise from the technology and stakeholder (human) perspectives of LLMs.\\n“edit model” trained on logged data from past programmer interactions. We found that highlighting\\ntokens with the highest predicted likelihood of being edited helped programmers work more efficiently\\nand was subjectively preferred, while using generation probabilities provided little benefit. This re-\\nsearch is exploratory in nature and we encourage future work that takes a human-centered perspective\\nto define uncertainty based on human needs.\\nWhat are the most effective ways to communicate uncertainty? Beyond how to quantify\\nuncertainty, a key consideration is how to best communicate it to stakeholders. The social science\\nliterature suggests that choosing an effective form of uncertainty communication requires articulating\\nwhat the uncertainty is regarding (e.g., uncertainty about an individual token or about a full output,\\nand which source of uncertainty), what form it is provided in (e.g., its precision and modality), and\\nwhat the effect is (e.g., on trust or behaviors), as well as taking into consideration of the characteristics\\nof the receiver [185]. For example, in our study on uncertainty in the context of code completion\\ntools [186], by soliciting participants’ feedback on different uncertainty communication design choices,\\nwe found that programmers prefer uncertainty about granular or meaningful blocks to guide them to\\nmake token-level changes and prefer less precise communication (as opposed to exact quantification)\\nfor easy processing—both ultimately supporting their goal of producing correct code efficiently.\\nAs discussed in Section 3.5, since language models output text, it is natural to consider communi-\\ncating uncertainty through language itself. Indeed, current LLM-infused chatbots and search engines\\nalready engage in hedging behavior and refuse to answer certain questions, often due to safety con-\\nsiderations. It is easy to imagine expanding these behaviors for uncertainty. However, research is\\nneeded to understand how people actually perceive them and how to enforce their calibration with the\\nunderlying uncertainty.\\n5 Summary and Discussion\\nWe have mapped out a roadmap for human-centered research on AI transparency in the era of LLMs by\\nreflecting on the unique challenges introduced by LLMs, synthesizing lessons learned from HCI/FATE\\nresearch on AI transparency, and exploring the applicability of existing transparency approaches—\\nmodel reporting, publishing evaluation results, providing explanations, and communicating uncer-\\n18 tainty. In Figure 2, we summarize the unique challenges (in purple italics) and some of the open\\nquestions that arise in considering LLMs from the perspectives of the technology and of the stake-\\nholder (human). This illustration highlights the complexity of the technology space (differentiating\\nbase LLM, adapted LLM, and LLM-powered applications), the diversity of the stakeholders, and the\\nneed to attend to the socio-organizational aspects that the technology and stakeholders are situated in.\\nHere we summarize the open questions in a more general fashion rather than in relation to a specific\\ntransparency approach, as we note that all transparency approaches for LLMs require similar consider-\\nations and face common challenges. This mapping is not meant to precisely categorize or diagnose the\\nopen questions, but to elucidate how the development of effective transparency approaches for LLMs\\nrequires research attending to multiple aspects and the interplay among them. We invite future work\\nto further expand these lists of challenges and open questions.\\nWe want to mention a few additional areas of consideration and directions of research. One area\\nwe have not yet touched on is transparency around the provenance of AI-generated text. Regulatory\\ndiscussions around AI transparency often center on obligations to reveal that an AI system is in use for\\ncertain tasks. For example, Article 52 of the proposed EU AI Act requires that providers of certain AI\\nsystems design them in such a way that it is clear that people are interacting with an AI system. It also\\nrequires that AI systems generating manipulated images, audio, or video (“deep fakes”) disclose that\\nthis content has been generated or manipulated by an AI system. For images and video, watermarking\\ntechniques can be used to combat the spread of deep fakes [e.g., 205], but techniques for tracking the\\nprovenance of text are still relatively unexplored. Very recently some progress has been made towards\\ndeveloping techniques to watermark text output by LLMs without a substantial sacrifice in quality,\\nfor example by softly increasing the probability of certain randomly selected tokens [99], though it is\\ntoo early to know whether such techniques will work in practical settings. There is also an active line\\nof research on post-hoc detection of artificially generated text [86, 179, 206]. While these are largely\\ntechnical challenges, there are additionally open questions around how to more effectively disclose that\\npeople are interacting with an AI system or that the text they are reading is AI-generated.\\nWe also highlight an additional dimension of transparency: to help people understand the temporal\\nchanges (or lack thereof) of the model. This dimension is especially important for LLMs as it is\\nknown that the base models are constantly being updated by the model providers, and these updates\\npropogate into LLM-infused applications. Not only is it necessary to track and maintain provenance\\ninformation about a model’s architecture, training process, training datasets, and adaptation details,\\nas well as its functions and evaluation results, but more research is also needed on how to characterize\\nand communicate the impact of any changes on end-users of different LLM-infused applications.\\nAnother key question around AI transparency is the role that regulators, advocates, and the general\\npublic should play. As an example, the research community has argued for the importance of external\\naudits of algorithms and models, especially those that act as gatekeepers or otherwise impact people’s\\nlives [51, 140, 165]. Recent research has begun to dig into ways of developing auditing procedures to\\naddress the particular governance challenges posed by LLMs [144], but many open questions remain,\\nfrom what methods and metrics to use (as discussed in Section 4.2) to how to account for risks that\\ncannot be addressed on the technology level. Engaging stakeholders who have an outside view can\\nhelp ensure that audits are conducted fairly and in such a way as to capture risks of harm to their\\ncommunities. There are also open challenges around how to effectively set up feedback mechanisms\\nand other ways for end-users or those impacted by an LLM’s outputs to contest those outputs, as well\\nas how to incorporate such feedback to identify and address patterns of failure.\\nFinally, while we have focused on LLMs in this paper, we note that many of the challenges, lessons\\nlearned, potential approaches, and open problems that we explored also apply to other large-scale\\ngenerative models, including multimodal models that allow for both textual and visual input or output.\\nAs such models become more widespread, we encourage additional research on AI transparency for\\nthis larger class of models.\\nAcknowledgments\\nWe are grateful to our colleagues for many useful discussions and feedback on early drafts of this\\nwork, especially to Jordan Ash, Steph Ballard, Gagan Bansal, Susan Dumais, Susan Etlinger, Ruth\\nKikin-Gil, Sunnie Kim, Daniela Massiceti, Ida Momennejad, Cecily Morrison, Mickey Vorvoreanu,\\n19 Daricia Wilkinson, Ziang Xiao, Cyril Zhang, the MSR FATE group, and attendees of Microsoft’s\\nAether Transparency Working Group community sync.\\nReferences\\n[1] Gavin Abercrombie, Amanda Cercas Curry, Tanvi Dinkar, and Zeerak Talat. Mirages: On\\nanthropomorphism in dialogue systems. CoRR arXiv:2305.09800, 2023.\\n[2] Abubakar Abid, Maheen Farooqi, and James Zou. Persistent anti-muslim bias in large language\\nmodels. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society , pages\\n298–306, 2021.\\n[3] Ajay Agrawal, Joshua Gans, and Avi Goldfarb. ChatGPT and how AI disrupts industries.\\nHarvard Business Review , December 2022.\\n[4] Saleema Amershi, Maya Cakmak, William Bradley Knox, and Todd Kulesza. Power to the\\npeople: The role of humans in interactive machine learning. AI Magazine , 35(4):105–120, 2014.\\n[5] Mike Ananny and Kate Crawford. Seeing without knowing: Limitations of the transparency ideal\\nand its application to algorithmic accountability. new media & society , 20(3):973–989, 2018.\\n[6] Matthew Arnold, Rachel KE Bellamy, Michael Hind, Stephanie Houde, Sameep Mehta, Alek-\\nsandra Mojsilovi´ c, Ravi Nair, K Natesan Ramamurthy, Alexandra Olteanu, David Piorkowski,\\net al. Factsheets: Increasing trust in AI services through supplier’s declarations of conformity.\\nIBM Journal of Research and Development , 63(4/5):6–1, 2019.\\n[7] Zahra Ashktorab, Mohit Jain, Q. Vera Liao, and Justin D Weisz. Resilient chatbots: Repair\\nstrategy preferences for conversational breakdowns. In Proceedings of the 2019 CHI conference\\non human factors in computing systems , pages 1–12, 2019.\\n[8] Sandeep Avula, Bogeum Choi, and Jaime Arguello. The effects of system initiative during\\nconversational collaborative search. Proceedings of the ACM on Human-Computer Interaction ,\\n6(CSCW1):1–30, 2022.\\n[9] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,\\nAnna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional AI:\\nHarmlessness from AI feedback. CoRR arXiv:2212.08073, 2022.\\n[10] Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi, Ece Kamar,\\nMarco Tulio Ribeiro, and Daniel Weld. Does the whole exceed its parts? The effect of AI\\nexplanations on complementary team performance. In Proceedings of the 2021 CHI Conference\\non Human Factors in Computing Systems , pages 1–16, 2021.\\n[11] Gagan Bansal, Zana Bu¸ cinca, Kenneth Holstein, Jessica Hullman, Alison Marie Smith-Renner,\\nSimone Stumpf, and Sherry Wu. Workshop on trust and reliance in AI-human teams (TRAIT).\\nInExtended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems ,\\n2023. doi: 10.1145/3544549.3573831.\\n[12] Solon Barocas, Anhong Guo, Ece Kamar, Jacquelyn Krones, Meredith Ringel Morris, Jen-\\nnifer Wortman Vaughan, Duncan Wadsworth, and Hanna Wallach. Designing disaggregated\\nevaluations of AI systems: Choices, considerations, and tradeoffs. In Fourth AAAI/ACM Con-\\nference on Artificial Intelligence, Ethics, and Society (AIES) , 2021.\\n[13] Jasmijn Bastings and Katja Filippova. The elephant in the interpretability room: Why use\\nattention as explanation when we have saliency methods? CoRR arXiv:2010.05607, 2020.\\n[14] Anja Belz, Anastasia Shimorina, Shubham Agarwal, and Ehud Reiter. The reprogen shared task\\non reproducibility of human evaluations in nlg: Overview and results. In Proceedings of the 14th\\nInternational Conference on Natural Language Generation , pages 249–258, 2021.\\n20 [15] Emily M. Bender and Batya Friedman. Data statements for natural language processing: To-\\nward mitigating system bias and enabling better science. Transactions of the Association for\\nComputational Linguistics , 6:587–604, 2018.\\n[16] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On\\nthe dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021\\nACM Conference on Fairness, Accountability, and Transparency (FAccT) , pages 610–623, 2021.\\ndoi: 10.1145/3442188.3445922.\\n[17] Yoshua Bengio, R´ ejean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic\\nlanguage model. Journal of Machine Learning Research , 3(Feb):1137–1155, 2003.\\n[18] Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep\\nGhosh, Ruchir Puri, Jos´ e MF Moura, and Peter Eckersley. Explainable machine learning in\\ndeployment. In Proceedings of the 2020 conference on fairness, accountability, and transparency ,\\npages 648–657, 2020.\\n[19] Umang Bhatt, Javier Antor´ an, Yunfeng Zhang, Q. Vera Liao, Prasanna Sattigeri, Riccardo\\nFogliato, Gabrielle Melan¸ con, Ranganath Krishnan, Jason Stanley, Omesh Tickoo, et al. Un-\\ncertainty as a form of transparency: Measuring, communicating, and using uncertainty. In\\nProceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society , pages 401–413,\\n2021.\\n[20] Su Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu, Robert Sim, and Hanna Wallach. Stereo-\\ntyping norwegian salmon: An inventory of pitfalls in fairness benchmark datasets. In Proceedings\\nof the 59th Annual Meeting of the Association for Computational Linguistics and the 11th In-\\nternational Joint Conference on Natural Language Processing (Volume 1: Long Papers) , pages\\n1004–1015, 2021.\\n[21] Tolga Bolukbasi, Adam Pearce, Ann Yuan, Andy Coenen, Emily Reif, Fernanda Viegas, and\\nMartin Wattenberg. An interpretability illusion for bert. CoRR arXiv:2104.07143, 2021.\\n[22] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von\\nArx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolf-\\nsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen\\nCreel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Dur-\\nmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor\\nGale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori\\nHashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang,\\nThomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keel-\\ning, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Ku-\\nditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent,\\nXiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani,\\nEric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben New-\\nman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel\\nOrr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts,\\nAditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack\\nRyan, Christopher R´ e, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan\\nSrinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tram` er, Rose E. Wang,\\nWilliam Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga,\\nJiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia\\nZheng, Kaitlyn Zhou, and Percy Liang. On the opportunities and risks of foundation models.\\nCoRR arXiv:2108.07258, 2021.\\n[23] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie\\nMillican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego\\nde Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, T. W. Hennigan, Saffron Huang, Lorenzo\\nMaggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol\\nVinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and L. Sifre. Improving\\n21 language models by retrieving from trillions of tokens. In International Conference on Machine\\nLearning , 2021.\\n[24] Samuel R. Bowman. Eight things to know about large language models. CoRR arXiv:2304.00612,\\n2023.\\n[25] Glenn W Brier et al. Verification of forecasts expressed in terms of probability. Monthly weather\\nreview , 78(1):1–3, 1950.\\n[26] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\\nfew-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020.\\n[27] S´ eastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece\\nKamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi,\\nMarco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments\\nwith GPT-4. CoRR arXiv 2303.12712, 2023.\\n[28] Ben Buchanan, Andrew Lohn, Micah Musser, and Katerina Sedova. Truth, lies, and automation:\\nHow language models could change disinformation. Technical Report. Center for Security and\\nEmerging Technology, Georgetown University, 2021.\\n[29] ´Angel Alexander Cabrera, Erica Fu, Donald Bertucci, Kenneth Holstein, Ameet Talwalkar, Ja-\\nson I Hong, and Adam Perer. Zeno: An interactive framework for behavioral evaluation of\\nmachine learning. CoRR arXiv:2302.04732, 2023.\\n[30] ´Angel Alexander Cabrera, Marco Tulio Ribeiro, Bongshin Lee, Robert Deline, Adam Perer, and\\nSteven M Drucker. What did my AI learn? how data scientists make sense of model behavior.\\nACM Transactions on Computer-Human Interaction , 30(1):1–27, 2023.\\n[31] Michelle Carney, Barron Webster, Irene Alvarado, Kyle Phillips, Noura Howell, Jordan Griffith,\\nJonas Jongejan, Amit Pitaru, and Alexander Chen. Teachable machine: Approachable web-\\nbased tool for exploring machine learning classification. In Extended abstracts of the 2020 CHI\\nconference on human factors in computing systems , pages 1–8, 2020.\\n[32] Diogo V Carvalho, Eduardo M Pereira, and Jaime S Cardoso. Machine learning interpretability:\\nA survey on methods and metrics. Electronics , 8(8):832, 2019.\\n[33] Tongfei Chen, Jir´ ı Navr´ atil, Vijay Iyengar, and Karthikeyan Shanmugam. Confidence scoring\\nusing whitebox meta-models with linear classifier probes. In The 22nd International Conference\\non Artificial Intelligence and Statistics , pages 1467–1475. PMLR, 2019.\\n[34] Valerie Chen, Q. Vera Liao, Jennifer Wortman Vaughan, and Gagan Bansal. Understanding\\nthe role of human intuition on reliance in human-AI decision-making with explanations. CSCW\\n2023, 2023.\\n[35] Andrew R. Chow and Billy Perrigo. The AI arms race is changing everything. Time , February\\n2023.\\n[36] Paul F. Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep\\nreinforcement learning from human preferences. In Advances in Neural Information Processing\\nSystems , volume 30, 2017.\\n[37] Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, and Noah A\\nSmith. All that’s ‘human’is not gold: Evaluating human evaluation of generated text. In Pro-\\nceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the\\n11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) ,\\npages 7282–7296, 2021.\\n[38] Anamaria Crisan, Margaret Drouhard, Jesse Vig, and Nazneen Rajani. Interactive model cards:\\nA human-centered approach to model documentation. In 2022 ACM Conference on Fairness,\\nAccountability, and Transparency , pages 427–439, 2022.\\n22 [39] Marina Danilevsky, Kun Qian, Ranit Aharonov, Yannis Katsis, Ban Kawas, and Prithviraj Sen.\\nA survey of the state of explainable AI for natural language processing. CoRR arXiv:2010.00711,\\n2020.\\n[40] Thomas H. Davenport and Nitin Mittal. How generative AI is changing creative work. Harvard\\nBusiness Review , November 2022.\\n[41] Lydia DePillis and Steve Lohr. Tinkering with ChatGPT, workers wonder: Will this take my\\njob? New York Times , March 28 2023.\\n[42] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of\\ndeep bidirectional transformers for language understanding. In Proceedings of the 2019 Confer-\\nence of the North American Chapter of the Association for Computational Linguistics: Human\\nLanguage Technologies, Volume 1 (Long and Short Papers) , pages 4171–4186, June 2019. doi:\\n10.18653/v1/N19-1423.\\n[43] Mandeep K Dhami and David R Mandel. Communicating uncertainty using words and numbers.\\nTrends in Cognitive Sciences , 2022.\\n[44] Upol Ehsan, Q. Vera Liao, Michael Muller, Mark O Riedl, and Justin D Weisz. Expanding\\nexplainability: Towards social transparency in AI systems. In Proceedings of the 2021 CHI\\nConference on Human Factors in Computing Systems , pages 1–19, 2021.\\n[45] Upol Ehsan, Samir Passi, Q Vera Liao, Larry Chan, I Lee, Michael Muller, Mark O Riedl, et al.\\nThe who in explainable ai: How ai background shapes perceptions of ai explanations. arXiv\\npreprint arXiv:2107.13509 , 2021.\\n[46] Malin Eiband, Hanna Schneider, Mark Bilandzic, Julian Fazekas-Con, Mareike Haug, and Hein-\\nrich Hussmann. Bringing transparency design into practice. In 23rd International Conference\\non Intelligent User Interfaces , pages 211–223, 2018.\\n[47] Malin Eiband, Daniel Buschek, Alexander Kremer, and Heinrich Hussmann. The impact of\\nplacebic explanations on trust in intelligent systems. In Extended Abstracts of the 2019 CHI\\nConference on Human Factors in Computing Systems , 2019. doi: 10.1145/3290607.3312787.\\n[48] Motahhare Eslami, Karrie Karahalios, Christian Sandvig, Kristen Vaccaro, Aimee Rickman,\\nKevin Hamilton, and Alex Kirlik. First I “like” it, then I hide it: Folk theories of social feeds.\\nInProceedings of the 2016 CHI Conference on Human Factors in Computing Systems , pages\\n2371–2382, 2016.\\n[49] Allyson Ettinger. What bert is not: Lessons from a new suite of psycholinguistic diagnostics for\\nlanguage models. Transactions of the Association for Computational Linguistics , 8:34–48, 2020.\\n[50] Alexander R Fabbri, Wojciech Kry´ sci´ nski, Bryan McCann, Caiming Xiong, Richard Socher,\\nand Dragomir Radev. Summeval: Re-evaluating summarization evaluation. Transactions of the\\nAssociation for Computational Linguistics , 9:391–409, 2021.\\n[51] Gregory Falco, Ben Shneiderman, Julia Badger, Ryan Carrier, Anton Dahbura, David Danks,\\nMartin Eling, Alwyn Goodloe, Jerry Gupta, Christopher Hart, et al. Governing AI safety through\\nindependent audits. Nature Machine Intelligence , 3(7):566–571, 2021.\\n[52] Ed Felten, Manav Raj, and Robert Seamans. How will language modelers like ChatGPT affect\\noccupations and industries? CoRR arXiv:2303.01157, 2023.\\n[53] Heike Felzmann, Eduard Fosch-Villaronga, Christoph Lutz, and Aurelia Tam` o-Larrieux. Towards\\ntransparency by design for artificial intelligence. Science and Engineering Ethics , 26(6):3333–\\n3361, 2020.\\n[54] Shi Feng, Eric Wallace, Alvin Grissom II, Mohit Iyyer, Pedro Rodriguez, and Jordan Boyd-\\nGraber. Pathologies of neural models make interpretations difficult. In Proceedings of the 2018\\nConference on Empirical Methods in Natural Language Processing , 2018. doi: 10.18653/v1/\\nD18-1407.\\n23 [55] Michael Fernandes, Logan Walls, Sean Munson, Jessica Hullman, and Matthew Kay. Uncertainty\\ndisplays using quantile dotplots or cdfs improve transit decision-making. In Proceedings of the\\n2018 CHI Conference on Human Factors in Computing Systems , pages 1–12, 2018.\\n[56] Deep Ganguli, Danny Hernandez, Liane Lovitt, Amanda Askell, Yuntao Bai, Anna Chen, Tom\\nConerly, Nova Dassarma, Dawn Drain, Nelson Elhage, et al. Predictability and surprise in large\\ngenerative models. In 2022 ACM Conference on Fairness, Accountability, and Transparency ,\\npages 1747–1764, 2022.\\n[57] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath,\\nBen Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red teaming language models\\nto reduce harms: Methods, scaling behaviors, and lessons learned. CoRR arXiv:2209.07858,\\n2022.\\n[58] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wal-\\nlach, Hal Daum´ e Iii, and Kate Crawford. Datasheets for datasets. Communications of the ACM ,\\n64(12):86–92, 2021.\\n[59] Sebastian Gehrmann, Elizabeth Clark, and Thibault Sellam. Repairing the cracked foundation:\\nA survey of obstacles in evaluation practices for generated text. CoRR arXiv:2202.06935, 2022.\\n[60] Dedre Gentner and Albert L. Stevens. Mental Models . Lawrence Erlbaum Associates, 1983.\\n[61] Katy Ilonka Gero, Zahra Ashktorab, Casey Dugan, Qian Pan, James Johnson, Werner Geyer,\\nMaria Ruiz, Sarah Miller, David R Millen, Murray Campbell, et al. Mental models of AI agents\\nin a cooperative game setting. In Proceedings of the 2020 chi conference on human factors in\\ncomputing systems , pages 1–12, 2020.\\n[62] Ella Glikson and Anita Williams Woolley. Human trust in artificial intelligence: Review of\\nempirical research. Academy of Management Annals , 14(2):627–660, 2020.\\n[63] Jochen G¨ ortler, Fred Hohman, Dominik Moritz, Kanit Wongsuphasawat, Donghao Ren, Rahul\\nNair, Marc Kirchner, and Kayur Patel. Neo: Generalizing confusion matrix visualization to\\nhierarchical and multi-output labels. In Proceedings of the 2022 CHI Conference on Human\\nFactors in Computing Systems , pages 1–13, 2022.\\n[64] Arthur C Graesser, William Baggett, and Kent Williams. Question-driven explanatory reasoning.\\nApplied Cognitive Psychology , 10(7):17–31, 1996.\\n[65] Nico Grant and Karen Weise. In A.I. race, Microsoft and Google choose speed over caution. New\\nYork Times , April 2023.\\n[66] Ben Green and Yiling Chen. Algorithmic risk assessments can alter human decision-making\\nprocesses in high-stakes government contexts. Proceedings of the ACM on Human-Computer\\nInteraction , 5(CSCW2):1–33, 2021.\\n[67] Gabriel Grill and Nazanin Andalibi. Attitudes and folk theories of data subjects on transparency\\nand accuracy in emotion recognition. Proceedings of the ACM on Human-Computer Interaction ,\\n6(CSCW1):1–35, 2022.\\n[68] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural\\nnetworks. In International conference on machine learning , pages 1321–1330. PMLR, 2017.\\n[69] Nitish Gupta, Kevin Lin, Dan Roth, Sameer Singh, and Matt Gardner. Neural module networks\\nfor reasoning over text. CoRR arXiv:1912.04971, 2019.\\n[70] Sai Gurrapu, Ajay Kulkarni, Lifu Huang, Ismini Lourentzou, Laura Freeman, and Feras A\\nBatarseh. Rationalization for explainable nlp: A survey. CoRR arXiv:2301.08912, 2023.\\n[71] Sophia Hadash, Martijn C Willemsen, Chris Snijders, and Wijnand A IJsselsteijn. Improving\\nunderstandability of feature contributions in model-agnostic explainable AI tools. In Proceedings\\nof the 2022 CHI Conference on Human Factors in Computing Systems , pages 1–9, 2022.\\n24 [72] Alex Hanna and Tina M Park. Against scale: Provocations and resistances to scale thinking.\\nCoRR arXiv:2010.08850, 2020.\\n[73] Amy K. Heger, Liz B. Marquis, Mihaela Vorvoreanu, Hanna Wallach, and Jennifer Wort-\\nman Vaughan. Understanding machine learning practitioners’ data documentation perceptions,\\nneeds, challenges, and desiderata. Proceedings of the ACM on Human-Computer Interaction , 6\\n(CSCW2), 2022. doi: 10.1145/3555760.\\n[74] Denis J Hilton. Conversational processes and causal explanation. Psychological Bulletin , 107(1):\\n65, 1990.\\n[75] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom\\nHennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia\\nGuy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack W. Rae, and L. Sifre.\\nTraining compute-optimal large language models. In Advances in Neural Information Processing\\nSystems , 2022.\\n[76] Sarah Holland, Ahmed Hosny, Sarah Newman, Joshua Joseph, and Kasia Chmielinski.\\nThe dataset nutrition label: A framework to drive higher data quality standards. CoRR\\narXiv:1805.03677, 2018.\\n[77] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural\\ntext degeneration. In International Conference on Learning Representations , 2020.\\n[78] Sungsoo Ray Hong, Jessica Hullman, and Enrico Bertini. Human factors in model interpretabil-\\nity: Industry practices, challenges, and needs. Proceedings of the ACM on Human-Computer\\nInteraction , 4(CSCW1):1–26, 2020.\\n[79] Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification.\\nInProceedings of the 56th Annual Meeting of the Association for Computational Linguistics\\n(Volume 1: Long Papers) , pages 328–339, July 2018. doi: 10.18653/v1/P18-1031.\\n[80] David Howcroft, Anya Belz, Miruna Clinciu, Dimitra Gkatzia, Sadid A Hasan, Saad Mahamood,\\nSimon Mille, Emiel Van Miltenburg, Sashank Santhanam, and Verena Rieser. Twenty years of\\nconfusion in human evaluation: Nlg needs evaluation sheets and standardised definition. Asso-\\nciation for Computational Linguistics (ACL), 2020.\\n[81] Eyke H¨ ullermeier and Willem Waegeman. Aleatoric and epistemic uncertainty in machine learn-\\ning: An introduction to concepts and methods. Machine Learning , 110:457–506, 2021.\\n[82] Alon Jacovi and Yoav Goldberg. Towards faithfully interpretable nlp systems: How should we\\ndefine and evaluate faithfulness? CoRR arXiv:2004.03685, 2020.\\n[83] Alon Jacovi, Ana Marasovi´ c, Tim Miller, and Yoav Goldberg. Formalizing trust in artificial\\nintelligence: Prerequisites, causes and goals of human trust in ai. In Proceedings of the 2021\\nACM conference on fairness, accountability, and transparency , pages 624–635, 2021.\\n[84] Sarthak Jain and Byron C. Wallace. Attention is not Explanation. In Proceedings of the 2019\\nConference of the North American Chapter of the Association for Computational Linguistics:\\nHuman Language Technologies, Volume 1 (Long and Short Papers) , 2019. doi: 10.18653/v1/\\nN19-1357.\\n[85] Maurice Jakesch, Jeffrey T Hancock, and Mor Naaman. Human heuristics for AI-generated\\nlanguage are flawed. Proceedings of the National Academy of Sciences , 120(11):e2208839120,\\n2023.\\n[86] Ganesh Jawahar, Muhammad Abdul-Mageed, and Laks Lakshmanan, V.S. Automatic de-\\ntection of machine generated text: A critical survey. In Proceedings of the 28th Inter-\\nnational Conference on Computational Linguistics , pages 2296–2309, December 2020. doi:\\n10.18653/v1/2020.coling-main.208.\\n25 [87] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang,\\nAndrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation.\\nACM Computing Surveys , nov 2022. doi: 10.1145/3571730.\\n[88] Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig. How can we know when language\\nmodels know? on the calibration of language models for question answering. Transactions of the\\nAssociation for Computational Linguistics , 9:962–977, 2021.\\n[89] Philip Johnson-Laird. Mental Models: Towards a Cognitive Science of Language, Inference, and\\nConsciousness . Cambridge University Press, 1983.\\n[90] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez,\\nNicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston,\\nSheer El-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam\\nBowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion,\\nShauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei,\\nTom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared\\nKaplan. Language models (mostly) know what they know. CoRR arXiv:2207.05221, 2022.\\n[91] Harmanpreet Kaur, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna Wallach, and Jen-\\nnifer Wortman Vaughan. Interpreting interpretability: Understanding data scientists’ use of\\ninterpretability tools for machine learning. In Proceedings of the 2020 CHI Conference on Hu-\\nman Factors in Computing Systems (CHI) , 2020.\\n[92] Matthew Kay, Tara Kola, Jessica R Hullman, and Sean A Munson. When (ish) is my bus? user-\\ncentered visualizations of uncertainty in everyday, mobile predictive systems. In Proceedings of\\nthe 2016 chi conference on human factors in computing systems , pages 5092–5103, 2016.\\n[93] Frank C Keil. Explanation and understanding. Annu. Rev. Psychol. , 57:227–254, 2006.\\n[94] Nitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard\\nSocher. Ctrl: A conditional transformer language model for controllable generation. CoRR\\narXiv:1909.05858, 2019.\\n[95] Lina M. Khan. We must regulate A.I. Here’s how. New York Times , May 2023.\\n[96] Mehtab Khan and Alex Hanna. The subjects and stages of AI dataset development: A framework\\nfor dataset accountability. 2022.\\n[97] Been Kim, Rajiv Khanna, and Oluwasanmi O Koyejo. Examples are not enough, learn to\\ncriticize! criticism for interpretability. Advances in neural information processing systems , 29,\\n2016.\\n[98] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, et al.\\nInterpretability beyond feature attribution: Quantitative testing with concept activation vectors\\n(tcav). In International conference on machine learning , pages 2668–2677. PMLR, 2018.\\n[99] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein.\\nA watermark for large language models. CoRR arXiv:2301.10226, 2023.\\n[100] Daniel N. Kluttz, Nitin Kohli, and Deirdre K. Mulligan. Shaping our tools: Contestability as a\\nmeans to promote responsible algorithmic decision making in the professions. In Kevin Werbach,\\neditor, After the Digital Tornado: Networks, Algorithms, Humanity . Cambridge University Press,\\n2020.\\n[101] Bran Knowles. Explainable AI: Another successful failure? In 2022 CHI Workshop on Human-\\nCentered Explainable AI , 2022.\\n[102] Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions.\\nInProceedings of the 34th International Conference on Machine Learning (ICML) , pages 1885–\\n1894, 2017.\\n26 [103] Sarah Kreps, R Miles McCain, and Miles Brundage. All the news that’s ft to fabricate: AI-\\ngenerated text as a tool of media misinformation. Journal of Experimental Political Science , 9\\n(1):104–117, 2022.\\n[104] Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty: Linguistic invariances\\nfor uncertainty estimation in natural language generation. In The Eleventh International Con-\\nference on Learning Representations (ICLR) , 2023.\\n[105] Todd Kulesza, Simone Stumpf, Margaret Burnett, and Irwin Kwan. Tell me more? the effects\\nof mental model soundness on personalizing an intelligent agent. In Proceedings of the sigchi\\nconference on human factors in computing systems , pages 1–10, 2012.\\n[106] Sachin Kumar, Vidhisha Balachandran, Lucille Njoo, Antonios Anastasopoulos, and Yulia\\nTsvetkov. Language generation models can cause harm: So what can we do about it? An\\nactionable survey. In Proceedings of the The 17th Conference of the European Chapter of the\\nAssociation for Computational Linguistics , 2023.\\n[107] Vivian Lai, Yiming Zhang, Chacha Chen, Q. Vera Liao, and Chenhao Tan. Selective explanations:\\nLeveraging human input to align explainable ai. CSCW 2023 , 2023.\\n[108] Ellen J Langer, Arthur Blank, and Benzion Chanowitz. The mindlessness of ostensibly thoughtful\\naction: The role of “placebic” information in interpersonal interaction. Journal of Personality\\nand Social Psychology , 36(6):635, 1978.\\n[109] Markus Langer, Daniel Oster, Timo Speith, Holger Hermanns, Lena Kstner, Eva Schmidt, An-\\ndreas Sesing, and Kevin Baum. What do we want from explainable artificial intelligence (xai)? A\\nstakeholder perspective on XAI and a conceptual model guiding interdisciplinary XAI research.\\nArtificial Intelligence , 296, 2021. doi: 10.1016/j.artint.2021.103473.\\n[110] Raina Langevin, Ross J Lordon, Thi Avrahami, Benjamin R Cowan, Tad Hirsch, and Gary\\nHsieh. Heuristic evaluation of conversational agents. In Proceedings of the 2021 CHI Conference\\non Human Factors in Computing Systems , pages 1–15, 2021.\\n[111] Cheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang. Mixout: Effective regularization to\\nfinetune large-scale pretrained language models. In International Conference on Learning Rep-\\nresentations (ICLR) , 2020.\\n[112] Min Kyung Lee, Anuraag Jain, Hea Jin Cha, Shashank Ojha, and Daniel Kusbit. Procedural\\njustice in algorithmic fairness: Leveraging transparency and outcome control for fair algorithmic\\nmediation. Proceedings of the ACM on Human-Computer Interaction , 3(CSCW):1–26, 2019.\\n[113] Peter Lee, Carey Goldberg, and Isaac Kohane. The AI Revolution in Medicine: GPT-4 and\\nBeyond . Pearson, 2023.\\n[114] Tao Lei, Regina Barzilay, and Tommi Jaakkola. Rationalizing neural predictions. CoRR\\narXiv:1606.04155, 2016.\\n[115] Daliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix\\nYu, and Sanjiv Kumar. Large language models with controllable working memory. CoRR\\narXiv:2211.05110, 2022.\\n[116] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga,\\nYian Zhang, D. Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan,\\nBobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher R´ e, Diana\\nAcosta-Navas, Drew A. Hudson, E. Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu\\nRen, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel J. Orr, Lucia Zheng, Mert Yuksek-\\ngonul, Mirac Suzgun, Nathan S. Kim, Neel Guha, Niladri S. Chatterji, O. Khattab, Peter Hen-\\nderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, S. Ganguli, Tatsunori\\nHashimoto, Thomas F. Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li,\\nYifan Mai, Yuhui Zhang, and Yuta Koreeda. Holistic evaluation of language models. CoRR\\narXiv:2211.09110, 2022.\\n27 [117] Q. Vera Liao and S Shyam Sundar. Designing for responsible trust in AI systems: A commu-\\nnication perspective. In 2022 ACM Conference on Fairness, Accountability, and Transparency ,\\npages 1257–1268, 2022.\\n[118] Q. Vera Liao and Kush R. Varshney. Human-centered explainable AI (XAI): From algorithms\\nto user experiences. CoRR arXiv:2110.10790, 2021.\\n[119] Q. Vera Liao, Daniel Gruen, and Sarah Miller. Questioning the AI: informing design practices for\\nexplainable AI user experiences. In Proceedings of the 2020 CHI Conference on Human Factors\\nin Computing Systems , pages 1–15, 2020.\\n[120] Q. Vera Liao, Yunfeng Zhang, Ronny Luss, Finale Doshi-Velez, and Amit Dhurandhar. Con-\\nnecting algorithmic research and usage contexts: A perspective of contextualized evaluation for\\nexplainable ai. In Proceedings of the AAAI Conference on Human Computation and Crowdsourc-\\ning, volume 10, pages 147–159, 2022.\\n[121] Q. Vera Liao, Hariharan Subramonyam, Jennifer Wang, and Jennifer Wortman Vaughan. De-\\nsignerly understanding: Information needs for model transparency to support design ideation\\nfor AI-powered user experience. Proceedings of the 2023 CHI conference on human factors in\\ncomputing systems , 2023.\\n[122] Gabriel Lima, Nina Grgi´ c-Hlaˇ ca, Jin Keun Jeong, and Meeyoung Cha. The conflict between\\nexplainable and accountable decision-making algorithms. In 2022 ACM Conference on Fairness,\\nAccountability, and Transparency , pages 2103–2113, 2022.\\n[123] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization\\nbranches out , pages 74–81, 2004.\\n[124] Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic hu-\\nman falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational\\nLinguistics (Volume 1: Long Papers) , pages 3214–3252, 2022. doi: 10.18653/v1/2022.acl-long.\\n229.\\n[125] Stephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in\\nwords. Transactions on Machine Learning Research , 2022. ISSN 2835-8856.\\n[126] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen.\\nWhat makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out\\n(DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning\\nArchitectures , pages 100–114, May 2022. doi: 10.18653/v1/2022.deelio-1.10.\\n[127] Nelson F. Liu, Tianyi Zhang, and Percy Liang. Evaluating verifiability in generative search\\nengines. CoRR arXiv:2304.09848, 2023.\\n[128] Tania Lombrozo. Explanation and abductive inference. 2012.\\n[129] Tania Lombrozo. Explanatory preferences shape learning and inference. Trends in cognitive\\nsciences , 20(10):748–759, 2016.\\n[130] Scott Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In Advances\\nin Neural Information Processing Systems 30 (NIPS) , pages 4765–4774, 2017.\\n[131] Henrietta Lyons, Eduardo Velloso, and Tim Miller. Conceptualising contestability: Perspectives\\non contesting algorithmic decisions. Proceedings of the ACM on Human-Computer Interaction ,\\n5(CSCW1):1–25, 2021.\\n[132] Qing Lyu, Marianna Apidianaki, and Chris Callison-Burch. Towards faithful model explanation\\nin nlp: A survey. CoRR arXiv:2209.11326, 2022.\\n[133] Michael Madaio, Lisa Egede, Hariharan Subramonyam, Jennifer Wortman Vaughan, and Hanna\\nWallach. Assessing the fairness of AI systems: AI practitioners’ processes, challenges, and\\nneeds for support. Proceedings of the ACM on Human-Computer Interaction , 6(CSCW1). doi:\\n10.1145/3512899.\\n28 [134] Michael A Madaio, Luke Stark, Jennifer Wortman Vaughan, and Hanna Wallach. Co-designing\\nchecklists to understand organizational challenges and opportunities around fairness in AI. In\\nProceedings of the 2020 CHI Conference on Human Factors in Computing Systems , pages 1–14,\\n2020.\\n[135] Andreas Madsen, Siva Reddy, and Sarath Chandar. Post-hoc interpretability for neural NLP: A\\nsurvey. ACM Computing Surveys , 55(8):1–42, 2022.\\n[136] Kyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy Kanwisher, Joshua B Tenenbaum, and\\nEvelina Fedorenko. Dissociating language and thought in large language models: a cognitive\\nperspective. CoRR arXiv:2301.06627, 2023.\\n[137] Bertram F Malle. How the mind explains behavior: Folk explanations, meaning, and social\\ninteraction . MIT press, 2006.\\n[138] Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. On faithfulness and factu-\\nality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association\\nfor Computational Linguistics , pages 1906–1919, 2020. doi: 10.18653/v1/2020.acl-main.173.\\n[139] Albert Meijer. Understanding the complex dynamics of transparency. Public administration\\nreview , 73(3):429–439, 2013.\\n[140] Dana¨ e Metaxa, Joon Sung Park, Ronald E Robertson, Karrie Karahalios, Christo Wilson, Jeff\\nHancock, Christian Sandvig, et al. Auditing algorithms: Understanding algorithmic systems\\nfrom the outside in. Foundations and Trends in Human–Computer Interaction , 14(4):272–344,\\n2021.\\n[141] Gr´ egoire Mialon, Roberto Dessi, Maria Lomeli, Christoforos Nalmpantis, Ramakanth Pasunuru,\\nRoberta Raileanu, Baptiste Roziere, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard\\nGrave, Yann LeCun, and Thomas Scialom. Augmented language models: A survey. Transactions\\non Machine Learning Research , 2023. ISSN 2835-8856. Survey Certification.\\n[142] Tim Miller. Explanation in artificial intelligence: Insights from the social sciences. Artificial\\nintelligence , 267:1–38, 2019.\\n[143] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchin-\\nson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting.\\nInProceedings of the conference on fairness, accountability, and transparency , pages 220–229,\\n2019.\\n[144] Jakob M¨ okander, Jonas Schuett, Hannah Rose Kirk, and Luciano Floridi. Auditing large lan-\\nguage models: A three-layered approach. CoRR arXiv:2302.08500, 2023.\\n[145] Ida Momennejad, Hosein Hasanbeig, Felipe Vieira Frujeri, Hiteshi Sharma, Robert Ness, Nebojsa\\nJojic, Hamid Palangi, and Jonathan Larson. Evaluating cognitive maps in large language models:\\nNo emergent planning. Working paper, 2023.\\n[146] Ramaravind K Mothilal, Amit Sharma, and Chenhao Tan. Explaining machine learning classifiers\\nthrough diverse counterfactual explanations. In Proceedings of the 2020 conference on fairness,\\naccountability, and transparency , pages 607–617, 2020.\\n[147] Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well calibrated\\nprobabilities using bayesian binning. In Proceedings of the AAAI conference on artificial intelli-\\ngence , volume 29, 2015.\\n[148] Clifford Nass and Youngme Moon. Machines and mindlessness: Social responses to computers.\\nJournal of social issues , 56(1):81–103, 2000.\\n[149] Don A. Norman. Some observations on mental models. In R. M. Baecker and W. A. S. Buxton,\\neditors, Human-Computer Interaction: A Multidisciplinary Approach , pages 241–244. Morgan\\nKaufmann Publishers Inc., 1987.\\n29 [150] Donald A Norman. Some observations on mental models. In Mental models , pages 15–22.\\nPsychology Press, 2014.\\n[151] OpenAI. GPT-4 technical report. CoRR arXiv:2303.08774, 2023.\\n[152] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to\\nfollow instructions with human feedback. Advances in Neural Information Processing Systems ,\\n35:27730–27744, 2022.\\n[153] Raja Parasuraman and Dietrich H Manzey. Complacency and bias in human use of automation:\\nAn attentional integration. Human factors , 52(3):381–410, 2010.\\n[154] Forough Poursabzi-Sangdeh, Daniel G. Goldstein, Jake M. Hofman, Jennifer Wortman Vaughan,\\nand Hanna Wallach. Manipulating and measuring model interpretability. In 2021 ACM CHI\\nConference on Human Factors in Computing Systems , 2021.\\n[155] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\\nunderstanding with unsupervised learning. Technical report, OpenAI, 2018.\\n[156] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Lan-\\nguage models are unsupervised multitask learners. OpenAI white paper, 2019.\\n[157] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song,\\nJohn Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models:\\nMethods, analysis & insights from training gopher. CoRR arXiv:2112.11446, 2021.\\n[158] Inioluwa Deborah Raji, Emily Denton, Emily M Bender, Alex Hanna, and Amandalynne\\nPaullada. Ai and the everything in the whole wide world benchmark. In Thirty-fifth Conference\\non Neural Information Processing Systems Datasets and Benchmarks Track (Round 2) .\\n[159] Bogdana Rakova, Jingying Yang, Henriette Cramer, and Rumman Chowdhury. Where re-\\nsponsible AI meets reality: Practitioner perspectives on enablers for shifting organizational\\npractices. Proceedings of the ACM on Human-Computer Interaction , 5(CSCW1), 2021. doi:\\n10.1145/3449081.\\n[160] Amy Rechkemmer and Ming Yin. When confidence meets accuracy: Exploring the effects of\\nmultiple performance indicators on trust in machine learning models. In Proceedings of the 2022\\nchi conference on human factors in computing systems , pages 1–14, 2022.\\n[161] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. “Why should I trust you?”: Explain-\\ning the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International\\nConference on Knowledge Discovery and Data Mining (KDD) , pages 1135–1144, 2016.\\n[162] Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and\\nuse interpretable models instead. Nature machine intelligence , 1(5):206–215, 2019.\\n[163] Chris Russell. Efficient search for diverse coherent explanations. In Proceedings of the Conference\\non Fairness, Accountability, and Transparency (FAT*) , pages 20–28, 2019.\\n[164] Ananya B Sai, Akash Kumar Mohankumar, and Mitesh M Khapra. A survey of evaluation\\nmetrics used for nlg systems. ACM Computing Surveys (CSUR) , 55(2):1–39, 2022.\\n[165] Christian Sandvig, Kevin Hamilton, Karrie Karahalios, and Cedric Langbort. Auditing algo-\\nrithms: Research methods for detecting discrimination on internet platforms. Data and discrim-\\nination: converting critical concerns into productive inquiry , 22(2014):4349–4357, 2014.\\n[166] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili´ c, Daniel Hesslow, Roman\\nCastagn´ e, Alexandra Sasha Luccioni, Fran¸ cois Yvon, Matthias Gall´ e, et al. Bloom: A 176b-\\nparameter open-access multilingual language model. CoRR arXiv:2211.05100, 2022.\\n[167] Philipp Schmidt, Felix Biessmann, and Timm Teubner. Transparency and trust in artificial\\nintelligence systems. Journal of Decision Systems , 29(4):260–278, 2020.\\n30 [168] Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with\\npointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for\\nComputational Linguistics (Volume 1: Long Papers) , pages 1073–1083, Vancouver, Canada, July\\n2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1099.\\n[169] Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. Auto-\\nPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts.\\nInProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\\n(EMNLP) , pages 4222–4235, November 2020. doi: 10.18653/v1/2020.emnlp-main.346.\\n[170] Patrice Y Simard, Saleema Amershi, David M Chickering, Alicia Edelman Pelton, Soroush\\nGhorashi, Christopher Meek, Gonzalo Ramos, Jina Suh, Johan Verwey, Mo Wang, et al.\\nMachine teaching: A new paradigm for building machine learning systems. arXiv preprint\\narXiv:1707.06742 , 2017.\\n[171] Alison Smith-Renner, Ron Fan, Melissa Birchfield, Tongshuang Wu, Jordan Boyd-Graber,\\nDaniel S Weld, and Leah Findlater. No explainability without accountability: An empirical\\nstudy of explanations and feedback in interactive ML. In Proceedings of the 2020 chi conference\\non human factors in computing systems , pages 1–13, 2020.\\n[172] Kacper Sokol and Peter Flach. Explainability fact sheets: a framework for systematic assessment\\nof explainable approaches. In Proceedings of the 2020 Conference on Fairness, Accountability,\\nand Transparency , pages 56–67, 2020.\\n[173] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam\\nFisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri` a Garriga-Alonso, et al. Beyond the\\nimitation game: Quantifying and extrapolating the capabilities of language models. CoRR\\narXiv:2206.04615, 2022.\\n[174] Elias Storms, Oscar Alvarado, and Luciana Monteiro-Krebs. ’transparency is meant for con-\\ntrol’and vice versa: Learning from co-designing and evaluating algorithmic news recommenders.\\nProceedings of the ACM on Human-Computer Interaction , 6(CSCW2):1–24, 2022.\\n[175] Jiao Sun, Q. Vera Liao, Michael Muller, Mayank Agarwal, Stephanie Houde, Kartik Tala-\\nmadupula, and Justin D Weisz. Investigating explainability of generative AI for code through\\nscenario-based design. In 27th International Conference on Intelligent User Interfaces , pages\\n212–228, 2022.\\n[176] Harini Suresh, Steven R. Gomez, Kevin K. Nam, and Arvind Satyanarayan. Beyond expertise\\nand roles: A framework to characterize the stakeholders of interpretable machine learning and\\ntheir needs. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems\\n(CHI 21) , 2021. doi: 10.1145/3411764.3445088.\\n[177] Maxwell Szymanski, Martijn Millecamp, and Katrien Verbert. Visual, textual or hybrid: the\\neffect of user expertise on different explanations. In 26th International Conference on Intelligent\\nUser Interfaces , pages 109–119, 2021.\\n[178] Oyvind Tafjord, Bhavana Dalvi Mishra, and Peter Clark. Proofwriter: Generating implications,\\nproofs, and abductive statements over natural language. CoRR arXiv:2012.13048, 2020.\\n[179] Reuben Tan, Bryan Plummer, and Kate Saenko. Detecting cross-modal inconsistency to defend\\nagainst neural fake news. In Proceedings of the 2020 Conference on Empirical Methods in Nat-\\nural Language Processing (EMNLP) , pages 2081–2106, November 2020. doi: 10.18653/v1/2020.\\nemnlp-main.163.\\n[180] Anja Thieme, Ed Cutrell, Cecily Morrison, Alex Taylor, and Abigail Sellen. Interpretability as\\na dynamic of human-AI collaboration. ACM Interactions , 27(5):40–45, 2020.\\n[181] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Tim-\\noth´ ee Lacroix, Baptiste Rozi` ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\\nand efficient foundation language models. CoRR arXiv:2302.13971, 2023.\\n31 [182] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas\\nBlecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernan-\\ndes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-\\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian\\nKhabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut\\nLavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,\\nPushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta,\\nKalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiao-\\nqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng\\nYan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien\\nRodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation\\nand fine-tuned chat models. CoRR arXiv:2307.09288, 2023.\\n[183] Miles Turpin, Julian Michael, Ethan Perez, and Samuel R. Bowman. Language models don’t\\nalways say what they think: Unfaithful explanations in chain-of-thought prompting. CoRR\\narXiv:2305.04388, 2023.\\n[184] Berk Ustun, Alexander Spangher, and Yang Liu. Actionable recourse in linear classification.\\nInProceedings of the Conference on Fairness, Accountability, and Transparency (FAT*) , pages\\n10–19, 2019.\\n[185] Anne Marthe Van Der Bles, Sander Van Der Linden, Alexandra LJ Freeman, James Mitchell,\\nAna B Galvao, Lisa Zaval, and David J Spiegelhalter. Communicating uncertainty about facts,\\nnumbers and science. Royal Society open science , 6(5):181870, 2019.\\n[186] Helena Vasconcelos, Gagan Bansal, Adam Fourney, Q. Vera Liao, and Jennifer Wortman\\nVaughan. Generation probabilities are not enough: Exploring the effectiveness of uncertainty\\nhighlighting in AI-powered code completions. CoRR arXiv:2302.07248, 2023.\\n[187] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\n Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information\\nProcessing Systems , volume 30, 2017.\\n[188] Jennifer Wortman Vaughan and Hanna Wallach. A human-centered agenda for intelligible ma-\\nchine learning. In Marcello Pelillo and Teresa Scantamburlo, editors, Machines We Trust: Per-\\nspectives on Dependable AI . MIT Press, 2021.\\n[189] Oleksandra Vereschak, Gilles Bailly, and Baptiste Caramiaux. How to evaluate trust in ai-\\nassisted decision making? a survey of empirical methodologies. Proceedings of the ACM on\\nHuman-Computer Interaction , 5(CSCW2):1–39, 2021.\\n[190] Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and\\nStuart Shieber. Investigating gender bias in language models using causal mediation analysis.\\nAdvances in neural information processing systems , 33:12388–12401, 2020.\\n[191] Danding Wang, Wencan Zhang, and Brian Y Lim. Show or suppress? managing input uncertainty\\nin machine learning model explanations. Artificial Intelligence , 294:103456, 2021.\\n[192] Xinru Wang and Ming Yin. Are explanations helpful? A comparative study of the effects of\\nexplanations in AI-assisted decision-making. In 26th International Conference on Intelligent\\nUser Interfaces , pages 318–328, 2021.\\n[193] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani\\nYogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large\\nlanguage models. CoRR arXiv:2206.07682, 2022.\\n[194] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi,\\nQuoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language\\nmodels. In Advances in Neural Information Processing Systems , 2022.\\n32 [195] Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang,\\nMyra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et al. Ethical and social risks of harm\\nfrom language models. CoRR arXiv:2112.04359, 2021.\\n[196] Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor,\\nAmelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, Courtney Biles, Sasha Brown,\\nZac Kenton, Will Hawkins, Tom Stepleton, Abeba Birhane, Lisa Anne Hendricks, Laura Rimell,\\nWilliam Isaac, Julia Haas, Sean Legassick, Geoffrey Irving, and Iason Gabriel. Taxonomy of\\nrisks posed by language models. In Proceedings of the 2022 ACM Conference on Fairness,\\nAccountability, and Transparency (FAccT) , pages 214–229, 2022. doi: 10.1145/3531146.3533088.\\n[197] Christopher D Wickens, Benjamin A Clegg, Alex Z Vieane, and Angelia L Sebok. Complacency\\nand automation bias in the use of imperfect automation. Human factors , 57(5):728–739, 2015.\\n[198] Sarah Wiegreffe and Yuval Pinter. Attention is not not explanation. CoRR arXiv:1908.04626,\\n2019.\\n[199] Jeffery D Wilfong. Computer anxiety and anger: The impact of computer use, computer expe-\\nrience, and self-efficacy beliefs. Computers in human behavior , 22(6):1001–1011, 2006.\\n[200] Tongshuang Wu, Marco Tulio Ribeiro, Jeffrey Heer, and Daniel S Weld. Errudite: Scalable,\\nreproducible, and testable error analysis. In Proceedings of the 57th Annual Meeting of the\\nAssociation for Computational Linguistics , pages 747–763, 2019.\\n[201] Tongshuang Wu, Michael Terry, and Carrie Jun Cai. AI chains: Transparent and controllable\\nhuman-AI interaction by chaining large language model prompts. In Proceedings of the 2022\\nCHI Conference on Human Factors in Computing Systems , pages 1–22, 2022.\\n[202] Qian Yang, Yuexing Hao, Kexin Quan, Stephen Yang, Yiran Zhao, Volodymyr Kuleshov, and\\nFei Wang. Harnessing biomedical literature to calibrate clinicians’ trust in AI decision support\\nsystems. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems ,\\npages 1–14, 2023.\\n[203] Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Josh Tenenbaum.\\nNeural-symbolic vqa: Disentangling reasoning from vision and language understanding. Advances\\nin neural information processing systems , 31, 2018.\\n[204] Ming Yin, Jennifer Wortman Vaughan, and Hanna Wallach. Understanding the effect of accuracy\\non trust in machine learning models. In Proceedings of the 2019 CHI Conference on Human\\nFactors in Computing Systems (CHI) , 2019.\\n[205] Ning Yu, Vladislav Skripniuk, Sahar Abdelnabi, and Mario Fritz. Artificial fingerprinting for gen-\\nerative models: Rooting deepfake attribution in training data. In Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision (ICCV) , pages 14448–14457, October 2021.\\n[206] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roes-\\nner, and Yejin Choi. Defending against neural fake news. In Advances in Neural Information\\nProcessing Systems , volume 32, 2019.\\n[207] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore:\\nEvaluating text generation with bert. CoRR arXiv:1904.09675, 2019.\\n[208] Yunfeng Zhang, Q. Vera Liao, and Rachel KE Bellamy. Effect of confidence and explanation\\non accuracy and trust calibration in AI-assisted decision making. In Proceedings of the 2020\\nconference on fairness, accountability, and transparency , pages 295–305, 2020.\\n[209] Jiawei Zhou, Yixuan Zhang, Qianni Luo, Andrea G Parker, and Munmun De Choudhury. Syn-\\nthetic lies: Understanding AI-generated misinformation and evaluating algorithmic and human\\nsolutions. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems\\n(CHI 23) , 2023.\\n33 '}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_papers(\"New innovations regarding Local LLMs and OOLAMA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_index = build_index()\n",
    "arxiv_query_engine = arxiv_index.as_query_engine(top_k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='7dee4147-f1b2-439e-85f4-26c1f00aad66', embedding=None, metadata={'file_path': 'data/json/data_5.json', 'file_name': 'data_5.json', 'file_type': 'application/json', 'file_size': 73744, 'creation_date': '2024-01-22', 'last_modified_date': '2024-01-22', 'last_accessed_date': '2024-01-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='3549464c-2cfb-4931-9d7a-e32fc8f1394c', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': 'data/json/data_5.json', 'file_name': 'data_5.json', 'file_type': 'application/json', 'file_size': 73744, 'creation_date': '2024-01-22', 'last_modified_date': '2024-01-22', 'last_accessed_date': '2024-01-22'}, hash='49c7c1b193874418f2f12ab8b191079b34be05fe7a4c220ce1625d65ad420e47'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='a7137b54-7651-44cb-bcbe-044a9f0e1ae1', node_type=<ObjectType.TEXT: '1'>, metadata={'file_path': 'data/json/data_4.json', 'file_name': 'data_4.json', 'file_type': 'application/json', 'file_size': 41357, 'creation_date': '2024-01-22', 'last_modified_date': '2024-01-22', 'last_accessed_date': '2024-01-22'}, hash='e77cf23c666cca056c6528271d527785059c964dfe5163fe83a6b3898dd7eaee'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='19cd4bb4-4b65-49e0-8faf-c7d270496f86', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='eb770b4306e74ad2f3f6207d105d7f5671275a6b5dd2299a4069493765b063e5')}, hash='0a273d6afcb60800a88843cd8eb66a0fb309b480569ea861971f326b71395bb8', text='{\"url\": \"http://arxiv.org/pdf/2304.00612v1\", \"title\": \"Eight Things to Know about Large Language Models\", \"text\": \"Eight Things to Know about Large Language Models\\\\nSamuel R. Bowman1 2\\\\nAbstract\\\\nThe widespread public deployment of large lan-\\\\nguage models (LLMs) in recent months has\\\\nprompted a wave of new attention and engage-\\\\nment from advocates, policymakers, and scholars\\\\nfrom many \\\\ufb01elds. This attention is a timely re-\\\\nsponse to the many urgent questions that this tech-\\\\nnology raises, but it can sometimes miss important\\\\nconsiderations. This paper surveys the evidence\\\\nfor eight potentially surprising such points:\\\\n1.LLMs predictably get more capable with in-\\\\ncreasing investment, even without targeted\\\\ninnovation.\\\\n2.Many important LLM behaviors emerge un-\\\\npredictably as a byproduct of increasing in-\\\\nvestment.\\\\n3.LLMs often appear to learn and use repre-\\\\nsentations of the outside world.\\\\n4.There are no reliable techniques for steering\\\\nthe behavior of LLMs.\\\\n5.Experts are not yet able to interpret the inner\\\\nworkings of LLMs.\\\\n6.Human performance on a task isn\\\\u2019t an upper\\\\nbound on LLM performance.\\\\n7.LLMs need not express the values of their\\\\ncreators nor the values encoded in web text.\\\\n8.Brief interactions with LLMs are often mis-\\\\nleading.\\\\nIntroduction\\\\nLarge language models (LLMs, e.g. GPT-3, PALM,\\\\nLLaMA, and GPT-4; Brown et al., 2020; Chowdhery et al.,\\\\n2022; Touvron et al., 2023; OpenAI, 2023b) and products\\\\nbuilt on them, such as ChatGPT, have recently prompted\\\\nan enormous amount of attention from journalists, (Klein,\\\\n2023; Perrigo, 2023; Oliver, 2023), policymakers (J & C,\\\\n2023; Bartz, 2023; Lieu, 2023), and scholars from many\\\\n1New York University2Anthropic, PBC. Correspondence to:\\\\nSamuel R. Bowman <bowman@nyu.edu >.\\\\ufb01elds (Chan, 2022; Lund & Wang, 2023; Choi et al., 2023;\\\\nBiswas, 2023). This technology de\\\\ufb01es expectations in many\\\\nways, though, and it can be easy for brief discussions of it\\\\nto leave out important points.\\\\nThis paper presents eight potentially surprising claims that\\\\nI expect will be salient in at least some of the conversa-\\\\ntions that are springing up around LLMs. They re\\\\ufb02ect, to\\\\nthe best of my understanding, views that are reasonably\\\\nwidely shared among the researchers\\\\u2014largely based in pri-\\\\nvate labs\\\\u2014who have been developing these models. All the\\\\nevidence I present here, as well as most of the arguments,\\\\nare collected from prior work, and I encourage anyone who\\\\n\\\\ufb01nds these claims useful to consult (and directly cite) the\\\\nsources named here.\\\\nI do not mean for these claims to be normative in any signif-\\\\nicant way. Rather, this work is motivated by the recognition\\\\nthat deciding what we should do in light of this disruptive\\\\nnew technology is a question that is best led\\\\u2014in an informed\\\\nway\\\\u2014by scholars, advocates, and lawmakers from outside\\\\nthe core technical R&D community.\\\\n1. LLMs predictably get more capable with\\\\nincreasing investment, even without\\\\ntargeted innovation\\\\nScaling law results (Kaplan et al., 2020; Brown et al., 2020;\\\\nHoffmann et al., 2022) have been a major driving factor\\\\nin the recent surge of research and investment into LLMs\\\\n(Ganguli et al., 2022a). Scaling laws allow us to precisely\\\\npredict some coarse-but-useful measures of how capable\\\\nfuture models will be as we scale them up along three dimen-\\\\nsions: the amount of data they are fed, their size (measured\\\\nin parameters), and the amount of computation used to train\\\\nthem (measured in FLOPs).', start_char_idx=0, end_char_idx=3534, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.7735088158604257),\n",
       " NodeWithScore(node=TextNode(id_='1818108e-1508-4707-8964-d5723a99bf00', embedding=None, metadata={'file_path': 'data/json/data_9.json', 'file_name': 'data_9.json', 'file_type': 'application/json', 'file_size': 146682, 'creation_date': '2024-01-22', 'last_modified_date': '2024-01-22', 'last_accessed_date': '2024-01-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='d037d22c-bc10-4686-9946-80101595803b', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': 'data/json/data_9.json', 'file_name': 'data_9.json', 'file_type': 'application/json', 'file_size': 146682, 'creation_date': '2024-01-22', 'last_modified_date': '2024-01-22', 'last_accessed_date': '2024-01-22'}, hash='d17934f83ee40ccded39faa490a9804bff5f05ca558ce72c6228e5c5186acdce'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='58996ce7-5080-4ae6-8335-22e3db351602', node_type=<ObjectType.TEXT: '1'>, metadata={'file_path': 'data/json/data_9.json', 'file_name': 'data_9.json', 'file_type': 'application/json', 'file_size': 146682, 'creation_date': '2024-01-22', 'last_modified_date': '2024-01-22', 'last_accessed_date': '2024-01-22'}, hash='9c713f63eab4c08f4cf51cb3c19dc7bf9f0f999cc6b66ad8c875c028c0cc113a'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='18186f81-60ca-4b04-bd67-e3f73f728de0', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='74bbfa429075bebe65cc7d577d5111a9fe39ae3458f8cb7e520dde1f51080b8f')}, hash='d314b978be0e8c84b0e5bf5f3f9459b55d19a4902b179008574cb30a4e83c717', text='End-users may not interact with LLMs directly, but rather\\\\nthrough LLM-infused applications. Emerging applications include general and specialized chatbots,\\\\nweb search, programming assistants, productivity tools such as for writing support or presentation\\\\ngeneration, and text analysis tools such as for customer insights discovery. As LLMs\\\\u2019 capabilities\\\\ncontinue to be discovered, we can only expect the number and variety of LLM-infused applications\\\\nto grow. While any opacity of the model will likely trickle down to hinder the transparency of the\\\\napplications built on them, as mentioned above, the transparency requirements for LLM-infused ap-\\\\nplications will be different from the model as they serve a different set of stakeholders. Furthermore,\\\\njust as the models themselves are flexible, the use cases for LLM-infused applications can be flexible\\\\nand open-ended. For example, an LLM-infused search engine may be used to plan a trip, research\\\\na report, or write a poem\\\\u2014use cases that reflect different needs in terms of accuracy, verifiability of\\\\noutput, and likely the required transparency approaches.\\\\nTo further complicate transparency around LLM-infused applications, such applications may not\\\\nbe built on a single LLM, but may involve many interacting models and tools. For example, auxiliary\\\\nLLMs can be used to augment the output or expand the capabilities of a primary LLM. LLMs can\\\\nbe embedded in a complex system to operate other models or external services, for example through\\\\nplugins, allowing them to perform tasks like ordering groceries or booking flights with no human in the\\\\nloop. An application may also include other components like input or output filters. For example, an\\\\nLLM-infused search engine may rely on results obtained from a traditional search engine to \\\\u201cground\\\\u201d\\\\nits responses. Changes to any component can change the behavior of the application, making it more\\\\n4 difficult to understand its behavior. Approaches to transparency must therefore take into account all\\\\ncomponents and how they fit together rather than focusing on an LLM in isolation.\\\\nExpanded and Diverse Stakeholders. As the number of LLM-infused applications grows and\\\\npopular applications such as LLM-infused search engines expand their user bases, a larger number\\\\nof people\\\\u2014diverse along many dimensions\\\\u2014will interact with or be impacted by LLMs. Research\\\\nin AI transparency typically considers stakeholder groups like data scientists and model developers,\\\\nbusiness decision-makers, regulators and auditors, end-users, and impacted groups (i.e., the people\\\\nwho are directly or indirectly affected by a model or application) [78, 118, 188]. The use of LLMs\\\\nmay introduce new stakeholder groups with unique transparency needs. For example, it is increasingly\\\\ncommon for product teams to have dedicated prompt engineers\\\\u2014a role that, until recently, did not\\\\nexist\\\\u2014to streamline tasks, evaluate models, or contribute to model adaptation. As another example,\\\\nas LLMs are increasingly used for productivity support to augment people\\\\u2019s writing, we must consider\\\\nboth the creators of LLM-assisted articles and the consumers of these articles as \\\\u201cusers\\\\u201d of the LLM\\\\u2019s\\\\noutputs and support both groups\\\\u2019 transparency needs. Meanwhile, we must support any subjects\\\\nreferred to in the articles as \\\\u201cimpacted groups.\\\\u201d\\\\nAs the pre-trained nature of LLMs lowers the barrier to using and building on AI capabilities, we\\\\nbelieve application developers\\\\u2014including those working on model adaptation\\\\u2014will become a signif-\\\\nicant group and diverse in itself, potentially including developers, entrepreneurs, product managers,\\\\ndesigners, or essentially anyone. In some cases, the line between application developers and end-users\\\\nmay be blurred. Consider, for example, a writer who experiments with using an LLM for writing sup-\\\\nport. This writer might benefit from model transparency to assess the LLM\\\\u2019s suitability for different\\\\nwriting tasks and identify effective ways to adapt the model for each task.\\\\nRecent research has begun to inquire about the ecosystem of LLMs and the roles in it [22], from\\\\ndata creation, curation, model training, and model adaptation through to deployment. Identifying\\\\nthese LLM stakeholder roles and supporting their role-, task- and context-specific transparency needs\\\\nwill be of primary importance for the AI transparency research community.\\\\nRapidly Evolving and Often Flawed Public Perception.', start_char_idx=17423, end_char_idx=21963, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.7523891591134068)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_query_engine.retrieve(\"New innovations regarding Local LLMs and OOLAMA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaindex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
