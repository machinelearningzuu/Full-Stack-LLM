{"url": "http://arxiv.org/pdf/2310.19061v1", "title": "Multimodal ChatGPT for Medical Applications: an Experimental Study of GPT-4V", "text": "Multimodal ChatGPT for Medical Applications: an\nExperimental Study of GPT-4V\nZhiling Yan1\u2217Kai Zhang1\u2217Rong Zhou1Lifang He1Xiang Li2Lichao Sun1\u2020\n1Lehigh University,2Massachusetts General Hospital and Harvard Medical School\nAbstract\nIn this paper, we critically evaluate the capabilities of the state-of-the-art multi-\nmodal large language model, i.e., GPT-4 with Vision (GPT-4V), on Visual Question\nAnswering (VQA) task. Our experiments thoroughly assess GPT-4V\u2019s proficiency\nin answering questions paired with images using both pathology and radiology\ndatasets from 11 modalities (e.g. Microscopy, Dermoscopy, X-ray, CT, etc.) and\nfifteen objects of interests (brain, liver, lung, etc.). Our datasets encompass a\ncomprehensive range of medical inquiries, including sixteen distinct question types.\nThroughout our evaluations, we devised textual prompts for GPT-4V , directing it\nto synergize visual and textual information. The experiments with accuracy score\nconclude that the current version of GPT-4V is not recommended for real-world\ndiagnostics due to its unreliable and suboptimal accuracy in responding to diagnos-\ntic medical questions. In addition, we delineate seven unique facets of GPT-4V\u2019s\nbehavior in medical VQA, highlighting its constraints within this complex arena.\nThe complete details of our evaluation cases are accessible at Github.\n1 Introduction\nMedicine is an intrinsically multimodal discipline. Clinicians often use medical images, clinical\nnotes, lab tests, electronic health records, genomics, and more when providing care [Tu et al.,\n2023]. However, although many AI models have demonstrated great potential for biomedical\napplications [Lee et al., 2023b, Nori et al., 2023, Lee et al., 2023a], majority of them focus on just\none type of data [Acosta et al., 2022, Singhal et al., 2023, Zhou et al., 2023]. Some need extra\nadjustments to work well for specific tasks [Zhang et al., 2023a, Chen et al., 2023], while others\nare not available for public use [Tu et al., 2023, Li et al., 2023]. Previously, ChatGPT [OpenAI,\n2022] has shown potential in offering insights for both patients and doctors, aiding in minimizing\nmissed or misdiagnoses [Yunxiang et al., 2023, Han et al., 2023, Wang et al., 2023]. A notable\ninstance was when ChatGPT accurately diagnosed a young boy after 17 doctors had been unable to\nprovide conclusive answers over three years1. The advent of visual input capabilities in the GPT-4\nsystem, known as GPT-4 with Vision (GPT-4V) [OpenAI, 2023], further amplifies this potential.\nThis enhancement allows for more holistic information processing in a question and answer format,\npossibly mitigating language ambiguities\u2014particularly relevant when non-medical experts, such as\npatients, might struggle with accurate descriptions due to unfamiliarity with medical jargon.\n*These authors contributed equally to this work.\n\u2020Corresponding author.\nLichao Sun (lis221@lehigh.edu)\n1https://www.today.com/health/mom-chatgpt-diagnosis-pain-rcna101843\nPreprint. Under review.arXiv:2310.19061v1  [cs.CV]  29 Oct 2023 What is the name of the \nechocardiography view?\nA: Basal 3-chamber view       \nB: Parasternal short-axis view\nC: Apical four-chamber view  \nD: Subcostal view \nC: Apical four-chamber view.\nWhat is the texture of \nthe tumor? \nHyperechoic.\nWhere is the stenosis \nlocated in the RCA? \nIn the mid segment.\nWhere was the tumor \nlocated?\nThe tumor was located \nat the superior pole of \nthe right kidney .\nIs this Hyaline \nmembrane disease?\nYes\nWhat implies impending \nperforation?\nAreas of translucency or \nthinning indicate a risk \nof impending perforation.Neurology\nIn which two ventricles \ncan calcifications be \nseen on this CT scan?\nThe 3rd ventricle and \nthe lateral ventricles.\nBrain  CT\nBrain  MRI\nAngiography\nECHOUltrasound\nAbdominal MRIEndoscopic videos\nCardiology Oncology\nPathological \nimagesPathology\nSurgery\nWhere is the abnormality?\nLeft temporal lobe.\nFigure 1: The diagram of medical departments and their corresponding objects of interest and\nmodalities. We comprehensively consider 11 modalities across 15 objects of interest in the paper.\n1.1 Analysis Dimensions\nIn this paper, we will systematically examine how GPT-4V operates in the medical field using the\nVQA approach. We believe this might become a predominant method for future medical AI like daily\nhealthcare assistant. We will address questions in a hierarchical manner, emphasizing the step-by-step\nprocess of medical progression for an intelligent machine / agent. This approach will help us uncover\nwhat GPT-4V can offer and also delve into its limitations and challenges in real-world medical\napplications.\n\u2022In medical imaging, the fundamental aspect of machine understanding lies in recognizing the used\nmodalities, such as X-Ray, MRI, or microscopy. Following this recognition, an effective system\nshould discern specific objects within these images, from anatomical structures to distinct cellular\nconfigurations. The adeptness of GPT-4V in these preliminary tasks could offer insights into its\nproficiency in interpreting medical images across varied domains, setting the foundation for more\nadvanced evaluations.\n\u2022Central to medical analysis is the notion of localization, or the precise pinpointing of specific\nregions or objects within an image. Such capability is instrumental when demarcating between\nhealthy and pathological tissues or discerning the positions of tumors, vessels, and other salient\nstructures. For a model like GPT-4V , mastering this skill could usher in nuanced analyses, bolstering\nclinical diagnostics, treatment design, and disease tracking.\n\u2022Further deepening the analysis, the precision with which GPT-4V gauges the dimensions of regions\nof interest (ROIs) becomes paramount. Monitoring the dynamics of tumors, evaluating organ\ndimensions, or quantifying lesions holds clinical weight, aiding in diagnostics, surgical planning,\nand gauging ailment severity.\n\u2022Another layer of analytical depth involves the identification of morphological patterns. Such\npatterns\u2014be it the systematic cellular structures in pathology or attributes such as density, form,\nand opacity in radiology\u2014are instrumental for diagnostic deliberations. A case in point is the\npalisade-like cellular organization around necrotic zones, characteristic of glioblastoma multiforme,\na specific brain malignancy.\n2 \u2022Expanding the purview beyond mere visual cues, an integrative diagnostic modality combines\nimagery with textual descriptions, offering a holistic view of a patient\u2019s status. However, the\nefficacy of GPT-4V in such vision-language synthesis warrants exploration, particularly given\nconcerns of potential over-reliance on singular modes, leading to possible incomplete or skewed\ndiagnostic outcomes.\n\u2022A mere answer, devoid of context or clarity, often falls short in the medical domain. Therefore,\nassessing if GPT-4V elucidates its rationale, articulates clearly, and evinces assurance in its\nresponses becomes pivotal. Such a facet not only engenders trust among users but also aligns with\nthe gravity and precision the medical domain demands.\n\u2022Lastly, shaping the user-AI interaction framework remains crucial. Crafting an optimal prompt\ntemplate when querying GPT-4V or similar AI entities can drastically influence response accuracy.\nPreliminary observations suggest that while GPT-4V\u2019s immediate answers might sometimes falter,\ncertain prompt structures channel its analytical depth more effectively. Sharing such findings can\nguide users in their engagement with GPT-4V , optimizing outcomes in personal healthcare inquiries\nand consultations.\n1.2 Highlights\nIn this section, we provide a concise summary of our findings related to the characteristics of GPT-4V\nin the context of medical VQA. These characteristics, depicted in Section 5, directly correspond to\nthe research questions posed earlier:\n(1)GPT-4V consistently recognizes various medical imaging modalities and the objects within them.\n(2)For accurate localization, GPT-4V requires cues, particularly to consider the orientations of\nmedical images across different modalities.\n(3)GPT-4V finds it challenging to discern the size of Regions of Interest (ROI) or objects, especially\nwhen the assessment involves multiple slices, such as CT scans.\n(4)While GPT-4V has the capability to integrate both image and text inputs for diagnostic-related\nqueries, it displays tendencies towards visual and linguistic biases. Specifically, it might either\noveremphasize markings in images or rely excessively on text, neglecting the visual information in\nthe process.\n(5)GPT-4V typically offers cautious responses, emphasizing that it is not a medical professional\n(e.g., radiologist or pathologist). Nonetheless, its answers are thorough and come with detailed\nexplanations. It\u2019s important to note that these explanations, while informative, are not definitive facts\nand should be cross-checked by experts for accuracy.\n(6)Based on the statistical results concerning the accuracy of VQA, the current version of GPT-4V\nis not recommended for real-world diagnostics due to its unreliable and suboptimal accuracy in\nresponding to diagnostic medical questions (see Section 4 for details).\n1.3 Contributions\nThis report provides the following contributions to the community in the realm of medical AI:\n\u22c6We meticulously assess GPT-4V\u2019s performance in responding to visually paired medical queries,\nleveraging datasets from a diverse array of seven imaging modalities such as Microscopy, Der-\nmoscopy, X-ray, and CT, and centering our analysis on fifteen different clinical objects including\nthe brain, liver, and lung. Our comprehensive dataset, uniquely tailored, encompasses sixteen\ndifferent types of medical questions, providing a broad basis for evaluation.\n\u22c6The empirical results, derived from rigorous testing for accuracy, unequivocally suggest that\nthe current version of GPT-4V should not be employed for practical diagnostic purposes. Its\nperformance in responding to diagnostic medical questions demonstrates a lack of reliability and\naccuracy necessary for real-world application.\n\u22c6Our study delineates seven distinct dimensions of GPT-4V\u2019s operational capabilities within the\nmedical VQA context. These dimensions highlight the model\u2019s operational boundaries and shed\nlight on its adaptability and limitations in the demanding realm of medical inquiry.\n3 2 Experimental Setup\nWe outline the experimental setup or case studies employed to address the aforementioned questions\nand objectives. Since GPT-4V has not officially released APIs, we evaluated its capability for medical\nVQA through its dedicated chat interface (the ChatGPT webpage version), initiating our dialogue\nwith image inputs. To eliminate any interference, hints, or biases from a multi-round conversation, we\nbegan a new chat session for each new Q&A case. This ensured that GPT-4V did not unintentionally\nreference information from previous conversations related to different cases. In this report, we\nprioritize evaluating zero-shot performance using the accuracy metric. For closed-ended questions\nwith limited choices, this metric gauges the consistency of GPT-4V\u2019s answers with factual accuracy.\nFor open-ended queries, it assesses how often GPT-4V\u2019s responses contain the correct information.\nWe provide examples on the prompts utilized and the criteria for determining the correctness of the\nanswers in the following:\nClosed-ended Pathology VQA\nPrompt: [image] Is atrophy seen in the brain?\nGPT-4V: I am not a pathologist, but based on the image provided, it appears to be a represen-\ntation of two human brains. The key characteristic of brain atrophy is a noticeable reduction\nin the size or volume of the brain tissue. Comparing the two images, there might be some\ndifferences in the appearance of the gyri (raised portions) and sulci (grooves) between the\ntwo brains, suggesting potential volume changes.\nCriteria: The response exhibits considerable uncertainty, characterized by terms such as\nmight be\u2019 andpotential\u2019 . Furthermore, the answer lacks clarity and directness. For instance,\nwhile the phrase \u2018a noticeable reduction in size\u2019 suggests atrophy, the term \u2018volume changes\u2019\nis ambiguous regarding whether it denotes expansion or reduction. Consequently, we classify\nthis response from GPT-4V as incorrect.\nOpen-ended Radiology VQA\nPrompt: [image] What type of imaging does this not represent?\nGPT-4V: This is an X-ray image of the chest. Thus, it does not represent imaging modalities\nsuch as MRI (Magnetic Resonance Imaging), CT (Computed Tomography), Ultrasound, or\nPET (Positron Emission Tomography).\nCriteria: The reference answer is exclusively Ultrasound. In this instance, GPT-4V offers\na more comprehensive response that encompasses the reference answer. Thus, this case is\ncategorized as correct for accuracy computation. Notably, even if GPT-4V mentions CT or\nother correct type only instead of ultrasound, it is still deemed accurate, as GPT-4V distinctly\nidentifies the image as an X-ray.\nCriteria for assessing the accuracy of GPT-4V\u2019s responses are as follows:\n\u2022 GPT-4V should directly answer the question and provide the correct response.\n\u2022GPT-4V does not refuse to answer the question, and its response should encompass key points or\nsemantically equivalent terms. Any additional information in the response must also be manually\nverified for accuracy. This criterion is particularly applicable to open-ended questions.\n\u2022Responses from GPT-4V should be devoid of ambiguity. While answers that display a degree of\ncaution, like \u201cIt appears to be atrophy\u201d , are acceptable, ambiguous answers such as \u201cIt appears to\nbe volume changes\u201d are not permitted, as illustrated by the closed-ended pathology VQA example.\n\u2022GPT-4V needs to provide comprehensive answers. For instance, if the prompt is \u201cIn which two\nventricles . . . \u201d and GPT-4V mentions only one, the answer is considered incorrect.\n\u2022Multi-round conversations leading to the correct answer are not permissible. This is because they\ncan introduce excessive hints, and the GPT model is intrinsically influenced by user feedback, like\nstatements indicating \u201cYour answer is wrong, . . . \u201d will mislead the response easily.\n4 \u2022OpenAI has documented inconsistent medical responses within the GPT-4V system card2. This\nindicates that while GPT-4V may offer correct answers sometimes, it might falter in others. In\nour study, we permit only a single response from GPT-4V . This approach mirrors real-life medical\nscenarios where individuals have just one life, underscoring the notion that a virtual doctor like\nGPT-4V cannot be afforded a second chance.\nTo comprehensively assess GPT-4V\u2019s proficiency in medicine, and in light of the absence of an\nAPI which necessitates manual testing (thus constraining the scalability of our evaluation), we\nmeticulously selected 133 samples. These comprise 56 radiology samples sourced from VQA-RAD\n[Lau et al., 2018] and PMC-VQA [Zhang et al., 2023b], along with 77 samples from PathVQA [He\net al., 2020]. Detailed information about the data, including sample selection and the distribution of\nquestion types, can be found in Section 3.\n3 Data Collection\n3.1 Pathology\nThe pathology data collection process commences with obtaining question-answer pairs from\nPathVQA set [He et al., 2020]. These pairs involve tasks such as recognizing objects in the image\nand giving clinical advice. Recognizing objects holds fundamental importance for AI models to\nunderstand the pathology image. This recognition forms the basis for subsequent assessments.\nTo be more specific, we randomly select 63 representative pathology images, and manually select 77\nhigh quality questions from the corresponding question set. To ensure the diversity of the data, we\nselect images across microscopy, dermoscopy, WSI, with variety objects of interest: brain, liver, skin,\ncell, Heart, lung, vessel, and kidney, as shown in Table 1. On average, each image has 1.22 questions.\nThe maximum and minimum number of questions for a single image is 5 and 1 respectively. Figure 2\nshows some examples.\nThere are eight categories of questions: \"Anatomical Structures,\" \"Lesion & Abnormality Detec-\ntion,\" \"Disease Diagnosis,\" \"Temporal & History-Related Effects,\" \"Spatial Relationships,\" \"Contrast\nAgents & Staining,\" \"Microscopic Features,\" and \"Pathophysiological Mechanisms & Manifesta-\ntions.\" Table 2 shows the number of questions and percentage of each category. The eight categories\nencompass a comprehensive range of medical inquiries. \"Anatomical Structures\" pertain to specific\norgans or tissues within the body. \"Lesion & Abnormality Detection\" focus on the identification\nof unusual imaging findings or pathological abnormalities. \"Disease Diagnosis\" aims to determine\nspecific medical conditions from given symptoms or findings. \"Temporal & History-Related Effects\"\ndelve into the progression or changes over time, linking them to past medical events. \"Spatial Rela-\ntionships\" address the relative positioning of structures or abnormalities within the body. \"Contrast\nAgents & Staining\" relate to the use and interpretation of imaging contrasts or histological stains.\n\"Microscopic Features\" detail observations made at a cellular level, often in histology or cytology.\nFinally, \"Pathophysiological Mechanisms & Manifestations\" explore the underpinnings and outcomes\nof diseases, touching on both their causes and effects.\nThe questions are also defined into three difficulty levels: \"Easy,\" \"Medium,\" and \"Hard,\" as shown in\nTable 4 Questions about recognizing objects in the image are tend to be considered as easy samples.\nRecognizing objects holds fundamental importance for AI models to understand the pathology image.\nThis recognition forms the basis for subsequent assessments. Medium questions also ask GPT-4V to\nrecognize objects, but with more complicated scenario and less background information. Questions\nabout giving clinical advice are often categorized as challenging due to their demand for a holistic\nunderstanding.\nOverall, this approach allows us to comprehensively assess GPT-4V\u2019s performance across a range of\npathological scenarios, question types and modes.\n3.2 Radiology\nThe radiology data collection process commences with obtaining modality-related question-answer\npairs from VQA-RAD dataset [Lau et al., 2018]. These pairs involve tasks such as determining\n2https://openai.com/research/gpt-4v-system-card\n5 Figure 2: VQA samples from both pathology set and radiology set. Samples of pathology set are in\ngreen boxes, while radiology samples are in red boxes. Each question comes with a prompt, directing\nGPT-4V to consider both the visual and textual data. Questions and their corresponding ground truth\nanswers are denoted with [Question] and [GT] respectively.\n6 Table 1: Dataset evaluated in this paper. \"Num. Pairs\" refers to the number of text-image pairs of\neach dataset.\nDataset Source Data Image Modality Objects of interest Text Category Num. Pairs\nPathVQAPathology Questions\nfor Medical\nVisual Question Answering\n[He et al., 2020]Microscopy,\nDermoscopy,\nWSI,\nEndoscopic VideoBrain, Liver,\nSkin, Cell,\nHeart, Lung,\nVessel, KidneyClosed-ended,\nOpen-ended77\nVQA-RADClinicians asked naturally\noccurring questions of\nradiology images and\nprovided reference answers.\n[Lau et al., 2018]X-Ray,\nCT,\nMRIChest, Head,\nAbdomenClosed-ended,\nOpen-ended37\nPMC-VQAMixture of medical VQAs\nfrom PubmedCentral\u00ae\n[Zhang et al., 2023b].\nWe only select\nradiology-related pairs\nin this report.ECHO,\nAngiography,\nUltrasound,\nMRI,\nPETNeck, Heart,\nKidney, Lung,\nHead, Abdomen,\nPelvic, Jaw,\nVesselClosed-ended,\nOpen-ended19\nTable 2: Statistics of the pathology data based on question type.\nQuestion Type Total Number Percentage\nAnatomical Structures 9 11.69%\nLesion & Abnormality Detection 10 12.99%\nDisease Diagnosis 12 15.58%\nTemporal & History-Related Effects 6 7.79%\nSpatial Relationships 3 3.90%\nContrast Agents & Staining 8 10.39%\nMicroscopic Features 16 20.78%\nPathophysiological Mechanisms & Manifestations 13 16.88%\nthe imaging type and identifying the medical devices employed for capturing radiological images.\nRecognizing imaging types holds fundamental importance in the development of radiology AI models.\nThis recognition forms the basis for subsequent assessments, including evaluations of imaging density,\nobject size, and other related parameters. To ensure the diversity of modality-related data, we select\n10 images across X-ray, CT and MRI, and different anatomies: head, chest, and abdomen.\nTo ensure the diversity of modality-related data, we selected 10 images from various anatomical\nregions, including the head, chest, and abdomen, representing different imaging modalities such\nas X-ray, CT, and MRI. In our continued exploration of GPT-4V\u2019s capabilities, we employed three\nrepresentative images corresponding to modality-related pairs while utilizing the remaining questions.\nWe observed instances where GPT-4V exhibited misunderstandings, particularly in responding to\nposition- and size-related inquiries. To address these challenges, we further selected 10 size-related\npairs from VQA-RAD and 2 position-related pairs from VQA-RAD, supplemented by 6 position-\nrelated pairs from PMC-VQA [Zhang et al., 2023b].\nWe meticulously filtered these two datasets, manually selecting questions to balance the question types\nin terms of \"Modality Recognition,\" \"Structural Identification,\" \"Lesion & Abnormality Detection,\"\n\"Disease Diagnosis,\" \"Size & Extent Assessment,\" \u2019Spatial Relationships,\u2019 \u2019Image Technical Details,\u2019\nand \"Imaging Features,\" as well as varying the difficulty levels. To be more specific, \"Modality\nRecognition\" discerns the specific imaging modality, such as CT, MRI, or others. \"Structural\nIdentification\" seeks to pinpoint specific anatomical landmarks or structures within the captured\nimages. \"Lesion & Abnormality Detection\" emphasizes the identification of anomalous patterns or\naberrations. \"Disease Diagnosis\" aspires to deduce specific medical conditions based on imaging\nmanifestations. \"Size & Extent Assessment\" gauges the dimensions and spread of a lesion or\nabnormality. \"Spatial Relationships\" examines the relative positioning or orientation of imaged\nstructures. \"Image Technical Details\" delves into the nuances of the imaging process itself, such as\ncontrast utilization or image orientation. Lastly, \"Imaging Features\" evaluates characteristic patterns,\n7 Table 3: Statistics of the radiology data based on question type.\nQuestion Type Total Number Percentage\nModality Recognition 8 14.29%\nStructural Identification 12 21.43%\nLesion & Abnormality Detection 12 21.43%\nDisease Diagnosis 5 8.93%\nSize & Extent Assessment 9 16.07%\nSpatial Relationships 4 7.14%\nImage Technical Details 3 5.36%\nImaging Features 3 5.36%\nTable 4: Data statistics based on difficulty levels for the pathology and radiology sets.\nPathology Radiology\nDifficulty Total Number Percentage Difficulty Total Number Percentage\nEasy 20 26.0% Easy 16 28.6%\nMedium 33 42.9% Medium 22 39.3%\nHard 24 31.2% Hard 18 32.1%\ntextures, or attributes discernible in the image, pivotal for diagnostic interpretation. For difficulty\nlevel, similar to pathology data, questions related to diagnostics are often categorized as challenging\ndue to their demand for a holistic understanding. This comprehension necessitates a deep grasp of\npreliminary aspects, including modality, objects, position, size, and more. Furthermore, it requires\nthe ability to filter and extract key medical knowledge essential for accurate diagnostics.\nIn summary, this experiment encompasses a total of 56 radiology VQA samples, with 37 samples\nsourced from VQA-RAD and 19 samples from PMC-VQA. This approach allows us to comprehen-\nsively assess GPT-4V\u2019s performance across a range of radiological scenarios, question types and\nmodes.\n4 Experimental Results\n4.1 Pathology Accuracy\nFigure 3 shows the accuracy achieved in the pathology VQA task. Overall accuracy score is 29.9 %,\nwhich means that GPT-4V can not give accurate and effecient diagnosis at present. To be specific,\nGPT-4V shows 35.3 %performance in closed-ended questions. It performances worse than random\nguess (where the accuracy performance is 50 %). This means that the answer generated by GPT-4V\nis not clinically meaningful. Accuracy score on open-ended questions reflects GPT-4V\u2019s capability\nin understanding and inferring key aspects in medical images. This categorization rationalizes\nGPT-4V\u2019s comprehension of objects, locations, time, and logic within the context of medical image\nanalysis, showcasing its versatile capabilities. As can be seen in Figure 3, the score is relatively low.\nConsidering this sub-set is quite challenging [He et al., 2020], the result is acceptable.\nMeanwhile, we collect the QA pairs in a hierarchy method that all pairs are divided into three\ndifficulty levels. As shown in Figure 3, the accuracy score in \"Easy\" set is 75.00 %, higher than the\naccuracy in medium set by 59.80 %. The hard set gets the lowest accuracy, at 8.30 %. The accuracy\nscore experiences a decrease along with the increase of the difficulty level, which shows the efficiency\nand high quality of our collected data. The result demonstrates GPT-4V\u2019s proficiency in basic medical\nknowledge, including recognition of numerous specialized terms and the ability to provide definitions.\nMoreover, GPT-4V exhibits traces of medical diagnostic training, attempting to combine images with\nits medical knowledge to address medical questions. It also displays fundamental medical literacy,\noffering correct responses to straightforward medical queries. However, there is significant room\nfor improvement, particularly as questions become more complex and closely resemble real clinical\nscenarios.\n8 Figure 3: Results of pathology VQA task. The\nbar chart on the left is related to the accuracy\nresult of questions with different difficulty levels,\nwhile the right chart is the results of closed-ended\nquestions and open-ended questions, marked as\nClosed andOpen , respectively.\nFigure 4: Results of radiology VQA task. On the\nleft, we have a bar chart showcasing the accuracy\nresults for questions of varying difficulty levels.\nMeanwhile, on the right, outcomes for closed-\nended and open-ended questions are presented\nin separate charts.\n4.2 Radiology Accuracy\nThe accuracy results for the VQA task within radiology are presented in Figure 4. To be more precise,\nthe overall accuracy score for the entire curated dataset stands at 50.0%. To present the GPT-4V\u2019s\ncapability in different fine-grained views, we will show the accuracy in terms of question types\n(open and closed-end), difficulty levels (easy, medium, and hard), and question modes (modality,\nsize, position, . . . ) in the following. In more specific terms, GPT-4V achieves a 50% accuracy rate\nfor 16 open-ended questions and a 50% success rate for 40 closed-ended questions with limited\nchoices. This showcases GPT-4V\u2019s adaptability in handling both free-form and closed-form VQA\ntasks, although its suitability for real-world applications may be limited.\nIt\u2019s worth noting that closed-ended questions, despite narrowing the possible answers, do not nec-\nessarily make them easier to address. To further explore GPT-4V\u2019s performance across varying\ndifficulty levels, we present its accuracy rates: 81.25 %for easy questions, 59.09 %for medium\nquestions, and a mere 11.11 %for hard questions within the medical vision-language domain. Easy\nquestions often revolve around modality judgments, like distinguishing between CT and MRI scans,\nand straightforward position-related queries, such as object positioning. For an in-depth explanation\nof our question difficulty categorization method, please refer to Section 3.\nPosition and size are foundational attributes that hold pivotal roles across various medical practices,\nparticularly in radiological imaging: Radiologists depend on accurate measurements and spatial data\nfor diagnosing conditions, tracking disease progression, and formulating intervention strategies. To\nassess GPT-4V\u2019s proficiency in addressing issues related to positioning and size within radiology, we\nspecifically analyzed 8 position-related questions and 9 size-related questions. The model achieved\nan accuracy rate of 62.50 %for position-related queries and 55.56 %for size-related queries.\nThe lower accuracy observed for position-related questions can be attributed to two primary factors.\nFirstly, addressing these questions often requires a background understanding of the directional\naspects inherent in medical imaging, such as the AP or PA view of a chest X-ray. Secondly, the\ntypical workflow for such questions involves first identifying the disease or infection and then\nmatching it with its respective position within the medical image. Similarly, the reduced accuracy in\nresponding to size-related questions can be attributed to the model\u2019s limitations in utilizing calibrated\ntools. GPT-4V appears to struggle in extrapolating the size of a region or object of interest, especially\nwhen it cannot draw upon information about the sizes of adjacent anatomical structures or organs.\nThese observations highlight the model\u2019s current challenges in dealing with positioning and size-\nrelated queries within the radiological context, shedding light on areas where further development\nand fine-tuning may be needed to enhance its performance in this domain.\nIn the following, we will carefully select specific cases to provide in-depth insights into GPT-4V\u2019s\ncapabilities within various fine-grained perspectives.\n9 5 Features of GPT-4V with Case Studies\n5.1 Requiring Cues for Accurate Localization\nIn medical imaging analysis, the accurate determination of anatomical positioning and localization\nis crucial. Position is typically established based on the viewpoint, with standard conventions\ngoverning this perspective. Such conventions are foundational in radiology and provide consistency\nin interpretation across different medical platforms and professionals. As demonstrated in Figure 5,\nGPT-4V has the potential to autonomously leverage and comprehend these viewpoint conventions.\nSpecifically, when presented with the contextual information \" In dental radiographs, the images are\noriented as if you are looking at the patient directly ,\" GPT-4V was aptly able to utilize this knowledge\nand yield an accurate interpretation in response to the related question.\nHowever, the model\u2019s ability to consistently apply these conventions appears to be context-dependent.\nThis was evident in the VQA pair depicted in Figure 5. Here, GPT-4V initially overlooked the\ntraditional orientation of MRI imaging in its default response. It was only upon receiving an explicit\nhint about the imaging perspective that the model revised its answer, aligning it with the correct\ninterpretation.\nThis observation underscores a salient point: while GPT-4V is endowed with a vast reservoir of\nmedical knowledge and is capable of discerning context, its responses can sometimes hinge on the\nspecificity and clarity of the information provided, emphasizing the importance of user interaction\nand context provision to guide the model towards accurate conclusions. As our analysis is grounded\non zero-shot predictions, we maintain the view that in the bottom case, GPT-4V provide wrong\nanswer to the question without additional contexts or hints.\nFigure 5: Case for GPT-4V\u2019s requiring cues for accurate localization. [Question] and [GT] mark\nthe question and ground truth answer of the text-image pair, respectively. [Answer] refers to the\nanswer generated by GPT-4V . The upper case illustrates where GPT-4V autonomously considers the\nconvention of dental imaging, and answers the position-related question correctly. We feed a sub\nquestion to GPT-4V after the first question in the bottom case. It shows GPT-4V\u2019s ability to pinpoint\nthe locations of objects in radiology images irrespective of the traditional imaging orientation.\n5.2 Challenge in Assessing Object Size\nWhen assessing the difficulty level of size-related questions, GPT-4V typically categorizes them\nas at least medium, often leaning towards hard. In general, GPT-4V demonstrates the ability to\ndistinguish relative sizes. For example, when asked, \" Is the heart size normal? \" it can provide an\n10 answer based on the principle that \" Generally, the heart\u2019s width should not exceed half the width of\nthe chest, or the cardiothoracic ratio should be less than 0.5 .\" It\u2019s worth noting that GPT-4V tends to\nanswer correctly for most chest X-ray Q &A pairs but faces challenges when dealing with CT scans.\nA common response from GPT-4V when judging the size of objects in CT images is, \" Making a\ndefinitive assessment about the size and volume of [object] would require reviewing multiple slices to\nunderstand its entire length and width .\" This suggests that GPT-4V struggles to interpret the size of\none object relative to others or in the context of surrounding contours within CT images.\nFigure 6: Case for GPT-4V\u2019s relying excessively on text. [Answer] is generated by GPT-4V , while\n[Question] and [GT] are the question and ground truth answer of the text-image pair. Words in red\nshow that GPT-4V wrongly recognises the pattern in the image as antinuclear antibodies (ANA).\n5.3 Relying Excessively on Text\nIn the investigations into the capabilities of GPT-4V , a pronounced reliance on textual content rather\nthan the integration of accompanying visual cues has been observed. This inclination leans heavily\ninto the model\u2019s expansive medical knowledge without sufficiently factoring in the nuances provided\nby visual data. Taking the provided instance as a prime example (shown in Figure 6), the disparity\nbetween the model\u2019s output and the expected gold-standard response is evident. As highlighted, GPT-\n4V , influenced by the textual context mentioning \u201c systemic sclerosis and Sj\u00f6gren syndrome ,\u201d inferred\nthe presence of \u201c antinuclear antibodies (ANA) \u201d from the image. Contrastingly, the gold standard\nidentifies the image as showcasing \u201c anti-centromere antibodies (ACA) diseases .\u201d From a standpoint\nof logic, GPT-4V\u2019s inference isn\u2019t entirely baseless. ANA is a broad category of autoantibodies found\nin various autoimmune diseases, inclusive of systemic sclerosis and Sj\u00f6gren syndrome. Given the\nbroad nature of ANA, and the diseases it encompasses, the connection made by GPT-4V can be\nunderstood.\nHowever, the inadequacy lies in the nuanced distinction between ANA and ACA. While both are\nautoantibodies, their specificity, associated conditions, and staining patterns vary considerably. ACA,\nspecifically targeting the centromere, would manifest differently in fluorescent staining compared to\na generic ANA. Despite a passing mention of the image\u2019s fluorescence, GPT-4V\u2019s response remained\nsuperficial in its description of the image, devoid of a more informed interpretation of the centromere\nfluorescence. It\u2019s evident that while the model possesses the capability to describe images, it might\nnot be optimally integrating this information with its extensive textual knowledge. While GPT-4V\nexhibits profound medical knowledge and textual understanding, its underwhelming utilization of\nvisual data, especially in contexts demanding a synergy of both, remains a limitation.\n5.4 Overemphasizing Markings in Images\nAn emerging challenge observed in GPT-4V model, is the overemphasis on explicit markings or\nannotations within images, often at the expense of understanding the broader context and image\ninformation. As shown in Figure 7, GPT-4V tends to prioritize the symbols embedded within the\n11 coronary angiogram. Only RCA does not have explicit labeling, it results in the conclusion that\n\"The RCA is not visible in this image \". Instead of analyzing the structures present in the coronary\nangiogram of the image, the model became anchored to the absence of a textual label, revealing a\nshortcoming in its holistic understanding of the image content.\nFigure 7: Cases of overemphasizing markings in images. For the upper case, GPT-4V is susceptible to\nsymbols in the image. Due to the unlabelled RCA in the image, GPT-4V did not answer the question\ncorrectly, shown in [Answer]. The bottom case shows that because of the presence of an arrow in the\nimage, GPT-4V struggles to distinguish between contrasting queries and tends to provide identical\nresponses based solely on the arrow\u2019s indication, shown in [Answer-1] and [Answer-2], respectively.\nAnother evident manifestation of this challenge is observed when assessing lymph nodes in an image.\nIn the bottom case in Figure 7, GPT-4V\u2019s assessment was predominantly influenced by the presence\nof an arrow. Even when the query was modified from \"abnormal\" to \"normal,\" the model\u2019s focus\nremained unwaveringly on the marked element, reiterating its answer based on the arrow rather than\ngrasping the overall visual narrative. This example underscores a critical area for improvement. For\nrobust image interpretation in the VQA task, especially in the medical domain demanding precision,\nmodels should not only identify explicit markings but also appreciate the broader visual information\nto prevent such misconstruals.\n5.5 Not Suitable for Diagnostics\nWhile GPT-4V can analyze and provide insights on various topics, including medical VQA task, its\naccuracy is not guaranteed. An illustrative case is its interpretation of a given H &E stained slide\nwhere it inferred the presence of extracapillary proliferation, as shown in Figure 8. This conclusion,\nhowever, appears contradictory to the actual context. GPT-4V\u2019s determination was influenced by\nits perception of the deep purple regions as the crowded cellular accumulation outside the capillary\nloops. In reality, these visual features might be resultant perturbations introduced during the slide\npreparation, staining, or scanning processes.\nStepping back from the specific case, several fundamental reasons underscore why GPT-4V isn\u2019t\nsuitable for diagnostic purposes. Clinical cases in reality are intricate, and slides processed for\nhuman examination entail various perturbations, many of which are unavoidable. Without sufficient\nexperience to discern and eliminate the influence of such perturbations, precise diagnoses become\nchallenging. Furthermore, GPT-4V lacks the expertise of medical professionals who evaluate a\nholistic view of the slide, incorporate multiple imaging perspectives, and factor in patient history for\naccurate diagnoses. Consequently, GPT-4V\u2019s evaluations, though advanced, are limited in their scope\nand should not be used for medical evaluation.\n12 Figure 8: The case study of GPT-4V\u2019s not suitable for diagnostics. We ask GPT-4V two sequential\nquestions, marked as [Question] and [Sub-Question], respectively, and record its corresponding\nanswer in [Answer] and [Sub-Answer]. [GT] refers to the ground truth answer of the text-image pair.\n5.6 Cautious Answers\nIn the domain of medical analysis, GPT-4V consistently adopts a conservative approach, exemplified\nin Figure 9. Two salient examples illustrate this caution. In the upper instance, when tasked with\nidentifying a type of mass from a radiological image, GPT-4V declined, emphasizing the necessity of\nprofessional consultation. In the bottom one, faced with analyzing cardiac anatomy from a cross-\nsection of a heart, GPT-4V again demurred, noting the importance of comparing with a typical heart\nand soliciting expert medical advice.\nThis caution is rooted in the complexities and high stakes of medical decisions. Diagnoses often\nrequire comprehensive contextual knowledge beyond a single image. However, an inherent tension\nexists: while GPT-4V\u2019s conservative approach safeguards against potential harm or misrepresentation,\nit can sometimes seem overly cautious, potentially sidelining users\u2019 direct queries. This balance\nunderscores the challenge of leveraging artificial intelligence in medical contexts. GPT-4V\u2019s default\nto caution, even potentially at the expense of direct answer, reflects a prioritization of safety over\nimmediate information delivery.\n5.7 Thorough Answers with Details\nThis system is characterized by its capacity to elucidate its rationale alongside its answers. As\ndepicted in Figure 10, GPT-4V not only quantifies nucleated erythroid precursors present in the\nimage but also justifies its deduction by referencing the purplish-blue nucleus contrasted against a\npaler cytoplasm. Such elucidations foster users\u2019 deeper comprehension and permit validation of the\nsystem\u2019s methodologies. However, it\u2019s essential to note that these explanations might occasionally\nmiss the intricate nuances or complexities of certain topics.\nFurthermore, the system provides clarifications on terms present in the query or its response and\noffers supplementary context when requisite. This underscores its potential utility in educational\ncontexts. As exemplified in the bottom instance in Figure 10, GPT-4V autonomously elucidated\nthe concept of \" impending perforation in the intestines \". Furthermore, it elaborated on potential\nindicators for intestinal perforation, stating: \" Any focal point of severe discoloration, inflammation, or\nnecrosis (dead tissue) can also suggest areas at risk of perforation .\" Nonetheless, while the responses\nare comprehensive and largely accurate, they could be more concise and directly aligned with users\u2019\nexplicit queries. In instances of direct yes/no inquiries, excessive elaboration can be distracting and\npotentially obfuscate the primary message.\n13 Figure 9: Cases of cautious answers of GPT-4V . Question and ground truth answer are marked as\n[Question] and [GT], respectively. The answer generated by GPT-4V is represented as [Answer].\nIn cases of ambiguity within radiology and pathology domains, GPT-4V consistently recommends\ndirect consultation with medical professionals rather than providing definitive answers to users.\nFigure 10: Case study of GPT-4V\u2019s capability to answer thoroughly with details. [GT] refers to the\nground truth answer to the question. Additional details provided by GPT-4V are in red.\n6 Discussion and Limitation\nIn the study, we explore the zero-shot VQA capabilities of GPT-4V in radiology and pathology tasks.\nthe current study\u2019s breadth is constrained by the lack of APIs for multimodal input and the challenges\nposed by manual data input and response documentation. This scope offers avenues for expansion in\nsubsequent studies. a larger sample size might yield a more comprehensive evaluation.\nWe assess GPT-4V\u2019s capabilities in medicine from an AI practitioner\u2019s viewpoint rather than that of\nmedical practitioners. For professional medical insights regarding GPT-4V\u2019s responses, collaboration\nwith the medical community is essential. By involving subjec- matter experts, we can better ensure\nthat critical nuances are captured and conclusions are more precise.\nMoreover, the dataset primarily features an image with its corresponding question, omitting potentially\nvaluable context like patient history or varied imaging perspectives. Incorporating such comprehensive\n14 data could align more closely with the holistic approach medical professionals take, ensuring a more\nin-depth and accurate assessment by the model.\nThe basic prompt structure used in the experiment offers room for enhancement. The craft of\ndesigning impactful prompts can play a vital role in refining the quality of the answers. A more\nnuanced prompt might yield more consistent and insightful outcomes.\nGPT-4V\u2019s role in radiology and pathology is an emerging area with potential. Its diagnostic efficacy\nin these fields might see improvement with a broader dataset, enhanced prompt methodologies,\nand feedback from domain specialists. A collaborative approach could help navigate the present\nlimitations.\n7 Conclusion\nIn the study, we evaluate the zero-shot VQA capabilities of the current version of GPT-4V in\nthe realms of radiology and pathology using a hand-curated dataset. We identified seven unique\ncharacteristics of GPT-4V\u2019s performance in medical VQA, highlighting its constraints within this\narea. Due to the poor performance of GPT-4V on the medical VQA dataset, and considering the\nsevere consequences of erroneous results in the medical field, GPT-4V should not currently be used\nas a reliable tool for medical diagnosis and providing treatment suggestions.\nReferences\nJuli\u00e1n N Acosta, Guido J Falcone, Pranav Rajpurkar, and Eric J Topol. Multimodal biomedical ai.\nNature Medicine , 28(9):1773\u20131784, 2022.\nZhihong Chen, Shizhe Diao, Benyou Wang, Guanbin Li, and Xiang Wan. Towards unifying medical\nvision-and-language pre-training via soft prompts. arXiv preprint arXiv:2302.08958 , 2023.\nTianyu Han, Lisa C Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexan-\nder L\u00f6ser, Daniel Truhn, and Keno K Bressem. Medalpaca\u2013an open-source collection of medical\nconversational ai models and training data. arXiv preprint arXiv:2304.08247 , 2023.\nXuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. Pathvqa: 30000+ questions for\nmedical visual question answering, 2020.\nJason J Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. A dataset of clinically\ngenerated visual questions and answers about radiology images. Scientific data , 5(1):1\u201310, 2018.\nPeter Lee, Sebastien Bubeck, and Joseph Petro. Benefits, limits, and risks of gpt-4 as an ai chatbot\nfor medicine. New England Journal of Medicine , 388(13):1233\u20131239, 2023a.\nPeter Lee, Carey Goldberg, and Isaac Kohane. The AI revolution in medicine: GPT-4 and beyond .\nPearson, 2023b.\nChunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan\nNaumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training a large language-and-vision\nassistant for biomedicine in one day. arXiv preprint arXiv:2306.00890 , 2023.\nHarsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capabilities\nof gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375 , 2023.\nOpenAI. Chatgpt. https://openai.com/blog/chatgpt , 2022.\nOpenAI. Gpt-4v(ision) system card. https://cdn.openai.com/papers/GPTV_System_Card.\npdf, 2023.\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan\nScales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode\nclinical knowledge. Nature , 620(7972):172\u2013180, 2023.\nTao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang,\nAndrew Carroll, Chuck Lau, Ryutaro Tanno, Ira Ktena, et al. Towards generalist biomedical ai.\narXiv preprint arXiv:2307.14334 , 2023.\n15 Haochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, and Ting Liu. Huatuo:\nTuning llama model with chinese medical knowledge. arXiv preprint arXiv:2304.06975 , 2023.\nLi Yunxiang, Li Zihan, Zhang Kai, Dan Ruilong, and Zhang You. Chatdoctor: A medical chat model\nfine-tuned on llama model using medical domain knowledge. arXiv preprint arXiv:2303.14070 ,\n2023.\nKai Zhang, Jun Yu, Zhiling Yan, Yixin Liu, Eashan Adhikarla, Sunyang Fu, Xun Chen, Chen Chen,\nYuyin Zhou, Xiang Li, et al. Biomedgpt: A unified and generalist biomedical generative pre-trained\ntransformer for vision, language, and multimodal tasks. arXiv preprint arXiv:2305.17100 , 2023a.\nXiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi\nXie. Pmc-vqa: Visual instruction tuning for medical visual question answering. arXiv preprint\narXiv:2305.10415 , 2023b.\nRong Zhou, Houliang Zhou, Brian Y Chen, Li Shen, Yu Zhang, and Lifang He. Attentive deep\ncanonical correlation analysis for diagnosing alzheimer\u2019s disease using multimodal imaging\ngenetics. In International Conference on Medical Image Computing and Computer-Assisted\nIntervention , pages 681\u2013691. Springer, 2023.\n16 "}