{"url": "http://arxiv.org/pdf/2311.02782v3", "title": "Towards Generic Anomaly Detection and Understanding: Large-scale Visual-linguistic Model (GPT-4V) Takes the Lead", "text": "Towards Generic Anomaly Detection and Understanding:\nLarge-scale Visual-linguistic Model (GPT-4V) Takes the Lead\nYunkang Cao1\u2217, Xiaohao Xu2\u2217, Chen Sun3\u2217, Xiaonan Huang2and Weiming Shen1\n1Huazhong University of Science and Technology2University of Michigan, Ann Arbor\n3University of Toronto\nAbstract\nAnomaly detection is a crucial task across different domains and data types. However, existing anomaly\ndetection models are often designed for specific domains and modalities. This study explores the use\nof GPT-4V(ision), a powerful visual-linguistic model, to address anomaly detection tasks in a generic\nmanner. We investigate the application of GPT-4V in multi-modality, multi-domain anomaly detection\ntasks, including image, video, point cloud, and time series data, across multiple application areas, such as\nindustrial, medical, logical, video, 3D anomaly detection, and localization tasks. To enhance GPT-4V\u2019s\nperformance, we incorporate different kinds of additional cues such as class information, human expertise,\nand reference images as prompts. Based on our experiments, GPT-4V proves to be highly effective in\ndetecting and explaining global and fine-grained semantic patterns in zero/one-shot anomaly detection. This\nenables accurate differentiation between normal and abnormal instances. Although we conducted extensive\nevaluations in this study, there is still room for future evaluation to further exploit GPT-4V\u2019s generic\nanomaly detection capacity from different aspects. These include exploring quantitative metrics, expanding\nevaluation benchmarks, incorporating multi-round interactions, and incorporating human feedback loops.\nNevertheless, GPT-4V exhibits promising performance in generic anomaly detection and understanding,\nthus opening up a new avenue for anomaly detection.\nAll evaluation samples, including image and text prompts, will be available at https://github.com/caoyunkang/\nGPT4V-for-Generic-Anomaly-Detection .\nContents\n1 Introduction 5\n1.1 Motivation and Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n1.2 Our Approach: Prompting GPT-4V for Anomaly Detection . . . . . . . . . . . . . . . . . . . 5\n1.2.1 Prompt Designs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n1.2.2 Evaluation Scope: Modalities and Domains . . . . . . . . . . . . . . . . . . . . . . . . 6\n1.3 Limitations in Anomaly Detection Evaluation Based on GPT-4V . . . . . . . . . . . . . . . . 6\n2 Observations of GPT-4V on Multi-modal Multi-domain Anomaly Detection 7\n2.1GPT-4V can address multi-modality and multi-field anomaly detection tasks in zero/one-shot\nregime: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2.2 GPT-4V can understand both global and fine-grained semantics for anomaly detection: . . . 7\n2.3 GPT-4V can automatically reason for anomaly detection: . . . . . . . . . . . . . . . . . . . . 8\n2.4 GPT-4V can be enhanced with increasing prompts: . . . . . . . . . . . . . . . . . . . . . . . . 8\n\u2217Authors contribute equally. Email: cyk_hust@hust.edu.cn, xiaohaox@umich.edu, chrn.sun@mail.utoronto.ca,\nxiaonanh@umich.edu, wshen@ieee.orgarXiv:2311.02782v3  [cs.CV]  16 Nov 2023 2.5 GPT-4V can be constrained in real-world application but still promising: . . . . . . . . . . . 8\n3 Industrial Image Anomaly Detection 8\n3.1 Task Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n3.2 Testing philosophy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n3.3 Case Demonstration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n4 Industrial Image Anomaly Localization 10\n4.1 Task Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n4.2 Testing philosophy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n4.3 Case Demonstration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n5 Point Cloud Anomaly Detection 10\n5.1 Task Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n5.2 Testing philosophy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n5.3 Case Demonstration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n6 Logical Anomaly Detection 11\n6.1 Task Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n6.2 Testing philosophy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n6.3 Case Demonstration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n7 Medical Image Anomaly Detection 12\n7.1 Task Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n7.2 Testing philosophy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n7.3 Case Demonstration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n8 Medical Image Anomaly Localization 13\n8.1 Task Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n8.2 Testing philosophy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n8.3 Case Demonstration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n9 Pedestrian Anomaly Detection 14\n9.1 Task Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n9.2 Testing philosophy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n9.3 Case Demonstration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n10 Traffic Anomaly Detection 14\n10.1 Task Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n10.2 Testing philosophy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n10.3 Case Demonstration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n11 Time Series Anomaly Detection 15\n|2 11.1 Task Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n11.2 Testing philosophy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n11.3 Case Demonstration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n12 Prospect 15\n13 Conclusion 16\n|3 List of Figures\n1 The Diagram of Evaluation GPT-4V on Multi-modality/fields Anomaly Detection. . . . . . . 6\n2 Industrial Image Anomaly Detection: Case 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n3 Industrial Image Anomaly Detection: Case 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n4 Industrial Image Anomaly Detection: Case 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n5 Industrial Image Anomaly Detection: Case 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n6 Industrial Image Anomaly Detection: Case 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n7 Industrial Image Anomaly Detection: Case 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n8 Industrial Image Anomaly Localization: Case 1 . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n9 Industrial Image Anomaly Localization: Case 2 . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n10 Industrial Image Anomaly Localization: Case 3 . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n11 Point Cloud Anomaly Detection: Case 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n12 Point Cloud Anomaly Detection: Case 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n13 Point Cloud Anomaly Detection: Case 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n14 Point Cloud Anomaly Detection: Case 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n15 Point Cloud Anomaly Detection: Case 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n16 Point Cloud Anomaly Detection: Case 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n17 Logical Anomaly Detection: Case 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n18 Logical Anomaly Detection: Case 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n19 Logical Anomaly Detection: Case 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n20 Logical Anomaly Detection: Case 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n21 Medical Anomaly Detection: Case 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n22 Medical Anomaly Detection: Case 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n23 Medical Anomaly Detection: Case 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n24 Medical Anomaly Detection: Case 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n25 Medical Anomaly Detection: Case 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n26 Medical Anomaly Detection: Case 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n27 Medical Anomaly Detection: Case 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n28 Medical Anomaly Detection: Case 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n29 Medical Anomaly Localization: Case 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n30 Medical Anomaly Localization: Case 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n31 Medical Anomaly Localization: Case 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n32 Medical Anomaly Localization: Case 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n33 Pedestrian Anomaly Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n34 Traffic Anomaly Detection: Case 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n35 Traffic Anomaly Detection: Case 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n36 Time Series Anomaly Detection: Case 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n37 Time Series Anomaly Detection: Case 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n|4 1 Introduction\n1.1 Motivation and Overview\nAnomaly detection [ 20,19,72,10,78] involves identifying data patterns or data points that significantly\ndeviate from normality. These anomalies or outliers are rare, unusual, or inconsistent data points that deviate\nfrom the majority of the data. The primary objective of anomaly detection is to automatically detect and\npinpoint these irregularities, which may signify errors, fraud, unusual events, or other noteworthy phenomena,\nfacilitating further investigation or necessary action. Anomaly detection techniques have been widely employed\nin diverse domains, such as industrial inspection [ 29,98], medical diagonisis [ 107], video surveillance [ 84],\nfraud detection [30] and many other areas where identifying unusual instances is crucial.\nDespite the existence of numerous techniques [ 14,3,69,41,38,79,110,16,103] for anomaly detection, many\nexisting approaches predominantly rely on methods that describe the normal data distribution. They often\noverlook high-level perception and primarily treat it as a low-level task. However, practical applications\nof anomaly detection frequently necessitate a more comprehensive, high-level understanding of the data.\nAchieving this understanding entails at least three crucial steps:\n1.Understanding the Data Types and Categories: The first step involves a thorough comprehension\nof the data types and categories present in the dataset. Data can take various forms, including\nimages, videos, point clouds, time-series data, etc. Each data type may require specific methods and\nconsiderations for anomaly detection. Furthermore, different categories may have distinct definitions of\nnormal states.\n2.Determining Standards for Normal States: After obtaining the data types and categories, it would\nbe feasible to further reason the standards for normal states, which requires a high-level understanding\nof the data.\n3.Evaluating Data Conformance: The final step is to assess whether the provided data conforms to\nthe established standards for normality. Any deviation from these standards can be categorized as an\nanomaly.\nRecent advancements in large multimodal models (LMMs) [ 25,4,36,113,58,27,52] have shown robust\nreasoning capacity [ 55,57] and created new opportunities for improving anomaly detection. LMMs are\ntypically trained on extensive multimodal datasets [ 80], enabling them to effectively analyze various data\ntypes, including natural language and visual information. They hold the potential to address the challenges\nassociated with high-level anomaly detection [37, 17, 22, 112].\nMoreover, OpenAI recently introduced GPT-4V(ision) [ 101], a state-of-the-art LMM that has exhibited\nremarkable performance across various practical applications. However, it remains uncertain whether GPT-4V\ncan also exhibit robust capabilities for anomaly detection. The objective of this study is to bridge this\nknowledge gap by assessing the anomaly detection capabilities of GPT-4V.\n1.2 Our Approach: Prompting GPT-4V for Anomaly Detection\n1.2.1 Prompt Designs\nThe design of prompts plays a crucial role in effectively directing GPT-4V\u2019s attention toward the specific\naspects of the anomaly detection task. In this study, we primarily consider four types of prompts:\n1.Task Information Prompt : To prompt GPT-4V effectively for anomaly detection, it is essential to\nprovide clear task information. This study formulates the prompt as follows: \"Please determine whether\nthe image contains anomalies or outlier points.\"\n2.Class Information Prompt : The understanding of data types and categories is critical. In cases\nwhere GPT-4V may struggle to recognize the data class, explicit class information may be provided.\nFor instance, \"Please determine whether the image, which is related to the {CLS}, contains anomalies\nor defects.\"\n|5 Figure 1 |Comprehensive Evaluation of GPT-4V for Multi-modality Multi-task Anomaly Detection In this\nstudy, we conduct a thorough evaluation of GPT-4V in the context of multi-modality anomaly detection. We consider four\nmodalities: image, video, point cloud, and time series, and explore nine specific tasks, including industrial image anomaly\ndetection/localization, point cloud anomaly detection, medical image anomaly detection/localization, logical anomaly detection,\npedestrian anomaly detection, traffic anomaly detection, and time series anomaly detection. Our evaluation encompasses a\ndiverse range of 15 datasets.\n3.Normal Standard Prompt : GPT-4V may encounter difficulties in answering questions related to\ndetermining normal standards, and sometimes the standards even can not be examined without human\nexpertise. Hence, this study also explicitly provides the normal standards. For example, normal\nstandards for the breakfast box in MVTec-LOCO [ 7] could be expressed as follows: \"1. It should contain\ntwo oranges, one peach, and some cereal, nuts, and banana slices; 2. The fruit should be on the left side\nof the lunchbox, the cereal on the upper right, and the nuts and banana slices on the lower right of the\nlunchbox.\"\n4.Reference Image Prompt : To ensure better alignment between normal standards and images, a\nnormal reference image is provided alongside language prompts. For example, \"The first image is normal.\nPlease determine whether the second image contains anomalies or defects.\"\nThe study aims to explore how the use of these prompts, either individually or in different combinations\ndepending on certain cases, impacts GPT-4V\u2019s capacity for anomaly detection.\n1.2.2 Evaluation Scope: Modalities and Domains\nExtensive evaluations are conducted in this study to assess the capabilities of GPT-4V in anomaly detection,\nas Fig. 1 shows. From the perspective of modalities, we evaluate image (Section 3, 4, 6, 7, 8), point cloud\n(Section 5), video (Section 9, 10), and time series (Section 11). From the perspective of fields, industrial\ninspection (Section 3, 4, 6, 5), medical diagnosis (Section 7, 8), and video surveillance (Section 9, 10) are\nevaluated. To the best of our knowledge, this is the first study to investigate such a wide range of modalities\nand fields for anomaly detection.\n1.3 Limitations in Anomaly Detection Evaluation Based on GPT-4V\nThe analysis of this study is subject to certain limitations:\n1.Predominance of Qualitative Results : The analysis primarily relies on qualitative assessment,\n|6 lacking quantitative metrics that could offer a more objective evaluation of the model\u2019s performance\nin anomaly detection. Incorporating quantitative measures would provide a more robust basis for\nassessment.\n2.Scope of Evaluated Cases : The evaluation is confined to a limited scope of cases or scenarios. This\nnarrow focus may not fully capture the diverse challenges encountered in real-world anomaly detection\ntasks. Expanding the range of evaluated cases would yield a more comprehensive understanding of the\nmodel\u2019s capabilities.\n3.Single Interaction Evaluation : The study mainly concentrates on a single-round conversation. In\ncontrast, multi-round conversations, as observed in the in-context learning capacity of GPT-4V [ 101], can\nstimulate deeper interaction. The single-round conversation approach restricts the depth of interaction\nand may constrain the model\u2019s comprehension and its effectiveness in responding to anomaly detection\ntasks. Exploring multi-round interactions could reveal a more nuanced perspective of the model\u2019s\nperformance.\n2 Observations of GPT-4V on Multi-modal Multi-domain Anomaly Detection\nFollowing a thorough evaluation of GPT-4V\u2019s performance across various multi-modality and multi-field\nanomaly detection tasks, it becomes apparent that GPT-4V possesses robust anomaly detection capabilities.\nMore precisely, GPT-4V consistently excels in addressing the three previously mentioned challenges: compre-\nhending image context, discerning normal standards, and effectively comparing the provided image against\nthese standards. In addition to these fundamental findings, our assessments have yielded valuable insights.\n2.1GPT-4V can address multi-modality and multi-field anomaly detection tasks in\nzero/one-shot regime:\nAnomaly detection for multi-modality : GPT-4V\u2019s ability to handle diverse data modalities is demon-\nstrated by its consistent performance across various domains. For instance, it exhibits proficiency in identifying\nanomalies in images, point clouds, X-rays, etc., underscoring its adaptability to multi-modal tasks. This\nversatility allows it to transcend the limitations of single-modal anomaly detectors.\nAnomaly detection for multi-field : GPT-4V\u2019s performance across multiple fields, including industrial,\nmedical, pedestrian, traffic, and time series anomaly detection, showcases its ability to seamlessly adapt to\nthe distinct characteristics of each domain. Its consistent results affirm its broad applicability and versatility,\nmaking it a valuable tool for anomaly detection in a variety of real-world contexts.\nAnomaly detection in zero/one-shot regime : GPT-4V\u2019s evaluation in both zero-shot and one-shot\nsettings highlights its adaptability to different inference scenarios. In the absence of reference images, the\nmodel effectively relies on language prompts to detect anomalies. However, when provided with normal\nreference images, its anomaly detection accuracy is further enhanced. This flexibility enables GPT-4V to\ncater to a wide range of anomaly detection applications, whether with or without prior knowledge.\n2.2GPT-4V can understand both global and fine-grained semantics for anomaly detec-\ntion:\nGPT-4V\u2019s understanding of global semantics : GPT-4V\u2019s capacity to comprehend global semantics is\ndemonstrated in its ability to recognize overarching abnormal patterns or behaviors. For example, in traffic\nanomaly detection, it can discern the distinction between typical traffic flow and irregular events, providing a\nholistic interpretation of the data. This global understanding makes it well-suited for identifying anomalies\nthat deviate from expected norms in a broader context.\nGPT-4V\u2019s understanding of fine-grained semantics : GPT-4V\u2019s fine-grained anomaly detection capabil-\nities shine in cases where it not only detects anomalies but also precisely localizes them within complex data.\nFor instance, in industrial image anomaly detection, it can pinpoint intricate details like slightly tilted wicks\non candles or minor scratches or residues around the top rim of the bottle. This fine-grained understanding\nenhances its ability to detect subtle anomalies within complex data, contributing to its overall effectiveness.\n|7 2.3 GPT-4V can automatically reason for anomaly detection:\nThe model\u2019s strength in automatically reasoning the given complex normal standards and generating expla-\nnations for detected anomalies is a valuable feature. In logical anomaly detection, for example, GPT-4V\nexcels at dissecting complex rules and providing detailed analyses of why an image deviates from the expected\nstandards. This inherent reasoning ability adds a layer of interpretability to its anomaly detection results,\nmaking it a valuable tool for understanding and addressing irregularities in various domains.\n2.4 GPT-4V can be enhanced with increasing prompts:\nThe results of the evaluation highlight the positive impact of additional prompts on GPT-4V\u2019s anomaly\ndetection performance. The model\u2019s response to class information, human expertise, and reference images\nsuggests that providing it with more context and information significantly improves its ability to detect\nanomalies accurately. This feature allows users to fine-tune and enhance the model\u2019s performance by providing\nrelevant and supplementary information.\n2.5 GPT-4V can be constrained in real-world application but still promising:\nFrom the cases we test, we find there are still several gaps for GPT4V models to be applied in real world\nanomaly detection. For example, GPT-4V may face challenges in handling highly complex scenarios for\nindustrial application. Ethical constraints in the medical field also make it conservative and hesitate to give\nconfident answer. But we believe it remains promising in a wide range of anomaly detection tasks. To address\nthese challenges effectively, further enhancements, specialized fine-tuning, or complementary techniques may\nbe required. GPT-4V\u2019s potential for anomaly detection is evident, and ongoing research may continue to\nunlock its capabilities in even more complex scenarios.\n3 Industrial Image Anomaly Detection\n3.1 Task Introduction\nIndustrial image anomaly detection is a critical component of manufacturing processes aimed at upholding\nproduct quality [ 6,98,14]. Following the establishment of the MVTec AD dataset [ 6], various methods [ 15,\n45,22,17,15,46,92] have thrived in this field. These methods focus on determining whether testing images\ncontain anomalies, typically represented as local structural variants. Early methods [ 91,95,102,13,54,94]\nconcentrated on developing specific models for given categories, while recent approaches [ 45,22,17,112] target\na more general but challenging solution, i.e., developing a unified model for arbitrary product categories, which\nusually performs in few-shot [ 99,40] or even zero-shot [ 45,17,22] regime. As highlighted in [ 101], GPT-4V,\nequipped with extensive world knowledge, presents a promising solution for arbitrary category inspection.\n3.2 Testing philosophy\nDifferent prompts [ 101,56] could lead to different responses from GPT-4V. We aim to investigate the influence\nof different information on prompting GPT-4V for industrial anomaly detection. Following the previously\ndiscussed problems, this study further develops three prompts, a) class information: the names of the desired\ninspecting products, such as \"bottle\" and \"candle\", b) human expertise: the normal appearance and potential\nabnormal states and express them in languages, e.g., \"Normally, the image given should show a clean and\nwell-structured printed circuit board (PCB) with clear traces, soldered components, and distinct labels. It\nmay have defects such as bent pins, cold solder joints, missing components, or smudged labels\", c) reference\nimage: normal reference image to provide GPT-4V a better understanding of normality. We propose to\nevaluate GPT-4V in either a zero-shot setting, with only language prompts, or a one-shot setting, with one\nreference image provided along with the language prompts. For each setting, we test three different variants:\na) a naive prompt like \"Please determine whether the image contains anomalies or defects,\" b) with class\ninformation, and c) with human expertise.\n|8 Figure 2 |Industrial Image Anomaly Detection: Case 1, zero-shot, the Bottle category of MVTec AD [ 6].Yellow\nhighlights the given class information and normal and abnormal state descriptions. Green, red, and blue highlight the expected,\nincorrect, and additional information outputted by GPT-4V.\n|9 3.3 Case Demonstration\nFig. 2, 3, 4, 5, 6, 7 qualitatively demonstrate the effectiveness of GPT-4V for industrial image anomaly\ndetection. Even with a simple language prompt, GPT-4V effectively identifies anomalies in examined bottle\nand candle images, showcasing its capacity and versatility. Moreover, GPT-4V excels not only in detecting\ndesired anomalies but also in identifying fine-grained structural anomalies. As evident in Fig. 4, GPT-4V\nnoticed a slightly tilted wick on the bottom left candle, demonstrating its nuanced understanding. In complex\ncases like Fig. 6, GPT-4V recognizes the PCB in images and provides in-depth reasoning about anomalies, such\nas examining the proper seating of the ultrasonic sensor. However, GPT-4V overlooks the bent pin, resulting\nin an incorrect result. Nevertheless, GPT-4V showcases a strong grasp of image context and category-specific\nanomaly understanding.\n4 Industrial Image Anomaly Localization\n4.1 Task Introduction\nIndustrial image anomaly localization entails a more intricate process than mere image anomaly detection [ 76,\n93,12,13,65]. It goes beyond recognizing the abnormality within an image and extends to precisely\nidentifying the location of these anomalies. While GPT-4V has exhibited localization capabilities in various\ndomains [101, 97, 100], its potential for image anomaly localization warrants further exploration.\nRegrettably, GPT-4V does not currently have the capability to directly produce prediction masks. Some\nmethods have attempted to leverage GPT-4V by prompting it to provide bounding boxes [ 101,97]. However,\nthis approach appears to be imprecise and poses challenges for GPT-4V. In contrast, the approach presented\nby SoM [ 100] involves utilizing SAM [ 50] to generate visual prompts [ 81,50], which are presented in numbered\nmarkers. This visual prompting technique shifts the localization task from a pixel-level mask prediction task\nto a mask-level classification task, effectively reducing the associated complexities and increasing localization\nprecision.\n4.2 Testing philosophy\nTo harness the fine-grained localization capability of GPT-4V, we adopt the approach outlined in SoM [ 100].\nThis involves generating a set of image-mask pairs for prompting GPT-4V. In addition to the image-mask\npairs, we employ a straightforward language prompt that instructs the model, as follows: \"The first image\nneeds to be inspected. The second image contains its corresponding marks. Please determine whether the\nimage contains anomalies or defects. If yes, give a specific reason\".\n4.3 Case Demonstration\nFig. 8, 9, and 10 provide a visual representation of GPT-4V\u2019s performance in industrial anomaly localization.\nThese illustrations clearly illustrate GPT-4V\u2019s ability to accurately identify the second mask in Fig. 8 as a\ntwisted wire and the second mask in Fig. 9 as holes. These results serve as compelling evidence of GPT-4V\u2019s\nproficiency in localizing anomalies when guided by visual prompts.\nIt is important to acknowledge that GPT-4V does exhibit certain limitations when confronted with more\ncomplex scenarios, as evidenced in Fig. 10. However, the combination of visual prompting techniques and\nGPT-4V remains a promising approach for industrial anomaly localization.\n5 Point Cloud Anomaly Detection\n5.1 Task Introduction\nGeometrical information, as discussed in references such as PAD [ 111], Real3D [ 59], and MVTec-3D [ 8], holds\na crucial role in fields like industrial anomaly detection, especially when dealing with categories lacking textual\ninformation. Recently, MVTec 3D [ 8] and Real3D [ 59] have recognized the growing need for such information\nand have introduced a point cloud anomaly detection task. This task focuses on the identification of anomalies\n|10 within the provided point clouds [32].\nIt is important to note that the success achieved in industrial image anomaly detection is not fully mirrored\nin point cloud anomaly detection. This disparity is primarily attributed to the reliance of industrial image\nanomaly detection on robust pre-trained networks [ 12,75,39]. Conversely, due to the scarcity of extensive\npoint cloud data, the capabilities of pre-trained networks for point clouds currently fall short, leading to\nsuboptimal performance for some methods [96, 21, 9, 77].\nIn contrast, CPMF [ 16] proposes a novel approach by transforming point clouds into depth images, thereby\nopening up the possibility of leveraging image-based foundation models for point cloud anomaly detection.\nThis innovative method has shown the potential to deliver significantly improved results in point cloud\nanomaly detection.\n5.2 Testing philosophy\nTo employ GPT-4V in the context of point cloud anomaly detection, we adopt the methodology presented\nin CPMF [ 16] to transform point clouds into multi-view depth images. In our evaluation, we adhere to the\nprinciples commonly used in industrial image anomaly detection, specifically the zero/one-shot approach, with\nthe inclusion of three distinct variations of language prompts.\n5.3 Case Demonstration\nFig. 11, 12, 13, 14, 15, 16 provide a visual representation of the performance of GPT-4V in point cloud anomaly\ndetection. These illustrations serve to qualitatively illustrate the proficiency of GPT-4V in comprehending\nmulti-modality data.\nSpecifically, GPT-4V demonstrates its capability to accurately identify the presence of a small protrusion\nor bump on the top left part of the torus in the bagel (Fig. 11). Moreover, the introduction of additional\ninformation, such as class information and human expertise, enhances the performance of GPT-4V in point\ncloud anomaly detection, allowing it to effectively detect anomalies in the rope (Fig. 15 and 16).\nHowever, it is noteworthy that GPT-4V may occasionally misidentify artificially introduced elements during\nthe rendering process as anomalies, as observed in Fig. 14. It is possible that improvements in rendering\nquality could further enhance the capacity of GPT-4V in this context.\n6 Logical Anomaly Detection\n6.1 Task Introduction\nIn addition to structural anomalies, there exists another type of anomaly, named logical anomalies [ 7]. Logical\nanomalies generally refer to incorrect combinations of components, commonly encountered in the context\nof anomaly detection in assemblies. For instance, a screw bag should contain matched screws, nuts, and\nwashers. This necessitates that the model is capable of understanding fine-grained information in images\nand determining attributes of the components within the image, such as component type, length, color,\nquantity, and so forth. This places higher demands on the model. Existing logical anomaly detection\nmethods [ 63,103,5,106] typically relied on solely visual context and have achieved promising detection\nperformance. However, these approaches do not genuinely comprehend the content of images; instead, they\nrely on global-local correspondences [ 98] for logical anomaly detection. This does not effectively address logical\nanomaly detection. In contrast, GPT-4V possesses robust image understanding capabilities, allowing for a\nbetter comprehension of image content. By providing predefined normal rules manually, GPT-4V might be\ncapable of determining whether an image adheres to normal rules, thereby enabling a more rational approach\nto logical anomaly detection.\n6.2 Testing philosophy\nTo ensure an effective assessment of testing images, it is crucial to provide clear guidelines defining the\nexpected normal state for GPT-4V. This enables GPT-4V to evaluate the conformity of testing images with\n|11 the established standards, relying on an analysis of image content in relation to these norms. Consequently,\nour approach involves presenting GPT-4V with both testing images and descriptive language articulating\nthe expected normal standards. However, it is worth noting that GPT-4V might encounter difficulties\nin comprehending the nuances of normal standards when presented with language alone. To enhance its\nunderstanding and alignment of normal standards with the context of normal images, we propose the inclusion\nof a reference image illustrating the desired normal state. Therefore, our experimental design encompasses\nboth zero-shot and one-shot settings to assess the effectiveness of this approach.\n6.3 Case Demonstration\nThe evaluation results, as depicted in Fig. 17, 18, 19, 20, unequivocally highlight the robust image compre-\nhension and logical reasoning capabilities of GPT-4V. For instance, in Fig. 17, GPT-4V demonstrates its\nproficiency in interpreting intricate standards, encompassing criteria such as the presence of \"1. It should\ncontain two oranges, one peach, and some cereal, nuts, and banana slices; 2. The fruit should be on the left\nside of the lunch box, the cereal on the upper right, and the nuts and banana slices on the lower right of the\nlunch box\". GPT-4V adeptly breaks down this complex task into subcomponents, identifying and localizing\nthe various items before calculating their quantities and positions. Ultimately, GPT-4V accurately concludes\nthat the provided breakfast box does not adhere to the stipulated standards.\nMoreover, visual references play a pivotal role in enhancing GPT-4V\u2019s performance. In Fig. 18, without the\naid of a visual reference, GPT-4V erroneously classifies the juice bottle as a normal one. However, when\npresented with a referenced image, GPT-4V effectively comprehends the rule \"2. To prevent bottle explosions,\nensure the juice is filled to about 3cm below the bottle\u2019s opening\" and delivers a correct analysis.\nNonetheless, GPT-4V may encounter challenges in scenarios where its ability to contextualize images is\nconstrained. Notably, GPT-4V fails to detect a broken cable in Fig. 19 and inaccurately quantifies washers in\nFig. 20. The limitations of GPT-4V, particularly in matters of fine-grained details like counting, have been\naddressed in prior research [ 101]. Furthermore, it is worth noting that multi-round conversations and specific\nlanguage prompts can significantly impact GPT-4V\u2019s performance in such cases.\n7 Medical Image Anomaly Detection\n7.1 Task Introduction\nAnomaly detection, also known as outlier detection, is a pivotal task in the domain of medical imaging,\naimed at identifying abnormal patterns that do not conform to expected behavior[ 31]. These abnormalities or\nanomalies could be indicative of a wide range of medical conditions or diseases[citation]. The primary goal of\nanomaly detection is to accurately discern these irregularities from a plethora of medical imaging data, thereby\naiding in early diagnosis and effective treatment planning. Current medical anomaly detection methods can\nbe categorized into reconstruction-based methods [ 23] [35] [90], GAN-based [ 61], self-supervised methods [ 82]\n[88] [87] and pre-train methods [ 75] [28] [60] [62] Although these methods have achieved great improvements,\na unified anomaly detection model across different diseases and modalities still remains an unsolved challenge.\nAs highlighted in [ 71] and [97], GPT-4V, equipped with numerous multi-modal knowledge, shows promising\nfuture in enhancing the performance of anomaly detection tasks in various medical imaging modalities.\n7.2 Testing philosophy\nWe aim to investigate the generalization abilities of GPT-4V on medical anomaly detection. Thus medical\nimages on across different diseases and modalities are used, including Head MRI, Head CT, Retinal OCT,\nChest X-ray and so on. For the text prompt, we also take the previous multi-step prompt to test its zero-shot\nand one-shot abilities. There are generally three types of prompts, a)general medical information, the disease\nand modalities of the medical images, such as \"Chest X-ray Image\" or \"Head CT Image\" b)human expertise,\nbased on the general medical information, we further give the possible disease name in the medical image,\ne.g.\"The image should be classified as normal or hemorrhage\", c) reference image: normal reference image to\nprovide GPT-4V a better understanding of nomrality.\n|12 We propose to evaluate GPT-4V in either a zero-shot setting, with only language prompts, or a one-shot\nsetting, with one reference image provided along with the language prompts. For each setting, we test three\ndifferent variants: a) a naive prompt like \"Please determine whether the image contains anomalies\" b)general\nmedical information, and c) with human expertise.\n7.3 Case Demonstration\nFig.21, 23, 25 and 27 show the GPT-4V\u2019s zero-shot inference ability. GPT-4V is capable of automatically\nrecognizing medical image modalities and anatomical structures, even without general medical information\nprompts. The superior image caption ability enables GPT-4V to describe the spatial and textural anomalies\nin the image. However, due to ethical restrictions, the GPT-4V model tends to give conservative answers when\nlack of sufficient information. The introduction of both general medical information and human expertise\nsuccessfully leads GPT-4V to generate more concrete and accurate answers, as shown in Fig 21, 23 and\n25. However, GPT-4V fails to recognize anomalies in Fig 27, even with enough information provided. The\nabnormal area is not obvious in the image, so it turns out that it has high requirements for the medical\nimage quality. When a visual reference is added, the GPT-4V\u2019s image caption ability successfully describe the\ndifference between normal and abnormal images, which is shown in Fig 22, 24 26 and 28.\n8 Medical Image Anomaly Localization\n8.1 Task Introduction\nFollowing the detection of medical anomaly, the subsequent critical task is anomaly localization, which\nentails pinpointing the exact spatial location of the identified anomaly within the medical image [ 88] [104].\nAccurate localization is imperative for clinicians to understand the extent and nature of the pathology, which\nin turn informs the course of clinical intervention. However, the real-world clinical scenario, such as tumor\nanomaly localization, is more complex, where either normal or abmoral cases have multiple types of tumors.\nEstablishing a direct relationship between image pixels and excessive semantics (types of tumors) is diffcult for\nreal world medical image anomaly localization. Several methods, including self-supervised based method [ 88]\nand cluster-based method [ 104] have been proposed to deal with the medical image anomaly localization\ntask. Inspired by [ 100], we would like to examine the localization ability of GPT-4V model, under the visual\nprompts.\n8.2 Testing philosophy\nTo test the GPT-4V\u2019s ability on medical image localization, we utilize several diseases categories and modalities,\nincluding abdominal CT image, endoscopy image, head MRI image and skin lesion image. Both diseased area\nand manually synthetic abnormal are taken into consideration to test its robustness. The visual prompts\nproposed by [100] are also used to harness the fine-grained localization abilities of GPT-4V, including a set\nof image-mask pairs and corresponding index numbers to each mask. Thus, the input images are the raw\nimages with the augmented one with masks and numbers. We also adopt a straightforward text prompt to\nintroduce the relationship between the two input images, as follows: \"The first image needs to be inspected.\nThe second image contains its corresponding marks. Please determine whether the image contains anomalies\nor defects. If yes, give a specific reason\"\n8.3 Case Demonstration\nThe qualitative results are shown in Fig 29 30 31 and 32. Under the instruction of visual prompts in the\nimages, the GPT-4V tends to learn and caption the areas around the marks. For easily recognized and\nlocated cases, such as Fig 30 31 and 32, GPT-4V can clearly tell the difference between the anomaly areas\nand backgrounds. But GPT-4V fails in Fig 29, a synthetic case where the region-of-interest shares a similar\ntexture and shape with the background. This indicates that this model still needs to improve its detection\nand localization abilities under adversarial attack and complex backgrounds.\n|13 9 Pedestrian Anomaly Detection\n9.1 Task Introduction\nPedestrian anomaly detection, a subset of video anomaly detection, is dedicated to recognizing irregular\nactivities within pedestrian interactions captured in video streams. Traditional methodologies, as referenced\nby various studies [ 1,69,33,109,64,86,105,44], primarily rely on rule-based approaches and manually\nengineered features. In recent times, there has been a noticeable shift towards the adoption of deep learning\ntechniques [ 38,24,66,74,73,53,42,43,41] for pedestrian anomaly detection. The complexity of pedestrian\nanomaly detection arises from the need to accurately identify abnormal behaviors within the context of diverse\nand dynamic pedestrian interactions. This is further compounded by the varying environmental conditions\nin which these interactions take place. To ensure precise analysis, a substantial contextual understanding is\nessential. While existing methods have demonstrated promising performance in pedestrian anomaly detection,\nit is worth considering that GPT-4V, with its advanced contextual comprehension capabilities, has the\npotential to significantly enhance the performance of this task.\n9.2 Testing philosophy\nWe utilize the GPT-4V model, which currently only accepts image format visual input, for pedestrian anomaly\ndetection. To prompt the model, we select two images from the video dataset. In addition to the image\nprompt, we include a simple text prompt asking the model to determine if the video frames contain anomalies\nor outlier points and provide a specific reason if so.\n9.3 Case Demonstration\nIn Fig. 33, we illustrate a scenario (from UCF-Crime datadet [ 85]) where a pedestrian aggresses another\non the road. The GPT-4V model recognizes the aggressive behavior as an anomaly when compared to\ntypical interactions. Additionally, it suggests caution due to the \"LiveLeak\" watermark, implying a need for\nfurther analysis with sufficient contextual information before drawing conclusions. The model\u2019s adeptness\nat discerning aggressive behavior, even in the absence of technical anomalies, demonstrates its potential to\nidentify social anomalies within visual data.\n10 Traffic Anomaly Detection\n10.1 Task Introduction\nTraffic anomaly detection primarily aims at identifying the commencement and conclusion of abnormal\nevents, with lesser emphasis on spatial localization. Various methodologies [ 38,70,24,67,68,35,35] have\nbeen devised to model normalcy and discern regular patterns in video frames. The prevailing challenge for\nanomaly detection in traffic scenarios is the development of robust algorithms that can effectively differentiate\nbetween normal and abnormal vehicles and driving behaviors, thereby ensuring the safety and reliability of\nthe autonomous vehicle system. Integrating GPT4v into traffic anomaly detection promises to refine the\nprecision and speed of current systems. GPT4v, which has the ability to conduct high-level understanding, is\nadept at parsing the intricacies of traffic data, thereby sharpening the discrepancy between normal variations\nand true anomalies. This precision is critical for developing real-time monitoring systems that deliver accurate\nalerts while minimizing false positives.\n10.2 Testing philosophy\nWe employ GPT-4V for traffic anomaly detection, which, as of now, only accepts visual input in image format.\nTo engage the model, we select a representative image from the traffic scene, accompanied by a succinct text\nprompt. This prompt requests the model to ascertain whether the image frames harbor anomalies or outlier\npoints, and if found, to elucidate the specific reasons for such irregularities.\n|14 10.3 Case Demonstration\nAs depicted in Fig. 34 and 35, by scrutinizing the spatial-temporal dynamics within the traffic scenes from\na traffic anomaly detection dataset [ 48], GPT-4V proficiently differentiates between standard traffic flow\nand anomalous events. Beyond merely identifying outliers in traffic patterns, the model extends its utility\nby offering insightful elucidations concerning the abnormal nature of the scenarios. For instance, in Fig.34,\nthe model effectively explicates an abnormal vehicular maneuver that collides with the roadside barrier\nand deviates from typical driving behavior. Harnessing its deep comprehension of the underlying patterns\nand relationships within the traffic data, the model employs interpretable techniques to unravel the factors\ncontributing to the anomaly, thereby providing a nuanced understanding that could be pivotal for enhancing\nthe safety and reliability of autonomous driving systems.\n11 Time Series Anomaly Detection\n11.1 Task Introduction\nTime series anomaly detection refers to the task of identifying unusual or abnormal patterns, events, or\nbehaviors in sequential data over time, that deviate significantly from the expected or normal behavior. Time\nseries anomaly detection models can be categorized as supervised or unsupervised algorithms. Supervised\nmethods perform well when anomaly labels are available, such as AutoEncoder [ 79] and RobustTAD[ 34].\nUnsupervised algorithms are suitable when obtaining anomaly labels is challenging. This has led to the\ndevelopment of new unsupervised methods, including DAGMM [ 115] and OmniAnomaly [ 83]. Unsupervised\ndeep learning methods excel in time series anomaly detection, leveraging representation learning and a\nreconstruction approach to accurately identify anomalies without the need for labeled data [110, 47, 108].\n11.2 Testing philosophy\nTo exploit GPT-4V for time series anomaly detection, we plot time series into images and then deliver the\ntesting data to GPT-4V. Specifically, we select two instances [ 2,89] along with a simple text prompt asking\nthe model to determine if the image contains anomalies or outlier points and provide a specific reason if so.\n11.3 Case Demonstration\nAs illustrated in Fig. 36 and 37, by examining the temporal dependencies and trends within the time series,\nGPT-4V adeptly differentiates between normal fluctuations and anomalous behavior. Beyond merely detecting\noutliers in the time series curves, the model extends its utility by offering insightful explanations regarding the\nabnormal nature of the data. For instance, in Fig. 37, the model effectively elucidates the abnormal peak in\nthe time series. Drawing upon its profound understanding of the underlying patterns and relationships within\nthe data, the model employs interpretability techniques to illuminate the factors contributing to the anomaly.\n12 Prospect\nThe future evaluation and utilization of GPT-4V for anomaly detection hold significant promise in addressing\ncomplex challenges across various domains. As a versatile language model, GPT-4V demonstrates its potential\nin anomaly detection, and the following prospects aim to refine its capabilities, foster integration, and elevate\nits performance.\n1.Quantitative Analysis : Incorporating quantitative metrics, such as Precision, Recall, and F1-score,\nalongside AUC-ROC and MAP, in future evaluations will provide a more comprehensive understanding of\nGPT-4V\u2019s anomaly detection performance. This quantification will empower a more objective assessment\nof the model\u2019s capabilities and its adaptation to diverse anomaly detection tasks.\n2.Expanding Evaluation Scope : Expanding the scope to include real-world challenges, such as varying\nlighting conditions and occlusions in image-based anomaly detection, and different types of anomalies in\ntime-series data, offers a more realistic view of GPT-4V\u2019s adaptability and limitations. The inclusion of\nsynthetic and real-world anomalies adds depth to the evaluation process.\n|15 3.Multi-round Interaction Evaluation : The potential of multi-round conversations for GPT-4V\u2019s\niterativelearningandadaptationtofeedbackprovidesadynamicframeworkforenhancingitsperformance\nin anomaly detection. It is a promising avenue for scenarios where ongoing refinement is crucial, such as\ncybersecurity.\n4.Incorporation of Human Feedback : Utilizing human feedback loops presents the opportunity for\ndomain experts to refine GPT-4V\u2019s understanding of complex or nuanced anomalies. The collaboration\nbetween the model and experts promises to address real-world challenges effectively.\n5.Integration of Auxiliary Data : Exploring the impact of integrating auxiliary data, such as additional\nsensor readings or metadata, is instrumental in enhancing GPT-4V\u2019s understanding and accuracy in\nidentifying anomalies across various domains. This comprehensive approach aligns with real-world data\nscenarios.\n6.Comparison with Specialized Models : Comparative evaluations against specialized anomaly\ndetection models are essential to identify the specific strengths and weaknesses of GPT-4V. These\nassessments will clarify the domains and use cases where GPT-4V\u2019s versatility excels or where specialized\nmodels remain superior.\n7.Real-Time Performance Assessment : Evaluating GPT-4V\u2019s real-time performance is crucial for\napplications requiring rapid anomaly detection. This prospect ensures the model\u2019s suitability for\ntime-critical or online anomaly detection tasks.\n8.Transfer Learning Evaluation : Assessing the effectiveness of transfer learning in fine-tuning GPT-4V\nfor specific anomaly detection tasks can pave the way for broader generalization. It enhances the model\u2019s\nadaptability in diverse anomaly detection scenarios.\n9.Hybrid Model Development : The development of hybrid models combining GPT-4V with other\nmachine learning or deep learning approaches offers an innovative approach to address anomaly de-\ntection challenges. These hybrids aim to leverage GPT-4V\u2019s linguistic capabilities while enhancing its\nperformance in specialized scenarios.\nIn summation, these prospects set the stage for a comprehensive and multifaceted exploration of GPT-4V\u2019s\nanomaly detection potential. By combining quantitative metrics, real-world challenges, human feedback,\nauxiliary data integration, comparative assessments, and real-time capabilities, we can unlock the full scope\nof GPT-4V\u2019s utility in addressing anomalies across diverse fields. The journey towards improved anomaly\ndetection with GPT-4V is one of collaboration, adaptation, and innovation, promising exciting developments\nin the years to come.\n13 Conclusion\nIn conclusion, the assessment of GPT-4V\u2019s capabilities in anomaly detection signifies a notable advancement\nin the realm of versatile and adaptable AI models. GPT-4V demonstrates exceptional proficiency in iden-\ntifying anomalies across diverse modalities and fields, offering both comprehensive and nuanced semantic\ncomprehension. Its ability to deduce anomalies and its responsiveness to an expanding array of prompts\nunderscore its versatility and potential. Nevertheless, like any technology, there remains room for further\nenhancement, particularly in intricate and subtle scenarios.\nThe opportunities delineated in this evaluation propose promising avenues for future research and development.\nThe inclusion of quantitative metrics, broadening the spectrum of evaluations, embracing human input, and\nintegrating supplementary data all contribute to augmenting the performance of GPT-4V. Comparative\nassessments against specialized models and the exploration of hybrid models further enrich the landscape\nof anomaly detection. Real-time assessment and the incorporation of transfer learning hold the promise of\naddressing time-sensitive situations and generalizing anomaly detection across diverse domains.\nAs we embark on this journey to unlock the full potential of GPT-4V, collaboration, adaptability, and\ninnovation will serve as the foundational pillars of our success. The evaluation and utilization of GPT-4V for\nanomaly detection do not merely signify an exploration of technology but also serve as a testament to the\nongoing evolution of AI and its transformative impact on real-world applications. Keeping these prospects in\nmind, the future of anomaly detection holds significant promise, and GPT-4V stands at the forefront of this\ncaptivating evolution.\n|16 Figure 3 |Industrial Image Anomaly Detection: Case 1, one-shot, the Bottle category of MVTec AD [ 6].Yellow\nhighlights the given class information and normal and abnormal state descriptions. Green, red, and blue highlight the expected,\nincorrect, and additional information outputted by GPT-4V.\n|17 Figure 4 |Industrial Image Anomaly Detection: Case 2, zero-shot, the Candle category of VisA [ 116].Yellow\nhighlights the given class information and normal and abnormal state descriptions. Green, red, and blue highlight the expected,\nincorrect, and additional information outputted by GPT-4V.\n|18 Figure 5 |Industrial Image Anomaly Detection: Case 2, one-shot, the Candle category of VisA [ 116].Yellow\nhighlights the given class information and normal and abnormal state descriptions. Green, red, and blue highlight the expected,\nincorrect, and additional information outputted by GPT-4V.\n|19 Figure 6 |Industrial Image Anomaly Detection: Case 3, zero-shot, the PCB2 category of VisA [ 116].Yellow\nhighlights the given class information and normal and abnormal state descriptions. Green, red, and blue highlight the expected,\nincorrect, and additional information outputted by GPT-4V.\n|20 Figure 7 |Industrial Image Anomaly Detection: Case 3, one-shot, the PCB2 category of VisA [ 116].Yellow\nhighlights the given class information and normal and abnormal state descriptions. Green, red, and blue highlight the expected,\nincorrect, and additional information outputted by GPT-4V.\n|21 Figure 8 |Industrial Image Anomaly Localization: Case 1, zero-shot, the Bottle category of MVTec AD [ 6].\nYellow highlights the given class information and normal and abnormal state descriptions. Green, red, and blue highlight the\nexpected, incorrect, and additional information outputted by GPT-4V.\n|22 Figure 9 |Industrial Image Anomaly Localization: Case 2, the Hazelnut category of MVTec AD [ 6].Yellow\nhighlights the given class information and normal and abnormal state descriptions. Green, red, and blue highlight the expected,\nincorrect, and additional information outputted by GPT-4V.\n|23 Figure 10 |Industrial Image Anomaly Localization: Case 3, the Capsule category of VisA [ 116].Yellow highlights\nthe given class information and normal and abnormal state descriptions. Green, red, and blue highlight the expected, incorrect,\nand additional information outputted by GPT-4V.\n|24 Figure 11 |Point Cloud Anomaly Detection: Case 1, zero-shot, the Bagel category of MVTec 3D [ 8].Yellow\nhighlights the given class information and normal and abnormal state descriptions. Green, red, and blue highlight the expected,\nincorrect, and additional information outputted by GPT-4V.\n|25 Figure 12 |Point Cloud Anomaly Detection: Case 1, one-shot, the Bagel category of MVTec 3D [ 8].Yellow\nhighlights the given class information and normal and abnormal state descriptions. Green, red, and blue highlight the expected,\nincorrect, and additional information outputted by GPT-4V.\n|26 Figure 13 |Point Cloud Anomaly Detection: Case 2, zero-shot, the Peach category of MVTec 3D [ 8].Yellow\nhighlights the given class information and normal and abnormal state descriptions. Green, red, and blue highlight the expected,\nincorrect, and additional information outputted by GPT-4V.\n|27 Figure 14 |Point Cloud Anomaly Detection: Case 2, one-shot, the Peach category of MVTec 3D [ 8].Yellow\nhighlights the given class information and normal and abnormal state descriptions. Green, red, and blue highlight the expected,\nincorrect, and additional information outputted by GPT-4V.\n|28 Figure 15 |Point Cloud Anomaly Detection: Case 3, zero-shot, the Rope category of MVTec 3D [ 8].Yellow\nhighlights the given class information and normal and abnormal state descriptions. Green, red, and blue highlight the expected,\nincorrect, and additional information outputted by GPT-4V.\n|29 Figure 16 |Point Cloud Anomaly Detection: Case 3, one-shot, the Rope category of MVTec 3D [ 8].Yellow\nhighlights the given class information and normal and abnormal state descriptions. Green, red, and blue highlight the expected,\nincorrect, and additional information outputted by GPT-4V.\n|30 Figure 17 |Logical Anomaly Detection: Case 1, the Breakfast Box category of MVTec LOCO [ 7].Yellow highlights\nthe given class information and normal and abnormal state descriptions. Green, red, and blue highlight the expected, incorrect,\nand additional information outputted by GPT-4V.\n|31 Figure 18 |Logical Anomaly Detection: Case 2, the Juice Bottle category of MVTec LOCO [ 7].Yellow highlights\nthe given class information and normal and abnormal state descriptions. Green, red, and blue highlight the expected, incorrect,\nand additional information outputted by GPT-4V.\n|32 Figure 19 |Logical Anomaly Detection: Case 3, the Splicing Connector category of MVTec LOCO [ 7].Yellow\nhighlights the given class information and normal and abnormal state descriptions. Green, red, and blue highlight the expected,\nincorrect, and additional information outputted by GPT-4V.\n|33 Figure 20 |Logical Anomaly Detection: Case 4, the Screw Bag category of MVTec LOCO [ 7].Yellow highlights\nthe given class information and normal and abnormal state descriptions. Green, red, and blue highlight the expected, incorrect,\nand additional information outputted by GPT-4V.\n|34 Figure 21 |Medical Anomaly Detection: Case 1, the Chest X-ray [ 49].Yellow highlights the given class information\nand normal and abnormal state descriptions. Green, red, and blue highlight the expected, incorrect, and additional information\noutputted by GPT-4V.\n|35 Figure 22 |Medical Anomaly Detection: Case 1, the Chest X-ray [ 49].Yellow highlights the given class information\nand normal and abnormal state descriptions. Green, red, and blue highlight the expected, incorrect, and additional information\noutputted by GPT-4V.\n|36 Figure 23 |Medical Anomaly Detection: Case 2, the Retinal OCT [ 49].Yellow highlights the given class information\nand normal and abnormal state descriptions. Green, red, and blue highlight the expected, incorrect, and additional information\noutputted by GPT-4V.\n|37 Figure 24 |Medical Anomaly Detection: Case 2, the Retinal OCT [ 49].Yellow highlights the given class information\nand normal and abnormal state descriptions. Green, red, and blue highlight the expected, incorrect, and additional information\noutputted by GPT-4V.\n|38 Figure 25 |Medical Anomaly Detection: Case 3, the Head CT [ 51].Yellow highlights the given class information and\nnormal and abnormal state descriptions. Green, red, and blue highlight the expected, incorrect, and additional information\noutputted by GPT-4V.\n|39 Figure 26 |Medical Anomaly Detection: Case 3, the Head CT [ 51].Yellow highlights the given class information and\nnormal and abnormal state descriptions. Green, red, and blue highlight the expected, incorrect, and additional information\noutputted by GPT-4V.\n|40 Figure 27 |Medical Anomaly Detection: Case 4, Head MRI Image [ 18].Yellow highlights the given class information\nand normal and abnormal state descriptions. Green, red, and blue highlight the expected, incorrect, and additional information\noutputted by GPT-4V.\n|41 Figure 28 |Medical Anomaly Detection: Case 4, Head MRI Image [ 18].Yellow highlights the given class information\nand normal and abnormal state descriptions. Green, red, and blue highlight the expected, incorrect, and additional information\noutputted by GPT-4V.\n|42 Figure 29 |Medical Anomaly Localization: Case 1, Abdonimal CT Localization [ 114].Yellow highlights the given\nclass information and normal and abnormal state descriptions. Green, red, and blue highlight the expected, incorrect, and\nadditional information outputted by GPT-4V.\n|43 Figure 30 |Medical Anomaly Localization: Case 2,Head MRI Localization [ 114].Yellow highlights the given class\ninformation and normal and abnormal state descriptions. Green, red, and blue highlight the expected, incorrect, and additional\ninformation outputted by GPT-4V.\n|44 Figure 31 |Medical Anomaly Localization: Case 3, Skin Lesion Localization [ 26].Yellow highlights the given class\ninformation and normal and abnormal state descriptions. Green, red, and blue highlight the expected, incorrect, and additional\ninformation outputted by GPT-4V.\n|45 Figure 32 |Medical Anomaly Localization: Case 4, Endoscopy Localization [ 11].Yellow highlights the given class\ninformation and normal and abnormal state descriptions. Green, red, and blue highlight the expected, incorrect, and additional\ninformation outputted by GPT-4V.\n|46 Figure 33 |Pedestrian Anomaly Detection: Case 1, from UCF-Crime Dataset [ 85]. Green highlights the expected\ninformation outputted by GPT-4V.\n|47 Figure 34 |Traffic Anomaly Detection: Case 1, from Kaggle Accident Detection [ 48]. Green highlights the expected\ninformation outputted by GPT-4V.\n|48 Figure 35 |Traffic Anomaly Detection: Case 2, from Kaggle Accident Detection [ 48]. Green highlights the expected\ninformation outputted by GPT-4V.\n|49 Figure 36 |Time Series Anomaly Detection: Case 1, from Outlier Detection Dataset [ 89].Green highlights the\nexpected information outputted by GPT-4V.\n|50 Figure 37 |Time Series Anomaly Detection: Case 2, from Catfish Sales Dataset [ 2]. Green highlights the expected\ninformation outputted by GPT-4V.\n|51 References\n[1]Amit Adam, Ehud Rivlin, Ilan Shimshoni, and Daviv Reinitz. Robust real-time unusual event detection\nusing multiple fixed-location monitors. IEEE Transactions on Pattern Analysis and Machine Intelligence ,\n30(3):555\u2013560, 2008.\n[2]Neptune AI. Anomaly detection in time series. https://neptune.ai/blog/anomaly-detection-in-time-series , 2023.\nAccessed: 2023-11-04.\n[3]Samet Ak\u00e7ay, Amir Atapour-Abarghouei, and T. Breckon. Ganomaly: Semi-supervised anomaly\ndetection via adversarial training. In Asian Conference on Computer Vision , 2018.\n[4]Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint\narXiv:2305.10403 , 2023.\n[5]Kilian Batzner, Lars Heckler, and Rebecca K\u00f6nig. Efficientad: Accurate visual anomaly detection at\nmillisecond-level latencies. arXiv preprint arXiv:2303.14535 , 2023.\n[6]Paul Bergmann, Kilian Batzner, Michael Fauser, David Sattlegger, and Carsten Steger. The MVTec\nanomaly detection dataset: A comprehensive real-world dataset for unsupervised anomaly detection.\nInternational Journal of Computer Vision , 129(4):1038\u20131059, 2021.\n[7]Paul Bergmann, Kilian Batzner, Michael Fauser, David Sattlegger, and Carsten Steger. Beyond dents\nand scratches: Logical constraints in unsupervised anomaly detection and localization. International\nJournal of Computer Vision , 130(4):947\u2013969, 2022.\n[8]Paul Bergmann, Xin Jin, David Sattlegger, and Carsten Steger. The MVTec 3d-AD dataset for\nunsupervised 3d anomaly detection and localization. In Proceedings of the 17th International Joint\nConference on Computer Vision, Imaging and Computer Graphics Theory and Applications , pages\n202\u2013213, 2022.\n[9]Paul Bergmann and David Sattlegger. Anomaly detection in 3d point clouds using deep geometric\ndescriptors. arXiv preprint arXiv:2202.11660 , 2022.\n[10]Ane Bl\u2019azquez-Garc\u2019ia, Angel Conde, Usue Mori, and Jos\u00e9 Antonio Lozano. A review on outlier/anomaly\ndetection in time series data. ACM Computing Surveys , 54:1 \u2013 33, 2020.\n[11]Hanna Borgli, Vajira Thambawita, Pia H Smedsrud, Steven Hicks, Debesh Jha, Sigrun L Eskeland,\nKristin Ranheim Randel, Konstantin Pogorelov, Mathias Lux, Duc Tien Dang Nguyen, et al. Hyperkvasir,\na comprehensive multi-class image and video dataset for gastrointestinal endoscopy. Scientific data ,\n7(1):283, 2020.\n[12]Yuxuan Cai, Dingkang Liang, Dongliang Luo, Xinwei He, Xin Yang, and Xiang Bai. A discrepancy\naware framework for robust anomaly detection. IEEE Transactions on Industrial Informatics , pages\n1\u201310, 2023.\n[13]Yunkang Cao, Yanan Song, Xiaohao Xu, Shuya Li, Yuhao Yu, Yifeng Zhang, and Weiming Shen.\nSemi-supervised knowledge distillation for tiny defect detection. In 2022 IEEE 25th International\nConference on Computer Supported Cooperative Work in Design (CSCWD) , pages 1010\u20131015. IEEE,\n2022.\n[14]Yunkang Cao, Qian Wan, Weiming Shen, and Liang Gao. Informative knowledge distillation for image\nanomaly segmentation. Knowledge-Based Systems , 248:108846, 2022.\n[15]Yunkang Cao, Xiaohao Xu, Zhaoge Liu, and Weiming Shen. Collaborative discrepancy optimization for\nreliable image anomaly localization. IEEE Transactions on Industrial Informatics , pages 1\u201310, 2023.\n[16]Yunkang Cao, Xiaohao Xu, and Weiming Shen. Complementary pseudo multimodal feature for point\ncloud anomaly detection. arXiv preprint arXiv:2303.13194 , 2023.\n[17]Yunkang Cao, Xiaohao Xu, Chen Sun, Yuqi Cheng, Zongwei Du, Liang Gao, and Weiming Shen. Segment\nany anomaly without training via hybrid prompt regularization. arXiv preprint arXiv:2305.10724 , 2023.\n[18] Navoneel Chakrabarty. Brain mri images for brain tumor detection, 2019.\n[19]Raghavendra Chalapathy and Sanjay Chawla. Deep learning for anomaly detection: A survey. arXiv\npreprint arXiv:1901.03407 , 2019.\n|52 [20]Varun Chandola, Arindam Banerjee, and Vipin Kumar. Anomaly detection: A survey. ACM Computing\nSurveys, 41(3), jul 2009.\n[21]Rui Chen, Guoyang Xie, Jiaqi Liu, Jinbao Wang, Ziqi Luo, Jinfan Wang, and Feng Zheng. Easynet: An\neasy network for 3d industrial anomaly detection. Proceedings of the 31st ACM International Conference\non Multimedia , 2023.\n[22]Xuhai Chen, Yue Han, and Jiangning Zhang. A zero-/few-shot anomaly classification and segmentation\nmethod for CVPR 2023 VAND workshop challenge tracks 1&2: 1st place on zero-shot AD and 4th place\non few-shot AD. arXiv preprint arXiv:2305.17382 , 2023.\n[23]Xiaoran Chen, Suhang You, Kerem Can Tezcan, and Ender Konukoglu. Unsupervised lesion detection\nvia image restoration with a normative prior. Medical image analysis , 64:101713, 2020.\n[24]Yong Shean Chong and Yong Haur Tay. Abnormal event detection in videos using spatiotemporal\nautoencoder. In International symposium on neural networks , pages 189\u2013196. Springer, 2017.\n[25]Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language\nmodeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.\n[26]Noel CF Codella, David Gutman, M Emre Celebi, Brian Helba, Michael A Marchetti, Stephen W\nDusza, Aadi Kalloo, Konstantinos Liopyris, Nabin Mishra, Harald Kittler, et al. Skin lesion analysis\ntoward melanoma detection: A challenge at the 2017 international symposium on biomedical imaging\n(isbi), hosted by the international skin imaging collaboration (isic). In 2018 IEEE 15th international\nsymposium on biomedical imaging (ISBI 2018) , pages 168\u2013172. IEEE, 2018.\n[27]Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang\nLi, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with\ninstruction tuning. arXiv preprint arXiv:2305.06500 , 2023.\n[28]ThomasDefard, AleksandrSetkov, AngeliqueLoesch, andRomaricAudigier. Padim: apatchdistribution\nmodeling framework for anomaly detection and localization. In International Conference on Pattern\nRecognition , pages 475\u2013489. Springer, 2021.\n[29]Jan Diers and Christian Pigorsch. A survey of methods for automated quality control based on images.\nInternational Journal of Computer Vision , 2023.\n[30]Min Du, Feifei Li, Guineng Zheng, and Vivek Srikumar. Deeplog: Anomaly detection and diagnosis from\nsystem logs through deep learning. In Proceedings of the 2017 ACM SIGSAC Conference on Computer\nand Communications Security , 2017.\n[31]Tharindu Fernando, Harshala Gammulle, Simon Denman, Sridha Sridharan, and Clinton Fookes. Deep\nlearning for medical anomaly detection\u2013a survey. ACM Computing Surveys (CSUR) , 54(7):1\u201337, 2021.\n[32]Alberto Floris, Luca Frittoli, Diego Carrera, and Giacomo Boracchi. Composite layers for deep anomaly\ndetection on 3d point clouds. arXiv preprint arXiv:2209.11796 , 2022.\n[33]Harrou Fouzi and Ying Sun. Enhanced anomaly detection via pls regression models and information\nentropy theory. In IEEE Symposium Series on Computational Intelligence (SSCI) , pages 383\u2013388, 2015.\n[34]Jingkun Gao, Xiaomin Song, Qingsong Wen, Pichao Wang, Liang Sun, and Huan Xu. Robusttad:\nRobust time series anomaly detection via decomposition and convolutional neural networks. arXiv\npreprint arXiv:2002.09545 , 2020.\n[35]Dong Gong, Lingqiao Liu, Vuong Le, Budhaditya Saha, Moussa Reda Mansour, Svetha Venkatesh, and\nAnton van den Hengel. Memorizing normality to detect anomaly: Memory-augmented deep autoencoder\nfor unsupervised anomaly detection. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 1705\u20131714, 2019.\n[36]Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei\nZhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vision and language model for dialogue with\nhumans, 2023.\n[37]Zhaopeng Gu, Bingke Zhu, Guibo Zhu, Yingying Chen, Ming Tang, and Jinqiao Wang. Anomalygpt:\nDetecting industrial anomalies using large vision-language models. arXiv preprint arXiv:2308.15366 ,\n2023.\n|53 [38]Mahmudul Hasan, Jonghyun Choi, Jan Neumann, Amit K Roy-Chowdhury, and Larry S Davis. Learning\ntemporal regularity in video sequences. In Proceedings of the IEEE conference on computer vision and\npattern recognition , pages 733\u2013742, 2016.\n[39]Lars Heckler, Rebecca K\u00f6nig, and Paul Bergmann. Exploring the importance of pretrained feature\nextractors for unsupervised anomaly detection and localization. In 2023 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition Workshops (CVPRW) , pages 2917\u20132926, 2023.\n[40]Chaoqin Huang, Haoyan Guan, Aofan Jiang, Ya Zhang, Michael Spratling, and Yan-Feng Wang.\nRegistration based few-shot anomaly detection. In European Conference on Computer Vision , pages\n303\u2013319. Springer, 2022.\n[41]Chao Huang, Chengliang Liu, Jie Wen, Lian Wu, Yong Xu, Qiuping Jiang, and Yaowei Wang. Weakly su-\npervised video anomalydetectionvia self-guided temporal discriminative transformer. IEEE Transactions\non Cybernetics , pages 1\u201314, 2022.\n[42]Chao Huang, Jie Wen, Yong Xu, Qiuping Jiang, Jian Yang, Yaowei Wang, and David Zhang. Self-\nsupervised attentive generative adversarial networks for video anomaly detection. IEEE Transactions\non Neural Networks and Learning Systems , pages 1\u201315, 2022.\n[43]Chao Huang, Zehua Yang, Jie Wen, Yong Xu, Qiuping Jiang, Jian Yang, and Yaowei Wang. Self-\nsupervision-augmented deep autoencoder for unsupervised visual anomaly detection. IEEE Transactions\non Cybernetics , 52(12):13834\u201313847, 2022-12.\n[44]Tsuyoshi Id\u00e9, Ankush Khandelwal, and Jayant Kalagnanam. Sparse gaussian markov random field\nmixtures for anomaly detection. In IEEE 16th International Conference on Data Mining (ICDM) , pages\n955\u2013960, 2016.\n[45]Jongheon Jeong, Yang Zou, Taewan Kim, Dongqing Zhang, Avinash Ravichandran, and Onkar Dabeer.\nWinclip: Zero-/few-shot anomaly classification and segmentation. arXiv preprint arXiv:2303.14814 ,\n2023.\n[46]Yuxin Jiang, Yunkang Cao, and Weiming Shen. A masked reverse knowledge distillation method\nincorporating global and local information for image anomaly detection. Knowledge-Based Systems ,\n280:110982, 2023.\n[47]Yang Jiao, Kai Yang, Dongjing Song, and Dacheng Tao. Timeautoad: Autonomous anomaly detection\nwith self-supervised contrastive loss for multivariate time series. IEEE Transactions on Network Science\nand Engineering , 9(3):1604\u20131619, 2022.\n[48]C. Kay. Accident detection from cctv footage. https://www.kaggle.com/datasets/ckay16/accident-detection-from-\ncctv-footage , 2022. Kaggle dataset.\n[49]Daniel S Kermany, Michael Goldbaum, Wenjia Cai, Carolina CS Valentim, Huiying Liang, Sally L\nBaxter, Alex McKeown, Ge Yang, Xiaokang Wu, Fangbing Yan, et al. Identifying medical diagnoses\nand treatable diseases by image-based deep learning. cell, 172(5):1122\u20131131, 2018.\n[50]Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\nXiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment\nanything. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) ,\npages 4015\u20134026, October 2023.\n[51] Felipe Campos Kitamura. Head ct - hemorrhage, 2018.\n[52]Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and Jianfeng\nGao. Multimodal foundation models: From specialists to general-purpose assistants. arXiv preprint\narXiv:2309.10020 , 2023.\n[53]Chun-Liang Li, Kihyuk Sohn, Jinsung Yoon, and Tomas Pfister. Cutpaste: Self-supervised learning for\nanomaly detection and localization. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 9664\u20139674, 2021.\n[54]Yufei Liang, Jiangning Zhang, Shiwei Zhao, Runze Wu, Yong Liu, and Shuwen Pan. Omni-frequency\nchannel-selection representations for unsupervised anomaly detection. arXiv preprint arXiv:2203.00259 ,\n2022.\n|54 [55]Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou.\nHallusionbench: You see what you think? or you think what you see? an image-context reasoning\nbenchmark challenging for gpt-4v (ision), llava-1.5, and other multi-modality models. arXiv preprint\narXiv:2310.14566 , 2023.\n[56]Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning large\nmulti-modal model with robust instruction tuning. arXiv preprint arXiv:2306.14565 , 2023.\n[57]Fuxiao Liu, Yaser Yacoob, and Abhinav Shrivastava. Covid-vts: Fact extraction and verification on\nshort video platforms. arXiv preprint arXiv:2302.07919 , 2023.\n[58]Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint\narXiv:2304.08485 , 2023.\n[59]Jiaqi Liu, Guoyang Xie, Rui Chen, Xinpeng Li, Jinbao Wang, Yong Liu, Chengjie Wang, and Feng\nZheng. Real3d-ad: A dataset of point cloud anomaly detection. arXiv preprint arXiv:2309.13226 , 2023.\n[60]Mingxuan Liu, Yunrui Jiao, and Hong Chen. Skip-st: Anomaly detection for medical images using\nstudent-teacher network with skip connections. In 2023 IEEE International Symposium on Circuits and\nSystems (ISCAS) , pages 1\u20135, 2023.\n[61]Mingxuan Liu, Yunrui Jiao, Hongyu Gu, Jingqiao Lu, and Hong Chen. Data augmentation using\nimage-to-image translation for tongue coating thickness classification with imbalanced data. In 2022\nIEEE Biomedical Circuits and Systems Conference (BioCAS) , pages 90\u201394, 2022.\n[62]Mingxuan Liu, Yunrui Jiao, Jingqiao Lu, and Hong Chen. Anomaly detection for medical images using\nteacher-student model with skip connections and multi-scale anomaly consistency. TechRxiv , 2023.\n[63]Tongkun Liu, Bing Li, Xiao Du, Bingke Jiang, Xiao Jin, Liuyi Jin, and Zhu Zhao. Component-aware\nanomaly detection framework for adjustable and logical industrial visual inspection. arXiv preprint\narXiv:2305.08509 , 2023.\n[64]Cewu Lu, Jianping Shi, and Jiaya Jia. Abnormal event detection at 150 fps in matlab. In IEEE\nInternational Conference on Computer Vision , pages 2720\u20132727, 2013.\n[65]Ruiying Lu, YuJie Wu, Long Tian, Dongsheng Wang, Bo Chen, Xiyang Liu, and Ruimin Hu. Hier-\narchical vector quantized transformer for multi-class unsupervised anomaly detection. arXiv preprint\narXiv:2310.14228 , 2023.\n[66]Weixin Luo, Wen Liu, and Shenghua Gao. Remembering history with convolutional lstm for anomaly\ndetection. In IEEE International Conference on Multimedia and Expo (ICME) , pages 439\u2013444, 2017.\n[67]Weixin Luo, Wen Liu, and Shenghua Gao. Remembering history with convolutional lstm for anomaly\ndetection. In 2017 IEEE International conference on multimedia and expo (ICME) , pages 439\u2013444.\nIEEE, 2017.\n[68]Weixin Luo, Wen Liu, and Shenghua Gao. A revisit of sparse coding based anomaly detection in\nstacked rnn framework. In Proceedings of the IEEE International Conference on Computer Vision ,\npages 341\u2013349, 2017.\n[69]Vijay Mahadevan, Weixin Li, Viral Bhalodia, and Nuno Vasconcelos. Anomaly detection in crowded\nscenes. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition , pages\n1975\u20131981, 2010.\n[70]Jefferson Ryan Medel and Andreas Savakis. Anomaly detection in video using predictive convolutional\nlong short-term memory networks. arXiv preprint arXiv:1612.00390 , 2016.\n[71] OpenAI. Gpt-4v(ision) system card. 2023.\n[72]Guansong Pang, Chunhua Shen, Longbing Cao, and Anton van den Hengel. Deep learning for anomaly\ndetection. ACM Computing Surveys , 54:1 \u2013 38, 2020.\n[73]Hyunjong Park, Jongyoun Noh, and Bumsub Ham. Learning memory-guided normality for anomaly\ndetection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,\npages 14372\u201314381, 2020.\n[74]Mahdyar Ravanbakhsh, Moin Nabi, Enver Sangineto, Lucio Marcenaro, Carlo Regazzoni, and Nicu\nSebe. Abnormal event detection in videos using generative adversarial nets. In IEEE International\nConference on Image Processing (ICIP) , pages 1577\u20131581, 2017.\n|55 [75]Tal Reiss, Niv Cohen, Liron Bergman, and Yedid Hoshen. Panda: Adapting pretrained features for\nanomaly detection and segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 2806\u20132814, 2021.\n[76]Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Sch\u00f6lkopf, Thomas Brox, and Peter Gehler.\nTowards total recall in industrial anomaly detection. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 14318\u201314328, 2022.\n[77]Marco Rudolph, Tom Wehrbein, Bodo Rosenhahn, and Bastian Wandt. Asymmetric student-teacher\nnetworks for industrial anomaly detection. In 2023 IEEE/CVF Winter Conference on Applications of\nComputer Vision (WACV) , pages 2591\u20132601, 2022.\n[78]Lukas Ruff, Jacob R. Kauffmann, Robert A. Vandermeulen, Gregoire Montavon, Wojciech Samek,\nMarius Kloft, Thomas G. Dietterich, and Klaus-Robert Muller. A unifying review of deep and shallow\nanomaly detection. Proceedings of the IEEE , 109(5):756\u2013795, 2021.\n[79]Mayu Sakurada and Takehisa Yairi. Anomaly detection using autoencoders with nonlinear dimensionality\nreduction. In Proceedings of the MLSDA 2014 2nd Workshop on Machine Learning for Sensory Data\nAnalysis, MLSDA\u201914, page 4\u201311, New York, NY, USA, 2014. Association for Computing Machinery.\n[80]Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi\nCherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open\nlarge-scale dataset for training next generation image-text models. Advances in Neural Information\nProcessing Systems , 35:25278\u201325294, 2022.\n[81]Aleksandar Shtedritski, Christian Rupprecht, and Andrea Vedaldi. What does clip know about a red\ncircle? visual prompt engineering for vlms. In Proceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV) , pages 11987\u201311997, October 2023.\n[82]Kihyuk Sohn, Chun-Liang Li, Jinsung Yoon, Minho Jin, and Tomas Pfister. Learning and evaluating\nrepresentations for deep one-class classification. In International Conference on Learning Representations ,\n2020.\n[83]Ya Su, Youjian Zhao, Chenhao Niu, Rong Liu, Wei Sun, and Dan Pei. Robust anomaly detection for\nmultivariate time series through stochastic recurrent neural network. In Proceedings of the 25th ACM\nSIGKDD international conference on knowledge discovery & data mining , pages 2828\u20132837, 2019.\n[84]Waqas Sultani, Chen Chen, and Mubarak Shah. Real-world anomaly detection in surveillance videos.\nIn2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2018.\n[85]Waqas Sultani, Chen Chen, and Mubarak Shah. Real-world anomaly detection in surveillance videos. In\nProceedings of the IEEE conference on computer vision and pattern recognition , pages 6479\u20136488, 2018.\n[86]Hanlin Tan, Yongping Zhai, Yu Liu, and Maojun Zhang. Fast anomaly detection in traffic surveillance\nvideo based on robust sparse optical flow. In IEEE international conference on acoustics, speech and\nsignal processing (ICASSP) , pages 1976\u20131980, 2016.\n[87]Yu Tian, Fengbei Liu, Guansong Pang, Yuanhong Chen, Yuyuan Liu, Johan W Verjans, Rajvinder\nSingh, and Gustavo Carneiro. Self-supervised pseudo multi-class pre-training for unsupervised anomaly\ndetection and segmentation in medical images. Medical Image Analysis , 90:102930, 2023.\n[88]Yu Tian, Guansong Pang, Fengbei Liu, Yuanhong Chen, Seon Ho Shin, Johan W Verjans, Rajvinder\nSingh, and Gustavo Carneiro. Constrained contrastive distribution learning for unsupervised anomaly\ndetection and localisation in medical images. In Medical Image Computing and Computer Assisted\nIntervention\u2013MICCAI 2021: 24th International Conference, Strasbourg, France, September 27\u2013October\n1, 2021, Proceedings, Part V 24 , pages 128\u2013140. Springer, 2021.\n[89]Stack Exchange User. Simple outlier detection for time series. https://stats.stackexchange.com/questions/\n427327/simple-outlier-detection-for-time-series , 2021. Accessed: 2023-11-04.\n[90]Shashanka Venkataramanan, Kuan-Chuan Peng, Rajat Vikram Singh, and Abhijit Mahalanobis. At-\ntention guided anomaly localization in images. In European Conference on Computer Vision , pages\n485\u2013503. Springer, 2020.\n[91]Qian Wan, Yunkang Cao, Liang Gao, Weiming Shen, and Xinyu Li. Position encoding enhanced feature\nmapping for image anomaly detection. In 2022 IEEE 18th International Conference on Automation\nScience and Engineering (CASE) , pages 876\u2013881. IEEE, 2022-08-20.\n|56 [92]Qian Wan, Liang Gao, and Xinyu Li. Logit inducing with abnormality capturing for semi-supervised\nimage anomaly detection. IEEE Transactions on Instrumentation and Measurement , 71:1\u201312, 2022.\n[93]Qian Wan, Liang Gao, Xinyu Li, and Long Wen. Industrial image anomaly localization based on\ngaussian clustering of pretrained feature. IEEE Transactions on Industrial Electronics , 69(6):6182\u20136192.\n[94]QianWan, LiangGao, XinyuLi, andLongWen. Unsupervisedimageanomalydetectionandsegmentation\nbased on pretrained feature mapping. 19(3):2330\u20132339, 2023-03.\n[95]Guodong Wang, Shumin Han, Errui Ding, and Di Huang. Student-teacher feature pyramid matching\nfor anomaly detection. In British Machine Vision Conference , 2021.\n[96]Yue Wang, Jinlong Peng, Jiangning Zhang, Ran Yi, Yabiao Wang, and Chengjie Wang. Multimodal\nindustrial anomaly detection via hybrid fusion. arXiv preprint arXiv:2303.00601 , 2023.\n[97]Chaoyi Wu, Jiayu Lei, Qiaoyu Zheng, Weike Zhao, Weixiong Lin, Xiaoman Zhang, Xiao Zhou, Ziheng\nZhao, Ya Zhang, Yanfeng Wang, and Weidi Xie. Can GPT-4v(ision) serve medical applications? case\nstudies on GPT-4v for multimodal medical diagnosis. arXiv preprint arXiv:2310.09909 , 2023.\n[98]Guoyang Xie, Jinbao Wang, Jiaqi Liu, Jiayi Lyu, Yong Liu, Chengjie Wang, Feng Zheng, and Yaochu\nJin. IM-IAD: Industrial image anomaly detection benchmark in manufacturing. arXiv preprint\narXiv:2301.13359 , 2023.\n[99]Guoyang Xie, Jingbao Wang, Jiaqi Liu, Feng Zheng, and Yaochu Jin. Pushing the limits of fewshot\nanomaly detection in industry vision: Graphcore. In International Conference on Learning Representa-\ntions, 2023.\n[100]Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting\nunleashes extraordinary visual grounding in GPT-4v. arXiv preprint, arXiv:2310.11441 , 2023.\n[101]Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang.\nThe dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421 ,\n2023.\n[102]Haiming Yao, Wei Luo, Wenyong Yu, Xiaotian Zhang, Zhenfeng Qiang, Donghao Luo, and Hui Shi.\nDual-attention transformer and discriminative flow for industrial visual anomaly detection. IEEE\nTransactions on Automation Science and Engineering , pages 1\u201315, 2023.\n[103]Haiming Yao, Wenyong Yu, Wei Luo, Zhenfeng Qiang, Donghao Luo, and Xiaotian Zhang. Learning\nglobal-local correspondence with semantic bottleneck for logical anomaly detection. IEEE Transactions\non Circuits and Systems for Video Technology , pages 1\u20131, 2023.\n[104]Mingze Yuan, Yingda Xia, Hexin Dong, Zifan Chen, Jiawen Yao, Mingyan Qiu, Ke Yan, Xiaoli Yin,\nYu Shi, Xin Chen, et al. Devil is in the queries: Advancing mask transformers for real-world medical\nimage segmentation and out-of-distribution localization. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 23879\u201323889, 2023.\n[105] Andrei Zaharescu and Richard Wildes. Anomalous behaviour detection using spatiotemporal oriented\nenergies, subset inclusion histogram comparison and event-driven processing. In European Conference\non Computer Vision , pages 563\u2013576. Springer, 2010.\n[106]J. Zhang, Masanori Suganuma, and Takayuki Okatani. Contextual affinity distillation for image anomaly\ndetection. arXiv preprint arXiv:2307.03101, , 2023.\n[107]Jianpeng Zhang, Yutong Xie, Yi Li, Chunhua Shen, and Yong Xia. Covid-19 screening on chest x-ray\nimages using deep learning based anomaly detection. arXiv preprint arXiv:2003.12338 , 2020.\n[108]Yuxin Zhang, Jindong Wang, Yiqiang Chen, Han Yu, and Tao Qin. Adaptive memory networks with\nself-supervised learning for unsupervised anomaly detection. IEEE Transactions on Knowledge and\nData Engineering , 2022.\n[109]Bin Zhao, Li Fei-Fei, and Eric P. Xing. Online detection of unusual events in videos via dynamic sparse\ncoding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages\n3313\u20133320, 2011.\n[110]Hang Zhao, Yujing Wang, Juanyong Duan, Congrui Huang, Defu Cao, Yunhai Tong, Bixiong Xu, Jing\nBai, Jie Tong, and Qi Zhang. Multivariate time-series anomaly detection via graph attention network.\nIn2020 IEEE International Conference on Data Mining (ICDM) , pages 841\u2013850. IEEE, 2020.\n|57 [111]Qiang Zhou, Weize Li, Lihan Jiang, Guoliang Wang, Guyue Zhou, Shanghang Zhang, and Hao Zhao.\nPad: A dataset and benchmark for pose-agnostic anomaly detection. arXiv preprint arXiv:2310.07716 ,\n2023.\n[112]Qihang Zhou, Guansong Pang, Yu Tian, Shibo He, and Jiming Chen. Anomalyclip: Object-agnostic\nprompt learning for zero-shot anomaly detection. arXiv preprint arXiv:2310.18961 , 2023.\n[113]Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing\nvision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592 ,\n2023.\n[114]David Zimmerer, Peter M. Full, Fabian Isensee, Paul J\u00e4ger, Tim Adler, Jens Petersen, Gregor K\u00f6hler,\nTobias Ross, Annika Reinke, Antanas Kascenas, Bj\u00f8rn Sand Jensen, Alison Q. O\u2019Neil, Jeremy Tan,\nBenjamin Hou, James Batten, Huaqi Qiu, Bernhard Kainz, Nina Shvetsova, Irina Fedulova, Dmitry V.\nDylov, Baolun Yu, Jianyang Zhai, Jingtao Hu, Runxuan Si, Sihang Zhou, Siqi Wang, Xinyang Li, Xuerun\nChen, Yang Zhao, Sergio Naval Marimont, Giacomo Tarroni, Victor Saase, Lena Maier-Hein, and Klaus\nMaier-Hein. Mood 2020: A public benchmark for out-of-distribution detection and localization on\nmedical images. IEEE Transactions on Medical Imaging , 41(10):2728\u20132738, 2022.\n[115]Bo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cristian Lumezanu, Daeki Cho, and Haifeng\nChen. Deep autoencoding gaussian mixture model for unsupervised anomaly detection. In International\nconference on learning representations , 2018.\n[116]Yang Zou, Jongheon Jeong, Latha Pemula, Dongqing Zhang, and Onkar Dabeer. Spot-the-difference self-\nsupervised pre-training for anomaly detection and segmentation. In European Conference on Computer\nVision, pages 392\u2013408. Springer, 2022.\n|58 "}