{"url": "http://arxiv.org/pdf/2401.02147v1", "title": "Exploring Boundary of GPT-4V on Marine Analysis: A Preliminary Case Study", "text": "EXPLORING BOUNDARY OF GPT-4V ONMARINE\nANALYSIS : A P RELIMINARY CASE STUDY\nZiqiang Zheng1, Yiwei Chen1, Jipeng Zhang1, Tuan-Anh Vu1, Huimin Zeng2, Yue Him Wong Tim3, Sai-Kit Yeung1\n1The Hong Kong University of Science and Technology,\n2University of Science and Technology of China,3Shenzhen University\n{zzhengaw,ychenmb,jzhanggr,tavu }@connect.ust.hk ,saikit@ust.hk\nABSTRACT\nLarge language models (LLMs) have demonstrated a powerful ability to answer\nvarious queries as a general-purpose assistant. The continuous multi-modal large\nlanguage models (MLLM) empower LLMs with the ability to perceive visual\nsignals. The launch of GPT-4 (Generative Pre-trained Transformers) has generated\nsignificant interest in the research communities. GPT-4V(ison) has demonstrated\nsignificant power in both academia and industry fields, as a focal point in a new\nartificial intelligence generation. Though significant success was achieved by\nGPT-4V , exploring MLLMs in domain-specific analysis ( e.g., marine analysis) that\nrequired domain-specific knowledge and expertise has gained less attention. In\nthis study, we carry out the preliminary and comprehensive case study of utilizing\nGPT-4V for marine analysis. This report conducts a systematic evaluation of\nexisting GPT-4V , assessing the performance of GPT-4V on marine research and\nalso setting a new standard for future developments in MLLMs. The experimental\nresults of GPT-4V show that the responses generated by GPT-4V are still far away\nfrom satisfying the domain-specific requirements of the marine professions. All\nimages and prompts used in this study will be available at https://github.com/hkust-\nvgd/Marine GPT-4V Eval\nCONTENTS\n1 Introduction 1\n2 Experiments 2\n2.1 Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n2.2 Perception . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2\n2.2.1 Marine object recognition . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.2.2 Fine-grained marine object recognition . . . . . . . . . . . . . . . . . . . 12\n2.2.3 Robustness Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.2.4 Physical World Knowledge Understanding . . . . . . . . . . . . . . . . . 18\n2.3 Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n2.3.1 Object counting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n2.3.2 Recognizing all the objects . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n2.4 Domain-specific Question-Answering . . . . . . . . . . . . . . . . . . . . . . . . 23\n2.5 Marine Cultural Understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n2.6 Advanced Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n2.6.1 Coral coverage estimation . . . . . . . . . . . . . . . . . . . . . . . . . . 31arXiv:2401.02147v1  [cs.CL]  4 Jan 2024 Work in progress\n2.6.2 Benthic Composition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n2.6.3 Relationship Summarization and Event Detection . . . . . . . . . . . . . . 34\n2.6.4 Framework and Flowchart Understanding . . . . . . . . . . . . . . . . . . 37\n2.6.5 Aesthetic evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n2.6.6 Temporal Sequence Understanding . . . . . . . . . . . . . . . . . . . . . 41\n2.7 Prompt Engineering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n3 Discussions and Future Directions 45\n3.1 Discussions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n3.2 Future Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n4 Conclusion 45 Work in progress\nLIST OF FIGURES\n1 Filename comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n2 Testing with random filenames . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n3 Testing with misleading filenames . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n4 Testing with meaningful filenames . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n5 Wide spectrum of marine object recognition . . . . . . . . . . . . . . . . . . . . . 9\n6 Marine object recognition under challenging conditions Case 1 . . . . . . . . . . . 10\n7 Marine object recognition under challenging conditions Case 2 . . . . . . . . . . . 11\n8 Fine-grained marine object recognition . . . . . . . . . . . . . . . . . . . . . . . . 13\n9 Pairwise comparing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n10 Cross-view fish re-identification . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n11 Robustness analysis Case 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n12 Robustness analysis Case 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n13 Robustness analysis Case 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n14 Physical world knowledge understanding . . . . . . . . . . . . . . . . . . . . . . 19\n15 Object counting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n16 Recognizing all the objects within images . . . . . . . . . . . . . . . . . . . . . . 22\n17 Marine multiple choice question answering . . . . . . . . . . . . . . . . . . . . . 23\n18 Domain-specific question answering . . . . . . . . . . . . . . . . . . . . . . . . . 24\n19 Understanding domain-specific figures and tables . . . . . . . . . . . . . . . . . . 25\n20 Multi-round conversation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n21 Marine logo understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n22 Artist image understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n23 Landmark recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n24 Coral coverage estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n25 Coral composition and coral bleaching detection . . . . . . . . . . . . . . . . . . . 32\n26 Benthic composition estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n27 Relationship summarization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n28 Event detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n29 Scientific figure understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n30 Illustration figure understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n31 Framework and flowchart understanding . . . . . . . . . . . . . . . . . . . . . . . 39\n32 Aesthetic evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n33 Temporal content understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n34 Prompt engineering Case 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n35 Prompt engineering Case 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n36 Prompt engineering Case 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 Work in progress\n1 I NTRODUCTION\nLarge language models (LLMs) (Raffel et al., 2020; Chiang et al., 2023; Zhang et al., 2022; Touvron\net al., 2023a;b; Ouyang et al., 2022; OpenAI, 2023; Brown et al., 2020; Scao et al., 2022) demonstrated\nan impressive ability to handle a large range of user-tailored tasks. As a general-purpose assistant,\nChatGPT/GPT-4 (OpenAI, 2023; Ouyang et al., 2022) could understand human intents and complete\nvarious real-world tasks. The development of multi-modal large language models (Li et al., 2023c;\nZhu et al., 2023; Zheng et al., 2023c; Peng et al., 2023a; Team et al., 2023; Alayrac et al., 2022)\n(MLLMs) such as GPT-4V represents an important step towards more sophisticated AI systems\nwith the ability to receive both textual inputs and visual data. The integration of vision in language\nmodels has marked a significant milestone. GPT-4V showcased impressive general-purpose visual\nunderstanding and reasoning abilities. The advent of GPT-4V has expanded AI applications, aligning\nwith the multi-modal capabilities of the human brain. In detail, GPT-4V extends the abilities of\nGPT-4 to analyze and interpret images and has attracted significant attention across both academia\nand industry.\nExisting open-source general-purpose MLLMs (Liu et al., 2023; Peng et al., 2023b; Li et al., 2023a)\noften lack in image-text analysis (Lu et al., 2022) due to limited model size and data scale. It is\nstill unclear how GPT-4V , and MLLMs built on GPT-4, perform various multimodal understanding\ntasks. Though vision capabilities embodied in GPT-4 have pioneered new avenues for advanced\nimage-text analysis, the challenges (Fu et al., 2023a; Singh et al., 2023) of evaluating how GPT-4V\naccurately perceives visual signals and measuring the effectiveness of such a system arise. To evaluate\nwhether GPT-4V could achieve robust visual perception and mimic the inherently subjective and\nassociative processes of human perception, recent studies (Yang et al., 2023; Zhang et al., 2023; Fu\net al., 2023b; Ge et al., 2023; Bubeck et al., 2023) have been conducted to evaluate the performance\nof GPT-4V in different areas, such as recommendation (Zhou et al., 2023), medical analysis (Li et al.,\n2023b), radiological (Busch et al., 2023), mathematic (Gao et al., 2023), and general-purpose visual\nanalysis tasks (Yang et al., 2023; Bubeck et al., 2023). Evaluating the performance of GPT-4V in\nthese areas will provide insights into the flexibility of GPT-4V as the AI assistant. However, there\nare few attempts (Palnitkar et al., 2023; Zheng et al., 2023c) to utilize GPT-4V for more advanced\nanalysis, which requires advanced and domain-specific knowledge and expertise.\nTo bridge this gap, we present a preliminary case study investigating the marine analysis based on\nGPT-4V . We explore whether GPT-4V could serve as an effective visual perception system and a\nprofessional expert for sensitive, informative, and accurate knowledge delivery. We construct a series\nof qualitative test samples spanning multiple purposes in the field of marine analysis and employ\nthese samples to assess the quality of the responses generated by GPT-4V .\nWe propose to evaluate the performance of GPT-4V on marine analysis from the following aspects:\nperception ,statistics ,domain-specific question answering ,marine culture understanding ,advanced\nfunctions andprompt engineering . We pick up images that are not accessible online or private data,\ncombined with manually crafted prompts to build the evaluation samples. Evaluation results on\nour constructed testing samples prove that GPT-4V has a remarkable OCR, event detection, and\nframework understanding ability across various conditions, due to its robust visual-text comprehension\ncapabilities and extensive knowledge. However, we have also observed the intrinsic limitations of\nusing GPT-4V for marine analysis. GPT-4V only demonstrates very limited fine-grained marine\nobject recognition ability and is easily misled by meticulously forged filenames (we observe that\nGPT-4V will read the filenames of uploaded images as context prompts). Besides, GPT-4V cannot\nperform complicated object counting and detect all the objects within the visual images since it is\nmainly performing image-level understanding. GPT-4V also failed to accurately capture subtle details\nin images and respond with the required domain-specific information. We finally demonstrate that\nGPT-4V cannot conduct advanced marine analysis as a professional analysis tool. We summarize our\nfindings as follows.\n\u2022In this study, we embark on an in-depth analysis of GPT-4V on domain-specific marine analysis.\nThe expert capacity of GPT-4V has been measured for applying the learned domain knowledge and\nskills to the professional domains. Our study holds significant importance for the marine research\ncommunity, providing valuable insights and guidance for future exploration of utilizing MLLMs\nfor domain-specific analysis.\n1 Work in progress\n\u2022We demonstrate several limitations of GPT-4V on marine analysis. Despite these limitations, we\nalso aim to include a list of potential abilities of GPT-4V that we have identified as a domain-\nspecific analysis tool. We hope that these explorations and our constructed domain-specific testing\nsamples can offer valuable insights and serve as domain-specific benchmark data for evaluating\nMLLMs on domains with professional knowledge.\n\u2022We also acknowledge GPT-4V could be easily misled by the wrong prompts ( e.g., the filenames\nof visual images), demonstrating GPT-4V leans towards the text prompts and without looking at\nthe visual elements within the images. The hallucination happens a lot when GPT-4V is asked to\nanswer domain-specific questions.\n2 E XPERIMENTS\n2.1 A PPROACH\nData construction . To avoid the testing sample leakage, all the samples involved in this study are\nfrom different sources: 1) private data collection contributed by marine biologists (Zheng et al.,\n2023a); 2) manually cropped frames from YouTube videos; 3) Internet images posted after the release\nof GPT-4V APIs; 4) framework and flowchart images from research articles and books (Haixin et al.,\n2023; Ziqiang et al., 2023); and 5) images from public datasets (Beijbom et al., 2015) and our newly\ncreated images. To promote the consistency and reliability of our study and increase the robustness\nof our findings, we make sure that every case has at least 10 testing samples with high diversity.\nPrompt design . GPT-4V has been demonstrated to support a diverse range of visual processing\nbased on various signed prompts (Wang et al., 2022; Peng et al., 2023a). This inspires us to design\nthe various prompts. Our prompts in this study are characterized by a rich diversity and complexity\nof instructions to enable GPT-4V to generate comprehensive and descriptive responses, which are\naligned with the user intents.\nEvaluation metric . In each testing case, we compute the accuracy of GPT-4V on a wide range of\nvisual tasks. For those object recognition tasks with ground truth labeled by the domain experts, we\nevaluate whether GPT-4V could yield satisfactory object recognition performance according to the\ngenerated labels. For those evaluation metrics with human judgment involved, we mainly design two\nprotocols (Zhang et al., 2023; Ge et al., 2023): pairwise comparison andimage-based scoring . For\npairwise comparison, we judge whether the two images come from the same identity or the same\nspecies. For pairwise scoring, we ask both GPT-4V and human labelers to generate scores on a scale\nof 1 to 10. The ground truths under the two protocols are both generated by human experts.\n2.2 P ERCEPTION\nIn this section, our goal is to assess the performance of GPT-4V in various challenging vision tasks.\nThe involved tasks demand a powerful visual perception ability to understand the real world. Our\nexperiments focus on the ability of GPT-4V to sense the visual contents and then perform image-level,\nobject-level and attribute-level comprehension.\nWe first explore whether GPT-4V could really understand the visual content of the given marine\nimages or just respond without looking at the visual signals. We perform experiments using the\nsame images under three settings: 1) with random filename; 2) with meticulously forged misleading\nfilename; and 3) with meaningful andaligned filename. The experimental results are illustrated in\nFigure 1. The filenames and the ground truths of the marine objects are also provided as references.\nAs illustrated, we observe that GPT-4V will recognize the marine objects within the given image\nunder the first setting since no side clues are provided. GPT-4V tends to describe all the appeared\nmeaningful objects and usually yields longer responses. Under the second setting, with the misleading\nfilename given, GPT-4V will respond according to the given file name and generate some \u201cfalse\npromise\u201d that does notappear in the image. GPT-4V could be easily deceived by the meticulously\nforged filenames and yield some wrong answers. We guess that GPT-4V would read the filename of\nthe uploaded image and regard such filename as the context prompt when generating the responses. It\nwill easily produce a hallucination if the wrong context prompts do not exist in the image. As for the\nfinal setting, when the correct and aligned filenames are given, GPT-4V could generate meaningful\nand satisfactory responses. However, we cannot claim that GPT-4V could really understand the visual\n2 Work in progress\nFigure 1: The marine object recognition results under three different settings: left column (with\nrandom filename); middle column (with meticulously forged misleading filename); and right column\n(with meaningful andaligned filename). The texts in red represent the wrong responses and texts in\ngreen indicate the correct responses. The prompts are \u201cRecognize the object in this figure\u201d.\n3 Work in progress\ncontents of uploaded images since abstracted conception names have already leaked in the filenames.\nMore inference results under the three settings are provided in Figure 2, Figure 3, and Figure 4,\nrespectively.\nConsidering the conception leakage issue, we rename all the images in all our experiments to\nmeaningless filenames to avoid information leakage and ensure fair testing .\n4 Work in progress\nFigure 2: The marine object recognition results under the setting with random filenames. The prompts\nare \u201cRecognize the object in this figure\u201d.\n5 Work in progress\nFigure 3: The marine object recognition results under the setting with meticulously forged misleading\nfilenames. The prompts are \u201cRecognize the object in this figure\u201d.\n6 Work in progress\nFigure 4: The marine object recognition results under the setting with meaningful andaligned\nfilenames. The prompts are \u201cRecognize the object in this figure\u201d.\n7 Work in progress\n2.2.1 M ARINE OBJECT RECOGNITION\nWide spectrum of marine object recognition . We first explore whether GPT-4V could recognize a\nwide range of marine objects. We pick up 300 different marine images that contain the salient visual\nelements from one single marine species. In other words, there are 300 different marine species\ninvolved in our experiments. These images are manually cropped from the Youtube videos or the\nMVK dataset (Truong et al., 2023; Zheng et al., 2023a) The ground truth of the appeared marine\nobjects is labeled by domain experts and we manually compared the recognized object names with\nthe ground truth for computing the recognition accuracy. Some marine object recognition results\nare provided in Figure 5. As illustrated, GPT-4V failed to accurately recognize marine objects that\nare not relatively common. There is still a very large room to improve the recognition accuracy of\nGPT-4V on marine object recognition.\nMarine object recognition under challenging conditions . We then test whether GPT-4V is capable\nof depicting the key visual elements under some challenging conditions, including crowded scene ,\nobjects with weird appearances ,fluffy object ,irregular boundary ,tiny object ,camouflaged object ,\nobject detection under occlusion ,low visibility , and optical artifacts . All the experimental results\nare reported in Figure 6 and Figure 7, respectively. For these testing experiments, we make sure\nthere are at least 10 images under each experimental setting. We compute the recognition accuracy\nunder those diverse settings. We observe that GPT-4V has a poor ability to accurately recognize the\nvisual elements under challenging conditions. We guess that such failure of GPT-4V may be subject\nto the minority training data from the marine field. More training data collected under challenging\nconditions should be further included to promote the recognition ability of GPT-4V in challenging\nconditions.\n8 Work in progress\nFigure 5: The marine object recognition results of recognizing a wide spectrum of marine objects.\nThe prompts are \u201cRecognize this image and tell me the species name of the recognized objects\u201d. The\nground truths are also provided.\n9 Work in progress\nFigure 6: The marine object recognition results under challenging conditions. The prompts are\n\u201cRecognize the object in this image and tell me the species name of the recognized objects\u201d. The\nground truths are also provided.10 Work in progress\nFigure 7: The marine object recognition results under challenging conditions. The prompts are\n\u201cRecognize the object in this image and tell me the species name of the recognized objects\u201d.11 Work in progress\n2.2.2 F INE-GRAINED MARINE OBJECT RECOGNITION\nWe test whether GPT-4V could discriminate very similar marine objects ( e.g., fine-grained object\nrecognition) and generate different responses based on given visual contents. We report the fine-\ngrained object recognition results of GPT-4V in Figure 8. As demonstrated, GPT-4V failed to tell\nthe differences of close-related marine objects with similar appearances. The fine-grained object\nrecognition ability is required in the marine analysis field since it could enable diversity monitoring\nand reduce the human labor from the domain experts on species identification. There is still a far\naway from utilizing GPT-4V for marine species identification.\nWe then perform the pairwise comparing, formulating a pair of images and asking GPT-4V whether\nthe objects within the two images belong to the same marine species. Figure 9 illustrates the pairwise\ncomparing performance. We formulate 20 pairs and compute the correct rate of GPT-4V on this task.\nCross-view fish re-identification . We have also performed experiments to ask the GPT-4V to judge\nwhether the objects within the images captured under different camera views ( e.g., frontal, bird and\nside views) are the same object. Figure 10 demonstrates that GPT-4V has a poor ability to retrieve\nobjects with camera view changes. GPT-4V refused to respond to the matching question even though\nthe two fishes from the two visual images share very different appearances.\n12 Work in progress\nFigure 8: The fine-grained marine object recognition results of GPT-4V . The prompts are \u201cRecognize\nthe object in this image and tell me the species name of the recognized object\u201d.\n13 Work in progress\nFigure 9: Utilize GPT-4V for pairwise comparing. The prompts are \u201cCompare whether the two fishes\nin the two images belong to the same species\u201d.\nFigure 10: The cross-view fish re-identification performance of GPT-4V . The prompts are \u201cCompare\nwhether the two fishes in the two images belong to the same species\u201d.\n14 Work in progress\n2.2.3 R OBUSTNESS ANALYSIS\nFigure 11: The marine object recognition of GPT-4V on 360\u25e6and fisheye images. The prompts are\n\u201cDescribe this figure in detail and recognize the object within this figure\u201d.\n15 Work in progress\nFigure 12: The marine object recognition of GPT-4V on sonar and lidar images. The prompts are\n\u201cDescribe this figure in detail and recognize the object within this figure\u201d.\n16 Work in progress\nIn this section, we test the robustness of GPT-4V in recognizing various formats of visual signals,\nsuch as the fisheye (Zheng et al., 2023b), 360\u25e6(Huang et al., 2023), sonar (Xie et al., 2022) and Lidar\nimages. Figure 11 illustrates the recognition results of GPT-4V on 360\u25e6and fisheye images. GPT-4V\ncould observe the distortion of 360\u25e6images but cannot explicitly explain why the distortion happens.\nIn most cases, it could accurately recognize the visual elements from the visual images, however,\nit seems to have hallucination on the components in the submarine images where the visibility is\nlow and images tend to be more murky, showing its limited robustness to fisheye and 360\u25e6images.\nWhat\u2019s more, it is an expert at recognizing how the images are captured through the edge or border of\nthe viewpoint. We report the further object recognition results of GPT-4V on sonar images and Lidar\nimages in Figure 12. GPT-4V can recognize the general shape of the existing objects but cannot\neffectively detect what kind of stuff they are in sonar images due to the appearance shift. But for Lidar\nimages in which objects\u2019 appearance doesn\u2019t shift a lot, GPT-4T can exactly describe the element in\ndetail, showing a very good understanding of the image.\nFigure 13: The marine object recognition results of GPT-4V on the images with highlighted regions.\nWe then identify whether GPT-4V could effectively recognize object regions with highlighted masks\nas demonstrated in Figure 13, exploring the referring comprehension ability of GPT-4V . The partial\nparts of the whole image are highlighted by purple and we ask GPT-4V to identify the highlighted\nregions. Furthermore, GPT-4V is asked to compute the cover of the highlighted coral regions. GPT-4V\ncould generate the Python codes to compute the cover statistics. However, GPT-4V would self-define\nthe RGB value range of \u201cpurple\u201d without explanation. However, such a definition could be wrong\nand cannot handle visual images with high complexity.\n17 Work in progress\n2.2.4 P HYSICAL WORLD KNOWLEDGE UNDERSTANDING\nWe finally explore whether GPT-4V could really understand the physical world knowledge, for\nexample, the spatial, size, color and texture attributes of the existing objects within the images. We\nexplore the capability of GPT-4V to apply common sense knowledge in understanding visual contents\nwithin images. We have investigated the models\u2019 ability to comprehend visual information via\nthe application of knowledge, which encompasses commonsense, subject knowledge, multicultural\ncustoms, and world knowledge. The results are illustrated in Figure 14. GPT-4V shows its strong\ncapability of understanding the physical world knowledge like spatial, size and texture attributes and\nit also has great robustness to the wrong knowledge that does not correspond with the image and\ncorrect it. Even if we provide it with some really misleading images with close view of a dolphin and\na far view of a blue whale, it could still correctly tell the real size of these objects.\n18 Work in progress\nFigure 14: GPT-4V could understand the physical world knowledge.\n19 Work in progress\n2.3 S TATISTICS\nIn this section, we aim to explore the ability of GPT-4V to perform visual statistics based on the\nvisual contents, such as object counting and summarizing all the appeared objects within images.\n2.3.1 O BJECT COUNTING\nWe perform object counting experiments under five settings: 1) fewer than 10 objects; 2) 10-20\nobjects; 3) 20-50 objects; 4) 50-100 objects and 5) more than 100 objects. All the qualitative results\nhave been reported in Figure 15. As demonstrated, GPT-4V only demonstrates a limited ability\nto count the existing objects within the images, especially if the objects are occluded together or\nthe objects are tiny. Meanwhile, since the GPT-4V directly yields the estimation results of objects\nwithout explicitly localizing the objects ( e.g., bounding box), the estimation results will likely be not\naccurate. Furthermore, we have also observed that GPT-4V tends to generate an exact number of\npresented objects within the images when there are few objects visible. In contrast, GPT-4V instead\nyields a rough number of the object counting results. To avoid potential mistakes, GPT-4V outputs a\nrange ( e.g., more than 100) for the estimated objects. In summary, the external object detection tools\nfor localizing the objects should be integrated to promote the object counting ability of MLLM.\n2.3.2 R ECOGNIZING ALL THE OBJECTS\nWe then explore the ability of GPT-4V to recognize all the existing objects within the given visual\nimages and list the corresponding names of all the recognized objects. Figure 16 demonstrates the\nrecognition results under the crowded and structured palette. The GPT-4V struggles to recognize\nall the objects within the images and only lists very few common object categories. Furthermore,\nwe observe that GPT-4V could summarize the implicit intention of such visual images and try to\nsummarize the relationships between the recognized objects. However, due to the large number of\nobjects, some less commonly known species, and the low image resolution, GPT-4V shows a very\nlimited performance on recognizing all the objects in one single image while it could still understand\nsome general information of the image, like title, colors and common features of objects. Similar to\nthe object counting task, GPT-4V tends to discard many objects within the images and only tries to\nrecognize some common objects easy to recognize to avoid making mistakes, but this also makes it\nhard for GPT-4V to recognize all the objects existing in the image.\n20 Work in progress\nFigure 15: The marine object counting results under different settings.\n21 Work in progress\nFigure 16: Utilize GPT-4V to recognize all the objects within the visual images.\n22 Work in progress\n2.4 D OMAIN -SPECIFIC QUESTION -ANSWERING\nwe examine the ability of GPT-4V to apply knowledge in the fields of marine to understand visual\nimages. We observe that GPT-4V possesses the relevant subject knowledge associated with the\nfollowing cases.\nFigure 17: The performance of GPT-4V on answering the marine multiple choice questions. The\nprompts are \u201canswer the question within this image\u201d. We observe that GPT-4V demonstrates a strong\nOCR ability.\nMultiple choice questions . We first explore the ability of GPT-4V to answer the marine multiple-\nchoice questions. We upload the manually written marine questions and corresponding choices to\nGPT-4V and ask GPT-4V to generate the answers in Figure 17. As demonstrated, GPT-4V has shown\na strong optical character recognition (OCR) ability to extract the correct text information from the\nuploaded images and a satisfactory promise for handling basic marine knowledge. We have manually\nconstructed 100 multiple-choice questions, which come from marine biology, oceanography, and\ngeology. The accuracy of GPT-4V is computed to quantitatively assess the quality of GPT-4V in\nanswering the domain-specific questions.\nDomain-specific VQA . We evaluate whether GPT-4V could understand the user intent of the domain\nexperts and the ability of GPT-4V for abstract visual reasoning and scientific problem-solving. Such\nabilities are required for marine researchers to analyze the data (figures and tables) collected to\ngain insights into various aspects of marine research fields. Results are reported in Figure 18 and\nFigure 19, respectively. As demonstrated in Figure 18, GPT-4V could understand most elements\nof the left scientific figure but make a tiny mistake about the temperature range. Besides, GPT-\n4V could understand the temporal changes within the scientific figure and conclude the implicit\nintention. It could accurately describe the coral status of each sub-figure and conclude the progression\n23 Work in progress\nchanges. We have also included more visual scientific examples essential for handling marine biology,\nengineering, oceanography, and etc.\nFigure 18: The performance of GPT-4V on answering domain-specific questions.\nFurthermore, we feed GPT-4V with scientific figures and tables from the field of marine engineering\nas reported in Figure 19. GPT-4V could effectively understand the flowchart. GPT-4V could describe\nthe logic inside of the flowchart and respond with more reasoning details. GPT-4V could also\nunderstand the tables in detail. When being asked a question that requires intermediate reasoning\nprocedures, GPT-4V could answer correctly with detailed reasoning procedures. However, GPT-4V\nstill has difficulties in providing a precise answer in some cases, which is mainly constrained by the\nunsatisfactory OCR accuracy in Figure 19.\nMulti-round conversation . We finally assess the ability of GPT-4V to support multi-round conversa-\ntions. Users could ask different questions for comprehensive analysis, as demonstrated in Figure 20.\nOur study suggests that GPT-4V , could generate corresponding responses aligned with the user intent\nand cover the detailed information. However, GPT-4V struggles with the marine object recognition.\nWith the wrongly identified marine objects, GPT-4V leads to error accumulation, which suggests\nthat GPT-4V only responds based on the previously generated keywords (as the context prompt)\n24 Work in progress\nFigure 19: The performance of GPT-4V on understanding domain-specific figures and tables.\n25 Work in progress\nwithout looking at the visual contents. How to alleviate the hallucination of MLLMs is a valuable\nand important future research direction.\nFigure 20: The GPT-4V could support multi-round conversation, however, leads to error accumulation.\n26 Work in progress\n2.5 M ARINE CULTURAL UNDERSTANDING\nWe investigate the ability of GPT-4V to recognize logos, landmarks, artist images, and more in\nFigure 21, Figure 22, and Figure 22.\nIn Figure 21, GPT-4V could effectively recognize the globally known NOAA logo and yield a detailed\ndescription of the appearance of the logo. However, there is still a hallucination with the description\nof the NOAA logo. We guess the generated responses are from the training corpus of GPT-4V rather\nthan being aligned with the visual elements. As for the novel logos, GPT-4V could describe the\nappearance of the designed logos. The feature patterns of the logos are comprehensively described\nand GPT-4V could assess the artistic and literary representations of themes and species.\nWe then ask GPT-4V to perform marine artist image recognition and description as illustrated in\nFigure 22. GPT-4V could efficiently describe the visual elements of marine artist images. We\npresent the capacity of GPT-4V to depict the appearance of the cartoon images, paintings, and actual\nphotographs. GPT-4V demonstrates a strong ability to assess the aesthetic quality of visual images\nand describe the partial parts of each image.\nFinally, we report the landmark recognition performance of GPT-4V in Figure 23. GPT-4V can\nidentify the marine vestige and statures. The detailed appearances of recognized ruins are further\ndescribed in detail, demonstrating the strong ability of GPT-4V to perceive the visual images.\nHowever, GPT-4V cannot accurately discriminate the statures with irregular shapes and poses.\n27 Work in progress\nFigure 21: Utilize GPT-4V for marine logo understanding.\n28 Work in progress\nFigure 22: Utilize GPT-4V for marine artist image understanding.\n29 Work in progress\nFigure 23: Utilize GPT-4V for marine landmark recognition.\n30 Work in progress\n2.6 A DVANCED FUNCTIONS\nIn this section, we aim to explore the possibility of utilizing GPT-4V for some advanced and\ncomplicated functions in the marine research field, such as coral coverage estimation, benthic\ncomposition statistic, multi-modal reasoning, relationship summarization, and etc.\n2.6.1 C ORAL COVERAGE ESTIMATION\nCoral reefs are among the most biodiverse ecosystems on our planet and provide habitat for countless\nmarine species. Monitoring coral coverage allows researchers to assess the overall health and\ncondition of these ecosystems. In this section, we aim to explore the feasibility of utilizing GPT-4V\nfor coral coverage estimation. Figure 24 represents some preliminary results of coral coverage\nestimation. GPT-4V avoids directly outputting the coral coverage and instead attempts to generate\nsome computer vision processing codes for coral coverage estimation. The generated coral coverage\nis far away from the real ground truth. Besides, GPT-4V may lead to the ignorance of the tiny corals\nor the minority coral types and then result in wrong policy making.\nFigure 24: Utilize GPT-4V for coral coverage estimation.\nWe then examine the ability of GPT-4V to discriminate the coral reef composition from the visual\nimages in Figure 25. GPT-4V could accurately recognize the coral reefs and missed the brain coral\nreefs. Moreover, we have also explored the ability of GPT-4V to understand the coral bleaching,\nwhich is linked to warming seas, can lead to declines in coral coverage. When being asked whether\nthe coral reefs are bleached, GPT-4V has made a wrong judgment. GPT-4V cannot understand the\nmeaning of \u201cbleaching\u201d and describes the degree of coral bleaching due to the lack of a reference\ncolor bar.\n31 Work in progress\nFigure 25: Utilize GPT-4V for coral composition estimation and coral bleaching detection.\n2.6.2 B ENTHIC COMPOSITION\nUnderstanding the benthic composition from the captured visual images could help researchers\ncharacterize and classify marine ecosystems based on the types of organisms and substrate present.\nDifferent benthic communities support distinct sets of species and play unique ecological roles. We\nexplore the potential of utilizing GPT-4V to generate the benthic analysis data, which could be further\nused for monitoring the impact of factors like pollution, climate change, and habitat destruction. The\nresults are illustrated in Figure 26. We first ask GPT-4V to generate the benthic composition data (the\ncomposition of non-creatures and creatures) from the uploaded visual image and then identify how\nmany types of coral reefs. Furthermore, we examine the ability of GPT-4V for benthic invertebrate\nidentification ( e.g., corals, sponges, mollusks, and worms), algae, and even certain fish species.\nOur experimental results show that GPT-4V nearly cannot achieve benthic composition statistics\nwithout utilizing an external professional analysis tool or being fed corresponding analysis data\nfor final report generation. Even though GPT-4V could generate some very naive computer vision\nprocessing codes for analysis, the analyzed outputs are still very far from the requirement of a\nprofessional expert. Meanwhile, the whole processing and analysis procedure lacks the reasoning\nsteps and support of the domain-specific evidence.\n32 Work in progress\nFigure 26: Utilize GPT-4V for the benthic composition estimation.\n33 Work in progress\n2.6.3 R ELATIONSHIP SUMMARIZATION AND EVENT DETECTION\nFigure 27: Utilize GPT-4V for relationship summarization from visual images. The prompts are\n\u201cSummarize the relationship between the objects within this figure\u201d.\nRelationship summarization . Exploring the relationships between marine creatures allows conserva-\ntionists to make informed decisions about protecting vulnerable or endangered species. In this section,\nwe assess the ability of GPT-4V to comprehend how different creatures interact and summarize\n34 Work in progress\nFigure 28: Utilize GPT-4V for event detection. The prompts are \u201cDescribe the event in this figure\u201d.\n35 Work in progress\nthe relationship between them, such as predator-prey relationships, symbiosis, competition, and\nmutualism. Such summarized marine relationships could gain insights into the behavior, evolution,\nand adaptation of species. It is worth noting that we mainly focus on the relationship summarization\nfrom the perspective of marine biology research. The qualitative results are reported in Figure 27. As\ndemonstrated, GPT-4V has shown a satisfactory ability to understand and describe some well-known\nrelationships between recognized objects, such as the symbiotic relationship between clownfish and\nthe sea anemone. But in contrast, when GPT-4V fails to recognize the marine objects accurately, it\nwill generate totally irrelative responses, and the responses are nearly based on its \u201cimagination\u201d.\nEvent detection . Through event detection, domain experts could predict and mitigate the impacts of\nevents like climate change and pollution. Some preliminary case studies about event detection are\nillustrated in Figure 28. We collect more samples about 1) identifying irregular behaviors, such as\nillegal fishing, vessel collisions, or suspicious activities, which can be crucial for maritime safety and\nsecurity; 2) monitoring the changes of marine conditions, such as water levels, wave patterns, and\ncoastal erosion; and 3) detecting abnormal events in marine images, which can help identify unusual\nevents such as oil spills, coral bleaching, and marine pollution. Detecting these abnormalities early\nallows for a rapid response to mitigate environmental damage and protect marine ecosystems. The\nexcitement of unveiling the unknown serves as a powerful motivator for researchers and explorers.\nFrom the early exploration as demonstrated in Figure 28, GPT-4V possesses a strong ability to\nunderstand the event presented in the visual images.\n36 Work in progress\n2.6.4 F RAMEWORK AND FLOWCHART UNDERSTANDING\nWe test whether GPT-4V showcases some detailed reasoning procedures and the ability to understand\nthe inside intention of the designed images, including the framework and flow chart images. GPT-\n4V is required to explain the whole framework step by step and describe the intermediate step in\ndetail. We provide visual reasoning results of GPT-4V from various fields in Figure 29 (scientific\nfigure understanding), Figure 30 (implicit intention understanding), and Figure 31 (the framework\nunderstanding), respectively. Our exploration targets how GPT-4V understands and reasons for the\nhigh-level information from the figures as a whole.\nFigure 29: Utilize GPT-4V for scientific figure understanding in the marine research field.\nAs shown in Figure 29, GPT-4V has demonstrated a very strong OCR ability to extract text infor-\nmation from visual images. It could summarize the hierarchical relationship between different parts\n37 Work in progress\nFigure 30: Utilize GPT-4V for illustration figure understanding. GPT-4V could understand the\nimplicit intention.\n38 Work in progress\nFigure 31: Utilize GPT-4V for framework and flowchart understanding. GPT-4V could explain the\nintermediate procedures step by step and explain the whole framework.\n39 Work in progress\nand extract the key elements of the whole figure. Besides, GPT-4V can understand the structure\ninformation and guess the source and usage of the uploaded scientific images.\nFurthermore, we observe that GPT-4V could understand the motivation of the illustration figures\nas demonstrated in Figure 30. It could accurately describe the inside motivation of drawn figures.\nHowever, we have also observed the hallucination of GPT-4V . It will generate some information\nthat does not exist within the image based on some extracted keywords ( e.g., \u201cDA VIS-2017\u201d). We\nattribute this phenomenon to the reason that GPT-4V may overfit its training data. How to prevent\nsuch hallucinations and alleviate the over-claim of GPT-4V is an important and valuable research\ndirection.\nFinally, we explore the ability of GPT-4V to understand and explain the framework or flowchart step\nby step in Figure 31. GPT-4V could accurately describe each part of the whole framework in detail\nand summarize the relationship between each part. Also, it demonstrates a satisfactory performance\nto understand the overall intention of the whole framework.\n2.6.5 A ESTHETIC EVALUATION\nFigure 32: Utilize GPT-4V for the aesthetic quality estimation. GPT-4V could explain the criteria of\nits assessments.\nWe have also assessed the ability of GPT-4V to do the aesthetic evaluation. We manually constructed\n50 marine images with high diversity then we uploaded the visual images to GPT-4V to generate\nthe aesthetic score (scale of 10) based on the visual contents. To quantitatively evaluate the ability\n40 Work in progress\nof GPT-4V for aesthetic assessment, we ask expert-level human labelers (3 annotators) to give the\nsubjective scores towards the given marine images and we compute the mean value and the standard\ndeviation. Then we first evaluate the alignment between the scores from GPT-4V and human labelers\nin terms of aesthetic measuring. We provide some qualitative results of GPT-4V in Figure 32. We\nobserve that the scores generated by GPT-4V are highly correlated with human rating. GPT-4V\nsuccessfully identifies the aesthetic quality of visual elements within the images and provides a\ncomprehensive explanation for its scores. Our results reveal that GPT-4V achieves a promising\nagreement with humans on aesthetic quality assessment.\n2.6.6 T EMPORAL SEQUENCE UNDERSTANDING\nFigure 33: Utilize GPT-4V for temporal content understanding from the video sequence.\nWe finally explore the potential ability of GPT-4V for temporal sequence understanding. Given the\nconsecutive image frames sampled from the video sequence ( e.g., uniformly sampling 8 frames),\nwe concatenate the sampled frames to one image and then ask GPT-4V to summarize the event that\nhappened in the given video sequence. The temporal sequence understanding requires the MLLMs\nto fully comprehend the information within the visual sequence. Understanding the event of a\nmarine clip could be very valuable for detecting the abnormal behavior of marine creatures and then\npreventing the potential disaster. The results are illustrated in Figure 33. As illustrated in Figure,\nGPT-4V demonstrates the capability to recognize the action in the images and provide a detailed\ndescription. It has shown a promising potential to understand scenes from video and visual story\ngeneration.\n41 Work in progress\n2.7 P ROMPT ENGINEERING\nIn this section, we aim to explore the effectiveness of introducing the current prompt engineering\ntechniques designed for general-purpose MLLMs for marine research. We mainly focus on three\nsettings: 1) few-shot prompts; 2) self-consistency and 3) chain-of-thoughts.\nUnder the first setting, we feed the GPT-4V with few-shot samples with corresponding annotations\nto guide GPT-4V as a domain expert and help it better understand our questions. Then we ask the\nGPT-4V for a similar question as shown in Figure 34. We observe that GPT-4V will still make\nmistakes and generate wrong responses even the few-shot prompts provided. We attribute this failure\nto the limited visual perception ability of GPT-4V . GPT-4V cannot effectively perform fine-grained\nobject recognition.\nFigure 34: The marine object recognition results of GPT-4V with few-shot prompts provided.\nTo explore the self-consistency of the GPT-4V , we ask the GPT-4V to do the object counting task\nbased on various prompts and we then perform voting to get the final object count result. Through\n42 Work in progress\nthis, we aim to measure the self-consistency of GPT-4V for the same visual input and the robustness\nof its generated responses. Through voting or feeding GPT-4V with clearer prompts, GPT-4V could\ngenerate more reliable and accurate object counting results as demonstrated in Figure 35.\nFigure 35: The self-consistency analysis of GPT-4V . Through voting, GPT-4V could generate more\nreliable responses.\nFinally, we refer to the design of the chain-of-thoughts Yang et al. (2023) and add some simple\nexplanations in our input prompts. The GPT-4V is asked to follow our explanation procedure and\nunderstand the reasoning inside the recognition. In this way, GPT-4V could describe more about its\njudgment and illustrate more supporting evidence. The results are reported in Figure 36. We observe\nthat GPT-4V sucks the ability to accurately recognize marine objects even GPT-4V could generate\nplausible and detailed descriptions about the wrongly recognized object.\nTo sum up, the current prompt engineering techniques cannot heavily promote the visual recognition\nability of GPT-4V on marine images. GPT-4V will still make mistakes for fine-grained marine object\nrecognition and prompt engineering cannot alleviate the hallucination issue, effectively. To address\nthese issues, more training data from the marine field should be included for further promoting the\nrecognition ability of GPT-4V .\n43 Work in progress\nFigure 36: We prompt GPT-4V with detailed reasoning procedure and ask GPT-4V to explain its\nidentification procedure.\n44 Work in progress\n3 D ISCUSSIONS AND FUTURE DIRECTIONS\n3.1 D ISCUSSIONS\nPossible for educational tool? While the performance of the GPT-4V is promising, we ask whether\nGPT-4V could be viewed as a potential educational tool that may in the future augment, but not\nreplace, the nuanced analysis provided by trained marine professionals. GPT-4V could also play as\na pivotal role in fostering a deeper understanding and appreciation for marine life among users of\nall ages and backgrounds. Through our findings in this study, we conclude that GPT-4V is far from\ngenerating valuable insights for domain experts.\nPossible for labeling tool? With easy access to GPT-4V , it could actively encourage citizen science\nparticipation as a labeling tool, transforming ordinary individuals into valuable contributors to\nmarine research. From our findings, we observe that GPT-4V cannot serve as a labeling tool for a\nwide spectrum of marine images since GPT-4V still makes many mistakes for challenging images.\nMoreover, such labeling is also only limited to image-level scene understanding. GPT-4V cannot\ngenerate accurate descriptions for the very fine-grained details.\nSample Bias . In our study, the testing samples are manually constructed, inevitably incorporating\nindividual preferences and subjectivity. More importantly, our involved testing samples may not\ncomprehensively represent real-world cases, and potentially over-estimate or down-estimate the\nchallenges of utilizing GPT-4V for marine analysis.\n3.2 F UTURE WORKS\nOur findings emphasize the need for continued research to enhance the accuracy and expertise of\nresponses generated by GPT-4V . We hope that this study can inspire more comprehensive and targeted\nresearch into utilizing multimodal systems such as GPT-4V for domain-specific research and analysis.\nBy harnessing the capabilities of these models, we can better meet the professional demands of\nexperts, ultimately including the domain experts in the major users of GPT-4V . Furthermore, based\non the feedback and further prompts from the domain experts, a fundamental question arises, could\nGPT-4V revise its responses over time? Such feedback-driven MLLM would further promote the\nuser experience for obtaining more precise responses.\nThrough our experimental results, we have observed that GPT-4V cannot achieve fine-grained and\naccurate marine object recognition to satisfy the requirements of the domain experts. More training\ndata from the marine field should be included to promote the visual recognition ability of GPT-4V .\nFurthermore, we also demonstrate that GPT-4V has shown a very limited ability to handle advanced\nmarine analysis ( e.g., counting, coverage estimation, composition statistic, etc) without utilizing an\nexternal professional tool. More domain-specific instruction-following data should be constructed to\nhelp GPT-4V yield explicit intermediate analysis results.\n4 C ONCLUSION\nIn this paper, our investigation of GPT-4V on marine analysis demonstrates some valuable findings\nand insights of MLLMs concerning visual understanding, logical reasoning, and expert capacity,\nindicating that there remains a considerable distance toward strong artificial intelligence as a domain\nexpert.\n45 Work in progress\nREFERENCES\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language\nmodel for few-shot learning. Advances inNeural Information Processing Systems , 35:23716\u2013\n23736, 2022.\nOscar Beijbom, Peter J Edmunds, Chris Roelfsema, Jennifer Smith, David I Kline, Benjamin P Neal,\nMatthew J Dunlap, Vincent Moriarty, Tung-Yung Fan, Chih-Jui Tan, et al. Towards automated\nannotation of benthic survey images: Variability of human experts and operational modes of\nautomation. PloS one, 10(7):e0130312, 2015.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances inneural information processing systems, 33:1877\u20131901, 2020.\nS\u00b4ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence:\nEarly experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.\nFelix Busch, Tianyu Han, Marcus Makowski, Daniel Truhn, Keno Bressem, and Lisa Adams.\nFrom text to image: Exploring gpt-4vision\u2019s potential in advanced radiological analysis across\nsubspecialties. arXiv preprint arXiv:2311.14777, 2023.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot impressing\ngpt-4 with 90%* chatgpt quality. Seehttps://vicuna. lmsys. org(accessed 14April 2023), 2023.\nChaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin,\nJinrui Yang, Xiawu Zheng, et al. Mme: A comprehensive evaluation benchmark for multimodal\nlarge language models. arXiv preprint arXiv:2306.13394, 2023a.\nChaoyou Fu, Renrui Zhang, Haojia Lin, Zihan Wang, Timin Gao, Yongdong Luo, Yubo Huang,\nZhengye Zhang, Longtian Qiu, Gaoxiang Ye, et al. A challenger to gpt-4v? early explorations of\ngemini in visual expertise. arXiv preprint arXiv:2312.12436, 2023b.\nJiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong,\nJianhua Han, Hang Xu, Zhenguo Li, et al. G-llava: Solving geometric problem with multi-modal\nlarge language model. arXiv preprint arXiv:2312.11370, 2023.\nWentao Ge, Shunian Chen, Guiming Chen, Junying Chen, Zhihong Chen, Shuo Yan, Chenghao Zhu,\nZiyue Lin, Wenya Xie, Xidong Wang, et al. Mllm-bench, evaluating multi-modal llms using gpt-4v.\narXiv preprint arXiv:2311.13951, 2023.\nLiang Haixin, Zheng Ziqiang, Ma Zeyu, and Sai-Kit Yeung. Marinedet: Towards open-marine object\ndetection. arXiv preprint arXiv:2310.01931, 2023.\nHuajian Huang, Yinzhe Xu, Yingshu Chen, and Sai-Kit Yeung. 360vot: A new benchmark dataset for\nomnidirectional visual object tracking. In Proceedings oftheIEEE/CVF International Conference\nonComputer Vision, pp. 20566\u201320576, 2023.\nBo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A\nmulti-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023a.\nChunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan\nNaumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training a large language-and-vision\nassistant for biomedicine in one day. arXiv preprint arXiv:2306.00890, 2023b.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597 ,\n2023c.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv\npreprint arXiv:2304.08485, 2023.\n46 Work in progress\nPan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord,\nPeter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for\nscience question answering. Advances inNeural Information Processing Systems , 35:2507\u20132521,\n2022.\nOpenAI. Gpt-4 technical report, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. Advances inNeural Information Processing Systems , 35:\n27730\u201327744, 2022.\nAadi Palnitkar, Rashmi Kapu, Xiaomin Lin, Cheng Liu, Nare Karapetyan, and Yiannis Aloi-\nmonos. Chatsim: Underwater simulation with natural language prompting. arXiv preprint\narXiv:2308.04029, 2023.\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with\ngpt-4. arXiv preprint arXiv:2304.03277, 2023a.\nZhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu\nWei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint\narXiv:2306.14824, 2023b.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. TheJournal ofMachine Learning Research, 21(1):5485\u20135551, 2020.\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman\nCastagn \u00b4e, Alexandra Sasha Luccioni, Fran c \u00b8ois Yvon, Matthias Gall \u00b4e, Jonathan Tow, Alexander M.\nRush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Beno \u02c6\u0131t\nSagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas\nBekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan,\nPedro Ortiz Suarez, Victor Sanh, Hugo Lauren c \u00b8on, Yacine Jernite, Julien Launay, Margaret\nMitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy,\nAnna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher\nKlamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, and et al. BLOOM: A 176b-\nparameter open-access multilingual language model. CoRR , abs/2211.05100, 2022. doi: 10.48550/\narXiv.2211.05100. URL https://doi.org/10.48550/arXiv.2211.05100 .\nMukul Singh, Jos \u00b4e Cambronero, Sumit Gulwani, Vu Le, and Gust Verbruggen. Assessing gpt4-v on\nstructured reasoning tasks. arXiv preprint arXiv:2312.11524, 2023.\nGemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu\nSoricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable\nmultimodal models. arXiv preprint arXiv:2312.11805, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth \u00b4ee\nLacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aur \u00b4elien Rodriguez, Armand\nJoulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language\nmodels. CoRR , abs/2302.13971, 2023a. doi: 10.48550/arXiv.2302.13971. URL https://doi.\norg/10.48550/arXiv.2302.13971 .\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\nand fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.\nQuang-Trung Truong, Tuan-Anh Vu, Tan-Sang Ha, Jakub Loko \u02c7c, Yue-Him Wong, Ajay Joneja, and\nSai-Kit Yeung. Marine video kit: a new marine video dataset for content-based analysis and\nretrieval. In International Conference onMultimedia Modeling, pp. 539\u2013550. Springer, 2023.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and\nHannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions.\narXiv preprint arXiv:2212.10560, 2022.\n47 Work in progress\nKaibing Xie, Jian Yang, and Kang Qiu. A dataset with multibeam forward-looking sonar for\nunderwater object detection. Scientific Data, 9(1):739, 2022.\nZhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Li-\njuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint\narXiv:2309.17421, 9(1), 2023.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt\nShuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer.\nOPT: open pre-trained transformer language models. CoRR , abs/2205.01068, 2022. doi: 10.48550/\narXiv.2205.01068. URL https://doi.org/10.48550/arXiv.2205.01068 .\nXinlu Zhang, Yujie Lu, Weizhi Wang, An Yan, Jun Yan, Lianke Qin, Heng Wang, Xifeng Yan,\nWilliam Yang Wang, and Linda Ruth Petzold. Gpt-4v (ision) as a generalist evaluator for vision-\nlanguage tasks. arXiv preprint arXiv:2311.01361, 2023.\nZiqiang Zheng, Tan-Sang Ha, Yingshu Chen, Haixin Liang, Apple Pui-Yi Chui, Yue-Him Wong,\nand Sai-Kit Yeung. Marine video cloud: A cloud-based video analytics platform for collaborative\nmarine research. In OCEANS 2023-Limerick, pp. 1\u20136. IEEE, 2023a.\nZiqiang Zheng, Zhichao Xin, Zhibin Yu, and Sai-Kit Yeung. Real-time gan-based image enhancement\nfor robust underwater monocular slam. Frontiers inMarine Science, 2023b.\nZiqiang Zheng, Jipeng Zhang, Tuan-Anh Vu, Shizhe Diao, Yue Him Wong Tim, and Sai-Kit Yeung.\nMarinegpt: Unlocking secrets of ocean to the public. arXiv preprint arXiv:2310.13596, 2023c.\nPeilin Zhou, Meng Cao, You-Liang Huang, Qichen Ye, Peiyan Zhang, Junling Liu, Yueqi Xie, Yining\nHua, and Jaeboum Kim. Exploring recommendation capabilities of gpt-4v (ision): A preliminary\ncase study. arXiv preprint arXiv:2311.04199, 2023.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En-\nhancing vision-language understanding with advanced large language models. arXiv preprint\narXiv:2304.10592, 2023.\nZheng Ziqiang, Xie Yaofeng, Liang Haixin, Yu Zhibin, and Sai-Kit Yeung. Coralvos: Dataset and\nbenchmark for coral video segmentation. arXiv preprint arXiv:2310.01946, 2023.\n48 "}