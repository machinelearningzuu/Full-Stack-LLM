{"url": "http://arxiv.org/pdf/2309.17421v2", "title": "The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)", "text": "The Dawn of LMMs:\nPreliminary Explorations with GPT-4V(ision)\nZhengyuan Yang\u2217, Linjie Li\u2217, Kevin Lin\u2217, Jianfeng Wang\u2217, Chung-Ching Lin\u2217,\nZicheng Liu, Lijuan Wang\u2217\u2660\nMicrosoft Corporation\n\u2217Core Contributor\u2660Project Lead\nAbstract\nLarge multimodal models (LMMs) extend large language models (LLMs) with\nmulti-sensory skills, such as visual understanding, to achieve stronger generic in-\ntelligence. In this paper, we analyze the latest model, GPT-4V(ision) [ 99\u2013101,1]1,\nto deepen the understanding of LMMs. The analysis focuses on the intriguing\ntasks that GPT-4V can perform, containing test samples to probe the quality and\ngenericity of GPT-4V\u2019s capabilities, its supported inputs and working modes, and\nthe effective ways to prompt the model. In our approach to exploring GPT-4V,\nwe curate and organize a collection of carefully designed qualitative samples\nspanning a variety of domains and tasks. Observations from these samples demon-\nstrate that GPT-4V\u2019s unprecedented ability in processing arbitrarily interleaved\nmultimodal inputs and the genericity of its capabilities together make GPT-4V a\npowerful multimodal generalist system. Furthermore, GPT-4V\u2019s unique capability\nof understanding visual markers drawn on input images can give rise to new human-\ncomputer interaction methods such as visual referring prompting. We conclude the\nreport with in-depth discussions on the emerging application scenarios and the fu-\nture research directions for GPT-4V-based systems. We hope that this preliminary\nexploration will inspire future research on the next-generation multimodal task\nformulation, new ways to exploit and enhance LMMs to solve real-world problems,\nand gaining better understanding of multimodal foundation models. Finally, we\nacknowledge that the model under our study is solely the product of OpenAI\u2019s\ninnovative work, and they should be fully credited for its development. Please\nsee the GPT-4V contributions paper [ 101] for the authorship and credit attribution:\nhttps://cdn.openai.com/contributions/gpt-4v.pdf .\nContents\nList of Figures 4\n1 Introduction 8\n1.1 Motivation and Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n1.2 Our Approach in Exploring GPT-4V . . . . . . . . . . . . . . . . . . . . . . . . . 8\n1.3 How to Read this Report? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n1This report explores GPT-4V(ision) with the vision capability and refers to the model as \u201cGPT-4V,\u201d following\nthe OpenAI reports [100, 99]. We refer to the text-only version of the model as \u201cGPT-4 (no vision)\u201d [99].arXiv:2309.17421v2  [cs.CV]  11 Oct 2023 2 GPT-4V\u2019s Input Modes 10\n2.1 Text-only Inputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.2 Single Image-text Pair . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.3 Interleaved Image-text Inputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n3 GPT-4V\u2019s Working Modes and Prompting Techniques 12\n3.1 Following Text Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n3.2 Visual Pointing and Visual Referring Prompting . . . . . . . . . . . . . . . . . . . 15\n3.3 Visual + Text Prompting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n3.4 In-context Few-shot Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n4 Vision-Language Capability 26\n4.1 Image Description on Diverse Domains . . . . . . . . . . . . . . . . . . . . . . . 26\n4.2 Object Localization, Counting, and Dense Captioning . . . . . . . . . . . . . . . . 38\n4.3 Multimodal Knowledge and Commonsense . . . . . . . . . . . . . . . . . . . . . 43\n4.4 Scene Text, Table, Chart, and Document Reasoning . . . . . . . . . . . . . . . . . 49\n4.5 Multilingual Multimodal Understanding . . . . . . . . . . . . . . . . . . . . . . . 58\n4.6 Coding Capability with Vision . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n5 Interaction with Humans: Visual Referring Prompting 66\n5.1 Understand Pointing Inputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n5.2 Visual Referring Prompting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n5.3 Generate Pointing Outputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n6 Temporal and Video Understanding 73\n6.1 Multi-image Sequencing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n6.2 Video Understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n6.3 Visual Referring Prompting for Grounded Temporal Understanding . . . . . . . . . 79\n7 Abstract Visual Reasoning and Intelligence Quotient Test 80\n7.1 Abstract Visual Stimuli . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n7.2 Discovery and Association of Parts and Objects . . . . . . . . . . . . . . . . . . . 80\n7.3 Wechsler Adult Intelligence Scale . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n7.4 Raven\u2019s Progressive Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n8 Emotional Quotient Test 86\n8.1 Read Emotion from Facial Expressions . . . . . . . . . . . . . . . . . . . . . . . 86\n8.2 Understand How Visual Content Arouses Emotions . . . . . . . . . . . . . . . . . 87\n8.3 Emotion Conditioned Output . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89\n9 Emerging Application Highlights 90\n9.1 Spot the Difference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90\n2 9.2 Industry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90\n9.3 Medical . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n9.4 Auto Insurance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104\n9.5 Customized Captioner . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\n9.6 Image Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n9.7 Embodied Agent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116\n9.8 GUI Navigation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\n10 LMM Powered Agents 150\n10.1 Multimodal Plugins . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n10.2 Multimodal Chains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n10.3 Self-Reflection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n10.4 Self-Consistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155\n10.5 Retrieval-Augmented LMMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156\n11 Conclusions 156\n11.1 Summary and Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156\n11.2 Towards Future LMMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156\n3 List of Figures\n1 Section 2.3: multi-image inputs and interleaved image-text pairs. . . . . . . . . . . 11\n2 Section 3.1: following text instructions. . . . . . . . . . . . . . . . . . . . . . . . 12\n3 Section 3.1: constrained prompting. . . . . . . . . . . . . . . . . . . . . . . . . . 13\n4 Section 3.1: condition on good performance. . . . . . . . . . . . . . . . . . . . . . 14\n5 Section 3.2: different modes of visual pointing. . . . . . . . . . . . . . . . . . . . 16\n6 Section 3.2: visual pointing and visual referring prompting. . . . . . . . . . . . . . 17\n7 Section 3.3: multimodal example-grounded instruction. . . . . . . . . . . . . . . . 18\n8 Section 3.4: few-shot comparison example 1, Part I: zero-shot. . . . . . . . . . . . 20\n9 Section 3.4: few-shot comparison example 1, Part II: one-shot. . . . . . . . . . . . 21\n10 Section 3.4: few-shot comparison example 1, Part III: two-shot. . . . . . . . . . . 22\n11 Section 3.4: few-shot comparison example 2, Part I: zero-shot. . . . . . . . . . . . 23\n12 Section 3.4: few-shot comparison example 2, Part II: one-shot. . . . . . . . . . . . 24\n13 Section 3.4: few-shot comparison example 2, Part III: two-shot. . . . . . . . . . . 25\n14 Section 4.1: celebrity recognition and description. . . . . . . . . . . . . . . . . . . 28\n15 Section 4.1: landmark recognition and description. . . . . . . . . . . . . . . . . . 29\n16 Section 4.1: landmark recognition and description. . . . . . . . . . . . . . . . . . 30\n17 Section 4.1: food recognition and description. . . . . . . . . . . . . . . . . . . . . 31\n18 Section 4.1: medical image understanding. . . . . . . . . . . . . . . . . . . . . . . 32\n19 Section 4.1: medical image understanding. . . . . . . . . . . . . . . . . . . . . . . 33\n20 Section 4.1: logo recognition and description. . . . . . . . . . . . . . . . . . . . . 34\n21 Section 4.1: in-the-wild logo recognition and description. . . . . . . . . . . . . . . 35\n22 Section 4.1: scene understanding. . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n23 Section 4.1: counterfactual examples. . . . . . . . . . . . . . . . . . . . . . . . . 37\n24 Section 4.2: spatial relationship understanding. . . . . . . . . . . . . . . . . . . . 39\n25 Section 4.2: object counting. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n26 Section 4.2: object localization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n27 Section 4.2: dense captioning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n28 Section 4.3: joke and meme understanding. . . . . . . . . . . . . . . . . . . . . . 44\n29 Section 4.3: science and encyclopedia knowledge. . . . . . . . . . . . . . . . . . . 45\n30 Section 4.3: science and encyclopedia knowledge. . . . . . . . . . . . . . . . . . . 46\n31 Section 4.3: science and encyclopedia knowledge. . . . . . . . . . . . . . . . . . . 47\n32 Section 4.3: multimodal commonsense. . . . . . . . . . . . . . . . . . . . . . . . 48\n33 Section 4.4: scene text recognition. . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n34 Section 4.4: visual math reasoning. . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n35 Section 4.4: flow chart understanding. . . . . . . . . . . . . . . . . . . . . . . . . 52\n36 Section 4.4: flow chart understanding. . . . . . . . . . . . . . . . . . . . . . . . . 53\n37 Section 4.4: chart understanding and reasoning. . . . . . . . . . . . . . . . . . . . 54\n38 Section 4.4: table understanding and reasoning. . . . . . . . . . . . . . . . . . . . 55\n4 39 Section 4.4: document understanding. . . . . . . . . . . . . . . . . . . . . . . . . 56\n40 Section 4.4: long document understanding. . . . . . . . . . . . . . . . . . . . . . 57\n41 Section 4.5: multilingual image descriptions. . . . . . . . . . . . . . . . . . . . . 59\n42 Section 4.5: multilingual scene text recognition. . . . . . . . . . . . . . . . . . . . 60\n43 Section 4.5: multilingual text recognition, translation, and description reasoning. . 61\n44 Section 4.5: multilingual multiculture understanding. . . . . . . . . . . . . . . . . 62\n45 Section 4.6: generate LaTex codes based on the hand-written input. . . . . . . . . . 63\n46 Section 4.6: reconstruct table in Markdown or LaTex code. . . . . . . . . . . . . . 64\n47 Section 4.6: write codes to replicate the input figure. . . . . . . . . . . . . . . . . 65\n48 Section 5.1: understand pointing inputs for grounded description. . . . . . . . . . . 67\n49 Section 5.1: grounded description with text-format region coordinates. . . . . . . . 68\n50 Section 5.2: examples of visual referring prompting. . . . . . . . . . . . . . . . . 70\n51 Section 5.2: examples of visual referring prompting. . . . . . . . . . . . . . . . . 71\n52 Section 5.3: generate pointing outputs. . . . . . . . . . . . . . . . . . . . . . . . . 72\n53 Section 6.1: video frames understanding. . . . . . . . . . . . . . . . . . . . . . . . 74\n54 Section 6.2: long-term temporal order reasoning. . . . . . . . . . . . . . . . . . . 75\n55 Section 6.2: short-term temporal order reasoning. . . . . . . . . . . . . . . . . . . 76\n56 Section 6.2: short-term and long-term temporal anticipation. . . . . . . . . . . . . 77\n57 Section 6.2: temporal localization and reasoning. . . . . . . . . . . . . . . . . . . 78\n58 Section 6.3: grounded temporal understanding. . . . . . . . . . . . . . . . . . . . 79\n59 Section 7.1: abstract visual stimuli. . . . . . . . . . . . . . . . . . . . . . . . . . . 81\n60 Section 7.2: part-object association. . . . . . . . . . . . . . . . . . . . . . . . . . 82\n61 Section 7.3: WAIS IQ test. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n62 Section 7.4: RPM IQ test, entire question page. . . . . . . . . . . . . . . . . . . . 84\n63 Section 7.4: RPM IQ test, processed sub-figures. . . . . . . . . . . . . . . . . . . 85\n64 Section 8.1: read emotions from facial expressions. . . . . . . . . . . . . . . . . . 86\n65 Section 8.2: how different visual contents may arouse emotions. . . . . . . . . . . 87\n66 Section 8.2: image aesthetics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88\n67 Section 8.3: emotion conditioned outputs. . . . . . . . . . . . . . . . . . . . . . . 89\n68 Section 9.1: spot the differences. . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n69 Section 9.1: spot the differences. . . . . . . . . . . . . . . . . . . . . . . . . . . . 93\n70 Section 9.2: industrial defect detection. . . . . . . . . . . . . . . . . . . . . . . . 94\n71 Section 9.2: industrial defect detection. . . . . . . . . . . . . . . . . . . . . . . . 95\n72 Section 9.2: industrial defect detection. . . . . . . . . . . . . . . . . . . . . . . . 96\n73 Section 9.2: safety inspection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n74 Section 9.2: grocery checkout. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n75 Section 9.3: radiology report generation on medical images. . . . . . . . . . . . . 100\n76 Section 9.3: radiology report generation on medical images. . . . . . . . . . . . . 101\n77 Section 9.3: radiology report generation on medical images. . . . . . . . . . . . . 102\n5 78 Section 9.3: radiology report generation with diagnosis history. . . . . . . . . . . . 103\n79 Section 9.4: auto damage evaluation. . . . . . . . . . . . . . . . . . . . . . . . . . 105\n80 Section 9.4: insurance reporting. . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\n81 Section 9.5: customized captioner for photo organization. . . . . . . . . . . . . . . 108\n82 Section 9.5: customized captioner for photo organization. . . . . . . . . . . . . . . 109\n83 Section 9.5: dense captioning with segmentation cut-outs. . . . . . . . . . . . . . . 110\n84 Section 9.6: evaluation of images generated from texts. . . . . . . . . . . . . . . . 112\n85 Section 9.6: evaluation of images generated from texts. . . . . . . . . . . . . . . . 113\n86 Section 9.6: image editing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114\n87 Section 9.6: image editing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n88 Section 9.7: embodied agent using coffee machine. . . . . . . . . . . . . . . . . . 117\n89 Section 9.7: embodied agent using coffee machine. . . . . . . . . . . . . . . . . . 118\n90 Section 9.7: embodied agent indoor navigation. . . . . . . . . . . . . . . . . . . . 119\n91 Section 9.7: embodied agent indoor navigation. . . . . . . . . . . . . . . . . . . . 120\n92 Section 9.8: web browsing for Mapo Tofu recipe. . . . . . . . . . . . . . . . . . . 122\n93 Section 9.8: web browsing for Mapo Tofu recipe. . . . . . . . . . . . . . . . . . . 123\n94 Section 9.8: web browsing for Mapo Tofu recipe. . . . . . . . . . . . . . . . . . . 124\n95 Section 9.8: web browsing for Mapo Tofu recipe. . . . . . . . . . . . . . . . . . . 125\n96 Section 9.8: web browsing for Mapo Tofu recipe. . . . . . . . . . . . . . . . . . . 126\n97 Section 9.8: web browsing for today\u2019s news. . . . . . . . . . . . . . . . . . . . . . 127\n98 Section 9.8: web browsing for today\u2019s news. . . . . . . . . . . . . . . . . . . . . . 128\n99 Section 9.8: web browsing for today\u2019s news. . . . . . . . . . . . . . . . . . . . . . 129\n100 Section 9.8: web browsing for today\u2019s news. . . . . . . . . . . . . . . . . . . . . . 130\n101 Section 9.8: web browsing for today\u2019s news. . . . . . . . . . . . . . . . . . . . . . 131\n102 Section 9.8: web browsing for today\u2019s news. . . . . . . . . . . . . . . . . . . . . . 132\n103 Section 9.8: online shopping. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133\n104 Section 9.8: online shopping. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134\n105 Section 9.8: online shopping. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\n106 Section 9.8: online shopping. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136\n107 Section 9.8: online shopping. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137\n108 Section 9.8: online shopping. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\n109 Section 9.8: online shopping. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\n110 Section 9.8: online shopping. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\n111 Section 9.8: online shopping. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141\n112 Section 9.8: notification understanding. . . . . . . . . . . . . . . . . . . . . . . . 142\n113 Section 9.8: notification understanding. . . . . . . . . . . . . . . . . . . . . . . . 143\n114 Section 9.8: notification understanding. . . . . . . . . . . . . . . . . . . . . . . . 144\n115 Section 9.8: watching videos. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\n116 Section 9.8: watching videos. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146\n6 117 Section 9.8: watching videos. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147\n118 Section 9.8: watching videos. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148\n119 Section 9.8: watching videos. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\n120 Section 10.1: multimodal plugins. . . . . . . . . . . . . . . . . . . . . . . . . . . 150\n121 Section 10.2: multimodal chains. . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\n122 Section 10.3: self-reflection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\n123 Section 10.3: self-reflection. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\n124 Section 10.4: self-consistency. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155\n7 1 Introduction\n1.1 Motivation and Overview\nThe breakthroughs in large language models (LLMs) [ 23,99,30,11,123,53] have shown remarkable\nversatilities and capabilities across various domains and tasks. The next evolution in this field,\nlarge multimodal models (LMMs), aims to expand upon the capabilities of LLMs by integrating\nmulti-sensory skills to achieve even stronger general intelligence. Given the dominance of the\nvisual in human senses [ 33,58], many LMM studies start with extending the vision capability.\nPreliminary research investigations either finetune a vision encoder to align with a frozen pre-trained\nLLM [ 125,7,71,55,42,13,48,157,79,35,146], or use a vision-language model to convert visual\ninputs to text descriptions that LLMs can understand [ 149,141,131,54,113,142]. However, most\nexisting models [ 13,48,157,79,35,69] are of limited model and data scales, potentially restricting\nthe emergence of various intriguing abilities. Consequently, it remains unclear what are the status\nquo and emergent multimodal abilities of LMMs that are developed based on the state-of-the-art\nLLMs, such as GPT-4 (no vision) [ 99] and PaLM [ 30,11]. In this paper, we report our preliminary\nexplorations with (an early version of) GPT-4V, a state-of-the-art LMM with vision, built based on\nthe SOTA LLM and trained with a large scale of multimodal data.\nOur exploration of GPT-4V is guided by the following questions.\n1.What are GPT-4V\u2019s supported inputs and working modes? The genericity of multimodal\nmodels inevitably requires the system to work with the arbitrary mix of different input\nmodalities. GPT-4V shows unprecedented ability in understanding and processing an\narbitrary mix of input images, sub-images, texts, scene texts, and visual pointers. We\nalso demonstrate that GPT-4V well supports the test-time techniques observed in LLMs,\nincluding instruction following [ 102], chain-of-thoughts [ 136,66], in-context few-shot\nlearning [23], etc.\n2.What are the quality and genericity of GPT-4V\u2019s capabilities on different domains and tasks?\nWe sample queries covering a wide range of domains and tasks to understand GPT-4V\u2019s\ncapabilities, including open-world visual understanding, visual description, multimodal\nknowledge, commonsense, scene text understanding, document reasoning, coding, temporal\nreasoning, abstract reasoning, emotion understanding, and many more. GPT-4V shows\nimpressive human-level capabilities across many of the experimented domains.\n3.What are effective ways to use and prompt GPT-4V? GPT-4V is strong in understanding\npixel space edits, such as visual pointers and scene texts drawn on input images. Inspired by\nthis capability, we discuss the \u201cvisual referring prompting\u201d that directly edits input images to\ninstruct the task of interest. Visual referring prompting can be seamlessly used together with\nother image and text prompts, presenting a nuanced interface for instruction and example\ndemonstrations.\n4.What are promising future directions? Given GPT-4V\u2019s strong capability across domains and\ntasks, we ask what is the next step for multimodal learning, and more broadly for artificial\nintelligence. We organize our thoughts and explorations into two perspectives, i.e., emergent\nnovel application scenarios to focus on, and the future research directions for GPT-4V-based\nsystems. We present our preliminary explorations to inspire future studies.\nGuided by the aforementioned problems, we comprehensively organize and list our explored quali-\ntative results. The report contains minimal quantitative benchmark results, and instead consists of\nmainly selected interesting qualitative examples. Despite being less rigorous, this design allows\nfor providing a more comprehensive analysis covering a broad range of domains, tasks, working\nmodes, and prompting techniques, under a fixed capacity. We believe this organized collection of\nexplorations will inspire future works in emerging novel applications, next-generation multimodal\ntask formulation, and developing advanced LMM-based intelligent systems.\n1.2 Our Approach in Exploring GPT-4V\nGoal of this report. The standard approach for evaluating a system is by benchmarking it against a\nseries of carefully designed datasets, each representing a specific domain and task. One challenge is\nthat some of the existing benchmarks may not be suitable for evaluating LMMs anymore. For example,\n8 the image captioning outputs of LMMs are much richer and contain more detailed descriptions than\nthe ground truths in the image captioning benchmark datasets [ 27]. There is also a lack of public\ninformation regarding GPT-4V\u2019s large-scale pre-training, which may violate the train-test setup for\ncertain existing datasets and invalidate those benchmark numbers. Because of this, restricting the\nevaluation to existing benchmarks and metrics may unintentionally narrow the scope of GPT-4V\u2019s\nassessment. Developing a comprehensive list of next-generation evaluation tasks and benchmarks\nwould be the ideal ultimate solution. However, we left those as future work due to the significant\nefforts required.\nIn lieu of quantitative benchmarking, this paper focuses on using qualitative results to provide a\nglimpse of GPT-4V\u2019s new capabilities and potential emerging use cases. Our goal is to discover and\npreview what GPT-4V might already be capable of, even though these novel capabilities may not yet\nbe entirely reliable. We hope this collection of explorations will inspire future research in establishing\nquantitative benchmarks for next-generation multimodal tasks, modernizing existing benchmarks,\nfurther improving model performance and system reliability, and sparkling innovation in emerging\nuse cases. Following this, we will delve into the core designs for our approach to exploring GPT-4V.\nSample selection guidance. This report focuses on presenting qualitative results to showcase the\npotential capabilities of GPT-4V, rather than providing comprehensive quantitative benchmark results.\nThis naturally raises the question of the reliability of the showcased examples. The examples featured\nin this report may require careful instruction tuning to amplify GPT-4V\u2019s corresponding capabilities.\nIt should be noted that some complex cases may only work with the specifically designed prompts.\nAs such, the capabilities demonstrated may not consistently work across different samples. Instead of\nshowing only the reliable functionalities, the primary objective of this report is to provide readers\nwith a list of our discovered potential capabilities of GPT-4V, which might otherwise be overlooked\nafter a few unsuccessful trials.\nSample selection to prevent mere memorizing from training. A fundamental design consideration\nin qualitative reports [ 24] is discerning models\u2019 true capabilities from merely memorizing responses\nfrom training samples or making educated guesses based on hints from instructions and in-context\nexamples. We carefully control both the images and text in the input prompts to prevent them from\nbeing seen during GPT-4V training. We generate original text queries from scratch, and try to use\nimages that are either not accessible online or with a timestamp beyond April 2023. We will indicate\ninstances where a specific sample does not meet this criterion, e.g., deliberately using samples from\nspecific vision-language datasets. Beyond ensuring that samples are unseen, we incorporate rationale\nqueries into the process. These queries are designed to probe the model\u2019s reasoning process, thereby\nvalidating GPT-4V\u2019s possession of the intended capability.\nThe default working mode. As later detailed in Section 3, GPT-4V works effectively in different\nworking modes, including zero-shot learning with instructions, in-context few-shot learning, etc.\nAmong them, this report primarily focuses on zero-shot instruction tuning, as opposed to in-context\nfew-shot learning. This design is to prevent potential information leakage from in-context examples.\nWhile in-context few-shot examples can enhance performance and reliability, they do not consistently\nengender new capabilities. As such, we designate zero-shot as the default working mode for\npresentation, and reduce the use of in-context examples to minimize examples\u2019 impacts on the\nassessed capabilities.\n1.3 How to Read this Report?\nThis report documents the explorations of GPT-4V conducted by researchers in the computer vision\nand vision-language multimodal field. It is primarily geared towards fellow researchers in related\ndisciplines who seek to gain a qualitative impression of LMM\u2019s capabilities and understand its\ndifference from traditional vision-language models. The report is also prepared for professionals\nfor whom AI or computer science may be outside their specialties, to assist them in conceptualizing\nways LMMs can enhance their proficiency within their distinct domains of expertise.\nWe give an overview of the report, structured around the four core questions that guide our exploration.\n1.What are GPT-4V\u2019s supported inputs and working modes? Section 2 summarizes GPT-4V\u2019s\nsupported inputs and presents an overview of their corresponding use cases. Based on\nthe flexible interleaved image-text inputs, Section 3 discusses GPT-4V\u2019s different work-\ning modes, such as instruction tuning, in-context learning, and other emergent usages.\n9 The section covers the novel ways of using and prompting GPT-4V, aiming to provide a\ncomprehensive overview of how we will use GPT-4V in subsequent sections.\n2.What are the quality and genericity of GPT-4V\u2019s capabilities on different domains and tasks?\nThe exploration of this question makes up a large portion of the report. Section 4 provides a\ncomprehensive analysis covering a wide range of vision and vision-language scenarios, in-\ncluding image description and recognition on different domains, dense visual understanding,\nmultimodal knowledge, commonsense, scene text understanding, document reasoning, and\nmany more. We also separate out several novel and interesting capabilities. Section 6 studies\nGPT-4V\u2019s capability in temporal, motion, and video understanding. Section 7 explores the\nabstract visual understanding and reasoning capability, and Section 8 covers the emotion\nand sentiment understanding.\n3.What are effective ways to use and prompt GPT-4V? We start the discussion on this question\nfrom the working mode and prompting method introduction in Section 3. In Section 5, we\nhighlight one novel promoting technique, namely visual referring prompting, which draws\nvisual pointers and scene texts on input images to prompt GPT-4V. We demonstrate the flexi-\nble prompting methods, such as the combination of instruction and example demonstrations,\nthroughout the report in the given examples.\n4.What are promising future directions? Section 9 focuses on the novel use cases facilitated\nby GPT-4V. We hope these initial examples could inspire future works to design new task\nsetups and present rigorous benchmarks. Section 10 imagines powerful future systems\nthat can be built based on GPT-4V, such as the multimodal plugins, multimodal chains,\nself-reflection, self-consistency, and retrieval-augmented LMMs, etc.\nIn addition to this overview and the table of contents , we have also included a list of figures . The\nlist enumerates the qualitative examples detailed within the report, serving as an additional tool to\nhelp readers navigate to their scenarios of interest.\n2 GPT-4V\u2019s Input Modes\nThis section summarizes GPT-4V\u2019s supported inputs, i.e., functioning as a uni-model language model\nwith the text-only inputs, taking single image-text pair optionally with only a single image, and\ntaking interleaved image-text pairs optionally with only multiple image inputs. We next highlight the\nrepresentative use cases under these different input modes.\n2.1 Text-only Inputs\nGPT-4V\u2019s strong language capability enables it to serve as an effective unimodal language model [ 38,\n108,23] with text-only inputs. Operating exclusively with text for both input and output, GPT-4V is\ncapable of performing a wide variety of language and coding tasks. We refer readers to the GPT-4\ntechnical report [ 99] for the comprehensive and in-depth analysis of GPT-4V\u2019s language and coding\ncapabilities, as well as the comparison with GPT-4 (no vision).\n2.2 Single Image-text Pair\nGPT-4V, the latest large multimodal model, takes images and texts as inputs to generate textual\noutputs. In line with existing general-purpose vision-language models [ 9,81,73,8,70,122,120,\n155,28,83,45,74,57,64,72,132,29,140,41,7,128,46,40,158,69], GPT-4V can take a single\nimage-text pair or a single image as input to perform various vision and vision-language tasks,\nsuch as image recognition [ 37], object localization [ 153], image captioning [ 27], visual question\nanswering [ 12], visual dialogue [ 36], dense caption [ 62], and so on. We note that the text in the\nimage-text pair can be used either as instruction like \u201cdescribe the image\u201d for captioning, or as the\nquery input like the question in visual question answering. GPT-4V\u2019s exceptional intelligence is\nexemplified by its significantly enhanced performance and generalizability compared to prior arts. A\ncomprehensive analysis of its multimodal capabilities on various domains is detailed in Section 4.\n10 Sec. 2.3 Interleaved Image-text Inputs \nInterleaved Image-text Pairs \nPrompt: \nHow much should I pay for the beer on the table \naccording to the price on the menu? \nHere is a photo of the menu. \nGPT-4V: \nAccording to the menu, the price for a Magna beer \nis $6. Since there are two Magna beers on the table, \nthe total cost for the beers would be $12. Prompt: \nHow much did I pay for tax? \nWhere is the information? \nGPT-4V: \nIn the first receipt, you paid $3.72 for tax. The \ninformation is located at the bottom of the receipt. In \nthe second receipt, you paid $42.23 for tax. The \ninformation is located near the bottom of the receipt, \nunder the \"Sales Tax\" line. In the third receipt, you paid \n$4.58 for tax. The information is located at the bottom \nof the receipt, under the \"TAX\" line. In total, you paid \n$50.53 in tax across all three receipts. \nPrompt Continued Figure 1: GPT-4V can work with multi-image and interleaved image-text inputs. Check Section 2.3\nfor detailed discussions.\n2.3 Interleaved Image-text Inputs\nThe generality of GPT-4V is further enhanced by its ability to handle flexibly interleaved image-text\ninputs. The interleaved image-text inputs can be either visually centric such as multiple images\nwith a short question or instruction, text-centric such as a long webpage with two inserted images,\nor a balanced mixture of images and texts. This mode of mixed input provides flexibility for a\nwide array of applications. For example, it can compute the total tax paid across multiple receipt\nimages, as shown in Figure 1. It also enables processing multiple input images and extracting queried\ninformation. GPT-4V could also effectively associate information across interleaved image-text\ninputs, such as finding the beer price on the menu, counting the number of beers, and returning the\ntotal cost, as shown in Figure 1. Beyond direct applications, processing interleaved image-text inputs\nserves as a fundamental component for in-context few-shot learning and other advanced test-time\nprompting techniques, thereby further boosting GPT-4V\u2019s generality. We demonstrate these intriguing\nnovel usages in the next section, Section 3.\n11 3 GPT-4V\u2019s Working Modes and Prompting Techniques\nSec. 3.1 Following Text Instructions \nFollowing Text Instructions \nPrompt: \nDescribe the image with a sentence consisting \nof three/six/nine  words. \nGPT-4V: \nThree: Seaside dining table \nSix: Dining table overlooking ocean with beer \nNine: Outdoor restaurant table with ocean view \nand a beer \nPrompt: \nWhat would the missing image look like? \nGPT-4V: \nThe missing image would likely be a variation of the \ndiamond or star shape, as seen in the other three images. It \ncould be a diamond with a different pattern or color, or a star \nwith a different number of points or color. It is difficult to say  \nfor sure without more context, but it would likely follow the  \nsame theme as the other images. \nPrompt: \nWrite a sentence for the image consisting only \nof words starting with the letter 'B'/ 'D'/ 'T' .\nGPT-4V: \n'B': Beachside bar boasts beautiful blue \nbackdrop. \n'D': Dining deck displays delightful drinks, \ndistant docks. \n'T': Tabletop terrace teeming with tropical \ntranquility. \nPrompt: \nHere are three sub images arranged in a 2-by-2 matrix. First, \nlook at the two images in the first column, in the top left and \nbottom left. Then, find out the pattern in the first column \nbased on the left two images. Next, use the found pattern \nand the image in the top right to infer the missing figure. \nFinally, describe what would the missing image look like? \nGPT-4V: \nThe pattern in the first column is that the shape in the bottom \nleft image has a dot in the center of the shape in the top left \nimage. Therefore, the missing image in the bottom right \nwould be a star with a dot in the center, similar to the image \nin the top right but with a dot in the center. Prompt: \nWhat is the read of the \nspeed meter? \nGPT-V: \nThe speed of the speed meter is approximately 95 \nmiles per hour. \nPrompt: \nWhat is the read of the speed meter? To read a \nspeed meter, observe the needle on the dial which \npoints to your current speed represented in numbers, \non the outer edge of the dial. Yellow is the color of \nthe needle \nGPT-V: \nThe speed is around 119 mph, as the pointer is close \nto 120 mph but not quite 120 yet. \nFigure 2: GPT-4V can understand and follow text instructions, to generate the desired text outputs\nor learn to perform a new task. Red highlights the less informative answer. Check Section 3.1 for\ndetailed discussions.\n12 Constrained Prompting \nGPT-4V: \n{\"class\": \"D\", \"DLN\": \"D08954796\", \"DOB\": \"01/01/1974\", \"Name\": \"Jelani Sample\", \"Address\": \"123 Main St, \nPhoenix, AZ 85007\", \"EXP\": \"03/01/2024\", \"ISS\": \"03/01/2016\", \"SEX\": \"M\", \"HGT\": \"5-09\", \"WGT\": \"185 lbs\", \n\"EYES\": \"BRO\", \"HAIR\": \"N/A\" , \"DONOR\": \"Y\"} \nPrompt: \nPlease read the text in this image and return the information in the following JSON format (note xxx is placeholder, if \nthe information is not available in the image, put \"N/A\" instead). \n{\"class\": xxx, \"DLN\": xxx, \"DOB\": xxx, \"Name\": xxx, \"Address\": xxx, \"EXP\": xxx, \"ISS\": xxx, \"SEX\": xxx, \"HGT\": xxx, \n\"WGT\": xxx, \"EYES\": xxx, \"HAIR\": xxx, \"DONOR\": xxx} \n \nGPT-4V: \n{\"class\": \"C\", \"DLN\": \" 1234568\", \"DOB\": \"08/31/1977\", \"Name\": \"Ima Cardholder\", \"Address\": \"2570 24th Street, \nAnytown, CA 95818\", \"EXP\": \"08/31/2014\", \"ISS\": \"08/31/ 2011 \", \"SEX\": \"F\", \"HGT\": \"5-05\", \"WGT\": \"125 lbs\", \n\"EYES\": \"BRN\", \"HAIR\": \"BRN\", \"DONOR\": \"VETERAN\" }  \nPrompt: \nPlease read the text in this image and return the information in the following JSON format (note xxx is placeholder, if \nthe information is not available in the image, put \"N/A\" instead). \n{\"Surname\": xxx, \"Given Name\": xxx, \"USCIS #\": xxx, \"Category\": xxx, \"Country of Birth\": xxx, \"Date of Birth\": xxx, \n\"SEX\": xxx, \"Card Expires\": xxx, \"Resident Since\": xxx} \nGPT-4V: \n{\"Surname\": \"STEVENS\", \"Given Name\": \"ICHIKA\", \"USCIS #\": \"000-000-000\", \"Category\": \"IR1\", \"Country of Birth\": \n\"Japan\", \"Date of Birth\": \"18 AUG 1988\", \"SEX\": \"F\", \"Card Expires\": \"07/04/2027\", \"Resident Since\": \"07/05/2017\"} Sec. 3.1 Following Text Instructions \nPrompt: \nPlease read the text in this image and return the information in the following JSON format (note xxx is placeholder, if \nthe information is not available in the image, put \"N/A\" instead). \n{\"class\": xxx, \"DLN\": xxx, \"DOB\": xxx, \"Name\": xxx, \"Address\": xxx, \"EXP\": xxx, \"ISS\": xxx, \"SEX\": xxx, \"HGT\": xxx, \n\"WGT\": xxx, \"EYES\": xxx, \"HAIR\": xxx, \"DONOR\": xxx} \nFigure 3: Constrained prompting to return in JSON format. Images are example IDs for samples.\nRed highlights the wrong answer. Check Section 3.1 for detailed discussions.\n13 Condition on Good Performance \nGPT-4V: \nThere are 12 apples  in the image. \nPrompt: \nCount the number of apples in the image. Let\u2019s think step-by-step. \n \nGPT-4V: \nStep 1: Look at the image and identify all the apples. \nStep 2: Start counting the apples one by one. \nStep 3: Make sure not to miss any apples or count the same apple twice. \nStep 4: After counting all the apples, double check to make sure the count is correct. \nAnswer: There are 12 apples  in the image. Sec. 3.1 Following Text Instructions \nPrompt: \nCount the number of apples in the image. \nNote: Same query image as shown above, resized to save space \nPrompt: \nCount the number of apples in the image. Let\u2019s count the apples row-by-row. \n \nGPT-4V: \nFirst row: 4 apples \nSecond row: 3 apples \nThird row: 4 apples \nTotal number of apples in the image: 11 apples Note: Same query image as shown above, resized to save space \nPrompt: \nCount the number of apples in the image. First count how many rows of apples are there, and then count the  \napples in each row, and finally sum them up to get the total number.  \n \nGPT-4V: \nThere are 3 rows of apples in the image. \nThe top row has 5 apples. \nThe middle row has 4 apples. \nThe bottom row has 6 apples. \nTherefore, the total number of apples in the image is 5 + 4 + 6 = 15 apples.  Note: Same query image as shown above, resized to save space \nPrompt: \nYou are an expert in counting things in the image. Let's count the number of apples in the image below row by  row \nto be sure we have the right answer. \n \nGPT-4V: \nRow 1: 4 apples  \nRow 2: 4 apples  \nRow 3: 3 apples  \nTotal number of apples in the image: 11 apples Note: Same query image as shown above, resized to save space \nFigure 4: Condition on good performance to improve counting. Green (Red ) highlights the correct\n(wrong) answer. Blue indicates different ways to prompting in addition to the basic requirement of\n\u201cCount the number of apples in the image.\u201d Check Section 3.1 for detailed discussions.\n14 3.1 Following Text Instructions\nOne unique strength of GPT-4V is its generality, partially achieved via its strong capability in\nunderstanding and following text instructions [ 102,96,134,111]. Instructions provide a natural way\nto define and customize the desired output text for arbitrary vision-language use cases. Figure 2\nshows an example of image descriptions with constraints on sentence length and the words to use.\nAlternatively, on the input side, GPT-4V could understand the detailed instructions to perform\nchallenging tasks, such as enabling GPT-4V to better interpret the abstract reasoning question by\nproviding instructions on intermediate steps. The ability to learn new tasks from instructions shows\ngreat potential in adapting to various unseen applications and tasks, as detailed in Section 9. In line\nwith recent studies [ 7,13,48,157,79,35], the instructions discussed in this subsection are mostly in\nthe text format, providing language descriptions of the interested task. We will discuss GPT-4V\u2019s\nunique capability of following multimodal example-grounded instructions later in Section 3.3.\nIn addition, we showcase how text instructions play an important role in shaping GPT-4V\u2019s response\nwith two techniques adopted from LLM literature [ 3,156], (i) \u201cconstrained prompting\u201d so that\nGPT-4V responds in a certain format; and ( ii) \u201ccondition on good performance\u201d that explicitly asks\nfor good performance from GPT-4V.\nConstrained prompting. In Figure 3, we prompt GPT-4V to read the text in the image and return\nthe information in a specific JSON format. Although GPT-4V makes some mistakes in extracting\nthe corresponding information from driver\u2019s licenses, the responses are constrained to the JSON\nformat specified in the text instruction. We leverage this technique for certain application scenarios in\nSection 9.\nCondition on good performance. One observation about LLMs is that LLMs don\u2019t want to\nsucceed [ 10]. Rather, they want to imitate training sets with a spectrum of performance qualities. If\nthe user wants to succeed in a task given to the model, the user should explicitly ask for it, which\nhas proven useful in improving the performance of LLMs [ 156]. In the context of LMMs, we have\nsimilar observations. In Figure 4, we compare the model\u2019s response to different text instructions\nfor counting. We start with a simple and clear prompt: \u201cCount the number of apples in the image.\u201d\nHowever, GPT-4V incorrectly counts a total of 12 apples in the image. To improve its performance,\nwe explore the use of zero-shot chain-of-thought from [ 66] for LLMs by adding the phrase \u201cLet\u2019s\nthink step-by-step.\u201d Although GPT-4V\u2019s predicted steps are generally correct, they are not very\nhelpful for the final count, as it still arrives at the incorrect answer of \u201c12 apples.\u201d Next, we modify\nthe instruction to \u201cLet\u2019s count the apples row-by-row,\u201d which is more relevant to the visual input.\nWhile GPT-4V provides the correct total count, it makes mistakes in counting the second/third row.\nWhen we further expand the instruction to \u201cFirst count how many rows of apples there are, then\ncount the apples in each row, and finally sum them up to get the total number,\u201d the final answer\ndeviates even more from the correct answer (15 vs. 11). Finally, imitating \u201cLet\u2019s work this out in\na step by step way to be sure we have the right answer.\u201d in [ 156] for LLMs, we design the prompt\nas follows: \u201cYou are an expert in counting things in the image. Let\u2019s count the number of apples in\nthe image below row by row to be sure we have the right answer.\u201d. The first sentence in our prompt\nasks GPT-4V to assume the role of an expert in counting, and the second sentence explicitly instructs\nGPT-4V to succeed. With this design, GPT-4V successfully returns the correct answer for each row\nas well as the total count. Throughout the paper, we employ this technique in various scenarios for\nbetter performance.\n3.2 Visual Pointing and Visual Referring Prompting\nPointing is a fundamental aspect of human-human interaction [ 89]. To provide a comparable channel\nof interaction, various forms of \u201cpointing\u201d are studied to refer to an arbitrary spatial region of interest.\nFor example, as depicted in Figure 5, \u201cpointing\u201d can be represented as numerical spatial coordinates\nsuch as box coordinates and image crops, or visual markers overlaid on image pixels such as arrows,\nboxes, circles, and hand drawings. We observe that GPT-4V is particularly strong in understanding\nvisual pointers drawn directly on images. Given the flexibility of drawing on images, this capability\ncan be used as a natural approach for future human-computer interaction in the wild [ 90,117,157].\nTo this end, we explore a new prompting method named visual referring prompting, where people\nedit the pixel space of input images to specify the desired objective, such as drawing visual pointers\nor handwriting scene texts. As illustrated in Figure 6, visual referring prompting edits the image\n15 Crop Box \nPoint Box HandDrawing Circle Coordinate (0.47, 0.48, 0.55, 0.87) \n0 1\n1\nArrow \nCrop Box Point \nBox HandDrawing Circle Coordinate (0.47, 0.48, 0.55, 0.87) \nCrop Mask 0 1\n1\nArrow Figure 5: Different modes of \u201cvisual pointing\u201d in multimodal interaction.\npixels, instead of the conventional text prompts, to perform the task of interest. For example, it could\nbe a simple grounded description, which focuses on describing the pointed object while maintaining\nthe understanding of the global image context, as shown in Figure 6 (1,2). Visual referring prompting\nalso enables other novel use cases, such as associating the pointed object with an index written in\nscene text (Figure 6 (3)), or solving the question asked near the queried edge or angle (Figure 6 (4)).\nSection 5 will discuss visual referring prompting in more detail.\n3.3 Visual + Text Prompting\nVisual referring prompting can be smoothly used together with other image-text prompts, presenting\na nuanced interface that succinctly represents the problem of interest. Figure 7 presents two examples\nto showcase the flexibility of GPT-4V\u2019s prompt, particularly its proficiency in integrating different\ninput formats and seamlessly mixing instructions with examples in the inputs. GPT-4V\u2019s genericity\nand flexibility result in a human-like comprehension of multimodal instructions and an unprecedented\nability to adapt to unseen tasks.\nIntegrated multimodal instruction inputs. Existing models usually have implicit constraints on\nhow interleaved image-text inputs should be formatted, e.g., in-context few-shot learning requires\nimage-text pairs to share a similar format as the query input. In contrast, GPT-4V shows the genericity\nin processing an arbitrary mix of images, sub-images, texts, scene texts, and visual pointers. For\nexample, to illustrate the \u201cadding a line\u201d pattern in Figure 7, one could either point to the first column\nin the matrix image with a circle as in sub-figure (1), or incorporate the sub-images inline as in\nsub-figure (2). Similarly, for input query, one could either present a large figure with the question\nas scene texts as in sub-figure (1), or send the mix of texts and sub-images as in sub-figure (2). In\ncontrast to GPT-4V\u2019s flexibility, existing multimodal models are highly restricted in terms of how\nthey can combine images and texts, and the number of images they can process, thereby imposing\nlimitations on the model\u2019s capability and genericity.\nMultimodal example-grounded instruction. In addition to supporting more flexible input formats,\nGPT-4V\u2019s genericity also opens up more effective ways of illustrating the task to perform, compared\nwith the instruction-following mode and in-context few-shot learning. Instruction-following tech-\nniques [ 102,96,134,111], originally proposed for NLP tasks, intuitively focus on task instructions\npurely in the textual format. The text instruction is loosely related to the visual query input and thus\nmay not provide a clear task demonstration. While in-context few-shot learning [ 23,125,7] provides\ntest-time examples that contain both images and texts, these examples must align perfectly with\nthe format of the inference query, making them complex and lengthy to incorporate. Furthermore,\nin-context examples are usually used separately from instructions, requiring the model to infer the\ntask objective and thereby compromising the demonstration\u2019s effectiveness. In contrast, GPT-4V\u2019s\ncapability to comprehend multimodal instructions enables task demonstrations to be grounded onto\ncorresponding in-context examples, therefore more effectively illustrating the task of interest. For\nexample, in Figure 7, grounding instructions of \u201cfinding the pattern in the first column\u201d onto the key\nsteps in demonstration examples ( i.e., the circled pattern in (1) and corresponding sub-figures in (2))\nsimplifies the learning process and enhances the model\u2019s performance. This approach also mirrors\nthe human learning process, which involves abstract instructions paired with intuitive examples.\n16 Sec. 3.2 Visual Referring Prompting \n(1) Prompt: \nDescribe the pointed region in the image. \nGPT-4V: \nThe pointed region in the image is a row of \nhanging lights on a wire. The lights are small \nand round, and they are evenly spaced along \nthe wire. The wire is stretched between two \npoles and is located above a patio area with \ntables and umbrellas. The lights appear to be \ndecorative and are likely used to provide \nambient lighting in the evening. \n(2) Prompt: \nDescribe the pointed region in the image. \nGPT-4V: \nThe highlighted red region in the image is a table cell that \ncontains a numerical value of 122.3. It is located in the \"Test \nset\" section of the table, under the \"out\" column and the \"GIT2\" \nrow. This value represents the overall performance metric for \nthe GIT2 method on the test set, in the \"out\" category. \nVisual Pointing and Visual Referring Prompting \n(3) Prompt: \nWhat is in the circled glass? Object 1 or 2? \nDescribe what is Object 1 and object 2. Then \ncheck what is in the circled glass. \nGPT-4V: \nObject 1 is a glass bottle of water. It is clear \nand has a blue label on it. \nObject 2 is a glass bottle of Magna beer. It is \ngreen and has a red and white label on it. \nIn the circled glass, it appears to be a clear \nliquid, likely water. So, it is likely that the \ncontents of the circled glass are from Object 1, \nthe glass bottle of water. \n(4) Prompt: \nAnswer the question in the following figure. Provide \nintermediate steps. \nGPT-4V: \nTo find the length of the blue arrowed edge, we can use the \nPythagorean theorem since we have a right triangle. The \ntheorem states that a\u00b2 + b\u00b2 = c\u00b2, where a and b are the two legs \nof the right triangle and c is the hypotenuse. Plugging in the \nvalues we have, we get: \n4\u00b2 + 8\u00b2 = c\u00b2    16 + 64 = c\u00b2    80 = c\u00b2    \u221a80 = c    8.94 \u2248 c \nTherefore, the length of the blue arrowed edge is approximately \n8.94. \nTo find the angle at the green arrow, we can use the tangent \nfunction since we have the opposite and adjacent sides of the \nangle. The tangent function states that tan\u03b8 = \nopposite/adjacent. Plugging in the values we have, we get: \ntan\u03b8 = 4/8    tan\u03b8 = 1/2    \u03b8 = tan^-1(1/2)    \u03b8 \u2248 26.57\u00b0 \nTherefore, the angle at the green arrow is approximately \n26.57\u00b0. \nFigure 6: GPT-4V demonstrates the unique capability of understanding visual pointing directly\noverlaid on images. Based on such capability, we explore visual referring prompting that edits input\nimage pixels ( e.g., drawing visual pointers and scene texts) to prompt the task of interest. Check\nSection 3.2 for detailed discussions.\n17 (2) Prompt: \nFind out the pattern in the first column and use it to infer \nthe missing figure in the second column. \nFor example, \nTherefore, adds a line to image 2 \nwill be a diamond with a line through it. Sec. 3.3 Visual + Text Prompting \nMultimodal Example-grounded Instruction \n1.\n2.\n3.\n4.\nHere are three sub images arranged in a 2-by-2 matrix. \nWhat is the pattern between image 3 and image 1? \nWhat would be the next image to complete the matrix? \na.\nb.\nc.\nd.\ne.\nf.\nPrompt Continued (1) Prompt: \nFind out the pattern in the first column, and use it to infer \nthe missing figure in the second column. \nFor example, the circled region in the first left column \nshows the pattern of adding a line through it. Therefore, \nthe answer is a figure of a diamond \nwith a line through it, looking like \n.\nAnswer the question in the following figure. \nGPT-4V: \nThe pattern in the first column is that a dot is added in \nthe center of the figure. Therefore, the missing figure in \nthe second column would be a star with a dot in the \ncenter, which is option C. \nGPT-4V: \nThe pattern between image 3 and image 1 is that \nimage 3 has a dot in the center, while image 1 does \nnot. The next image to complete the matrix would be \noption c, a star with a dot in the center. This is because \nthe pattern seems to be that every other image has a \ndot in the center, and the previous image (image 3) \nhad a dot in the center. Additionally, the images \nalternate between a diamond and a star shape, so the \nnext image should be a star. \nFigure 7: GPT-4V is strong in interpreting an arbitrary mix of images, sub-images, texts, scene texts,\nand visual pointer inputs. These elements could serve as instructions, examples, or input queries,\nhelping GPT-4V to effectively perform novel tasks. Check Section 3.3 for detailed discussions.\n18 3.4 In-context Few-shot Learning\nIn-context few-shot learning is another intriguing emergent ability observed in LLMs [ 23,39,135,34].\nThat is, LLMs can generate desired outputs without parameter updates by prepending a few in-context\nexamples at inference time. The examples share the same format as the input query, and serve as\ndemonstrations to illustrate the desired outputs. Similar abilities were recently observed in multimodal\nmodels [ 125,7,55,42,151], where query inputs are formatted image-text pairs. Complementary to\ninstruction tuning, in-context learning \u201cteaches\u201d model to perform new tasks by providing in-context\nexamples with the same format during test time. We demonstrate the in-context few-shot learning\ncapacity of GPT-4V through a few compelling examples. We emphasize that in certain scenarios,\nin-context few-shot learning with a sufficient number of examples becomes essential, particularly\nwhen zero-shot or one-shot instruction approaches fall short. Figures 8-10 explore a challenging\nscenario involving the reading of a speed meter. In Figure 8, the zero-shot performance of GPT-4V\non a screenshot of a speed meter image from a video is depicted. Despite numerous attempts to\nprompt GPT-4V in a zero-shot manner, it struggles to accurately read the current speed displayed in\nthe image. The predictions it generates (22/30/40 mph) deviate significantly from the actual human\nreading of \u201capproximately 9 mph.\u201d Even when employing a 1-shot in-context example, as shown in\nFigure 9, using either a dissimilar example (Figure 9a) or a similar example (Figure 9b), GPT-4V\nstill fails to accurately locate the two numbers on the left and right sides of the yellow pointer. In\ncontrast, Figure 10 demonstrates that when provided with 2 in-context examples, one similar to the\nquery image and the other dissimilar, GPT-4V successfully predicts the speed reading as \u201caround 9\nmph\u201d by recognizing that the pointer is close to 10 mph but not quite there yet.\nThe comparison between zero-shot, 1-shot, and 2-shot performance for reasoning over a complex\nline plot is illustrated in Figures 11-13. The example we explore here presents a great difficulty level\nas it involves multi-hop reasoning. To answer the question \u201cIn the graph, which year has the highest\naverage gas price for the month of June,\u201d one needs to go through at least four steps: ( i) locating\nthe month of June on the x-axis, ( ii) comparing data points for each line in June, ( iii) identifying\nthe color of the line with the highest value, and ( iv) matching the color to the corresponding year\nin the legend at the top. Failure in any of these steps would lead to an incorrect prediction. As\ndepicted in Figure 11, even when prompted with \u201ctext instruction, think step-by-step\u201d in a zero-shot\nmanner, GPT-4V fails to correctly associate the colors with the years from the legend. Furthermore,\nit gets distracted by the highlighted gas price of $3.32in the graph. Similarly, in Figure 12, although\nGPT-4V shows some improvement in reading the legend (correcting the corresponding colors for\n2021 and 2022 compared to zero-shot), it still insists on answering with 2023 as the year with the\nhighest average gas price for the month of June, despite the fact that the chart only includes data\npoints until 01/17/2023. However, as we introduce another in-context example in Figure 13, GPT-4V\nfinally arrives at the correct answer (2022) and provides intermediate steps that explain its reasoning\nprocess, similar to the demonstration shown in the in-context examples.\nThese proof-of-concept examples vividly demonstrate the rising significance of in-context few-shot\nlearning for achieving improved performance with LMMs. This approach serves as a viable alternative\nto finetuning, analogous to the observations made in the context of LLMs [ 23,39,135,34]. Despite\nthe great importance of in-context few-shot learning in achieving better performance with LMMs,\nwe limit its use in this report to prevent the potential information leakage or undesired hints from\nin-context examples. We also leave the quantitative evaluation of few-shot learning\u2019s gain to future\nstudies.\n19 Zero-shot \nPrompt: \nWhat is the read of the speed meter? \nGPT-4V: \nThe read of the speed meter is approximately 22 miles per hour (mph). \nZero-shot (think step-by-step) \nPrompt: \nWhat is the read of the speed meter? Let's think step-by-step. \nGPT-4V: \nThere are two scales on this speedometer - one in miles per hour (mph) and one in kilometers per hour (km/h). \nThe needle is pointing between the 20 and 40 on the mph scale, and between the 40 and 60 on the km/h scale. \nSince the numbers on the scales increase in increments of 20, we can estimate that the needle is pointing at \napproximately 30 mph and 50 km/h. \nTherefore, the read of the speed meter is approximately 30 mph (or 50 km/h). \nZero-shot (w/ text instructions) \nPrompt: \nWhat is the read of the speed meter? To read this speed meter, observe the needle on the outer edge of the dial \nwhich points to your current speed represented in miles per hour. Yellow is the color of the needle. \nGPT-4V: \nIn this example, the needle is pointing to approximately 40 mph , so the current speed is approximately 40 miles \nper hour. Additionally, the inner circle of the dial shows the speed in kilometers per hour, so in this example, the \ncurrent speed is approximately 64 km/h. Note: Same query image as shown above, resized to save space Note: Same query image as shown above, resized to save space \nZero-shot (w/ text instructions, think step-by-step) \nPrompt: \nWhat is the read of the speed meter? To read this speed meter, observe the needle on the outer edge of the dial \nwhich points to your current speed represented in miles per hour. Yellow is the color of the needle. Let\u2019s think \nstep-by-step. \nGPT-4V: \n1. First, look at the outer edge of the dial and find the numbers that represent miles per hour. \n2. Next, find the yellow needle that points to the current speed. \n3. Observe where the needle is pointing and read the number on the outer edge of the dial. \n4. This number is the current speed in miles per hour. \nIn this example, the needle is pointing to approximately 40 mph, so the current speed is 40 miles per hour. Note: Same query image as shown above, resized to save space \nSec. 3.4 In-context Few-shot Learning Figure 8: Zero-shot performance under the challenging scenario of reading a speed meter. GPT-\n4V fails to read the speed meter accurately even with different ways of ZS prompting. Red highlights\nthe wrong answer. Check Section 3.4 for detailed discussions.\n20 In-context Few-shot (1-shot) \nPrompt: \nWhat is the read of the speed meter? \nThe yellow pointer is roughly at the middle between 80 and 100 mph. The middle between 80 and 100 is 90. \nThe speed is around 91 mph, as the pointer is just passing 90 mph. \nGPT-4V: \nIn this image, the yellow needle is roughly at the middle between 60 and 80 mph. The middle between 60 and  \n80 is 70. The speed is around 71 mph, as the needle is just passing 70 mph. \nSec. 3.4 In-context Few-shot Learning (a)\nIn-context Few-shot (1-shot) Sec. 3.4 In-context Few-shot Learning \nPrompt: \nWhat is the read of the speed meter? \nThe yellow pointer is  at the long tick between 20 and 40 mph. The long tick at the middle between 20 and 40 \nis 30. The speed is around 29 mph, as the pointer is close to 30 mph but not quite 30 yet. \nGPT-4V: \nThe yellow pointer is at the long tick between 40 and 60 mph. The long tick at the middle between 40 and 60 is  \n50. The speed is around 49 mph, as the pointer is close to 50 mph but not quite 50 yet. \n(b)\nFigure 9: One-shot (or prompting with multimodal example instruction) performance under the\nchallenging scenario of reading a speed meter. GPT-4V still fails with (a) dissimilar or (b) similar\n1-shot in-context example. Red highlights the wrong answer. Check Section 3.4 for detailed\ndiscussions.\n21 In-context Few-shot (2-shot) \nPrompt: \nWhat is the read of the speed meter? \nThe yellow pointer is roughly at the middle between 80 and 100 mph. The middle between 80 and 100 is 90. \nThe speed is around 91 mph, as the pointer is just passing 90 mph. \nThe yellow needle is roughly at the middle between 20 and 40 mph. The middle between 20 and 40 is 30. The \nspeed is around 29 mph, as the needle is close to 30 mph but not quite 30 yet. \nGPT-4V: \nThe yellow pointer is at the long tick between 0 and 20 mph. The long tick at the middle between 0 and 20 is  \n10. The speed is around 9 mph, as the pointer is close to 10 mph but not quite 10 yet .\nSec. 3.4 In-context Few-shot Learning Figure 10: Two-shot performance under the challenging scenario of reading a speed meter. GPT-\n4V now can read the speed accurately. Green highlights the correct answer. Check Section 3.4 for\ndetailed discussions.\n22 Zero-shot \nGPT-4V: \nBased on the graph, the year with the highest average gas price for the month of June is 2021, with a price of approximately $3.32 per  \ngallon \nZero-shot (think step-by-step) \nPrompt: \nIn the graph, which year has the highest average gas price for the month of June? Let's think step-by-step. \nGPT-4V: \nStep 1: Locate the month of June on the x-axis. \nStep 2: Look at the data points for each year in the month of June. \nStep 3: Compare the data points to see which year has the highest value. \nBased on these steps, we can see that the year 2021 has the highest average gas price for the month of June, with a value of  \napproximately $3.32 per gallon. \nZero-shot (w/ text instructions) \nPrompt: \nIn the graph, which year has the highest average gas price for the month of June? Step 1, read the title of the graph; Step 2 read the \nlegend of the graph to learn the correspondence between year and line color; Step 3 look at the graph and identify the month of June on \nthe x-axis; Step 4, look at the data points for each line in the month of June; Step 5, Compare the data points of each line to determine \nwhich one has the highest average gas price in June; Step 6, identify the color of that line, and match to the year in the legend on top. \nGPT-4V: \nBased on the above steps, we can see that the year 2021 (represented by the blue line) has the highest average gas price for the month  \nof June, at $3.32 per gallon.  Note: Same query image as shown above, resized to save space \nZero-shot (w/ text instructions, think step-by-step) \nPrompt: \nIn the graph, which year has the highest average gas price for the month of June? Step 1, read the title of the graph; Step 2 read the \nlegend of the graph to learn the correspondence between year and line color; Step 3 look at the graph and identify the month of June on \nthe x-axis; Step 4, look at the data points for each line in the month of June; Step 5, Compare the data points of each line to determine \nwhich one has the highest average gas price in June; Step 6, identify the color of that line, and match to the year in the legend on top. \nLet\u2019s think step by step. \nGPT-4V: \nStep 1: The title of the graph is \"National Gas Price Comparison / 2019-2023\". This tells us that the graph is comparing gas prices in the \nUnited States from 2019 to 2023. \nStep 2: The legend on top of the graph shows that the years 2019, 2020, 2021, 2022, and 2023 are represented by different colors: \norange, black, blue, green, and teal respectively. \nStep 3: Looking at the x-axis, we can see that the month of June is represented. \nStep 4: In the month of June, we can see that the data points for each line are as follows: - 2019 (orange): around $2.75 - 2020 (black): \naround $2.25 - 2021 (blue): around $3.32 - 2022 (green): around $3.00 - 2023 (teal): around $2.90 \nStep 5: Comparing these data points, we can see that the highest average gas price in June is $3.32. \nStep 6: The color of the line representing $3.32 is blue, which corresponds to the year 2021 in the legend. \nTherefore, the year with the highest average gas price for the month of June is 2021.  Sec. 3.4 In-context Few-shot Learning \nPrompt: \nIn the graph, which year has the highest average gas price for the month of June? \nNote: Same query image as shown above, resized to save space \nNote: Same query image as shown above, resized to save space \nFigure 11: Zero-shot performance under the challenging scenario of reading a line plot. GPT-4V fails\nto answer the question even with different ways of ZS prompting. Red highlights the wrong answer.\nCheck Section 3.4 for detailed discussions.23 In-context Few-shot (1-shot) Sec. 3.4 In-context Few-shot Learning \nPrompt: \nIn the graph, which year has the highest average gas price for the month of June? \nThis graph is a line plot for national gas price comparison from 2016 until 02/04/2019. The legend on top \nshows the line color of each year, red (2019), blue (2018), green (2017)  and orange (2016). Since the data \nis reported until Feb. 2019, only 3 years have datapoints for the month of June, 2018 (blue), 2017 (green) \nand 2016 (orange). Among them, blue line for 2018 is at the top for the month of June.  Hence, the year with \nthe highest average gas price for the month of June is 2018. \nGPT-4V: \nThis graph is a line plot for national gas price comparison from 2019 until 01/17/2023. The legend on top \nshows the line color of each year, red (2023), blue (2022), green (2021), black (2020) and orange (2019). \nSince the data is reported until Jan. 2023, only 4 year have datapoints for the month of June, 2022 (blue), \n2021 (green), 2020 (black) and 2019 (orange). Among them, red line for 2023 is at the top for the month of  \nJune. Hence, the year with the highest average gas price for the month of June is 2023.  \nFigure 12: One-shot (or prompting with multimodal example instruction) performance under the\nchallenging scenario of reading a line plot. GPT-4V still fails with 1-shot in-context example. Red\nhighlights the wrong answer. Check Section 3.4 for detailed discussions.\n24 In-context Few-shot (2-shot) \nPrompt: \nIn the graph, which year has the highest average gas price for the month of June? \nThis graph is a line plot for national gas price comparison from 2015 until 12/10/2018. The legend on top \nshows the line color of each year, red (2018), orange (2017), green (2016)  and orange (2017). Since the \ndata is reported until Dec. 2018, all 4 years have datapoints for the month of June. Among them, red line for \n2018 is at the top for the month of June.  Hence, the year with the highest average gas price for the month of \nJune is 2018. \nThis graph is a line plot for national gas price comparison from 2016 until 02/04/2019. The legend on top \nshows the line color of each year, red (2019), blue (2018), green (2017)  and orange (2016). Since the data \nis reported until Feb. 2019, only 3 years have datapoints for the month of June, 2018 (blue), 2017 (green) \nand 2016 (orange). Among them, blue line for 2018 is at the top for the month of June.  Hence, the year with \nthe highest average gas price for the month of June is 2018. \nGPT-4V: \nThis graph is a line plot for national gas price comparison from 2019 until 01/17/2023. The legend on top \nshows the line color of each year, red (2023), blue (2022), green (2021), orange (2020) and black (2019). \nSince the data is reported until Jan. 2023, only 4 years have datapoints for the month of June, 2022 (blue), \n2021 (green), 2020 (orange) and 2019 (black). Among them, blue line for 2022 is at the top for the month of \nJune. Hence, the year with the highest average gas price for the month of June is 2022.  Sec. 3.4 In-context Few-shot Learning \nFigure 13: Two-shot performance under the challenging scenario of reading a line plot. GPT-4V now\ncan answer the question of \u201cwhich year has the highest average gas price for the month of June?\u201d\ncorrectly. Check Section 3.4 for detailed discussions. Green highlights the correct answer.\n25 4 Vision-Language Capability\nUnderstanding and describing visual information plays a crucial role in human cognition. In this\nsection, we will investigate how GPT-4V can be utilized to comprehend and interpret the visual world.\nWe will start by examining the model\u2019s ability to generate open-ended descriptions for generic visual\ncaptioning.\nMoving forward, in Section 4.2, we will explore the application of GPT-4V in more advanced tasks,\nsuch as spatial relationship analysis, object localization, object counting, and dense captioning. In\nSection 4.3, we will delve into the model\u2019s capacity for multimodal knowledge and commonsense\nreasoning, and study whether the model can understand the context and relationships between\ndifferent types of information.\nAdditionally, in Section 4.4, we will assess the model\u2019s capability to extract and analyze information\nfrom various sources, including scene text, tables, charts, and documents. In Section 4.5, we will\nexplore GPT-4V\u2019s ability in comprehending and generating descriptions in multilingual scenarios.\nLastly, in Section 4.6, we will investigate the model\u2019s coding proficiency with visual information,\nexploring its ability to perform tasks with selected examples.\n4.1 Image Description on Diverse Domains\nWe access the model\u2019s capability and generalizability by providing a single image-text pair as input.\nWe prompt GPT-4V to generate natural language descriptions covering a variety of topics listed\nbelow.\nCelebrity recognition. Recognizing human appearance [ 49,80] presents a significant challenge due\nto its inherent variability. To assess GPT-4V\u2019s capabilities to recognize and describe the celebrities,\nwe conduct an experiment by providing a text prompt, \u201cDescribe the image,\u201d along with an input\ncelebrity image. In the top row of Figure 14, we observe that GPT-4V accurately identifies the\neight celebrities, despite their diverse backgrounds and fields. Furthermore, when we present a more\nspecific query, \u201cWho is the person in the image and what is the person doing?,\u201d as shown in the\nbottom row of Figure 14, GPT-4V comprehends that the current President of the United States is\ndelivering a speech at the 2023 G7 Summit. This illustrates the model\u2019s ability to generalize and\nhandle novel scenarios, such as the 2023 G7 Summit, which was not part of its training data.\nLandmark recognition. Landmarks exhibit considerable variations in appearance due to factors\nsuch as viewpoint changes, lighting conditions, occlusions, and seasonal changes. Recognizing\nlandmarks under these variations requires models to generalize well and handle the vast range of\nvisual appearances [ 152,5]. In the experiments, we employ a straightforward text prompt, \u201cDescribe\nthe landmark in the image,\u201d to test the model\u2019s capability. As shown in Figures 15-16, GPT-4V\ngenerates accurate and open-ended descriptions for each test image. For example, it accurately\nrecognizes Space Needle located in Seattle, Washington, understanding that the tower was built for\nthe 1962 World\u2019s Fair and has since become a symbol of the city. We have similar observations for\nother tested photos as well. The generated descriptions go beyond simple labels or generic phrases,\nproviding vivid and detailed narratives that capture the essence of the landmark.\nFood recognition. Recognizing food or dishes is a fascinating task [ 20,95], but it can be challenging\nto tackle due to the wide range of appearances and potential occlusions caused by other objects or\noverlapping ingredients. In our experiments, we employ a straightforward text prompt, asking the\nsystem to \u201cDescribe the name of the dish,\u201d for testing purpose. Figure 17 demonstrates the accurate\nrecognition of various dishes by GPT-4V. Additionally, GPT-4V effectively captures intricate details\nwithin the images, enabling it to identify specific ingredients, garnishes, or cooking techniques present\nin a dish.\nMedical image understanding. Medical images, such as X-rays and CT scans, can have large\nvariability due to patient populations and imaging equipment. Additionally, interpreting the visual\ncontent of these images requires expert knowledge. In Figure 18, we access GPT-4V\u2019s performance\nby providing the prompt, \u201cDescribe the image.\u201d The results show that GPT-4V recognizes both the\nteeth and jaw bones in the given X-ray. Furthermore, when we prompt with \u201cAre there wisdom teeth\nthat needs to be removed in this x-ray image?\u201d GPT-4V performs reasoning with the visual context,\nand explains that the wisdom teeth on the bottom left and right sides of the jaw are not fully emerged\nfrom the gum line, and this could be a reason for removal. We also conduct testing with other medical\n26 images, as shown in Figure 19. For these experiments, we use prompts such as \u201cWhat\u2019s wrong?\u201d\nor \u201cLook at the CT scan, tell me what\u2019s wrong.\u201d The observations reveal that GPT-4V can identify\ncommon conditions such as a Jones fracture. It could also point out potential concerns based on the\nCT scan of the lung. The experiments demonstrate GPT-4V\u2019s basic understanding of medical images.\nWe discuss the application of GPT-4V to the medical domain in Section 9.3.\nLogo recognition. We examine GPT-4V\u2019s ability in logo recognition. In Figure 20, we initiate\nthe experiments by providing the text prompt, \u201cDescribe the image.\u201d GPT-4V accurately identifies\nthe three logos depicted in the image. We then proceed to ask a more specific question, \u201cDescribe\nthe logos in details,\u201d GPT-4V provides elaborate descriptions, including the design, style, and\nrepresentation for each logo, respectively. Expanding the evaluation to a more challenging in-the-wild\nscenario, as shown in Figure 21, we experiment with logos that may be partially occluded, distorted,\nor situated in cluttered backgrounds. We employ the text prompt \u201cDescribe both the image and\nlogo in details\u201d for the in-the-wild experiment. As shown in Figure 21, GPT-4V demonstrates\nstrong capability in understanding logos in difficult scenarios. Notably, GPT-4V can also provide\ndescriptions for novel or emerging logos and icons, such as the recently released Microsoft 365\nCopilot.\nScene understanding. Scene understanding [ 76,32,154] is an important task in computer vision.\nWe examine the model\u2019s capability by providing a simple query \u201cDescribe the image.\u201d In Figure 22,\nGPT-4V is able to describe the road and the location and color of the vehicles. It can also read the\nsign and notice the speed limit for this road.\nCounterfactual examples. We conduct experiments by randomly selecting counterfactual examples\nfrom [ 78]. In Figure 23, we observe that GPT-4V correctly describes the image contents when faced\nwith misleading questions or instructions.\n27 Prompt: \nWho is the person in the image and what is the \nperson doing? \nGPT-4V: \nThe person in the image is the current President of \nthe United States, Joe Biden. He is standing at a \npodium, likely delivering a speech, at the G7 Summit \nin Hiroshima, Japan in 2023. \nPrompt: \nWho is the person in the image and what is the \nperson doing? \nGPT-4V: \nThe person in the image is Jensen Huang, the CEO \nand co-founder of NVIDIA. He is holding and \npresenting a product from NVIDIA, likely a graphics \nprocessing unit (GPU).  Prompt: \nDescribe the image \nGPT-4V: \nThe image shows a collage of 8 prominent personalities from different fields. On the top row, from left to right, \nthere is singer Justin Bieber, footballer Lionel Messi, footballer Cristiano Ronaldo, and entrepreneur Elon Musk. \nOn the bottom row, from left to right, there is model and reality TV star Kylie Jenner, singer Taylor Swift, actor \nRobert Downey Jr., and actor Scarlett Johansson. All individuals are shown in different poses and outfits, with \nvarious backgrounds. Celebrity Recognition and Description Sec. 4.1 Image Description on Diverse Domains \nFigure 14: Results on celebrity recognition and description. GPT-4V can recognize a variety of\ncelebrities and describe the visual information (including their profession, action, background, and\nthe event) in details. Check Section 4.1 for detailed discussions.\n28 Prompt: \nDescribe the landmark in the image. \n.\nGPT-4V: \nThe landmark in the image is the Space Needle, an \nobservation tower located in Seattle, Washington. It \nstands 605 feet tall and offers panoramic views of the \ncity and surrounding areas. The tower was built for \nthe 1962 World\u2019s Fair and has since become a \nsymbol of Seattle and a popular tourist attraction. The \ntower has a futuristic design with a large circular \nobservation deck at the top and a restaurant that \nrotates 360 degrees. The base of the tower is \nsurrounded by a park with trees and green spaces. Landmark Recognition and Description \nPrompt: \nDescribe the landmark in the image. \n.\nGPT-4V: \nThis image shows the Kinkaku-ji Temple, also known \nas the Golden Pavilion, located in Kyoto, Japan. The \ntemple is a three-story Buddhist temple that is \ncovered in gold leaf and sits on a pond, creating a \nstunning reflection on the water. The surrounding \nlandscape is filled with trees and rocks, creating a \npeaceful and serene atmosphere. It is a popular \ntourist attraction and a designated National Special \nHistoric Site. \nPrompt: \nDescribe the landmark in the image. \nGPT-4V: \nThe landmark in the image is the statue of George M. \nCohan in Times Square, New York City. It is a bronze \nstatue that stands on a granite pedestal in Duffy \nSquare, which is the northern part of Times Square. \nThe statue is a tribute to George M. Cohan, an \nAmerican entertainer, playwright, composer, and \nproducer, who is considered the father of American \nmusical comedy. The statue was unveiled in 1959 \nand has since become a popular tourist attraction in \nTimes Square. Prompt: \nDescribe the landmark in the image. \n.\nGPT-4V: \nThe landmark in the image is the famous Lombard \nStreet in San Francisco, California, known for its \nsteep hairpin turns. The street is lined with colorful \nflowers and shrubbery, and there are houses and \nbuildings on both sides. In the background, you can \nsee the city skyline and the Coit Tower on Telegraph \nHill. This street is a popular tourist attraction, and it is \noften considered one of the most crooked streets in \nthe world. \nSec. 4.1 Image Description on Diverse Domains Figure 15: Results on landmark recognition and description. GPT-4V accurately recognizes the\nlandmarks in the test images. It also generates vivid and detailed narratives that capture the essence\nof the landmarks. Check Section 4.1 for detailed discussions.\n29 Landmark Recognition and Description \nPrompt: \nDescribe the landmark in the image. \n.\nGPT-4V: \nThe landmark in the image is the Taipei 101 building, \nlocated in Taipei, Taiwan. This building, with a height \nof 508 meters (1,667 feet), was the tallest building in \nthe world from 2004 to 2010. It has 101 floors above \nground and 5 floors below ground, and is a popular \ntourist attraction in Taipei. The design of the building \nresembles a bamboo plant, symbolizing strength and \nresilience. \nPrompt: \nDescribe the landmark in the image. \n.\nGPT-4V: \nThe landmark in the image is the Fushimi Inari \nTaisha Shrine in Kyoto, Japan. It is famous for its \nthousands of vermilion torii gates that line the paths \nleading up to the shrine. The gates represent the \nboundary between the secular world and the sacred \nworld, and are donated by individuals and businesses \nto show their devotion to the deity Inari. The image \nshows a pathway lined with these gates leading up \ntowards the shrine, with stone steps in the center. \nPrompt: \nDescribe the landmark in the image. \n.\nGPT-4V: \nThe landmark in the image is the Manhattan Bridge, \nas seen from the DUMBO neighborhood in Brooklyn, \nNew York. The bridge, which spans the East River, \nconnects lower Manhattan with Brooklyn. It is a \nsuspension bridge with two large towers and was \ncompleted in 1909. In the image, the bridge is framed \nby two brick buildings, creating a unique \nview of the landmark. \nPrompt: \nDescribe the landmark in the image. \n.\nGPT-4V: \nThe landmark in the image is the interior of the \nTrump Tower in New York City. The focal point is the \nTrump Grill, which is a restaurant located on the \nlower level of the tower. The interior is decorated with \nred and gold accents, marble walls and columns, and \na festive holiday display with a Christmas tree and \npoinsettias. The space is opulent and luxurious, \ntypical of the Trump brand. \nSec. 4.1 Image Description on Diverse Domains Figure 16: Results on landmark recognition and description. GPT-4V accurately recognizes the\nlandmarks in the test images. It also generates vivid and detailed narratives that capture the essence\nof the landmarks. Check Section 4.1 for detailed discussions.\n30 Prompt: \nDescribe the name of the dish. \n.\nGPT-4V: \nThe dish in the image is a bowl of Mapo Tofu, a \npopular Chinese dish from the Sichuan province. It \nconsists of soft tofu cubes in a rich and spicy sauce \nmade from chili peppers, fermented bean paste, and \nSichuan peppercorns. The dish is garnished with thin \nslices of green onions. Food Recognition and Description \nPrompt: \nDescribe the name of the dish. \n.\nGPT-4V: \nUnagi Donburi is the name of the dish that is \noriginated from Japan. In Japanese cuisine, it is a \ntype of rice bowl dish typically served in a deep bowl. \nThe word \u201cUnagi\u201d means freshwater eel in Japanese \nand \u201cdonburi\u201d refers to a large bowl served with rice \nand other toppings. Unagi Donburi is a dish made \nwith grilled eel and a type of sweet and savory sauce \ncalled \u201cunagi sauce\u201d over white rice. It is often served \nwith pickled vegetables, sesame seeds, green \nonions, and seaweed. \nPrompt: \nDescribe the name of the dish. \n.\nGPT-4V: \nThis dish is called a Cajun Sampler. It is a sampler \nplatter that consists of four small bowls of traditional \nCajun dishes - gumbo, jambalaya, red beans and \nrice, and crawfish \u00e9touff\u00e9e. The bowls are served \nwith a side of sliced bread for dipping. Each bowl is \nfilled with white rice and topped with green onions. It \nis a popular dish in Louisiana and a great way to try a \nvariety of Cajun flavors. Prompt: \nDescribe the name of the dish.. \n.\nGPT-4V: \nThis dish is called Braised Short Rib with Chimichurri. \nIt features a succulent short rib that has been slow \ncooked until tender and topped with a flavorful \nchimichurri sauce. It is served alongside roasted \nvegetables for a complete meal. \nSec. 4.1 Image Description on Diverse Domains Figure 17: Results on food recognition and description. GPT-4V recognizes various dishes. It\nalso identifies specific ingredients, garnishes, or cooking techniques present in a dish image. Check\nSection 4.1 for detailed discussions.\n31 Medical Image Understanding \nPrompt: \nDescribe the image \nGPT-4V: \nThe image is a black and white X-ray of a human skull, viewed from the front. The top and bottom of the skull are \nvisible. The teeth and jaw bones are clearly visible. The nasal cavity and eye sockets are also visible but not as \nclear. The image is labeled with an \u201cL\u201d on the bottom right corner, indicating that it is the left side of the skull. \nSec. 4.1 Image Description on Diverse Domains \nPrompt: \nAre there wisdom teeth that needs to be removed in this x-ray image? \nGPT-4V: \nIt is difficult to say for sure whether the wisdom teeth need to be removed from this X-ray image, as there are \nother factors that should be considered, such as the age of the patient and the position of the teeth. However, it \nappears that the wisdom teeth on the bottom left and right sides of the jaw are partially impacted, meaning they \nare not fully emerged from the gum line. This could be a reason for removal, as impacted wisdom teeth can cause \npain, infection, and other problems. \nPrompt: \nWhich wisdom tooth is hardest to remove? \nGPT-4V: \nThe lower wisdom teeth, especially those that are impacted, are typically considered the hardest to remove. This \nis because the lower jawbone is denser than the upper jawbone, and the impacted wisdom teeth may be more \ndifficult to access. Additionally, the roots of the lower wisdom teeth are often longer and more curved than those \nof the upper wisdom teeth, making them more challenging to remove. \nNote: Same query image as shown above, resized to save space \nNote: Same query image as shown above, resized to save space Figure 18: Results on medical image understanding. GPT-4V recognizes both the teeth and jaw bones\nin the given X-ray, and explains that the partially emerged wisdom teeth on the bottom left and right\nsides of the jaw may necessitate removal. Check Section 4.1 for detailed discussions. The medical\nimages are collected from the Internet.\n32 Prompt: \nWhat\u2019s wrong? \nGPT-4V: \nThere appears to be a fracture or break in the fifth metatarsal bone (the bone on the outside of the foot that \nconnects to the pinky toe). This is commonly known as a \"Jones fracture.\" \nPrompt: \nLook at the CT scan of lung in the image below, tell me what's wrong. \nGPT-4V: \nThere appears to be multiple areas of consolidation and ground-glass opacities in both lungs, which could \nindicate a lung infection or inflammation. There is also a possible mass or nodule in the right upper lobe. Further \nevaluation and clinical correlation is needed to determine the exact diagnosis. \nMedical Image Understanding Sec. 4.1 Image Description on Diverse Domains Figure 19: Results on medical image understanding. GPT-4V can identify common conditions like a\nJones fracture. It could also point out potential concerns based on the CT scan of the lung. Check\nSection 4.1 for detailed discussions. The medical images are from the internet.\n33 Logo Recognition and Description \nPrompt: \nDescribe the image \nGPT-4V: \nThe image is composed of three different icons arranged horizontally, with a thin vertical line separating each icon. \nOn the left is an icon resembling the letter \u201cO\u201d with a blue horizontal line running through the middle. This is the \nlogo of the car brand BMW. In the middle is an icon composed of two overlapping triangles, with the points facing \nupwards. This is the logo of the car brand Infiniti. On the right is an icon composed of a shield-like shape with \nthree arrows pointing upwards. This is the logo of the car brand Maserati. \nSec. 4.1 Image Description on Diverse Domains \nPrompt: \nDescribe the logos in details \nGPT-4V: \nLeftmost logo is BMW\u2019s logo. It is a circle icon which is filled with black color. Inside the circle, there is a blue \nhorizontal line running through the middle. The area above the blue line is white and the area below the blue line \nis light blue. This logo represents a spinning propeller of an airplane, which is a nod to the company\u2019s history in \naviation. \nThe middle logo is the Infiniti logo. It is composed of two overlapping triangles, with the points facing upward. The \ntriangles are filled in with black color. The logo represents a mountain or a road, which is a nod to the company\u2019s \nethos of power, strength, and performance. \nThe rightmost logo is the Maserati logo. It is composed of a shield-like shape and has three red arrows pointing \nupward. The shield shape is filled in with a faint white color, and the area between the arrows is black. This logo is \na nod to the company\u2019s history in racing, with the arrows representing the company\u2019s victories. \nNote: Same query image as shown above, resized to \nsave space Figure 20: Results on logo recognition. GPT-4V correctly recognizes the logos and provides\ndetailed descriptions, including its design, color, shape, and symbol. Check Section 4.1 for detailed\ndiscussions.\n34 In-the-Wild Logo Recognition and Description \nPrompt: \nDescribe both the image and logo in details \nGPT-4V: \nThe image is of a McDonald\u2019s french fry container, filled with \ngolden brown french fries. The container is red, and has the \niconic McDonald\u2019s \u201cM\u201d logo on it. The background is a bright \nyellow, making the red container and fries stand out. \nThe logo is the McDonald\u2019s \u201cM\u201d which is a large golden arch, \nforming the letter \u201cM\u201d for McDonald\u2019s. It is one of the most \nrecognized logos in the world, and is often associated with the \nfast food chain. \nPrompt: \nDescribe both the image and logo in details \nGPT-4V: \nThe image shows a close-up of a person\u2019s foot wearing a white \nNike Air Force 1 sneaker. The sneaker is a low-top style with a \nwhite rubber sole and white laces. The shoe\u2019s upper is made of \na textured white leather, giving it a premium look. \nThe logo shown on the sneaker is the iconic Nike Swoosh. It is \ndisplayed on the side of the shoe in a silver color. The Swoosh \nis a curved, elongated checkmark shape with a pointed tip at \nthe end. It represents speed and motion, embodying the brand\u2019s \nathletic and sporty aesthetic. \nPrompt: \nDescribe both the image and logo in details \nGPT-4V: \nThe image shows a woman in traditional Japanese clothing \nstanding in front of the Olympic rings logo. The woman is \nwearing a purple kimono with a floral pattern and a red obi belt. \nShe is smiling and posing for the photo. \nThe Olympic rings logo is made up of five interlocking rings of \ndifferent colors: blue, yellow, black, green, and red. The rings \nare arranged in a way that they form a flower-like shape. The \nlogo is displayed on a grassy area with trees in the background. Prompt: \nDescribe both the image and logo in details \nGPT-4V: \nThe image shows two Coca-Cola bottles lying on a bed of ice cubes. \nThe bottles are wet, indicating that they are chilled and ready to drink. \nThe bottle on the left is a new grip bottle, as mentioned on the label. \nThe bottle on the right is a regular Coca-Cola bottle. Both bottles \nhave the iconic Coca-Cola logo on them. \nThe logo is written in a cursive font in white color, with a red \nbackground. The first \"C\" in Coca is larger than the rest of the letters, \nand the \"o\" in Cola is elongated. The logo is instantly recognizable \nand is synonymous with the brand. \nPrompt: \nDescribe both the image and logo in details \nGPT-4V: \nThe image shows two Starbucks cups, one is a clear plastic cup with \na lid and straw, and the other is a white paper cup with a lid. The clear \nplastic cup contains a cold drink with ice and whipped cream on top, \nwhile the white paper cup contains a hot drink. The cups are placed \non a wooden table with a blurred background of a Starbucks store. \nThe logo on the cups is the Starbucks logo, which is a green circle \nwith a white mermaid or siren in the center \nPrompt: \nDescribe both the image and logo in details \nGPT-4V: \nThe image is a screenshot from a presentation about Microsoft 365 \nCopilot. The image shows a man standing in front of a large screen \nwith the Microsoft 365 Copilot logo and some text on it. The man is \nwearing a blue sweater and is gesturing with his hands as he speaks. \nThe background is a modern-looking set with a green plant and a \ngray wall. \nThe logo for Microsoft 365 Copilot is a square with rounded corners. \nThe background of the logo is white, and the text \"Microsoft 365 \nCopilot\" is written in black. In the center of the logo, there is a colorful \nshape that resembles a cube with three sides visible. The colors of \nthe shape are blue, purple, and pink, and the shape is slightly tilted to \nthe right. The logo is simple and modern, with a clean and \nprofessional look. \nSec. 4.1 Image Description on Diverse Domains \nFigure 21: Results on in-the-wild logo recognition and description. GPT-4V demonstrates strong\ncapability in understanding logos in many scenarios, including occlusions, lighting conditions, and\norientations. GPT-4V can also describe novel icons, such as the recently released Microsoft 365\nCopilot. Check Section 4.1 for detailed discussions.\n35 Scene Understanding \nPrompt: \nDescribe the image \n.\nGPT-4V: \nThe image is a view of a road from the perspective of a car dashboard camera. The road has two lanes, and the \ncar is in the right lane behind a silver vehicle. There are two yellow signs on the side of the road warning drivers \nof an upcoming pedestrian crossing and a curve to the left. The sky is cloudy and gray, and trees and bushes \nline the sides of the road. Prompt: \nDescribe the image \nGPT-4V: \nThe image is a screenshot from a dashcam, taken from the inside of a car, showing the view of the road ahead. \nThe car is driving on a two-lane road, with a speed limit sign of 35 mph visible on the right side of the road. \nThere are trees on both sides of the road, and the sky is filled with dark clouds, suggesting that it might rain \nsoon. Orange cones are visible on the left side of the road, indicating a construction zone. In the distance, there \nis a bridge overpass. \nSec. 4.1 Image Description on Diverse Domains Figure 22: Results on scene understanding. GPT-4V is able to provide a detailed description regarding\nthe scenes and objects. Check Section 4.1 for detailed discussions.\n36 Prompt: \nCan you find the red ball near the gray cat in the \nimage? \nGPT-4V: \nSorry, I cannot answer this question as there is no \nred ball visible in the image. Counterfactual Examples Sec. 4.1 Image Description on Diverse Domains \nPrompt: \nCan you see the bright blue feathers on the bird's \nwing? \nGPT-4V: \nNo, I cannot see bright blue feathers on the bird's \nwing. It appears to have brown and white feathers. \nPrompt: \nCan you spot the pink elephant playing in the \ncorner? \nGPT-4V: \nSorry, I cannot because there is no pink elephant \nplaying in the corner of this image. There is only a \nchild wearing an orange shirt and a bear in the \nbackground. Prompt: \nCan you find the baseball player wearing a blue \nshirt? \nGPT-4V: \nNo, I cannot find a baseball player wearing a blue \nshirt in this image. All the players appear to be \nwearing black shirts. \nPrompt: \nTry to locate a large waterfall behind the trees. \nGPT-4V: \nSorry, I am not able to complete this task as there is \nno waterfall present in the given image. \nPrompt: \nCan you find the blue lighting fixture mounted to the \nceiling? \nGPT-4V: \nSorry, I cannot find a blue lighting fixture mounted to \nthe ceiling in this image. All I see is a black wrought \niron chandelier with white glass shades. \nFigure 23: Results on counterfactual examples. GPT-4V is able to provide factual descriptions\nregarding the scenes and objects in the images. Example images are from [ 78]. Check Section 4.1 for\ndetailed discussions.\n37 4.2 Object Localization, Counting, and Dense Captioning\nSpatial relationship understanding. Understanding the spatial relationship between humans and\nobjects in the image is a vital aspect of visual intelligence [ 61,14]. In Figure 24, GPT-4V showcases\npromising capabilities in this regard. It can identify the spatial relationship between the frisbee and\nthe man in the image. It can also recognize the spatial relationship between the man and the car in\nthe image, and point out that the camera perspective may affect their perceived size.\nObject counting. Figure 25 highlights our exploration of GPT-4V\u2019s capability in object counting. In\nour experiments, we employ the text prompt \u201cCount the number of X in the image\u201d to evaluate its\nperformance. The results indicate that GPT-4V can successfully count the number of objects, such\nas apples, oranges, and people, present in the image. However, challenges arise when objects are\noccluded, or the scene is cluttered, which can result in errors in the counting process. In the bottom\nleft of Figure 25, GPT-4V identifies 12 people, but the correct answer should be 11. This may be due\nto our limited text prompt used in this experiment, and further investigation in prompting techniques\nis needed.\nObject localization. Object localization [ 153,76,51] is a fundamental challenge in the field of\ncomputer vision. In our preliminary experiments, we address this task by utilizing a simple text\nprompt, \u201cLocalize each person in the image using a bounding box.\u201d The initial results of our object\nlocalization experiments are depicted in Figure 26. The findings suggest that GPT-4V demonstrates\nthe capability to generate bounding box coordinates in textual format, without separate textualized box\ntokens [ 25,140,129,26,82,105]. However, it is important to note that the generated bounding box\ncoordinates are not accurate. We rescaled the predicted bounding box coordinates during visualization.\nPromising localization results are observed when the scene or background is relatively simpler and\nless cluttered. Further prompting techniques are required to enhance object localization performance\nin more complex and crowded environments.\nDense captioning. Dense captioning [ 62,84] involves generating detailed description for each\nregion of interest in the given image. This advanced task in vision-language field typically requires a\ncomplex system that integrates multiple experts, such as object detector, celebrity recognition model,\nand image captioning model. In order to explore GPT-4V\u2019s capabilities in dense captioning, we\nuse an instructional prompt, as shown in Figure 27. The results are highly encouraging, as GPT-4V\nsuccessfully localizes and recognizes the individuals within the image, and then provides concise\ndescriptions for each scientist.\n38 Prompt: \nDoes the person bigger than the car? \nGPT-4V: \nNo, the car appears to be bigger than the person in this image. However, the perspective and distance of the \nobjects in the photo may affect their perceived size. Prompt: \nWhat is the spatial relation between the frisbee and the man? \nGPT-4V: \nBased on the image provided, the spatial relation between the frisbee and the man is that the frisbee is above \nand slightly in front of the man. It also appears to be moving away from the man, as if it has been thrown or \nreleased by him. Spatial Relationship Understanding Sec. 4.2 Object Localization, Counting, and Dense Captioning \nFigure 24: Results on spatial relationship understanding. GPT-4V recognizes the spatial relationship\nbetween the objects in the images. Example images are from [ 67,14]. Check Section 4.2 for detailed\ndiscussions.\n39 Object Counting \nPrompt: \nCount the number of apples in the image. \nGPT-4V: \nThere are 7 apples in the image. \nPrompt: \nCount the number of apples in the image. \nGPT-4V: \nThere are 16 apples in the image. \nPrompt: \nCount the number of oranges in the image. \nGPT-4V: \nThere are 3 oranges in the image. Prompt: \nCount the number of people in the image. \nGPT-4V: \nThere are 3 people in the image. \nPrompt: \nCount the number of people in the image. \nGPT-4V: \nThere are 4 people in the image. Prompt: \nCount the number of people in the image. \nGPT-4V: \nThere are 12 people in the image. \nSec. 4.2 Object Localization, Counting, and Dense Captioning Figure 25: Results on object counting. GPT-4V is able to determine the quantity of the specified\nobjects the image. Red highlights the wrong answer. Check Section 4.2 for detailed discussions.\n40 Object Localization Sec. 4.2 Object Localization, Counting, and Dense Captioning \nPrompt: \nLocalize each person in the image using bounding \nbox. The image size is (width, height) = (998,786) \nGPT-4V: \nThere is only one person in the image, so the \nbounding box for this person would be: \n- Top left: (254, 118) \n- Bottom right: (731, 693) \nExplanation: The top left point of the bounding box is \nat (x, y) = (254, 118), which is the top left corner of the \nperson in the image. The bottom right point of the \nbounding box is at (x, y) = (731, 693), which is the \nbottom right corner of the person in the image. This \nbounding box encompasses the entire person in the \nimage. \nVisualization of Detected Bbox: \nPrompt: \nLocalize each car in the image using bounding box. \nWhat is the image size of the input image? \nGPT-4V: \nThe image size of the input image is 500 x 281  pixels. \nBounding box for the black car: \n- Top left corner: (63, 114) \n- Bottom right corner: (196, 220) \nBounding box for the white car: \n- Top left corner: (287, 96) \n- Bottom right corner: (474, 220) \nVisualization of Detected Bbox:  \nPrompt: \nLocalize each person in the image using bounding box. What is the image size of the input image? \nGPT-4V: \nThe image size of the input image is 1000 x 333  pixels. \nBounding boxes for each person: \n- Person 1 (leftmost): (x1, y1) = (22, 108), (x2, y2) = (170, 300) \n- Person 2: (x1, y1) = (200, 80), (x2, y2) = (290, 270) \n- Person 3: (x1, y1) = (350, 70), (x2, y2) = (460, 280) \n- Person 4: (x1, y1) = (540, 80), (x2, y2) = (640, 270) \n- Person 5 (rightmost): (x1, y1) = (760, 100), (x2, y2) = (920, 290) \nNote: These bounding box coordinates are approximate and may not be exact. \nVisualization of Detected Bbox: \n frame sho Figure 26: Results on object localization. GPT-4V is able to generate and approximate the bounding\nbox coordinates for the specified objects in the image. When providing a simple text prompt only, the\nmodel may encounter challenges when dealing with more complex scenarios like object occlusions\nand cluttered scenes. Red highlights the wrong answer. We rescaled the predictions when visualizing\nthe bounding boxes. Check Section 4.2 for detailed discussions.\n41 Dense Captioning Sec. 4.2 Object Localization, Counting, and Dense Captioning \nPrompt: \nPlease follow the instructions \n1. Tell me the size of the input image; \n2. Localize each person in the image using bounding box; \n3. Recognize each person; \n4. Generate detailed caption for each bounding box. \nGPT-4V: \n1. The size of the input image is 1600 x 1067  pixels. \n2. Bounding box 1: (x1, y1) = (120, 250), (x2, y2) = (420, 950) - Person 1 \n    Bounding box 2: (x1, y1) = (500, 240), (x2, y2) = (800, 940) - Person 2 \n    Bounding box 3: (x1, y1) = (880, 230), (x2, y2) = (1180, 930) - Person 3 \n    Bounding box 4: (x1, y1) = (1260, 220), (x2, y2) = (1560, 920) - Person 4 \n3. Person 1: Yann LeCun \n    Person 2: Geoffrey Hinton \n    Person 3: Yoshua Bengio \n    Person 4: Andrew Ng \n4. Caption for bounding box 1: Yann LeCun, computer scientist and one of the founding fathers of artificial \nintelligence. \n    Caption for bounding box 2: Geoffrey Hinton, computer scientist and cognitive psychologist, known for his work \non artificial neural networks. \n    Caption for bounding box 3: Yoshua Bengio, computer scientist and co-founder of the Montreal Institute for \nLearning Algorithms. \n    Caption for bounding box 4: Andrew Ng, computer scientist and entrepreneur, known for his work on deep \nlearning and artificial intelligence. \nVisualization of Dense Captioning: \n frame sho Figure 27: Results on dense captioning. GPT-4V follows the text prompt and successfully generates\ndense captions for the input image. Red highlights the wrong answer. We rescaled the predictions\nwhen visualizing the bounding boxes. Check Section 4.2 for detailed discussions.\n42 4.3 Multimodal Knowledge and Commonsense\nJoke and meme. Jokes and memes often reference specific events, pop culture, or Internet trends. Un-\nderstanding these references requires being familiar with the relevant context and cultural knowledge.\nGrasping the visual elements, their relationship to the text, and the intended humorous effect can be\na complex task [ 99]. Moreover, memes are often user-generated, making them highly diverse and\never-expanding. To evaluate GPT-4V\u2019s ability in this domain, we input a pair of meme and text prompt\nto GPT-4V. The example text prompts include \u201cCan you explain the meme?\u201d and \u201cWhat is funny\nabout the image?\u201d Figure 28 shows the example results. We observe that GPT-4V has remarkable\nability to gather information from both visual and textual modalities, and then comprehend the humor\nembedded within memes.\nScience and knowledge. We further investigate GPT-4V\u2019s capability in tasks that requires reasoning\nwith scientific knowledge [ 85]. We conduct experiments by providing a text prompt question and\na corresponding image. The questions cover a wide range of topics, including geography, physics,\nbiology, and earth science. In Figures 29-31, we observe that GPT-4V is able to correctly answer\nthe science questions based on the visual context. For instance, in the bottom row of Figure 29,\nGPT-4V recognizes the average particle speed for both sample A and sample B. By considering the\nrelationship among particle speed, kinetic energy, and temperature, GPT-4V answers the question\ncorrectly. For another instance, as shown in the bottom row of Figure 30, GPT-4V takes into account\nthe visual arrows presented in the figure to identify the producer in the specific food web. Moreover,\nas shown in Figure 31, when we provide a more specific prompt, such as \u201cSuppose you are a teacher,\nplease use the figure to explain X,\u201d we observe the generated answer adopts a tutorial format and\nexplains the subject step by step.\nMultimodal commonsense. In Figure 32, we access the ability of GPT-4V in multimodal common-\nsense reasoning [ 148,52]. In our experiments, we observed that GPT-4V effectively utilizes the\nbounding boxes presented in the image as visual prompts ( e.g., [person1] and [person2]) to recognize\nthe actions performed by the individuals. As shown in the second example in Figure 32, based on\nthe formal dress worn by [person1] and [person2] and the floral decorations present in the scene, it\ncan be inferred that they are attending a wedding ceremony. Moreover, when we provide a more\nspecific input prompt, such as \"Suppose you are a detective, what can you infer from the visual\nclues?\", GPT-4V demonstrates the ability to discern numerous nuanced visual cues within the image\nand offers a list of plausible hypotheses.\n43 Joke and Meme \nPrompt: \nCan you explain this meme? \nGPT-4V: \nThis meme is poking fun at the idea of procrastination \nand how someone may put off doing a task until a \ncertain time, but if they miss that time, they will push \nit back even further. In this case, the person in the \nmeme planned to do something at 8 o\u2019clock, but \nsince it is now 8:05, they have decided to wait until 9 \no\u2019clock to do it. The image of Kermit the Frog relaxing \nin bed represents the person\u2019s desire to procrastinate \nand avoid the task. \nPrompt: \nWhat is funny about this image? \nGPT-4V: \nThere are a couple of things that people might find \nfunny about this image: \n- It is a cartoon of a rhinoceros painting, which is an \nunexpected and silly concept in itself. \n- The rhinoceros is painting a picture of itself, which \nadds an extra layer of humor. \n- The caption, \u201cYou see the world as you are!\u201d, is a \njoke about how we all have our own unique \nperspectives and can be a little self-centered at \ntimes. \nPrompt: \nCan you explain this meme? \nGPT-4V: \nThis meme highlights the deterioration of a person\u2019s \nhandwriting during an exam. The first two pages \nshow neat and legible handwriting, the middle page \nshows slightly messier handwriting, and the last two \npages show almost illegible handwriting. \nThe meme is poking fun at how, as an exam \nprogresses, a person\u2019s handwriting tends to get \nworse as they become more tired and rushed to \nfinish the exam. It also implies that the person\u2019s focus \nand attention to detail decrease towards the end of \nthe exam. \nPrompt: \nWhat is funny about this image? \nGPT-4V: \nThis image is funny for a couple of reasons. Firstly, it \nis a meme template that has been widely used and \nrecognized on the internet. The left side of the image \nis a screenshot from the reality TV show \"The Real \nHousewives of Beverly Hills\" where one of the cast \nmembers is angrily pointing and yelling. The right \nside of the image is a picture of a cat sitting at a \ndinner table with a plate of food in front of it, looking \nunimpressed. The humor in this image comes from \nthe contrast between the two sides - the angry, \ndramatic yelling on the left and the nonchalant cat on \nthe right. It also plays on the idea of cats acting like \nhumans and being unimpressed by our behavior. \nOverall, the image creates a funny and absurd \nsituation that makes people laugh. \nSec. 4.3 Multimodal Knowledge and Commonsense Figure 28: Results on joke and meme understanding. GPT-4V demonstrates the impressive capability\nto comprehend the humor embedded within memes. Check Section 4.3 for detailed discussions.\n44 Science and Knowledge \nPrompt: \nWhich of these oceans does the prime meridian intersect? \nGPT-4V: \nThe prime meridian intersects the following oceans: \n- Atlantic Ocean \n- Southern Ocean \n- Arctic Ocean \nPrompt: \nCompare the average kinetic energies of the particles in each sample. The diagrams below show two pure \nsamples of gas in identical closed, rigid containers. Each colored ball represents one gas particle. Both samples \nhave the same number of particles. Which sample has the higher temperature? Sample A or sample B? \nGPT-4V: \nBecause the average particle speed in sample A is higher than in sample B (600 m/s vs. 540 m/s), the particles in \nsample A have a higher average kinetic energy. Therefore, sample A has a higher temperature than sample B. \nSec. 4.3 Multimodal Knowledge and Commonsense Figure 29: Results on answering science questions. GPT-4V can understand the question textually\nand visually, and gather necessary information to answer the question. Example images are from\n[85]. Check Section 4.3 for detailed discussions.\n45 Prompt: \nWhich of these states is farthest north? \nGPT-4V: \nOut of the three states shown in the map - Arizona, Oklahoma, and Delaware, Delaware is the farthest north. \nPrompt: \nWhich of the organisms is the producer in this food web? \nGPT-4V: \nThe producers in this food web are the berries and flowers, grasses, and seeds. These are the organisms that \nuse energy from the sun to produce their own food through photosynthesis. \nScience and Knowledge Sec. 4.3 Multimodal Knowledge and Commonsense Figure 30: Results on answering science questions. GPT-4V can understand the question textually\nand visually, and gather necessary information to answer the question. Example images are from\n[85]. Check Section 4.3 for detailed discussions.\n46 Science and Knowledge \nPrompt: \nSuppose you are a teacher, please use the figure to explain the distinction between evaporation and \nevapotranspiration. \nGPT-4V: \nOkay class, today we are going to learn about the distinction between evaporation and evapotranspiration. Let\u2019s \ntake a look at this figure. \nFirst, let\u2019s understand what evaporation is. Evaporation is the process by which water is changed from its liquid \nstate to its gaseous state. This usually takes place at the surface of a liquid, like the ocean or a lake, when the \nwater molecules gain enough energy to break away and form water vapor. So in this figure, we can see that the \nwater in the lake and the dam is evaporating and becoming water vapor. \nNow, let\u2019s take a look at evapotranspiration. This is a combination of two processes - evaporation and \ntranspiration. Transpiration is the process by which plants release water vapor through their leaves. So, in this \nfigure, we can see that the trees and plants are releasing water vapor into the air, which is the transpiration part \nof evapotranspiration. At the same time, the water in the soil and the surface is also evaporating. This \ncombination of evaporation and transpiration is what we call evapotranspiration. \nIn summary, evaporation is the process by which water is changed from its liquid state to its gaseous state, while \nevapotranspiration is the combination of evaporation and transpiration. Both these processes contribute to the \nwater cycle and are important for maintaining the balance of water in our environment. \nSec. 4.3 Multimodal Knowledge and Commonsense \nFigure 31: Results on answering science questions. When we use a more specific text prompt like\n\u201cSuppose you are a teacher, please use the figure to explain X,\u201d we observe that GPT-4V can generate\na short tutorial for explaining the subject. Check Section 4.3 for detailed discussions.\n47 Multimodal Commonsense \nPrompt: \nWhat is [person3] doing? \nGPT-4V: \n[person3] is carrying a plate of food, likely serving it to the customers at the table. It appears that they are working \nas a waiter or server in a restaurant. \nPrompt: \nWhat are [person1] and [person2] doing? \nGPT-4V: \n[person1] and [person2] are walking down the aisle at a wedding ceremony. \nPrompt: \nSuppose you are a detective, what can you infer from the visual clues in the image? \nGPT-4V: \nFrom the visual clues in the image, I can infer that: \n- The room belongs to someone who is likely a young adult or teenager, as evidenced by the casual clothing and \nthe desk with a computer. \n- The person is likely a student or someone who works from home, as evidenced by the desk and computer. \n- The person is likely fashion-conscious, as evidenced by the multiple jackets and shoes. \n- The room is likely located in a colder climate, as evidenced by the jackets and boots. \n- The person is likely not very tidy, as evidenced by the cluttered desk and the clothes hanging on the door. \n- The room is likely located in an older or more rustic building, as evidenced by the exposed pipes and the \nworn-looking walls. \nSec. 4.3 Multimodal Knowledge and Commonsense Figure 32: Results on multimodal commonsense reasoning. Example images are from [ 148,52].\nCheck Section 4.3 for detailed discussions.48 4.4 Scene Text, Table, Chart, and Document Reasoning\nScene text recognition. Reading and understanding scene text in images is an important task in\nvision-language [ 118\u2013120,17]. In our experiments, we investigate GPT-4V\u2019s ability to recognize\nscene text by utilizing the input prompt \u201cWhat are all the scene text in the image?\u201d Figure 33\nshows the example results. We observe GPT-4V accurately identifies scene text in various scenarios,\nincluding both handwritten and printed text. In Section 4.5, we present further results on multilingual\nscenarios.\nVisual math reasoning. In Figure 34, GPT-4V demonstrates its capability in solving visual math\nproblems. In our experiments, we observe GPT-4V is able to extract essential information from the\nimage. For instance, in Figure 34, GPT-4V correctly identifies the presence of a right triangle (or\northogonal triangle) and determines that AB is 4 units and BC is 3 units. In addition, we note that\nGPT-4V tends to present solutions in a well-structured manner, solving the problem step by step,\nthereby showcasing its ability to provide clear explanations.\nChart understanding and reasoning. We further study GPT-4V\u2019s ability in chart understanding\nand reasoning. Figures 35-37 show the example results. In our preliminary explorations, GPT-4V\nexhibits the ability to provide detailed descriptions of charts. For example, in Figure 35, the model\ncorrectly explains the proposal process from the beginning to the end. In Figure 36, the model not\nonly understands the program in the given flow chat, but also translates the details to a python code.\nIn the bottom row of Figure 37, GPT-4V shows a clear understanding of both x- and y-axis, and\nexplains the key insight presented in the chart. Furthermore, in our experiments, we observe that\nGPT-4V can answer questions based on the chart. In the top row of Figure 37, GPT-4V correctly\ncalculates the average total fueling cost, excluding the Ford F150.\nTable understanding and reasoning. In Figure 38, we present our preliminary investigations into\ntable understanding and reasoning. Similar to the findings from chart experiments, GPT-4V shows\npromising results in understanding the details in the table, as well as in reasoning and accurately\nresponding to related questions.\nDocument understanding. Figure 39 shows the results on various types of documents, such as floor\nplan, poster, and exam paper. We observe GPT-4V demonstrates an understanding of the documents\nand provides reasonable responses. For instance, it accurately identifies the location of the bathroom\nfor the second bedroom in the floor plan. It also recognizes the Chinese dish \u201cHot dry noodles,\u201d and\nassociates it with the city of Wuhan by following the scene text. Moreover, GPT-4V is capable of\nreading an exam paper. It accurately reconstructs the table in Markdown, and then fills in the table\nwith the correct answers. We present more explorations in its coding ability in Section 4.6.\nIn Figure 40, we explore a more challenging case by providing a technical report [ 126] with multiple\npages as input. In our limited exploration, GPT-4V exhibits impressive results. It correctly describes\nthe main idea and their proposed method by considering the context across multiple pages. However,\nit may occasionally miss some implementation details. Please note that the dataset should contain\n1196+665=1861 examples, and the extracted features should include Histograms of Oriented Gradi-\nents (HOG). Instead of prompting all pages to the model simultaneously, we believe that exploring\nmore advanced prompting techniques, such as thinking step-by-step or employing in-context few-shot\napproaches, could potentially enhance the model\u2019s performance.\n49 Scene Text Recognition \nPrompt: \nWhat are all the scene text in the image? \nGPT-4V: \nThe scene text in the image are: \n- Malayan Tapir \n- Tapirus indicus \n- LOCATION: Malay Peninsula and \nSumatra, near water \n- RELATIVES: There are only five species \nof tapirs. They are most closely related to \nhorses and rhinos. \n- VANISHING? The forest provides tapirs \nwith shelter, food, and water, but forests \nare rapidly disappearing in Malaysia and \nSumatra. Sec. 4.4 Scene Text, Table, Chart, and Document Reasoning \nPrompt: \nWhat are all the scene text in the image? \nGPT-4V: \nThe scene text in the image are: \n- Vacation \n- Beach \n- Smile \n- Summer \n- Watermelon \n- Flip flops \n- Lemonade \nPrompt: \nWhat are all the scene text in the image? \nGPT-4V: \nThe scene text in the image is \n\"3,642,039,031,055\" . \nPrompt: \nWhat are all the scene text in the image? \nGPT-4V: \nThe scene text in the image are: \n- (NVDA) \n- NVIDIA \n- 356.85 \n- +51.47 \n- +16.85%  \nPrompt: \nWhat are all the scene text in the image? \nGPT-4V: \nThe scene text in the image are: \n- \"Royal\" \n- \"London\" \nPrompt: \nWhat are all the scene text in the image? \nGPT-4V: \nThere are three texts in the image: \n1) \u201cCOVID-19 TESTING\u201d \n2) \u201cPLEASE HAVE ID & INSURANCE \nCARD READY\u201d \n3) \u201cW @UW Medicine\u201d (this is the \nlogo/branding for the University of \nWashington Medicine \nPrompt: \nWhat are all the scene text in the image? \nGPT-4V: \nThe only scene text in the image is \"The \nEarth without art is just 'eh'\". \nPrompt: \nWhat are all the scene text in the image? \nGPT-4V: \nThe scene text in the image is \u201cThe Road \nto Hollywood: how some of us got here\u201d \nPrompt: \nWhat are all the scene text in the image? \nGPT-4V: \nThe scene text in the image are: \n- \"Westfield\" \n- \"westfield.com.au\" \nFigure 33: Results on scene text recognition. GPT-4V can recognize scene text in many challenging\nscenarios. Check Section 4.4 for detailed discussions.\n50 Visual Math \nPrompt: \nWhat is x in the equation? \nGPT-4V: \nWithout any context or \ninformation about what the \nequation is representing, we \ncan only assume that \"x\" is a \nvariable or unknown value in \nthe equation. \nIn this equation, (x + 3) is \nbeing squared and is equal to \n4. To solve for x, we can take \nthe square root of both sides \nof the equation to get rid of the \nexponent on the left side. This \nwould give us: \n\u221a((x + 3)\u00b2) = \u221a4 \nx + 3 = \u00b12 \nFrom here, we can solve for x \nby subtracting 3 from both \nsides. This would give us: \nx = -1 or x = -5 \nSo, in this equation, x could be \neither -1 or -5. \nSec. 4.4 Scene Text, Table, Chart, and Document Reasoning \nPrompt: \nWhat is the value of d in the last equation? \nGPT-4V: \n1) To solve the first equation, we need to \nisolate the x variable on one side of the \nequation and the constants on the other \nside. We can start by subtracting 4x from \nboth sides to get rid of the x term on the \nright side of the equation. This gives us: \n4x + 11 = 14 \nNext, we can subtract 11 from both sides \nto get rid of the constant term on the left \nside of the equation. This gives us: \n4x = 3 \nFinally, we can divide both sides by 4 to \nisolate the x variable on the left side of the \nequation. This gives us: \nx = 3/4 \n2) To solve the second equation, we can \nstart by subtracting 7d from both sides to \nget rid of the d term on the left side of the \nequation. This gives us: \n-4 = 4d - 9 \nNext, we can add 9 to both sides to get rid \nof the constant term on the right side of \nthe equation. This gives us: \n5 = 4d \nFinally, we can divide both sides by 4 to \nisolate the d variable on the right side of \nthe equation. This gives us: \nd = 5/4 \nSo, the value of d in the last equation is \n5/4. \nPrompt: \nWhat is the value of x in the image? \nGPT-4V: \nThe value of x in the image can be \nfound using the Pythagorean \ntheorem, which states that in a right \ntriangle, the square of the \nhypotenuse (the longest side, \nopposite the right angle) is equal to \nthe sum of the squares of the other \ntwo sides. \nIn this case, the hypotenuse is the \nside AC, and the other two sides are \nAB and BC. We know that AB is 4 \nand BC is 3, so we can plug these \nvalues into the equation: \na\u00b2 + b\u00b2 = c\u00b2 \n3\u00b2 + 4\u00b2 = c\u00b2 \n9 + 16 = c\u00b2 \n25 = c\u00b2 \nTherefore, c (the length of the \nhypotenuse) is 5. Since x is the \nlength of the hypotenuse, x is equal \nto 5. \nFigure 34: Results on visual math reasoning. GPT-4V is able to comprehend and solve visual math\nproblems with a well-structured solution. Check Section 4.4 for detailed discussions.\n51 Flow Chart Understanding \nPrompt: \nExplain the figure \nGPT-4V: \nThis figure represents a flow chart of a team\u2019s proposal process. The process begins with a team member \nsuggesting a project in a meeting. The team then discusses whether it is a good idea or not. If the idea is not \nconsidered a good one, the team rejects it and the process ends. \nIf the team thinks it might be a good idea but needs more information, a team member conducts further research \nand refines the idea into a proposal. The team then reviews the proposal and provides feedback. If the proposal \nis not okay, then the team member revises it and the team reviews it again. \nIf the proposal is okay, the team sends it to a final decision-making process. If the proposal is approved, then the \nteam proceeds with the project. This is the end of the proposal process. \nSec. 4.4 Scene Text, Table, Chart, and Document Reasoning \nPrompt: \nWhat is the estimated average standing charge for the year 2023 as shown in the image? \nGPT-V: \nBased on the image, we can see that the estimated average standing charge for the year 2023 is around \u00a3280 \n- \u00a3250. \nWe can come to this conclusion by looking at the \u201cElectricity price cap Q3\u201d and \u201cElectricity price cap Q4\u201d \nsections, which show a range of \u00a3280 to \u00a3250 for the year 2023. \nHowever, it is important to note that these are only estimates and the actual standing charge may vary \ndepending on various factors such as market conditions, government policies, and supplier pricing. \nFigure 35: Results on flow chart understanding. GPT-4V correctly describes the proposal process in\ndetails. Check Section 4.4 for detailed discussions.\n52 Flow Chart Understanding \nPrompt: \nCan you translate the flowchart to a python code? \nGPT-4V: \nSure, here\u2019s the Python code: \nnum1 = input(\"Enter first number: \") num2 = input(\"Enter second number: \") if num1 > num2: \nprint(num1) else: print(num2) \nExplanation: \n- The first two lines correspond to the \u201cInput num1\u201d and \u201cInput num2\u201d blocks in the flowchart, where the user is \nprompted to enter two numbers. \n- The next two lines correspond to the \u201cnum1 > num2?\u201d block, where a conditional statement is used to check if \nthe first number is greater than the second number. \n- If the condition is true, the next line (\u201cDisplay num1\u201d) is executed, and the first number is printed to the screen. \n- If the condition is false, the else statement is executed, and the second number is printed to the screen \n(\u201cDisplay num2\u201d). \n- The code then stops, corresponding to the \u201cStop\u201d block in the flowchart. \nSec. 4.4 Scene Text, Table, Chart, and Document Reasoning \nFigure 36: Results on flow chart understanding. GPT-4V is able to translate the flow chart to a python\ncode. Check Section 4.4 for detailed discussions.\n53 Chart Understanding and Reasoning \nPrompt: \nWhat is the average total fueling cost excluding the Ford F150? \nGPT-4V: \nTo find the average total fueling cost excluding the Ford F150, we need to add the total fueling costs of the \nToyota RAV4 and Honda Civic and divide by 2 (since there are 2 vehicles in the calculation). \n(82.56 + 70.55) \u00f7 2 = 76.55 \nTherefore, the average total fueling cost excluding the Ford F150 is $76.55. \nSec. 4.4 Scene Text, Table, Chart, and Document Reasoning \nPrompt: \nExplain the figure. \nGPT-4V: \nThe figure is a graph that shows the relationship between the quality of a paper and its impact on a person's \ncareer. The x-axis represents the quality of the paper, which ranges from bad to creative and original. The \ny-axis represents the impact the paper has on a person's career, which ranges from nothing to lots of impact. \nThe graph shows that a bad paper has little to no impact on a person's career, while a creative and original \npaper has a significant impact. The impact of the paper on a person's career increases as the quality of the \npaper improves. This suggests that the quality of a person's work is an important factor in determining their \nsuccess in their career. \nFigure 37: GPT-4V shows promising results in understanding the details in the chart, as well as in\nreasoning and accurately responding to related questions. Check Section 4.4 for detailed discussions.\n54 Table Understanding and Reasoning \nPrompt: \nHow many families are earning more than 13000 and owns more than 2 cars? \nGPT-4V: \nFrom the given table, we can clearly see that there are a total of 113 families that earn more than Rs. 13000 and \nown more than 2 cars (25 families in the 13000-16000 range and 88 families in the 16000 or more range). \nSec. 4.4 Scene Text, Table, Chart, and Document Reasoning \nPrompt: \nWhich country has the highest profit margin? \nGPT-4V: \nBased on the data provided, the country with the highest profit margin is Australia, with a profit margin of \n19,4%. However, the difference between the top three countries (USA, China and Australia) is minimal, with \nless than 0,3% difference in profit margin. \nFigure 38: We observe GPT-4V can understand the details in the table, and answer related questions.\nCheck Section 4.4 for detailed discussions.\n55 Document Understanding \nPrompt: \nDescribe the image. \nGPT-4V: \nThis image is a floor plan for an apartment or \ncondominium. It shows the layout of the rooms and the \ndimensions. There is a terrace, dining area, living room, \nfoyer, two bedrooms, a master bathroom, and a guest \nbathroom. The floor plan is black and white and is a \ntwo-dimensional drawing. \nSec. 4.4 Scene Text, Table, Chart, and Document Reasoning \nPrompt: \nDescribe the poster. Which city is this dish from? \nGPT-4V: \nThe poster is an advertisement for a Chinese dish \ncalled \"Hot dry noodles.\" The poster has a red \nbackground with white Chinese characters at the \ntop that read \"Hot dry noodles, the taste of China.\" \nBelow the text is a photo of the dish, which \nconsists of noodles with meat and vegetables on \ntop. The dish is served in a green bowl with red \nchili peppers on the side. The text at the bottom of \nthe poster is in Chinese and describes the dish as \nbeing spicy, fragrant, and delicious. \nThis dish is from the city of Wuhan, located in the \nHubei Province of China. It is a popular street food \nin the city and is considered one of the \"10 great \nbreakfasts\" in China. \nPrompt: \nAnswer the question in the image. \nGPT-4V: \nLayer | Number on figure 1 \n--- | --- \nOuter core | 3 \nMantle | 2 \nCrust | 1 \nInner core | 4 \nVisualization of the generated results: \nPrompt: \nWhere is the bathroom for the second bedroom? \nGPT-4V: \nAccording to this floor plan, the bathroom for the \nsecond bedroom appears to be located directly \nadjacent to the bedroom. It is labeled \"WC\" and is \na small room measuring 5'2\" x 5'2\". \nNote: Same query \nimage as shown on \nleft, resized to \nsave space Figure 39: Results on document understanding. GPT-4V recognizes three different types of document\nand answers the questions correctly. Check Section 4.4 for detailed discussions.\n56 Document Understanding \nPrompt: \nThis is a paper \"Paper Gestalt.\" \nDescribe the paper in details \nand highlight their contribution. \nPage 1 \nPage 2 \nPage 3 \nSec. 4.4 Scene Text, Table, Chart, and Document Reasoning \nPage 4 \nPage 5 \nGPT-4V: \nThe paper \"Paper Gestalt\" is a research \npaper written by Carven von \nBearnensquash from the Department of \nComputer Science at the University of \nPhoenix. The paper discusses the visual \nquality of a computer science research \npaper and how it affects the overall \nimpression of the paper. \nThe paper begins with an introduction to \nthe concept of paper gestalt, which is the \noverall impression of a paper. The author \nargues that the visual quality of a paper is \nan important factor in creating a good \nimpression. The paper then discusses \nprevious work in this area and highlights \nthe contribution of this paper, which is the \nuse of a supervised learning algorithm to \nevaluate the visual quality of a paper. \nThe paper then presents the methodology \nand experimental results. The author \ncollected a dataset of 1196  computer \nscience research papers and used a \nsupervised learning algorithm to evaluate \nthe visual quality of the papers. The \nalgorithm was trained on a set of features \nextracted from the papers, such as  the \nnumber of figures, the number of  \nequations, and the number of references.  \nThe results showed that the algorithm was \nable to accurately predict the visual quality \nof a paper with a high degree of accuracy. \nThe paper concludes with a discussion of \nthe results and future work. The author \nsuggests that future work could focus on \nimproving the algorithm by incorporating \nmore features and training it on a larger \ndataset. The paper also suggests that the \nalgorithm could be used to evaluate the \nvisual quality of other types of documents, \nsuch as technical reports and \ndissertations. \nOverall, the contribution of this paper is the \nuse of a supervised learning algorithm to \nevaluate the visual quality of a computer \nscience research paper. This is a novel \napproach that could help researchers \nimprove the visual quality of their papers \nand make a better impression on their \nreaders. \nPrompt Continued Figure 40: Results on document understanding. GPT-4V reads a multi-page technical report, under-\nstands the content in each section, and provides a summary of the contribution of this technical report.\nRed highlights the wrong answer. Check Section 4.4 for detailed discussions.\n57 4.5 Multilingual Multimodal Understanding\nWe assess GPT-4V\u2019s ability in comprehending multiple languages and modalities. First, we explore\nthis capability by evaluating natural images without scene text, as depicted in Figure 41. In the first\nrow of the figure, we provide the input text prompt \u201cDescribe the image\u201d in Chinese, French, and\nCzech, respectively. GPT-4V recognizes the input text prompts in different languages, and generates\ncorrect image descriptions in corresponding languages. In the second row of Figure 41, we provide\nthe input text prompt in English and specify the output language. GPT-4V follows the instruction and\ngenerates correct descriptions in the desired languages. In the bottom row of Figure 41, we provide\nan input prompt in Spanish, and ask GPT-4V to generate image descriptions in 20 different languages.\nWe observe that GPT-4V can process both the input and output text in different languages.\nFurthermore, we explore a scenario involving multilingual scene text recognition, where the input\nimage may contain scene text in various languages. As shown in Figure 42, GPT-4V correctly\nidentifies and understands the scene text from different scenes. As shown in the first two rows\nof Figure 43, we observe that GPT-4V can recognize the scene text, and translate it to a different\nlanguage. In the bottom row of Figure 43, we provide a screenshot of a Wikipedia website which is\nin Catalan, and instruct GPT-4V to summarize the information in 20 different languages. GPT-4V\nnot only recognizes the text in Catalan but also generates precise summaries and translates them into\ndifferent languages. This showcases GPT-4V\u2019s ability to comprehend and translate multilingual scene\ntext.\nWe also explore the capability of multicultural understanding [ 147,77]. Figure 44 shows the example\nresults in such a scenario. We observe that GPT-4V is capable of understanding cultural nuances and\ngenerating reasonable multilingual descriptions for the wedding images given.\nIn our exploration, we found that GPT-4V seamlessly comprehends and correctly generates descrip-\ntions in different languages, highlighting its versatility in handling diverse linguistic contexts.\n58 Multilingual Image Description Sec. 4.5 Multilingual Multimodal Understanding \nPrompt: \nDescribe the image in French \nGPT-4V: \nC'est une image d'un petit chien de race \nCorgi. Il est de couleur marron et blanc et \nporte un collier avec une m\u00e9daille en \nforme d'os. Il est en train de marcher sur \nl'herbe verte avec la langue pendante. \nPrompt: \nDescribir la imagen en 20 idiomas diferentes \nGPT-4V: \n1. Ingl\u00e9s: This is a beautiful view of Machu Picchu in Peru. \n2. Espa\u00f1ol: Esta es una hermosa vista de Machu Picchu en Per\u00fa. \n3. Franc\u00e9s: C'est une belle vue de Machu Picchu au P\u00e9rou. Prompt: \n\u63cf\u8ff0\u9019\u5f35\u5716\u7247 \nGPT-4V: \n\u9019\u5f35\u5716\u7247\u662f\u4e00\u96bb\u5c0f\u8c93\u5728\u8349\u5730\u4e0a\u8d70\u8def\u7684\n\u756b\u9762\u3002\u5c0f\u8c93\u662f\u6a58\u8272\u7684\uff0c\u6709\u85cd\u8272\u7684\u773c\u775b\uff0c\n\u770b\u8d77\u4f86\u5f88\u53ef\u611b\u3002\u5b83\u7684\u8eab\u9ad4\u5f88\u5c0f\uff0c\u56db\u80a2\n\u7e96\u7d30\uff0c\u6b63\u5728\u5c0f\u5fc3\u7ffc\u7ffc\u5730\u5728\u8349\u5730\u4e0a\u884c\n\u8d70\u3002 \nPrompt: \nPopi\u0161te obr\u00e1zek \nGPT-4V: \nNa obr\u00e1zku je mal\u00e9 oran\u017eov\u00e9 kot\u011b s \nmodr\u00fdma o\u010dima. Kot\u011b stoj\u00ed na zelen\u00e9 \ntr\u00e1v\u011b a sm\u011b\u0159uje dop\u0159edu. Jeho t\u011blo je v \npohybu a vypad\u00e1, \u017ee pr\u00e1v\u011b b\u011b\u017e\u00ed. Na \npozad\u00ed je vid\u011bt dal\u0161\u00ed \u010d\u00e1st tr\u00e1vy a nejasn\u011b i \ndal\u0161\u00ed ko\u010dka. \nPrompt: \nDescribe the image in Chinese \nGPT-4V: \n\u8fd9 \u662f\u4e00\u53ea\u5c0f\u67ef\u57fa\u72ac\uff0c\u5b83\u5728\u8349\u5730\u4e0a\u8d70\u7740\uff0c\u5634\u5df4\n\u5f20 \u5f00\uff0c\u820c \u5934 \u4f38\u51fa\u6765\uff0c\u597d\u50cf\u5f88\u5f00\u5fc3\u7684 \u6837 \u5b50\u3002\u5b83\n\u7684\u8eab\u4e0a\u6709\u4e00\u6761 \u9879 \u5708\uff0c\u4e0a\u9762\u6302\u7740\u4e00\u4e2a\u9aa8 \u5934 \u5f62\n\u72b6\u7684\u724c\u5b50\u3002  Prompt: \nD\u00e9crire l\u2019image \nGPT-4V: \nIl s\u2019agit d\u2019une photo d\u2019un chaton roux \nmarchant sur l\u2019herbe verte. Le chaton a les \nyeux bleus et semble \u00eatre en train de \nmarcher avec prudence. Il est entour\u00e9 \nd\u2019herbe et on peut voir une partie d\u2019un \nautre chat en arri\u00e8re-plan. \nGPT-4V output continued \n4. Alem\u00e1n: Dies ist eine sch\u00f6ne Aussicht auf Machu Picchu in Peru. \n5. Italiano: Questa \u00e8 una bella vista di Machu Picchu in Per\u00f9. \n6. Portugu\u00e9s: Esta \u00e9 uma bela vista de Machu Picchu no Peru. \n7. Ruso: \u042d\u0442\u043e \u043a\u0440\u0430\u0441\u0438\u0432\u044b\u0439 \u0432\u0438\u0434 \u043d\u0430 \u041c\u0430\u0447\u0443-\u041f\u0438\u043a\u0447\u0443 \u0432 \u041f\u0435\u0440\u0443. \n8. Chino: \u8fd9 \u662f\u79d8 \u9c81\u9a6c \u4e18\u6bd4\u4e18\u7684\u7f8e \u4e3d \u666f\u8272\u3002  \n9. Japon\u00e9s: \u3053\u308c\u306f\u30da\u30eb\u30fc\u306e\u30de\u30c1\u30e5\u30d4\u30c1\u30e5\u306e\u7f8e\u3057\u3044\u666f\u8272\u3067\u3059\u3002  \n10. Coreano: \uc774\uac83\uc740  \ud398\ub8e8\uc758  \ub9c8\ucd94 \ud53c\ucd94\uc758  \uc544\ub984\ub2e4\uc6b4  \uacbd\uce58\uc785\ub2c8\ub2e4 . \n11. Hindi: \u092f\u0939 \u092a \u0947 \u01fe \u0915 \u0947 \u092e\u093e\u091a\u0942 \u036a\u092a\u00cd\u091a\u0942  \u0915\u093e \u0938\u0941\u0902\u0926\u0930 \u01fa\u00e6\u092f \u0939 \u0948 \u0964 \n12. \u00c1rabe: .\u06be\u0630\u0627 \ufee3\ufee7\u0638\u0631 \ufe9f\ufee3\ufbfe\u0644 \ufedf\ufee3\ufe8e\ufeb7\u0648 \ufe91\ufbfe\ufe97\ufeb7\u0648 \ufed3\ufef2 \ufe91\ufbfe\u0631\u0648  \n13. Turco: Bu, Peru'daki Machu Picchu'nun g\u00fczel bir manzaras\u0131d\u0131r. \n14. Griego: \u0391\u03c5\u03c4\u03ae \u03b5\u03af\u03bd\u03b1\u03b9 \u03bc\u03b9\u03b1 \u03cc\u03bc\u03bf\u03c1\u03c6\u03b7 \u03b8\u03ad\u03b1 \u03c4\u03bf\u03c5 \u039c\u03ac\u03c4\u03c3\u03bf\u03c5 \u03a0\u03af\u03c4\u03c3\u03bf\u03c5 \u03c3\u03c4\u03bf \n\u03a0\u03b5\u03c1\u03bf\u03cd. \n15. Hebreo: .\u05d6\u05d4\u05d5 \u05e0\u05d5\u05e3 \u05d9\u05e4\u05d4 \u05e9\u05dc \u05de\u05d0\u05e6'\u05d5 \u05e4\u05d9\u05e6'\u05d5 \u05d1\u05e4\u05e8\u05d5  \n16. Sueco: Det h\u00e4r \u00e4r en vacker vy av Machu Picchu i Peru. \n17. Noruego: Dette er en vakker utsikt over Machu Picchu i Peru. \n18. Dan\u00e9s: Dette er en smuk udsigt over Machu Picchu i Peru. \n19. Holand\u00e9s: Dit is een prachtig uitzicht op Machu Picchu in Peru. \n20. Polaco: To pi\u0119kny widok na Machu Picchu w Peru. \nPrompt: \nDescribe the image in Czech \nGPT-4V: \nNa obr\u00e1zku je mal\u00fd hn\u011bd\u00fd pes s b\u00edl\u00fdm \nl\u00edmcem a kosti\u010dkou na obojku. Pes je na \nzelen\u00e9 tr\u00e1v\u011b a m\u00e1 otev\u0159enou tlamu. \nVypad\u00e1, \u017ee se bav\u00ed. \nFigure 41: Results on multilingual image descriptions. GPT-4V is able to generate image descriptions\nin different languages. Check Section 4.5 for detailed discussions.\n59 Multilingual Scene Text Recognition Sec. 4.5 Multilingual Multimodal Understanding \nPrompt: \nWhat are all the scene text in the image? \nGPT-4V: \nThere are two scene texts in the image: \n1) \"SUPER NINTENDO WORLD\" \n2) \" \u30b9\u30fc\u30d1\u30fc\u30cb\u30f3\u30c6\u30f3\u30c9\u30fc\u30ef\u30fc\u30eb\u30c9 \" (this is \nthe Japanese version of \"Super Nintendo \nWorld\") \nPrompt: \nWhat are all the scene text in the image? \nGPT-4V: \nThe scene text in the image are: \n- \u0391\u0398\u0397\u039d\u0391\u0399-\u03a1\u03a9\u039c\u0397-\u03a1\u0399\u039f\u039d \u0399\u0391\u03a0\u0395\u03a4\u03a1\u039f\u039d (Athens-Rome-Rion Iapetron) \n- 29 \u039c\u0391\u03aa\u039f\u03a5 1933 (29 May 1933) \n- \u0394\u03a1\u0391\u03a7 (Drachma) \n- \u0395\u039b\u039b\u0391\u03a3 (Greece) \n- \u03a3\u03a0\u0391\u0395\u03a1. \u03a4\u0391\u03a7\u03a5\u0394\u03a1. \u03a3\u03a5\u0393\u039a\u039f\u0399\u039d. \u0399\u03a4\u0391\u039b\u0399\u0391-\u0395\u039b\u039b\u0391\u0394\u039f\u03a3-\u03a4\u039f\u03a5\u03a1\u039a\u0399\u0391 \n(Airmail. Express. Communication. Italy-Greece-Turkey) \n- 120 (Value of the stamp in drachmas) \nPrompt: \nWhat are all the scene text in the image? \nGPT-4V: \nThe scene text in the image is an Arabic \nphrase \" \ufe91\ufeb3\u0645 \ufdf2 \u0627\ufedf\u0631\ufea3\ufee3\u0646 \u0627\ufedf\u0631\ufea3\ufbfe\u0645( \" Bismillah \nal-Rahman al-Rahim). It translates to \"In \nthe name of Allah, the Most Merciful, the \nMost Compassionate\". This phrase is \ncommonly used by Muslims at the \nbeginning of any action or speech as a way \nto seek blessings and guidance from God.  \nPrompt: \nWhat are all the scene text in the image? \nGPT-4V: \nThe scene text in the image are: \n- \" \u5c16\u6c99\u5480 \" (Chinese characters) \n- \"Tsim Sha Tsui\" (English text) \nPrompt: \nWhat are all the scene text in the image? \nGPT-4V: \nThe scene text in the image are: \n- WELCOME (English) \n- bienvenida (Spanish) \n- Bienvenue (French) \n- Willkommen (German) \n- benvenuto (Italian) \nPrompt: \nWhat are all the scene text in the image? \nGPT-4V: \nThe scene text in the image is \"MUSEE \nDU LOUVRE\" which is the name of the \nfamous art museum in Paris, France.   \nPrompt: \nWhat are all the scene text in the image? \nGPT-4V: \nThe scene text in the image is \"n\u00e3o \u00e9 normal ter medo de andar \nsozinho na rua!!\" which is in Portuguese and translates to \"It is not \nnormal to be afraid to walk alone on the street!!\" \nPrompt: \nWhat are all the scene text in the image? \nGPT-4V: \nThe scene text in the image are: \n- G.P. FIM DA EUROPA \n- ATLETISMO \n- Ant\u00f3nio Agostinho 1987 \n- Fernando Domingos 1988 \nFigure 42: Results on multilingual scene text recognition. GPT-4V can recognize scene text in\ndifferent languages. Check Section 4.5 for detailed discussions.\n60 Multilingual Text Recognition, Translation, and Description Sec. 4.5 Multilingual Multimodal Understanding \nPrompt: \nIf the image is presented in a language other \nthan English, translate the content to \nEnglish. \nGPT-4V: \nThe content in the image is in Portuguese \nand translates to \"Double tap to add a title. \nDouble tap to add a subtitle.\" \nPrompt: \nSummarize the info in 20 languages \nGPT-4V output continued  \n4. German: Die S\u00fcdalpen sind ein Gebirgszug in Neuseeland. \n5. Italian: Le Alpi meridionali sono una catena montuosa in Nuova Zelanda. \n6. Portuguese: Os Alpes do Sul s\u00e3o uma cordilheira na Nova Zel\u00e2ndia. \n7. Russian: \u042e\u0436\u043d\u044b\u0435 \u0410\u043b\u044c\u043f\u044b - \u0433\u043e\u0440\u043d\u044b\u0439 \u0445\u0440\u0435\u0431\u0435\u0442 \u0432 \u041d\u043e\u0432\u043e\u0439 \u0417\u0435\u043b\u0430\u043d\u0434\u0438\u0438. \n8. Chinese: \u5357\u963f \u5c14 \u5351\u65af\u5c71\u8109\u662f\u65b0\u897f \u5170 \u7684\u4e00\u6761\u5c71\u8109\u3002  \n9. Japanese: \u5357\u30a2\u30eb\u30d7\u30b9\u306f\u30cb\u30e5\u30fc\u30b8\u30fc\u30e9\u30f3\u30c9\u306e\u5c71\u8108\u3067\u3059\u3002  \n10. Korean: \ub0a8\ubd80 \uc54c\ud504\uc2a4\ub294  \ub274\uc9c8\ub79c\ub4dc\uc758  \uc0b0\ub9e5\uc785\ub2c8\ub2e4 . \n11. Hindi: \u0926\u0915\u094d\u0937\u093f\u0923\u0940 \u090f\u0932\u094d\u092a\u094d\u0938 \u0928\u094d\u092f\u0942\u091c\u0940\u0932\u0948\u0902\u0921 \u0915\u093e \u090f\u0915 \u092a\u0935\u0930\u094d\u0935\u0924 \u0936\u094d\u0930\u0943\u0902\u0916\u0932\u093e \u0939\u0948\u0964 \n12. Bengali: \u09a6\u09bf\u0995\u09cd\u09b7\u09bf\u09a3 \u0985\u09cd\u09af\u09be\u09b2\u09aa\u09b8  \u09bf\u09a8\u0989\u09bf\u099c\u09b2\u09cd\u09af\u09be\u09c7\u09a8\u09cd\u09a1\u09c7\u09b0  \u098f\u0987 \u0985\u09bf\u09a7\u0995  \u09aa\u09ac \u09b0\u09cd\u09ac \u09a4\u09ae\u09be\u09b2\u09be\u0964 \n13. Arabic: \u0627\ufef7\ufedf\u0628 \u0627\ufedf\ufe9f\ufee7\u0648\ufe91\ufbfe\ufe94 \u06be\ufef2 \ufeb3\ufee0\ufeb3\ufee0\ufe94 \ufe9f\ufe91\ufe8e\u0644 \ufed3\ufef2 \ufee7\ufbfe\u0648\u0632\ufbfe\ufee0\ufee7\u062f\u0627 . \n14. Turkish: G\u00fcney Alpler Yeni Zelanda'da bir da\u011f silsilesidir. \n15. Indonesian: Alpen Selatan adalah sebuah pegunungan di Selandia Baru. \n16. Vietnamese: D\u00e3y n\u00fai Alpes ph\u00eda Nam l\u00e0 m\u1ed9t d\u00e3y n\u00fai \u1edf New Zealand. \n17. Thai: \u0e40\u0e17\u0e37\u0e2d\u0e01\u0e40\u0e02\u0e32\u0e41\u0e2d\u0e25\u0e1b\uf709\u0e20\u0e32\u0e04\u0e43\u0e15\uf70b\u0e40\u0e1b\uf712\u0e19\u0e40\u0e17\u0e37\u0e2d\u0e01\u0e40\u0e02\u0e32\u0e43\u0e19\u0e19\u0e34\u0e27\u0e0b\u0e35\u0e41\u0e25\u0e19\u0e14\uf70e  \n18. Malay: Alpen Selatan adalah satu rangkaian gunung di New Zealand. \n19. Tagalog: Ang Timog Alps ay isang hanay ng mga bundok sa New \nZealand. \n20. Swahili: Milima ya Alpi Kusini ni safu ya milima nchini New Zealand. \nGPT-4V: \n1. English: The Southern Alps are a mountain range in \nNew Zealand. \n2. Spanish: Los Alpes del Sur son una cadena \nmonta\u00f1osa en Nueva Zelanda. \n3. French: Les Alpes du Sud sont une cha\u00eene de \nmontagnes en Nouvelle-Z\u00e9lande. Prompt: \nIf the image is presented in a language \nother than English, translate the content \nto English. \nGPT-4V: \nEnglish translation: \"Pictures [edit | edit \nwikitext] Stegosaurus's brain was located \nin its small head.\" \nPrompt: \nIf the image is presented in a language \nother than English, translate the content \nto English. \nGPT-4V: \nTranslation: Angelus 1st Grand Cru \nClass\u00e9 Chateau Angelus 1996 \nSaint-Emilion Grand Cru Appellation \nSaint-Emilion Grand Cru Controlled \nPrompt: \nIf the image is presented in a language \nother than English, translate the content \nto English. \nGPT-4V: \nThe text in the image is in Japanese and \ntranslates to \"Good morning, have a nice \nday.\" \nPrompt: \nIf the image is presented in a language \nother than English, translate the content \nto English. \nGPT-4V: \nThe text in the image is in Chinese and \ntranslates to \"Mapo Tofu.\" Mapo tofu is a \npopular Chinese dish made with tofu and \nminced meat in a spicy sauce.  \nPrompt: \nIf the image is presented in a language other \nthan English, translate the content to \nEnglish. \nGPT-4V: \nThe text in the image is in Korean and \ntranslates to \"Delicious food\". \nFigure 43: Results on multilingual text recognition, translation, and description. GPT-4V is able to\nrecognize, translate and generate descriptions in different languages. Check Section 4.5 for detailed\ndiscussions.\n61 Multilingual Multicultural Understanding \nPrompt: \nDescribe the image in both English and the local \nlanguage corresponding to the geolocation depicted \nin the image. \nGPT-4V: \nEnglish: This image shows a couple dressed in \ntraditional Chinese wedding attire, sitting in front of a \nwall of red roses and a gold Chinese character for \n\"double happiness.\" \nChinese (Simplified): \u8fd9\u5f20\u56fe \u7247\u5c55\u793a\u4e86\u4e00 \u5bf9 \u7a7f\u7740\u4e2d\u56fd \u4f20\n\u7edf \u5a5a\u793c\u670d\u88c5\u7684\u592b \u5987 \uff0c\u5750\u5728 \u7ea2 \u8272\u73ab\u7470\u548c\u91d1\u8272 \u201c \u53cc\u559c\u201d \u6c49 \u5b57\u524d\n\u9762\u3002 (Zh\u00e8 zh\u0101ng t\u00fapi\u00e0n zh\u01censh\u00ecle y\u012b du\u00ec chu\u0101nzhe \nzh\u014dnggu\u00f3 chu\u00e1nt\u01d2ng h\u016bnl\u01d0 f\u00fazhu\u0101ng de f\u016bf\u00f9, zu\u00f2 z\u00e0i \nh\u00f3ngs\u00e8 m\u00e9igu\u012b h\u00e9 j\u012bns\u00e8 \"shu\u0101ng x\u01d0\" h\u00e0nz\u00ec qi\u00e1nmi\u00e0n.) \nPrompt: \nDescribe the image in both English and the local \nlanguage corresponding to the geolocation depicted \nin the image. \nGPT-4V: \nEnglish: This image shows a Japanese couple in \ntraditional wedding attire. The man is wearing a black \nkimono with a white hakama and the woman is \nwearing a white kimono with a large headpiece. \nJapanese: \u3053\u306e\u753b\u50cf\u306f\u4f1d\u7d71\u7684\u306a\u7d50\u5a5a\u5f0f\u306e\u8863\u88c5\u3092\u7740\u305f\u65e5\n\u672c\u306e\u30ab\u30c3\u30d7\u30eb\u3092\u793a\u3057\u3066\u3044\u307e\u3059\u3002\u7537\u6027\u306f\u767d\u3044\u88b4\u3068\u9ed2\u3044\u7740\u7269\n\u3092\u7740\u3066\u304a\u308a\u3001\u5973\u6027\u306f\u5927\u304d\u306a\u982d\u98fe\u308a\u3068\u767d\u3044\u7740\u7269\u3092\u7740\u3066\u3044\u307e\n\u3059\u3002 (Kono gaz\u014d wa dent\u014d-teki na kekkon-shiki no \nish\u014d o kita Nihon no kappuru o shimeshiteimasu. \nDansei wa shiroi hakama to kuroi kimono o kite ori, \njosei wa \u014dkina kazari to shiroi kimono o kiteimasu.) \nPrompt: \nDescribe the image in both English and the local \nlanguage corresponding to the geolocation depicted \nin the image. \nGPT-4V: \nEnglish: The image shows a couple wearing \ntraditional Ethiopian clothing, standing in a garden \nwith a tree in the background. The woman is wearing \na white dress with colorful patterns and the man is \nwearing a white suit with similar patterns. \nAmharic (Ethiopia's official language): \u12ed\u1205 \u121d\u1235\u120d  \n\u1260\u12a2\u1275\u12ee\u1335\u12eb\u12ca  \u1263\u1205\u120d \u120d\u1265\u1235 \u12e8\u1208\u1260\u1231\u1275  \u1263\u1208\u1264\u1275\u1293  \u1263\u1208\u1264\u1275\u1295  \u1260\u1308\u1263\u122d  \n\u12ed\u1273\u12eb\u120d\u1362  \u1234\u1275\u12ee\u1271  \u1260\u1290\u133b\u1290\u1275  \u12eb\u1209 \u12a0\u1208\u1263\u1208\u1263\u1205\u122d  \u120d\u1265\u1236\u127d  \u1208\u1260\u1233\u1208\u127d  \u12a5\u1293 \n\u12c8\u1295\u12f5\u12ee\u1271\u121d  \u1260\u1218\u1233\u1233\u1205  \u12a0\u1208\u1263\u1208\u1263\u1205\u122d  \u1231\u1275 \u1208\u1260\u1233\u1208\u127d\u1362  \nPrompt: \nDescribe the image in both English and the local \nlanguage corresponding to the geolocation depicted \nin the image. \nGPT-4V: \nEnglish: This image shows a newlywed Indian couple \nsharing a kiss. The bride is wearing a traditional red \nsari and the groom is wearing a red turban and white \nsuit. They are sitting on a decorated stage with \nflowers. \nHindi (local language in India): \u092f\u0939 \u091b\u0935 \u090f\u0915 \n\u0928\u0935\u0935\u0935\u093e\u0939\u0924 \u092d\u093e\u0930\u0924\u0940\u092f \n\u091c\u094b\u095c\u0947 \u0915\u094b \u090f\u0915 \u091a\u0941\u0902\u092c\u0928 \u0936\u0947\u092f\u0930 \u0915\u0930\u0924\u0947 \u0939\u0941\u090f \u0926\u0916\u093e\u0924\u0940 \u0939\u0948\u0964 \u0926\u0941\u0932\u094d\u0939\u0928 \u090f\u0915 \n\u092a\u0930\u0902\u092a\u0930\u093e\u0917\u0924 \u0932\u093e\u0932 \u0938\u093e\u095c\u0940 \u092a\u0939\u0928 \u0930\u0939\u0940 \u0939\u0948 \u0914\u0930 \u0926\u0942\u0932\u094d\u0939\u093e \u090f\u0915 \u0932\u093e\u0932 \n\u092a\u0917\u095c\u0940 \u0914\u0930 \u0938\u092b\u0947\u0926 \u0938\u0942\u091f \u092a\u0939\u0928 \u0930\u0939\u093e \u0939\u0948\u0964 \u0935\u0947 \u092b\u0942\u0932\u094b\u0902 \u0938\u0947 \u0938\u091c\u0947 \u0939\u0941\u090f \n\u090f\u0915 \u092e\u0902\u091a \u092a\u0930 \u092c\u0948\u0920\u0947 \u0939\u0948\u0902\u0964 Sec. 4.5 Multilingual Multimodal Understanding \nFigure 44: Results on multilingual multiculture understanding. Check Section 4.5 for detailed\ndiscussions.\n62 4.6 Coding Capability with Vision\nSec. 4.6 Coding Capability with Vision \nPrompt: \nGPT -4V : \nGPT -4V : \nGPT -4V : GPT -4V : Prompt: Prompt: \nPrompt: \nFigure 45: GPT-4V\u2019s capability to generate LaTex codes based on the hand-written input. The\ninstruction is \u2018generate latex code.\u2019 for each case. The output is the LaTeX code and we show the\nrendered result. Although the model fails to write the code for the complex equation (bottom), we\ncan break it down into several simple equations, which GPT-4V is able to handle. Check Section 4.6\nfor detailed discussions.\nFigure 45 illustrates the ability to generate LaTeX code based on handwritten mathematical equations.\nThis functionality can assist users in writing equations in LaTeX more efficiently. Although the\nmodel is unable to generate code for longer equations, it can handle shorter equations effectively. By\nbreaking down longer equations into shorter components, the model is able to generate the appropriate\ncode. Figure 46 further demonstrates how GPT-4V can reconstruct a table in the input image into\nMarkDown/LaTex code.\nFigure 47 shows examples of writing code in Python, TikZ, and SVG to replicate the input figure.\nAlthough the resulting output is not an exact match, the layout is similar and the code can be easily\nmodified to meet specific needs.\n63 GPT-4V: \n| Country | Sales Volume | Revenue | Profit | \n| --- | --- | --- | --- | \n| USA | 40.080 | $ 15.971.421  | $3.086.421 | \n| China | 35.070 | $15.866.670 | $3.032.162 | \n| Australia | 27.054 | $14.812.566 | $2.868.636 | \n| India | 23.046 | $10.608.174 | $1.853.710 | \n| South Korea | 16.032 | $10.494.948 | $1.975.844 | \nPrompt: \nPlease read the table in this image and return a latex-style reconstructed table in text. \nGPT-4V: \nHere is the reconstructed table in latex: \n\\begin{tabular},{|c|c|c|c|} \\hline \nCountry & Sales Volume & Revenue & Profit \\\\ \\hline \nUSA & 40.080 & \\$ 15.971.887  & \\$3.068.421  \\\\ \\hline \nChina & 35.070 & \\$15.866.670 & \\$3.032.162 \\\\ \\hline \nAustralia & 27.054 & \\$14.812.566 & \\$2.868.636 \\\\ \\hline \nIndia & 23.046 & \\$10.608.174 & \\$1.853.710 \\\\ \\hline \nSouth Korea & 16.032 & \\$10.494.948 & \\$1.975.844 \\\\ \\hline \n\\end{tabular} Sec. 4.6 Coding Capability with Vision \nPrompt: \nPlease read the table in this image and return a markdown-style reconstructed table in text. \nVisualization of Reconstructed Table (Markdown): \nNote: Same query image as shown above, resized to \nsave space \nVisualization of Reconstructed Table (LaTex): Constrained Prompting Figure 46: GPT-4V\u2019s capability to generate Markdown/LaTex codes to reconstruct a table in the\nimage. Red highlights the errors in reconstruction. Check Section 4.6 for detailed discussions.\n64 Sec. 4.6 Coding Capability with Vision \nPrompt: \nGenerate python code to draw similar curves. \nGPT -4V : \nPrompt: \nWrite python codes to generate similar figures. \nGPT -4V : \nPrompt: \nWrite svg codes to generate following image. \nGPT -4V : Prompt: \nGenerate the following image in tikz. \nGPT -4V : Figure 47: GPT-4V\u2019s capability to write codes to replicate the input figure. We directly show the\nrendered figures by python/TikZ/SVG as GPT-4V\u2019s response. The rendered figure is roughly aligned\nwith the input figure, and the code can be easily adapted. GPT-4V Chart. Check Section 4.6 for\ndetailed discussions. 65 5 Interaction with Humans: Visual Referring Prompting\nPointing to a specific spatial location is an essential capability in human-computer interaction with\nmultimodal systems, such as conducting visually grounded dialogues. As shown in Section 5.1,\nGPT-4V can well understand the visual pointers directly drawn on images. Based on this observation,\nwe propose a novel model interaction method named \u201cvisual referring prompting.\u201d The core idea is to\ndirectly edit image pixel space to draw visual pointers or scene texts as human referring instructions,\nas highlighted in Figure 50. We detail its usages and advantages in Section 5.2. Finally, Section 5.3\nexplores having GPT-4V generate visual pointer outputs to interact with humans. These visual\npointers are intuitive for both humans and machines to generate and understand, making them a good\nchannel for human-computer interaction.\n5.1 Understand Pointing Inputs\nAs illustrated in Figure 48, GPT-4V can understand different types of visual markers directly overlaid\non images as a pointer, such as circles, boxes, and hand drawings. This ability helps GPT-4V generate\ngrounded captioning, which is a known challenging problem to have conventional vision-language\nmodels [ 128] generating visual descriptions focused on a specific area of interest. Dense captioning\nmethods [ 62,138] use cropped boxes or mask regions to generate localized descriptions, but often\nignore the global image context and produce sub-optimal descriptions. Visual pointing provides a\nnatural way to indicate the area of interest while maintaining the global image context. For example,\nthe top left example focuses on providing a comprehensive description of the pointed Magna beer,\nwhile also mentioning the global image context that the beer bottle is on the table.\nAn intuitive alternative to visual pointers overlaid on images is the region coordinates represented in\nthe numerical text format. As shown in Figure 49, GPT-4V can understand the coordinates out-of-box,\nachieving the ability of spatial referring via text tokens without extra box token finetuning as in prior\nvision-language models [ 129,143]. Despite the promising capability, we note that our current prompt\nis less precise spatially. For example, in the top left example in Figure 49, GPT-4V mentions the\nsurrounding objects napkin and water bottle, even though only the beer bottle is in the region (0.47,\n0.48, 0.55, 0.87). Overall, with respect to our experimented prompts, GPT-4V works more reliably\nwhen prompted with overlaid visual pointers, compared with text coordinates. This unique capability\nmotivates us to explore a new prompting method, namely visual referring prompting.\n66 Grounded Description (overlaid visual pointers) \nPrompt: \nDescribe the pointed region in the image. \nGPT-4V: \nThe pointed region in the image is a green glass bottle of \nMagna beer, with a red label and a gold cap. It is sitting on \na black table, surrounded by other items such as a water \nbottle, napkins, and umbrellas. \nPrompt: \nDescribe the pointed region in the image. \nGPT-4V: \nThe pointed region in the image contains a white folded \nnapkin and a silverware set wrapped in another white \nnapkin, placed on a black table. This is likely a table setting \nfor a meal at a restaurant. \nPrompt: \nDescribe the two pointed objects in the image. \nGPT-4V: \nThe first pointed object (indicated by the yellow arrow) is a \nwhite patio umbrella. It has a pointed top and is attached to \na metal pole. The umbrella is open and providing shade for \nthe outdoor seating area. The second pointed object \n(indicated by the red arrow) is a green glass bottle of \nMagna beer. The bottle has a pointed neck and is capped \nwith a gold-colored metal lid. It is sitting on a black table in \nfront of a white napkin. \nPrompt: \nDescribe the pointed region in the image. \nGPT-4V: \nThe pointed region in the image is a green beer bottle with \na red label that reads \"MAGNA\". The bottle is standing \nupright on a black table with a white napkin next to it. \nPrompt: \nDescribe the pointed region in the image. \nGPT-4V: \nThe pointed region in the image is a section of a wooden \nrailing with vertical slats. It is located on the edge of a patio \noverlooking the ocean. The railing appears to be painted \nwhite and is partially obscured. \nPrompt: \nDescribe the pointed region in the image. \nGPT-4V: \nThe pointed region in the image is a row of hanging lights \non a wire. The lights are small and round, and they are \nevenly spaced along the wire. The wire is stretched \nbetween two poles and is located above a patio area with \ntables and umbrellas. The lights appear to be decorative \nand are likely used to provide ambient lighting in the \nevening. \nSec. 5.1 Understand Pointing Inputs Figure 48: GPT-4V understands visual pointers directly overlaid on images. Conducting grounded\ndescription with both local and global visual information is one unique application scenario. Check\nSection 5.1 for detailed discussions.\n67 Sec. 5.1 Understand Pointing Inputs \nGrounded Description (text-format region coordinates) \nPrompt: \nLocate the region based on given coordinates and \ndescribe the contents in the region. Regions are \nrepresented by (x1,y1,x2,y2) coordinates. x1 x2 are the left \nand right most positions, normalized into 0 to 1, where 0 is \nthe left and 1 is the right. y1 y2 are the top and bottom \nmost positions, normalized into 0 to 1, where 0 is the top \nand 1 is the bottom. Describe the relative position of region \n(0.47, 0.48, 0.55, 0.87) in the image. Describe the contents \nin that relative position. No need to mention contents in \nother part of the image. \nGPT-4V: \nThe region (0.47, 0.48, 0.55, 0.87) is located in the \ncenter-right part of the image. It covers a small area of the \ntable and extends to the bottom of the image. The most \nprominent object in this region is a green bottle of Magna \nbeer. There is also a white napkin and a part of a water \nbottle visible in this region. The background shows a part \nof the patio and the ocean. \nPrompt: \nLocate the region based on given coordinates and \ndescribe the contents in the region. Regions are \nrepresented by (x1,y1,x2,y2) coordinates. x1 x2 are the left \nand right most positions, normalized into 0 to 1, where 0 is \nthe left and 1 is the right. y1 y2 are the top and bottom \nmost positions, normalized into 0 to 1, where 0 is the top \nand 1 is the bottom. Describe the relative position of region \n(0.01, 0.09, 0.29, 0.21) in the image. Describe the contents \nin that relative position. No need to mention contents in \nother part of the image. \nGPT-4V: \nThe region (0.01, 0.09, 0.29, 0.21) is located in the top left \ncorner of the image. It covers approximately 28% of the \nwidth and 12% of the height of the image. In this region, \nthere is a part of a white umbrella and a string of lights. \nThe background shows a clear blue sky and a palm tree. \nPrompt: \nLocate the region based on given coordinates and \ndescribe the contents in the region. Regions are \nrepresented by (x1,y1,x2,y2) coordinates. x1 x2 are the left \nand right most positions, normalized into 0 to 1, where 0 is \nthe left and 1 is the right. y1 y2 are the top and bottom \nmost positions, normalized into 0 to 1, where 0 is the top \nand 1 is the bottom. Describe the relative position of region \n(0.01, 0.67, 0.36, 0.91) in the image. Describe the contents \nin that relative position. No need to mention contents in \nother part of the image. \nGPT-4V: \nThe region (0.01, 0.67, 0.36, 0.91) in the image is located \nat the bottom left corner, covering approximately one third \nof the width and one fourth of the height of the image. In \nthis region, there is a black table with a white napkin, a \nbottle of Magna beer, and a glass bottle of water. The table \nis in the foreground, while the background shows a part of \nthe patio with white umbrellas and chairs. \nPrompt: \nLocate the region based on given coordinates and \ndescribe the contents in the region. Regions are \nrepresented by (x1,y1,x2,y2) coordinates. x1 x2 are the left \nand right most positions, normalized into 0 to 1, where 0 is \nthe left and 1 is the right. y1 y2 are the top and bottom \nmost positions, normalized into 0 to 1, where 0 is the top \nand 1 is the bottom. Describe the relative position of region \n(0.47, 0.48, 0.55, 0.87) in the image. Describe the contents \nin that relative position. No need to mention contents in \nother part of the image. \nGPT-4V: \nThe region (0.47, 0.48, 0.55, 0.87) is located towards the \nright side of the image, slightly below the center. It covers \na small portion of the table and the background. In this \nregion, there is a green bottle of Magna beer on the table, \nwith a white napkin next to it. In the background, there is a \npart of an umbrella and a few chairs. 0.48\n0.87\n0.470.55\n0.67\n0.91\n0.01 0.360.48\n0.87\n0.470.55\n0.09\n0.21\n0.01 0.29\nFigure 49: An alternative to visual pointers overlaid on images is the region coordinates represented\nin the numerical text format. GPT-4V can understand the coordinates, e.g., (0.47, 0.48, 0.55, 0.87),\n(0.01, 0.09, 0.29, 0.21), and (0.01, 0.67, 0.36, 0.91) that correspond to the center beer bottle, top-left\nstring lights, and bottom-left table set, respectively. We observe that GPT-4V works less reliably\nwhen prompted with text coordinates, compared with visual pointers in visual referring prompting.\nCheck Section 5.1 for detailed discussions.\n68 5.2 Visual Referring Prompting\nInspired by GPT-4V\u2019s strong capability in understanding visual pointing and scene text, we explore a\nnew method to interact with GPT-4V, namely the visual referring prompting . Instead of conventional\nprompting techniques that edit text space, visual referring prompting is a complementary technique\nthat directly edits the pixel space for input images for human-computer interaction. Such visual\nprompting could offer a more nuanced and comprehensive interaction with the image, potentially\nunlocking a wider array of responses from the model. For example, in Figure 50 (1), GPT-4V naturally\nassociates the arrow-pointed objects with the given object indexes, easing the remaining visual\nreasoning and text outputs; in (2), GPT-4V understands the questions written on the image and\npointed to the corresponding edge or angle, providing a nuanced interface for grounded visual\ndialogue; in (3), humans can point to arbitrary regions inside the figure to help GPT-4V better\nunderstand complicated documents and charts; in (4), the pattern can be concisely represented as an\narrow and the scene text \u201c+dot\u201d, therefore helping GPT-4V to predict the next image. Complementary\nto text prompts that are loosely grounded to images, visual referring prompting provides a novel\ninteraction method that could facilitate various use cases, with additional demonstrations in Figure 51\nand Section 9.\n5.3 Generate Pointing Outputs\nSection 5.1 discusses the ability of GPT-4V to understand visual pointing generated by humans. A\nnatural question is: Can GPT-4V generate its own pointing outputs, thereby facilitating a closed-loop\ninteraction process in human-computer interaction?\nFigure 52 explores generating visual pointing outputs by letting GPT-4V predict region coordinates\nin the text format. We prompt GPT-4V to ground the object referred by text ( e.g., the text of \u201cblue\nSubaru SUV\u201d) or a reference image ( e.g., the image of \u201cblack Audi sedan\u201d). Similar to the observation\nin having GPT-4V comprehend coordinates input, the model has a coarse understanding of spatial\nlocations, but it wasn\u2019t accurate with respect to the prompts used in the experiment. For example,\nin Figure 52\u2019s \u201cplot visualizations,\u201d GPT-4V can approximately identify the blue SUV and black\nsedan mentioned in the query, but it struggles to create a closely-fitted bounding box. We observe that\nincluding example-grounded instructions in the prompt helps GPT-4V to understand the definition of\ncoordinates and subsequently generate better pointing outputs.\nWhile the generated pointing outputs may not perfectly cover the queried region, they still provide a\nvaluable tool for model interaction, interpretation, and helping multi-step visual reasoning. Specifi-\ncally, the pointing outputs can be interpreted by humans to better understand GPT-4V\u2019s references,\nor by GPT-4V itself to enable further reasoning based on previous outputs. As shown in the bottom\nof Figure 52, GPT-4V is capable of interpreting the pointers it generates, and providing grounded\ndescriptions with the prompts in Figure 48. Furthermore, the iterative pointing generation and\nunderstanding by GPT-4V itself can help complicated multi-hop grounded visual reasoning tasks.\nGPT-4V is capable of deconstructing the question, generating distinct visual markers to iteratively\nfocus on different image regions for each sub-step, ultimately collating the information to formulate\nthe final answer.\n69 Sec. 5.2 Visual Referring Prompting \n(1) Prompt: \nWhat is in the circled glass? Object 1 or object 2? \nDescribe what is Object 1 and object 2. Then check what \nis in the circled glass. \nGPT-4V: \nObject 1 is a glass bottle of water. It is clear and has a blue \nlabel on it. \nObject 2 is a glass bottle of Magna beer. It is green and \nhas a red and white label on it. \nIn the circled glass, it appears to be a clear liquid, likely \nwater. So, it is likely that  the contents of the circled glass  \nare from Object 1, the glass bottle of water .\n 8(2) Prompt: \nAnswer the question in the following figure. Provide \nintermediate steps. \nGPT-4V: \nTo find the length of the blue arrowed edge, we can use \nthe Pythagorean theorem since we have a right triangle. \nThe theorem states that a\u00b2 + b\u00b2 = c\u00b2, where a and b are the \ntwo legs of the right triangle and c is the hypotenuse. \nPlugging in the values we have, we get: \n4\u00b2 + 8\u00b2 = c\u00b2    16 + 64 = c\u00b2    80 = c\u00b2    \u221a80 = c    8.94 \u2248 c \nTherefore, the length of the blue arrowed edge is  \napproximately 8.94. \nTo find the angle at the green arrow, we can use the \ntangent function since we have the opposite and adjacent \nsides of the angle. The tangent function states that tan\u03b8 = \nopposite/adjacent. Plugging in the values we have, we get: \ntan\u03b8 = 4/8    tan\u03b8 = 1/2    \u03b8 = tan^-1(1/2)    \u03b8 \u2248 26.57\u00b0 \nTherefore, the angle at the green arrow is approximately  \n26.57\u00b0. \n(3) Prompt: \nWhich circled line shows a different comparison trend? \nProvide intermediate steps. \nGPT-4V: \nThe circled line that shows a different comparison trend is \nthe USNCO Local Section Exam 2022 . Intermediate steps: \n1. Analyze the data in the table and look for any \ndiscrepancies in the comparison trends. 2. Notice that in \nall the rows except for the USNCO Local Section Exam \n2022, the GPT-4 (no vision) column has a lower score than \nthe GPT-4 column. 3. This means that in all the other rows, \nthe GPT-4 model with vision performed better than the \nGPT-4 model without vision. 4. However, in the USNCO \nLocal Section Exam 2022 row, the GPT-4 (no vision) \ncolumn has a higher score than the GPT-4 column. 5. This \nis a different comparison trend than the rest of the rows, \nindicating that the GPT-4 model without vision performed \nbetter than the GPT-4 model with vision in this particular \nexam. \n(4) Prompt: \nAnswer questions in the following figure. Provide \nintermediate steps. \nGPT-4V: \nThe next image in the sequence would be a star with a dot  \nin the center.  This is because the sequence appears to be \nfollowing a pattern where the shape alternates between a \ndiamond and a star, and a dot is added to the center of the \nshape every other image. Intermediate steps: \n- First, we can observe that the sequence alternates \nbetween a diamond and a star shape. \n- Next, we can see that a dot is added to the center of the \nshape every other image. \n- Based on this pattern, we can predict that the next image \nwill be a star with a dot in the center. \nFigure 50: Visual referring prompting directly edits the input image as input prompts, such as drawing\nvisual pointers and scene texts. Complementary to text prompts, visual referring prompting provides a\nmore nuanced and natural interaction, e.g., (1) associating pointed objects with an index, (2) pointing\nto the image for questioning, (3) highlighting lines in documents and tables, (4) drawing the pattern\non the image, and many other novel use cases. Check Section 5.2 for detailed discussions.\n70 Sec. 5.2 Visual Referring Prompting \nPrompt: \nWhat is the icon used for? \nGPT -4V : \nThe icon circled in the image is a microphone icon. It is \nused to activate the voice search feature on Bing. Users \ncan click on this icon and speak into their device's \nmicrophone to ask a question or search for something \ninstead of typing it in. This can be helpful for hands-free \nsearching or for users who may have difficulty typing. \nPrompt: \nWhat does the pointed news talk about? \nGPT -4V : \nThe pointed news is talking about the new feature of ChatGPT, \nwhich allows users to create personalized instructions. This \nfeature is said to help users better manage their conversations \nand improve their experience with the chatbot. \nPrompt: \nWhat does this part of the poster highlights? \nGPT -4V : \nThis part of the poster highlights the ReCo Framework, \nwhich is the main contribution of the research. It explains \nhow the framework works and what it does. The ReCo \nFramework is a region-controlled text-to-image generation \nmodel that can generate high-quality images based on text \ndescriptions. It also shows the different components of the \nframework and how they interact with each other. \nThe ReCo Framework is designed to be flexible and easy \nto extend, and it has been shown to improve image \ngeneration quality and achieve better semantic coherence \ncompared to other text-to-image generation methods. \nPrompt: \nWhat are the differences between these two experiments in the \nslides? \nGPT -4V : \nThere are a few key differences between the two experiments \nshown in the slides: \n1. The tasks: The first experiment (on the left) is focused on the \ntask of visual grounding, where the model is trained to identify \nand localize objects in an image. The second experiment (on the \nright) is focused on the task of image captioning, where the \nmodel is trained to generate a descriptive caption for an image. \n2. The datasets: The first experiment uses the Flickr30k Entities \ndataset, which contains images with annotated object bounding \nboxes. The second experiment uses the MSCOCO and VQA2 \ndatasets, which contain images with associated captions. \n3. The evaluation metrics: The first experiment uses the mean \nAverage Precision metric to evaluate the model's performance \non the visual grounding task. The second experiment uses the \nBLEU and CIDEr metrics to evaluate the model's performance \non the image captioning task. \nOverall, the two experiments are designed to evaluate the \nUniTAB model's performance on different tasks and datasets, \nand to demonstrate its ability to handle multiple tasks with a \nsingle model. \nFigure 51: Visual referring prompts enhance the seamless interaction between humans and computers.\nThis is evident in the integration with computer and mobile Graphical User Interfaces (GUIs), and the\nsupport provided in understanding documents and slides. Check Section 5.2 for detailed discussions.\n71 Sec. 5.3 Generate Pointing Outputs \nPrompt: \nLocate the referred object and represent the location of \nthe region. Regions are represented by (x1,y1,x2,y2) \ncoordinates. x1 x2 are the left and right most positions, \nnormalized into 0 to 1, where 0 is the left and 1 is the right. \ny1 y2 are the top and bottom most positions, normalized \ninto 0 to 1, where 0 is the top and 1 is the bottom. For \nexample, the (x1,y1,x2,y2) coordinates for that region with \nthe beer bottle is (0.47, 0.48, 0.55, 0.87). \nFor the image below, what is the (x1,y1,x2,y2) coordinates \nfor the region with the blue Subaru SUV? \nGPT-4V: \nThe (x1,y1,x2,y2) coordinates for the region with the blue \nSubaru SUV would be (0.68, 0.57, 0.95, 0.95). \nPlot Visualization: \nRefer to the query image below \nGrounded Description \nPrompt: \nDescribe the pointed region in the image. \nGPT-4V: \nThe pointed region in red is a part of a larger mural on the \nside of a building. The content in this specific region \nappears to be a depiction of a blue car, possibly an SUV, \nparked in a parking lot. The car is facing towards the right \nside of the image and appears to be in a parking spot with \na white line on the left side. \nPrompt: \nLocate the referred object and represent the location of the \nregion. Regions are represented by (x1,y1,x2,y2) coordinates. \nx1 x2 are the left and right most positions, normalized into 0 to \n1, where 0 is the left and 1 is the right. y1 y2 are the top and \nbottom most positions, normalized into 0 to 1, where 0 is the top \nand 1 is the bottom. For example, the (x1,y1,x2,y2) coordinates \nfor that region with the beer bottle is (0.47, 0.48, 0.55, 0.87). \nFor the image below, what is the (x1,y1,x2,y2) coordinates for \nthe region with object \nin it? \nGPT-4V: \nThe (x1,y1,x2,y2) coordinates for the region with the black Audi \ncar in it would be approximately (0.35, 0.55, 0.55, 0.85). This is \nbased on the left and right most positions of the car being \naround 35% and 55% of the image width, respectively, and the \ntop and bottom most positions being around 55% and 85% of \nthe image height, respectively. \nPlot Visualization: \nRefer to the query image below \nGrounded Description \nPrompt: \nDescribe the pointed region in the image. \nGPT-4V: \nThe object in the pointed region in red is a black car, specifically \na sedan, parked in a parking lot. It appears to be a newer model \nwith a shiny exterior and tinted windows. \n0.48\n0.87\n0.470.550.48\n0.87\n0.470.55Figure 52: GPT-4V can use its understanding of coordinates to generate visual pointing output,\nthereby grounding the textual or visually queried object. Using example-grounded instructions can\nhelp GPT-4V understand coordinate definitions and therefore generate better pointing. While output\nspatial regions are not precise, the approach enables an \u201cunderstanding ( i.e., grounded description)\nand generation\u201d loop for visual pointing, leading to an effective way of human-computer interaction.\nCheck Section 5.3 for detailed discussions.\n72 6 Temporal and Video Understanding\nIn this section, we discuss temporal and video understanding capabilities. Even though GPT-\n4V operates primarily on images as inputs, evaluating its understanding of temporal sequences\nand video content remains a crucial aspect of its overall assessment. This is because real-world\nevents unfold over time, and an AI system\u2019s ability to understand these dynamic processes is\ninstrumental in real-world applications. Capabilities like temporal anticipation, temporal ordering,\ntemporal localization, temporal reasoning, and grounded temporal understanding help to gauge\nthe model\u2019s proficiency in comprehending the sequence of events, anticipating future occurrences,\nand contextually analyzing activities over time, all within a series of static images. In spite of its\nimage-centric focus, GPT-4V is able to comprehend video and temporal sequences in a way that\u2019s\nsimilar to human comprehension. To enhance the versatility and applicability of a sophisticated\nAI model like GPT-4V, this aspect of testing is critical to its development and refinement. For the\nupcoming experiments in this section, we will use multiple selected video frames as inputs to test the\nmodel\u2019s abilities in understanding temporal sequences and video content.\n6.1 Multi-image Sequencing\nIn this subsection, we demonstrate that GPT-4V can accurately comprehend and analyze sequences\nof video frames. Within this frame-by-frame analysis, GPT-4V recognizes the scene in which the\nactivity is taking place, delivering a deeper contextual understanding. As shown in Figure 53, the\nmodel is not just confined to recognizing the environment; it also accurately interprets the actions\nbeing performed by individuals in the video. GPT-4V understands the sequence and context of\nvarious human poses and intelligently correlates them with the ongoing activity. By understanding\npose variations beyond just identification, GPT-4V can derive meaning from the subtleties of human\nmovement and action. As a result of this level of detailed understanding, GPT-4V can capture\nthe essence of what\u2019s happening in videos, offering rich and nuanced insights that go beyond just\nidentifying objects and scenes.\n6.2 Video Understanding\nTemporal ordering. Temporal ordering is a crucial element of temporal commonsense and forms\nan essential part of GPT-4V\u2019s capabilities evaluation. This involves providing the model with a\nseries of shuffled images and gauging its ability to discern cause and effect relationships as well\nas time progressions. An understanding of such relationships requires the ability to reorder the\nsequence in a logically coherent and temporally accurate manner. Figure 54 illustrates an example\nof long-term temporal ordering where GPT-4V is presented with a series of shuffled image frames\ndepicting a sushi-making event. Despite the disorder, GPT-4V effectively identifies the event and\ndetermines the appropriate temporal sequence of the sushi-making process. In addition, Figure 55\nprovides an example of short-term temporal ordering. Given a designated action, such as opening\nor closing a door, GPT-4V demonstrates its capability to comprehend the image\u2019s content and\ndetermine the correct sequential order of the events. These examples highlight GPT-4V\u2019s capability\nin temporal commonsense, reinforcing its ability to comprehend both long-term and short-term\nsequences accurately.\nTemporal anticipation. We demonstrate GPT-4V\u2019s ability to anticipate future events given a set of\ninitial frames. Long- and short-term examples are used to validate this capacity for anticipating future\nevents. The right side of Figure 56 illustrates GPT-4V\u2019s ability to anticipate short-term events with a\nsoccer penalty kick example. Given the first few frames, it accurately foresees the typical next actions\nof both the kicker and the goalkeeper, due to its understanding of the inherent structure and rules of\nthe game. In addition, as shown in The left side of Figure 56, the sushi preparation sequence illustrates\nGPT-4V\u2019s long-term anticipation capability. By understanding the activity based on visual cues,\nGPT-4V not only recognizes the current progress in sushi preparation but also accurately anticipates\nthe subsequent steps, demonstrating its capacity to interpret and predict complex, multi-step processes\nover an extended period. This combination of short-term and long-term temporal anticipation allows\nGPT-4V to capture and understand activities with varying temporal structures and complexities.\nTemporal localization and reasoning. Figure 57 illustrates GPT-4V\u2019s capabilities in temporal\nlocalization and reasoning. It accurately identifies the precise moment the player strikes the ball. Fur-\n73 Action Recognition Sec. 6.1 Multi-image Sequencing \nPrompt: \nExplain the images and the action the person is taking. \nGPT-4V: \nThe images show a person in a motion capture suit in a studio with multiple cameras and sensors. The person \nis performing a push-up exercise. In the first image, the person is in the starting position with their arms \nextended and body in a straight line. In the second image, the person is lowering their body towards the ground. \nIn the third image, the person is at the bottom of the push-up with their chest close to the ground. In the fourth \nimage, the person is pushing their body back up to the starting position. \nFigure 53: Sequences of video frames understanding: Interpreting human poses and deriving relevant\ninsights from video sequences. Check Section 6.1 for detailed discussions.\nthermore, GPT-4V showcases its understanding of cause and effect by inferring from the relationship\nbetween the goalkeeper and the ball to determine if the goalkeeper successfully blocks the ball. In\nthe context of the example given, understanding whether the goalkeeper can block the ball involves\nnot only recognizing the spatial positions of the goalkeeper and the ball but also understanding the\ndynamics of their interaction and predicting the outcome of these dynamics. This demonstrates a\nconsiderable level of sophistication in the model\u2019s reasoning abilities.\n74 Temporal Ordering Sec. 6.2 Video Understanding \nPrompt: \nAnalyze the images and decide the correct order in \nwhich they should be displayed. \nGPT-4V: \nThe correct order in which the images should be \ndisplayed is: \n1. e - Mixing the rice \n2. c - Placing the rice on the seaweed \n3. b - Adding the fillings \n4. a - Cutting the roll \n5. d - Finished sushi rolls on a plate. a\nb\ncd\nePrompt Continued \nFigure 54: Long-term temporal ordering: GPT-4V is presented with shuffled image frames depicting\na sushi-making event. While the sushi-making process is disordered, GPT-4V is able to identify the\nevent and determine the correct temporal sequence. Check Section 6.2 for detailed discussions.\n75 Temporal Ordering Sec. 6.2 Video Understanding \nPrompt: \nFor opening a door, decide the correct order in which \nthey should be displayed. \nGPT-4V: \nThe correct order would be: \n1. a - Person approaching the door \n2. b - Person opening the door \n3. c - Person entering through the door.  a\nb\ncPrompt: \nFor closing a door, decide the correct order in which \nthey should be displayed. \nGPT-4V: \nCorrect order: \n1) c - Person standing in doorway \n2) b - Person starting to close door \n3) a - Person closing door completely a\nb\nc\nFigure 55: Short-term temporal ordering: given a specified action, such as opening or closing a door,\nGPT-4V demonstrates its capability to comprehend the images\u2019 content and determine the correct\nsequential order corresponding to the specified action. Check Section 6.2 for detailed discussions.\n76 Temporal Anticipation Sec. 6.2 Video Understanding \nPrompt: \nPredict what will happen next based on the images. \nGPT-4V: \nBased on the images, it appears that the player in \nthe white jersey is taking a penalty kick. The next \nlikely event is that the player will kick the ball \ntowards the goal, attempting to score. The \ngoalkeeper in the black jersey will try to block the \nshot. Prompt: \nAnalyze the images and decide what the next steps \nshould be. \nGPT-4V: \nThe images show the process of making sushi rolls. \nThe first image shows rice being mixed with vinegar \nin a bowl. The second image shows the rice being \nspread onto a sheet of seaweed. The third image \nshows the addition of salmon and cucumber to the \nroll.  The next steps would be to roll the sushi using a \nbamboo mat, cut the roll into smaller pieces, and \nserve with soy sauce and wasabi. \nFigure 56: Short-term and long-term temporal anticipation: GPT-4V captures and understands activi-\nties with varying temporal structures and complexities. Check Section 6.2 for detailed discussions.\n77 Temporal Localization and Reasoning Sec. 6.2 Video Understanding \nPrompt: \nIn which frame does the player kick the ball? Did \nthe goalkeeper block the ball? \ncPrompt Continued \nGPT-4V: \nThe player kicks the ball at frame c. The ball was not \nblocked by the goalkeeper as it can be seen in frame f \nthat the ball is in the net and the goalkeeper is on the \nground. ba\nfed\nFigure 57: Temporal localization and reasoning: GPT-4V shows the capability in temporal localization\nby accurately identifying when the player strikes the ball. It also demonstrates cause-and-effect\nreasoning by determining whether the ball was blocked based on the goalkeeper-ball interaction.\nCheck Section 6.2 for detailed discussions.\n78 Grounded Temporal Understanding Sec. 6.3 Video Visual Referring Prompting \nPrompt: \nDescribe and summarize what the circled person \ndoing? \nGPT-4V: \nThe circled person is a man who is standing next to \na bench and appears to be in a conversation with \nanother man who is sitting on the bench. In the \nsecond image, the circled man is pointing towards \nsomething and in the third image, he is playfully \npretending to punch the other man. Overall, it \nseems like the circled man is engaging in a friendly \nconversation and joking around with the other man. Prompt: \nSummarize the activities of the person. \nGPT-4V: \nThe circled person is sitting on a bench and then \nstands up and adjusts his shirt. \nFigure 58: Grounded temporal understanding: GPT-4V can apply a temporal understanding to a\nspecific person of interest, indicated by a circle. Check Section 6.3 for detailed discussions.\n6.3 Visual Referring Prompting for Grounded Temporal Understanding\nSection 5 illustrates GPT-4V\u2019s capabilities in visual referring prompting. In this section, we aim\nto extend this capability by testing visual referring prompting for temporal understanding. This\nadvancement offers enhanced control over video comprehension tasks.\nGrounded temporal understanding. Grounded temporal understanding forms another crucial aspect\nof GPT-4V\u2019s capabilities, which we explore using pointing input in a sequence of image frames.\nFigure 58 exemplifies this by demonstrating how GPT-4V can apply a temporal understanding to a\nspecific person of interest, indicated by a circle. GPT-4V can accurately describe events in a way\nthat aligns with the corresponding temporal order, focusing on the activities of the circled individual.\nBeyond this, GPT-4V demonstrates a more refined understanding of the event, recognizing the nature\nof the interactions. For instance, GPT-4V can distinguish between friendly interactions and violent\nincidents, illustrating an ability to not only comprehend the temporal flow of events but also to\ninterpret the tone and nature of the interactions taking place. This indicates GPT-4V\u2019s capacity to\nprocess and comprehend complex temporal and social cues within a given sequence, adding a layer\nof depth to its understanding.\n79 7 Abstract Visual Reasoning and Intelligence Quotient Test\nUnderstanding and reasoning over abstract visual stimuli and symbols is one fundamental ability for\nhuman intelligence. This section examines if GPT-4V can abstract semantics from visual signals and\ncan perform different types of human Intelligence Quotient (IQ) tests.\n7.1 Abstract Visual Stimuli\nHumans can infer semantics from abstract and often ambiguous visual stimuli. Figure 59 explores\nhaving GPT-4V interpret tangram [ 31,97,43,59]. A tangram is a traditional geometric puzzle that\nconsists of seven flat pieces called tans, which are put together to form shapes without overlapping\nthe pieces. For example, GPT-4V interprets that sub-figure 7 in Figure 59 best illustrates a flying\ngoose and provides reasoning descriptions for other sub-figure, e.g., 4. person or robot, 9. boat or\nhat, and 10. dog or fox. GPT-4V also has the ability to understand other formats of abstract visual\ndiagrams [ 127,16,150], such as ASCII text art of cartoon characters in Figure 59 and symbolic\ninputs in Figures 61-62.\n7.2 Discovery and Association of Parts and Objects\nDiscovering and associating object parts [ 139,44] is another important abstract visual reasoning\ncapability. Humans can easily discover how object parts may compose a semantically meaningful\nobject. Figure 60 designs examples to probe GPT-4V\u2019s capability in associating object parts. In the\nleft example, we ask GPT-4V to localize an object part based on its semantic meaning. In the right\nexample, GPT-4V is asked to associate object parts segmented by SAM [ 65]. GPT-4V can process\nfigures for all object parts and associate them in a semantically meaningful to form the boy visualized\nin the bottom right.\n7.3 Wechsler Adult Intelligence Scale\nSection 7.1 demonstrates the abstract visual understanding capability of GPT-4V. As a further\nchallenge, GPT-4V is asked to perform different abstract reasoning tasks, sourced from human\nIntelligence Quotient (IQ) tests. The Wechsler Adult Intelligence Scale [ 133] is recognized as one\nof the \u201cgold standard IQ tests,\u201d and is designed to provide a comprehensive measurement of an\nindividual\u2019s cognitive abilities using a series of sub-tests. Figure 61 shows representative questions\nand GPT-4V\u2019s outputs from each sub-test category. GPT-4V shows promises in abstract reasoning,\nanswering questions with texts only, symbolic visual inputs, and natural images. For example,\nthe bottom right sample shows that GPT-4V can interpret the analogy question and find the best\ncomparison of shoes.\n7.4 Raven\u2019s Progressive Matrices\nRaven\u2019s Progressive Matrices (RPM) [ 109] is another well-known non-verbal intelligence test devel-\noped to measure abstract reasoning and problem-solving abilities. The test is designed to minimize\nthe influence of language, culture, and formal education on test performance, making it suitable\nfor testing AI models [ 16,150,55]. Each test sample contains three or eight images, arranged in\n2-by-2 or 3-by-3 matrices with one figure missing. The goal is to select the next image from multiple\ncandidate images by identifying patterns in the provided samples. In our approach, we challenge\nGPT-4V by sending the entire question page as a single image, instead of converting it into interleaved\nimage-text pairs, similar to the human approach to IQ tests. As shown in Figure 62, GPT-4V can\ngenerate reasonable answers without processed text descriptions or sub-figures. However, we also\nnotice that breaking down the entire question image into interleaved text and sub-figures, such as in\nFigure 63, does simplify the task and let GPT-4V produce more reliable answers.\n80 Sec. 7.1 Abstract Visual Stimuli \n7.\n8.\n9.\n10.\nGPT -4V : \nThe tangram that best illustrates a flying goose is number \n7. It has the triangular shape of a goose's body and \nwings, and the smaller triangle at the top could represent \nthe head and neck. \nThe other figures could represent various things: \n1. A rocket or a tower \n2. A butterfly or a bat \n3. A question mark or a snake \n4. A person or a robot \n5. A mountain range or a crown \n6. A house or a temple \n7. A flying goose (as mentioned above) \n8. A bird or a fish \n9. A boat or a hat \n10. A dog or a fox. Prompt: \nA tangram is a geometric puzzle that consists of \nseven flat pieces to form shapes. Here are ten \nabstract tangram. Which one best illustrates a \nflying goose? What do other figures look like? \n1.\n2.\n3.\n4.\n5.\n6.\nPrompt Continued \nPrompt: \nHere is an abstract figure. Can you identify how many characters are there, and the cartoon from which they \noriginate? \nGPT -4V : \nThere are four characters in this abstract figure. They originate from the cartoon \"South Park.\" The characters \nare Eric Cartman, Kyle Broflovski, Stan Marsh, and Kenny McCormick. \nFigure 59: Understanding abstract visual stimuli such as tangram [ 59] and ASCII text art. Check\nSection 7.1 for detailed discussions.\n81 Sec. 7.2 Association of Parts and Objects \nDiscovery and Association of Parts and Objects \nPrompt: \nA tangram is a geometric puzzle that consists of seven \nflat pieces to form shapes. Locate the referred object \nand represent the location of the region. Regions are \nrepresented by (x1,y1,x2,y2) coordinates. x1 x2 are \nthe left and right most positions, normalized into 0 to 1, \nwhere 0 is the left and 1 is the right. y1 y2 are the top \nand bottom most positions, normalized into 0 to 1, \nwhere 0 is the top and 1 is the bottom. For example, \nthe (x1,y1,x2,y2) coordinates for that region with the \nbeer bottle is (0.47, 0.48, 0.55, 0.87). \nThe image below that shows a flying goose. Which \npart represents the head of the bird? \nWhat is the (x1,y1,x2,y2) coordinates for the region \nwith the head of the bird? \nGPT-4V: \nThe head of the bird is represented by the small \ntriangle at the top of the image. The (x1,y1,x2,y2) \ncoordinates for the region with the head of the bird \nwould be approximately (0.42, 0.15, 0.58, 0.35). \nPlot Visualization: \nRefer to the query image below \nGrounded VQA \nPrompt: \nWhat is the highlighted part? \nGPT-4V: \nThe highlighted part is the head of the goose. 0.48\n0.87\n0.470.55Prompt: \nHow are those visual parts related, and can they be \ncombined to form a single object such as a boy? If so, \nhow to arrange them? \nGPT-4V: \nThe three visual parts are related in that they are all \ncomponents that could be used to create a single \nobject. In this case, the object is a boy. \nTo combine these parts to form a boy, we can arrange \nthem in the following way: \n- The first image (the cardboard with orange and \ngreen shapes) could be used as the body of the boy. \n- The second image (the pair of jeans) could be used \nas the legs of the boy. \n- The third image (the cardboard with yellow, red, and \nblue shapes) could be used as the head of the boy. \nBy arranging these parts in this way, we can create a \nsingle object that resembles a boy. \nPlot Visualization: \n0.15\n0.35\n0.420.58\nFigure 60: Understanding part-object association in abstract and natural images. Check Section 7.2\nfor detailed discussions.\n82 Sec. 7.3 Wechsler Adult Intelligence Scale \nPrompt: \nWhich one of the five is least like the \nother four? \na. Carrot \nb. Potato \nc. Corn \nd. Apple \ne. Bean \nGPT -4V : \nd. Apple \nExplanation: Carrot, potato, corn, and \nbean are all vegetables, while apple is \na fruit. \nPrompt: \nWhich one of the five is least like the \nother four? \na.\nb.\nc.\nd.\ne.\nGPT -4V : \nAnswer: d.  is the least like the other \nfour because it is the only one that \ndoes not have a central point \nconnecting all the other points. Prompt: \nTom, twelve years old, is three times as old as his sister. How old will \nTom be when he is twice as old as his sister? \na. 18 \nb. 21 \nc. 16 \nd. 20 \ne. 15 \nGPT -4V : \nIf Tom is three times as old as his sister, then his sister is one-third of \nhis age. So, his sister is 4 years old. Now, we need to figure out when \nTom will be twice as old as his sister. This means that his sister's age \nwill be half of his age. So, we need to find a number that is double the \nage of 4. The answer is 8. So, Tom will be 16 years old when he is \ntwice as old as his sister. The correct answer is c. 16. \nPrompt: \nWhich one of the five designs makes the best comparison? \na.\nb.\nc.\nd.\ne.\nGPT -4V : \nAnswer: c. The black dress shoe \nExplanation: The analogy is comparing a hand to a glove and a foot to \na shoe. The best comparison would be the black dress shoe as it is \nan item of clothing that is worn on the foot, just like a glove is worn on \nthe hand. \nFigure 61: Example questions from the Wechsler Adult Intelligence Scale (WAIS) [ 133]. Check\nSection 7.3 for detailed discussions.\n83 Sec. 7.4 Raven's Progressive Matrices \nRPM: Entire Question Page \nPrompt: \nFind out the pattern in the first column and use it to infer the missing figure in the second column. \nGPT-4V: \nThe correct drawing that fits the empty space is the upside-down triangle. This is because the pattern in the grid is \nthat each row and column has one of each shape in different orientations. \nPrompt: \nFind out the pattern in the first two rows and use it to infer the missing figure in the third row. \nGPT-4V: \nThe pattern in the first two rows is that the figures are rotated 90 degrees clockwise in each subsequent cell. \nTherefore, the missing figure in the third row should be a rotated version of the figure in the previous cell. \nLooking at the options, the correct answer is OC, as it is the figure that is rotated 90 degrees clockwise from the \nfigure in the previous cell. \nFigure 62: Example questions from the Raven\u2019s Progressive Matrices [ 109,55]. We challenge\nGPT-4V by sending the entire question page as a single image, mimicking how humans look at the\nIQ tests. Check Section 7.4 for detailed discussions.\n84 RPM: Processed Sub-figures \nPrompt: \n1.\n2.\n3. \n4.\nHere are three sub images arranged in a 2-by-2 \nmatrix. Figure out which figure of a to f that should \nlogically replace the missing figure. What would be \nthe next image to complete the matrix? \na.\nb. \nc. \nd. \ne. \nf. \nThe first image in the first column, image 1, is a \ntriangle. The first image in the second column, \nimage 3, is a triangle with a line through it. The \nsecond image in the first column, image 2, is a \ndiamond. Therefore, the second image in the \nsecond column, image 4 should continue this \npattern and be a diamond with a line through it. \nOption b is the only image that fits this pattern, as it \nis a diamond with a line through it. Therefore, the \ncorrect answer is b. 1.\n2.\n3. \n4.\nHere are three sub images arranged in a 2-by-2 \nmatrix. Figure out which figure of a to f that should \nlogically replace the missing figure. What would be \nthe next image to complete the matrix? \na.\nb. \nc. \nd. \ne. \nf. \nGPT-4V: \nThe first image in the first column, image 1, is a \ndiamond. The first image in the second column, \nimage 3, is a diamond with a dot in the middle. The \nsecond image in the first column, image 2, is a star. \nTherefore, the second image in the second column, \nimage 4 should continue this pattern and be a star \nwith a dot in the middle. Option c is the only image \nthat fits this pattern, as it is a star with a dot in the \nmiddle. Therefore, the correct answer is c. \nPrompt Continued Sec. 7.4 Raven's Progressive Matrices Figure 63: Instead of sending the entire question page as a single image, we may also process the\nimage into multiple sub-figures and optionally provide detailed instructions and examples to further\nboost the answer accuracy. Check Section 7.4 for detailed discussions.\n85 8 Emotional Quotient Test\nSec. 8.1 Read Emotion from Facial Expressions \nPrompt: \nIdentify and read emotions of people from their faces \nas shown in the image below. \nGPT -4V : \nThe person in the image appears to be displaying a \ncombination of emotions. The furrowed brows and \ndownturned mouth suggest anger or frustration, while \nthe tense jaw and narrowed eyes could indicate \ndisgust or contempt. Overall, it seems like the person \nis experiencing negative emotions and is likely upset or \nunhappy. \nPrompt: \nIdentify and read emotions of people from their faces \nas shown in the image below. \nGPT -4V : \nThe person in the image appears to be experiencing a \nnegative emotion, likely frustration or disappointment. \nThis is evident from the furrowed brows, downward \ngaze, and hand on the forehead. Prompt: \nIdentify and read emotions of people from their faces \nas shown in the image below. \nGPT -4V : \nThe emotion shown in the image is fear or anxiety. The \nperson's eyes are wide open and their mouth is slightly \nopen, indicating a sense of shock or surprise. Their \nhands are covering their mouth, which could suggest \nthat they are trying to suppress a scream or gasp. \nPrompt: \nIdentify and read emotions of people from their faces \nas shown in the image below. \nGPT -4V : \nThe person in the image is displaying a happy emotion. \nThis can be seen from the wide smile and the crinkles \naround the eyes. \nFigure 64: GPT-4V can reliably identify and read the emotions of people from their facial expressions.\nCheck Section 8.1 for detailed discussions.\nWhen interacting with humans, it is important that GPT-4V has the empathy and Emotional Quotient\n(EQ) to understand and share the feelings of humans. Inspired by the definition of the human EQ\ntest [ 92,91,21], we examine GPT-4V\u2019s capability in (1) identifying and reading human emotions\nfrom their facial expressions, (2) understanding how different visual contents may arouse emotions,\nand (3) generating proper text outputs conditioned on the desired emotional and sentiment.\n8.1 Read Emotion from Facial Expressions\nAs shown in Figure 64, GPT-4V can reliably identify and read the emotions of people from their\nfacial expressions. It also provides reasonable rationales for the visual cues observed to make the\nemotion interpretation, indicating a good understanding of the facial emotions.\n86 Sec. 8.2 How Visual Content Arouses Emotions \nPrompt: \nAfter seeing the below image, how people might \nemotionally feel and react? \n.\nGPT -4V : \nPeople might feel happy, content, and relaxed when \nseeing this image. They might react with a smile or a \nfeeling of warmth and comfort. This image appears to \nbe of a couple at a music festival, so people might also \nfeel a sense of excitement and anticipation for the \nmusic and performances. Prompt: \nAfter seeing the below image, how people might \nemotionally feel and react? \nGPT -4V : \nPeople might feel angry, frustrated, or upset after \nseeing this image. They might react by feeling a \nsense of injustice or wanting to take action to support \nthe man in the image. Some people might also feel \nfear or anxiety about the situation and the presence of \nthe police. \nPrompt: \nAfter seeing the below image, how people might \nemotionally feel and react? \nGPT -4V : \nPeople might feel a sense of awe and wonder at the \nbeauty of the sunset and the colors in the sky. They \nmay feel peaceful and calm, or even inspired. Some \nmay react with a sense of gratitude for being able to \nwitness such a beautiful moment in nature. Others may \nfeel a sense of longing to be in a place like this, or a \ndesire to travel and see more of the world's beauty. Prompt: \nAfter seeing the below image, how people might \nemotionally feel and react? \nGPT -4V : \nPeople might feel scared, anxious, or uneasy after \nseeing this image. The dark tones and shadows \ncreate a sense of mystery and unease, and the \nsilhouette of the figure on the stairs could be \ninterpreted as ominous or threatening. Some people \nmight react by feeling a sense of dread or fear, while \nothers might be intrigued or curious about the context \nof the image. \nFigure 65: GPT-4V understands how different visual contents may arouse human emotions. Check\nSection 8.2 for detailed discussions.\n8.2 Understand How Visual Content Arouses Emotions\nWe next analyze GPT-4V\u2019s ability on visual sentiment analysis, i.e., understanding humans\u2019 emotional\nresponse after seeing the visual contents. Such ability is critical for GPT-4V to anticipate how visual\ncontents may arouse human emotions and thereby react properly. As shown in Figure 65, GPT-4V can\ninterpret visual sentiments such as content, anger, awe, and fear, based on both the semantic contents\nand the image style. These capabilities are essential in use cases such as home robots.\n87 Sec. 8.2 How Visual Content Arouses Emotions \nImage Aesthetics \nPrompt: \nAesthetics reflects the perceived beauty or visual \nappeal of an object, design, or piece of art by human. \nAmong the images provided below, based on societal \nstandards and norms, which one more people will \nconsider having a higher aesthetics? \nGPT-4V: \nBased on societal standards and norms, more people \nwill likely consider the second image (the colorful \nhouses) as having a higher aesthetics. This is \nbecause the image is more visually appealing and \nhas a more artistic and creative feel to it. The first \nimage (the house) is more plain and ordinary, and \nwhile it may be considered aesthetically pleasing to \nsome, it is not as striking or eye-catching as the \nsecond image. Prompt: \nAesthetics reflects the perceived beauty or visual \nappeal of an object, design, or piece of art by human. \nAmong the images provided below, based on societal \nstandards and norms, which one more people will \nconsider having a higher aesthetics? \nGPT-4V: \nMost people would likely consider the first image to \nhave a higher aesthetic value. This is because the \nimage is more visually appealing, with a variety of \ncolors, buildings, and greenery. The second image is \nless visually appealing, with a more monochromatic \ncolor scheme and less variety in the buildings and \nlandscape. Additionally, the first image is of a more \niconic and recognizable location (New York City), which \nmay also contribute to its perceived aesthetic value. \nFigure 66: GPT-4V judges image aesthetics based on societal standards and norms. Check Section 8.2\nfor detailed discussions.\nIn addition to interpreting visual sentiment, GPT-4V also aligns with human subjective judgments\nsuch as aesthetics. Figure 66 shows examples of GPT-4V judging image aesthetics based on societal\nstandards.\n88 Sec. 8.3 Emotion Conditioned Output \nPrompt: \nDescribe the image in a humorous way. \nGPT -4V : \nIt looks like the sun is trying to hide behind the \nhorizon, but it's not doing a very good job. It's like it's \nplaying peek-a-boo with us. And the seaweed is just \nchilling there, like \"Hey, don't mind us, we're just \nhanging out.\" The rocks are probably feeling left out, \nthey're like \"Hey, what about us? We're part of this \nbeach too! \nPrompt: \nDescribe the image to make people feel uneasy and \nanxious. \nGPT -4V : \nThe image is of a dark, shadowy staircase. The only \nlight comes from a single, dim bulb, casting eerie \nshadows on the walls. The railing is old and rusted, \nwith peeling paint and jagged edges. The stairs \nthemselves are uneven and creak underfoot. The air is \nthick and musty, and there is a feeling of unease as if \nsomeone is lurking in the shadows. The silence is \ndeafening, and every creak and groan of the old \nstaircase echoes through the empty space. \nPrompt: \nDescribe the image to lessen someone's desire to \nvisit.\nGPT -4V : \nThe image shows a rocky beach at sunset with \nseaweed and algae covering the ground. The water \nis murky and there are no clear areas to swim or \nrelax. The rocks are sharp and slippery, making it \ndifficult to walk or explore. The sunset is partially \nblocked by clouds. It seems like a difficult and \nunpleasant place to navigate. Prompt: \nDescribe the image to make people feel relax and \ncomforting. \nGPT -4V : \nThe image is of a dimly lit staircase with ornate railings. \nThe light is soft and creates a peaceful atmosphere. \nThe shadows on the wall add depth and texture to the \nimage, making it feel cozy and inviting. The overall \nmood of the image is calm and serene, making it a \nperfect place to relax and unwind. \nFigure 67: GPT-4V generates proper text based on the perceived or desired emotions, making its\ncommunication with humans comforting and effective. Check Section 8.3 for detailed discussions.\n8.3 Emotion Conditioned Output\nBased on the perceived emotions, GPT-4V effectively generates proper text outputs conditioned\non the desired emotion. For example, in Figure 67, GPT-4V can follow the prompt to describe\nthe right-side scary image in a way that makes it more horrifying or becoming comforting. This\ndemonstrates GPT-4V\u2019s potential to enable emotion-aware human-robot communication.\n89 9 Emerging Application Highlights\nIn this section, we showcase a myriad of high-value application scenarios and new use cases that can\nbe potentially enabled by the remarkable capabilities of GPT-4V. While it is true that some of these\napplication scenarios can be accomplished by meticulously curating the training data for finetuning\nexisting Vision and Language (VL) models, we want to emphasize that the true power of GPT-4V\nlies in its ability to perform effortlessly right out of the box. Moreover, we present how GPT-4V\nseamlessly integrates with external tools and plugins, further expanding its potential and enabling\neven more innovative and collaborative applications.\n9.1 Spot the Difference\nWe begin with a generic use case inspired by the brain-teasing game \u201cSpot the Difference.\u201d In\nFigures 68-69, we provide GPT-4V two visually similar images that contain subtle differences in\ncertain regions. The task given to GPT-4V is to identify all the differences between the two images.\nAmong the four examples, GPT-4V successfully identifies the regions or components that differ\nin the images. However, it falls short in providing accurate explanations for what is depicted in\neach image. To delve deeper into GPT-4V\u2019s capabilities, let\u2019s focus on the first example shown in\nFigure 68. Although GPT-4V fails to recognize that the discrepancy lies in the number of cuts in the\nhairband rather than the shade of the hair, it correctly identifies that the crown, the bow of the dress,\nand the hair differ between the two images. While GPT-4V\u2019s predictions in the \u201cSpot the Difference\u201d\ngame are not perfect, its ability to compare the content in two images proves valuable in real-life\napplications, such as defect detection, which we will explore in the following subsections.\n9.2 Industry\nDefect detection. Throughout the history of manufacturing, computer vision techniques have\nplayed a crucial role. One specific application scenario is defect detection, which is an essential step\nin manufacturing processes to ensure product quality. Detecting faults or defects in a timely manner\nand taking appropriate actions are vital for minimizing operational and quality-related costs.\nIn this scenario, we demonstrate the defect detection capabilities of GPT-4V by presenting images of\ndefective products in Figures 70-71. For commonly encountered products in real-life (e.g., hazelnut,\nfabric, screw, and car bumper in Figure 70), GPT-4V confidently identifies the defects such as small\nholes in the hazelnut/fabric, stripped heads of screws, and dents in car bumpers. However, when\nit comes to uncommon product images (e.g., the metal parts in Figures 70-71) or products with\nvariations in appearance (e.g., the pill in Figure 71), GPT-4V may hesitate or even refuse to make\npredictions. An interesting case in Figure 71 involves a car tire, where multiple defects can be\nobserved in the image, including dirt on the wheel, damage to the outer edge of the rim, and signs of\nwear on the tire. GPT-4V only focuses on the minor defect (dirt on the wheel) and fails to mention\nthe major defect (damage to the outer edge of the rim) that would require repair.\nGiven the success of GPT-4V in \u201cSpot the Difference\u201d scenario shown in Section 9.1, we explore\nthe idea of incorporating a reference image to illustrate what a defect-free product should look like,\nwith the aim of improving the failure cases depicted in Figure 71. The results of this approach\nare presented in Figure 72. By including the reference image and refining the prompt, GPT-4V\nsuccessfully identifies defects in all three failure cases in single-image defect detection. These\npromising findings highlight a potential high-value application of GPT-4V for defect detection in the\nmanufacturing industry.\nSafety inspection. Figure 73 presents an exploration of Personal Protective Equipment (PPE)\ncounting for safety inspection. The inadequate usage or failure to wear PPE, such as helmets,\nharnesses, and gloves, in work environments like construction sites, significantly increases the risk\nlevel associated with work activities. To effectively address this issue, computer vision techniques\nhave been employed as a solution to monitor PPE compliance and promptly identify any violations\nof safety regulations. Taking helmets as an example, a safety inspection system is necessary to\naccurately detect and report the number of employees who are not wearing helmets.\nIn Figure 73a, we assess the performance of GPT-4V by directly instructing it to count the individuals\nwearing helmets. GPT-4V provides a response of \u201c8 persons wearing helmets,\u201d which matches the\n90 total count of people shown in the image, suggesting there is no alerting safety violations. Obviously,\nGPT-4V fails to detect the 3 individuals who are not wearing helmets, thus compromising their\npersonal safety. This task poses a considerable challenge for GPT-4V, as it involves detecting people\nin the image, determining whether they are wearing helmets, and calculating the final count of people\nwho are not wearing the helmets.\nIn Figure 73b, instead of presenting GPT-4V with the original image containing all 8 individuals,\nwe provide cropped regions of the detected persons with an external person detector. This approach\ndivides the PPE counting workload into two steps: relying on an off-the-shelf person detector for\nperson detection and leveraging GPT-4V\u2019s robust visual reasoning capabilities and its ability to\nhandle interleaved image-text inputs for identifying the safety issues. As we can see, GPT-4V can\ncorrectly count the person who is not wearing the helmet, also demonstrating the benefit of tool use\nand divide-and-conquer.\nGrocery checkout. Self-checkout machines have become increasingly popular in major retailers\nlike Walmart, Target and CVS to expedite the checkout process for customers and reduce the workload\nfor employees. However, the actual experience with self-checkout machines may be frustrating for\ncustomers. Users still need to search for the product barcode or manually enter codes for fresh items\nlike apples, which can be time-consuming, particularly for those unfamiliar with the system. In\nFigure 74, we provide a simplified prototype to demonstrate the potential of GPT-4V in enabling an\nautomatic self-checkout system that can identify and ring up items without user intervention.\nWhen presented with a photograph of a shopping basket containing five grocery items, as shown\nin Figure 74a, GPT-4V fails to accurately identify the products within the basket. It mistakenly\nidentifies strawberries as raspberries, crab dip as Greek yogurt, and includes salmon fillets that are\nnot even present in the basket. However, in Figure 74b, we improve the prompt by augmenting it with\ncatalog images of grocery products retrieved from the retail website. As a result, GPT-4V successfully\nidentifies all five items in the basket. This successful demonstration allows the self-checkout system\nto proceed with retrieving the prices for each identified product from the database. While this is a\nsimple example, it represents a significant step forward toward an automated self-checkout system.\nFurther research and development can explore more complex and realistic scenarios to fully automate\nthe self-checkout process, making it more efficient and convenient for customers.\n91 Prompt: \nCompare Image 1 and image 2, tell me about the differences between image 1 and image 2. \nImage 1 \nGPT -4V : \nThere are a few differences between image 1 and image 2: \n1. The crown: in image 1, the crown is pink with a blue gem in the center. In image 2, the crown is gold with \nred gems and a cross on top. \n2. The dress: in image 1 the dress has a red bow in the center. In image 2, the bow is missing. \n3. The hair: in image 1, the hair is a lighter shade of brown compared to image 2.   Sec. 9.1 Spot the Difference \nImage 2 \nPrompt: \nCompare Image 1 and image 2, tell me about 3 differences between image 1 and image 2. \nImage 1 \nGPT -4V : \n1) In image 1, there are two blue coral reefs while in Image 2, there is only one blue coral reef. \n2) In image 2, there is an additional green plant that is not present in image 1. \n3) In image 1, the turtle\u2019s head is facing downwards. In image 2 the turtle\u2019s head is facing upwards. Image 2 \nFigure 68: Spot the differences. Red highlights the inaccurate description about the differences.\nCheck Section 9.1 for detailed discussions.\n92 Prompt: \nCompare Image 1 and image 2, tell me about the 3 differences between image 1 and image 2. \nImage 1 \nGPT -4V : \n1) In image 1, there is no sun in the background, while in image 2, there is a sun in the top left corner. \n2) In image 1, there is a single bamboo leaf on the ground, while in image 2, there is a pile of bamboo  \nleaves on the ground. \n3) In image 1, the panda is holding a single bamboo stalk , while in image 2, the panda is holding multiple  \nbamboo stalks. Image 2 \nPrompt: \nCompare Image 1 and image 2, tell me about 3 differences between image 1 and image 2. \nImage 1 \nGPT -4V : \n1) Image 1 has spikes on its back while image 2 does not. \n2) Image 2 has a tail while image 1 does not. \n3) Image 2 has a mouth with teeth  while image 1 does not have a mouth Image 2 \nSec. 9.1 Spot the Difference Figure 69: Spot the differences. Red highlights the inaccurate description about the differences.\nCheck Section 9.1 for detailed discussions.\n93 Defect Detection GPT-4\nV\nPrompt: \nWhat is wrong with the object in the image? \nGPT-4V: \nThere is a small hole in the object, which appears \nto be a hazelnut. This could be a sign of insect \ndamage or rot. \nPrompt: \nWhat is wrong with the object in the image? \nGPT-4V: \nIt is difficult to tell exactly what the object in the image \nis, but it appears to be a car bumper or fender with a \nlarge dent in it. The problem with the object is that it is \ndamaged and dented, which may affect the \nappearance and functionality of the car. \nPrompt: \nWhat is wrong with the object in the image? \nGPT-4V: \nThe object in the image appears to be a screw or \nbolt with a damaged or stripped head. The \ngrooves in the head, which would normally allow \na screwdriver or wrench to grip and turn the \nscrew, are worn away and no longer functional. Prompt: \nWhat is wrong with the object in the image? \nGPT-4V: \nThere is a small tear or hole in the fabric. \nPrompt: \nWhat is wrong with the object in the image? \nGPT-4V: \nIt is difficult to say for sure what is wrong with the  \nobject in the image as there is no context or  \ninformation about what the object is or what it is  \nsupposed to look like. However, it appears that there \nmay be a crack on one of the edges of the object. \nPrompt: \nWhat is wrong with the object in the image? \nGPT-4V: \nIt is difficult to say for sure what is wrong with the  \nobject in the image as there is no context or  \ninformation about what the object is or what it is  \nsupposed to look like.  However, it appears that \nthe copper wires in the blue and brown sections \nare frayed and not neatly arranged, which could \nindicate damage or poor quality. \nSec. 9.2 Industry Figure 70: Defect detection with a single image. Yellow highlights the cases when GPT-4V is\nhesitating to make the predictions. Check Section 9.2 for detailed discussions.\n94 Defect Detection GPT-V Sec. 9.2 Industry \nPrompt: \nWhat is wrong with the object in the image? \nGPT-4V: \nSorry, I cannot answer this question as there is no  \ncontext or information about what the object is or what  \nit is supposed to look like.   Prompt: \nWhat is wrong with the object in the image? \nGPT-4V: \nThere is not enough information provided to  \ndetermine if there is anything wrong with the  \nobject in the image. It appears to be a capsule  \nmedication with the number 500 printed on it, but  \nwithout further context, it is impossible to know if  \nthere is anything wrong with it.  \nPrompt: \nWhat is wrong with the object in the image? \nGPT-4V: \nThere is large amount of brake on the rim of the wheel. This is typically caused by the brake pads wearing \ndown and creating dust as they come in contact with the rotors. Additionally, the tire appears to be low on air \npressure, which can affect the vehicle\u2019s handling and fuel economy. \nFigure 71: Failure examples of defect detection with a single image. Red highlights the cases when\nGPT-4V fails. Check Section 9.2 for detailed discussions.\n95 Defect Detection GPT-4\nV\nPrompt: \nGiven Image 1, tell me what is wrong in image 2. \nImage 1 \nGPT-4V: \nIn image 2, there is a small white mark on the \norange part of the pill. This could be a defect or \ndamage to the pil. Image 2 \nPrompt: \nGiven Image 1, tell me what is wrong in image 2. \nImage 1 \nGPT-4V: \nIn image 2, there is a small white spot on the \nlower left side of the inner circle. This could be a \ndefect or damage on the surface of the object. Image 2 \nPrompt: \nGiven image 1, tell me if there are any defects \nseen on the object in image 2? \nImage 1 \nGPT-4V: \nYes, there are a few defects seen on the object in image 2: \n- There is a significant amount of dirt and grime on the wheel, which is not present in image 1. \n- The center cap is missing from the wheel in image 2. \n- There appears to be some damage to the outer edge of the rim, which is not present in image 1. \n- The tire tread appears to be more worn in image 2 than in image 1. Image 2 \nSec. 9.2 Industry Figure 72: Defect detection with the help of a reference image. Red highlights inaccurate descriptions.\nCheck Section 9.2 for detailed discussions.\n96 Safety Inspection GPT-4\nV\nPrompt: \nHow many people are wearing helmet? \nGPT-4V: \nThere are 8 persons wearing helmets in the image .\nSec. 9.2 Industry (a)\nSafety Inspection GPT-4\nV\nPrompt: \nPlease determine whether the person in the \nimage wears a helmet or not. And summarize \nhow many people are wearing helmets. \nImage 1 \nGPT-4V: \nImage 1: Not wearing a helmet \nImage 2: Not wearing a helmet \nImage 3: Wearing a helmet \nImage 4: Wearing a helmet \nImage 5: Wearing a helmet \nImage 6: Wearing a helmet \nImage 7: Wearing a helmet \nImage 8: Not wearing a helmet \nSummary: Out of 8 people, 5 are wearing helmets  \nand 3 are not wearing helmets.  \nImage 2 \nPrompt Continued \nImage 3 \nImage 4 \nImage 5 \nImage 6 \nImage 7 \nImage 8 \nSec. 9.2 Industry \n(b)\nFigure 73: Application Highlights on Safety Inspection: Personal Protective Equipment (PPE)\nCounting. GPT-4V fails with zero-shot prompting in (a), while succeeds with single person crops in\n(b). Red (Green ) highlights the wrong (correct) answer. Check Section 9.2 for detailed discussions.\n97 Grocery Checkout GPT-4\nV\nPrompt: \nFor below image, find what products are presented in the basket. Try to be accurate \nGPT-4V: \nPink tulips, salmon fillets , strawberries, apple, greek yogurt , sliced ham, raspberries  \nSec. 9.2 Industry (a)\nGrocery Checkout GPT-4\nV\nPrompt: \nHere is a list of reference images: \nShrimp \nGPT-4V: \nThere are 5 products in the basket that are exactly the same \ntype as the reference images: \n- Smoked ham \n- Apple \n- Pink tulips \n- Crab dip \n- Strawberries Crab dip Prompt Continued \nSmoked Ham \nBanana Cheese \nStrawberries \nPink Tulips \nApple \nBread \nFor below image, find what products are presented in the basket \naccording to reference image above. Only tell me products that \nare exactly same type of reference images. \nSec. 9.2 Industry \n(b)\nFigure 74: Application Highlights on Grocery Checkout. GPT-4V fails with zero-shot prompting\nin (a), while succeeds when prompting with reference product images in (b). Red highlights the\nproducts that are not in the basket. Check Sections 9.2, 10.5 for detailed discussions.\n98 9.3 Medical\nIn Section 4.1, the effectiveness of GPT-4V in medical image understanding is demonstrated through\nFigures 18-19. Furthermore, we conducted a detailed investigation into the application of GPT-4V in\nradiology report generation, as depicted in Figures 75-78. In this scenario, we provided GPT-4V with\nvarious medical images and tasked it with generating complete radiology reports. Since assessing the\naccuracy of the generated reports requires domain knowledge, we sought the evaluation of a medical\nprofessional.\nFigure 75 showcases two accurate examples: one involving an abdominal X-ray image and another\nfeaturing an MRI of the right knee. In both cases, GPT-4V correctly identified the study and provided\nan accurate diagnosis. Moving on to Figure 76, we observe the generated radiology reports by GPT-\n4V for X-ray images of the hand/wrist. While GPT-4V successfully diagnosed the recommended\nmanagement based on the first X-ray image, it missed the obvious distal radial fracture present in\nthe second X-ray image. Nevertheless, the generated reports maintain a high-quality format that can\nserve as a template, thus reducing the workload for medical professionals when drafting reports.\nIn Figure 77, we explore two additional examples involving a Chest CT and a Brain MRI. In the Chest\nCT case, GPT-4V mistakenly identified the mentioned nodule on the left side instead of the right side,\nand it also hallucinated the measurements. The ability to process interleaved image-text pairs also\nallows GPT-4V to reference prior medical scans and diagnosis histories, which is shown to be critical\nin medical professionals\u2019 diagnosing processes [ 15]. Figure 78 shows examples of understanding the\nsymptom progression from multiple Chest X-Ray scans [ 60,15]. These illustrations shed light on\nthe potential of GPT-4V to serve as an AI assistant for radiology report generation. However, it is\ncrucial to have the generated reports evaluated by medical professionals to ensure their correctness\nand accuracy.\n99 GPT-4\nV\nPrompt: \nWrite a radiology report for this image as an expert radiologist.. \nGPT-4V: \nRadiology Report \nDate: 03/23/2021 \nExam: Abdominal x-ray \nHistory: Patient presents with abdominal pain and distention \nFindings: The abdominal x-ray demonstrates a dilated stomach and small bowel loops, suggestive of a partial small bowel obstruction. \nThere is also a small amount of air in the rectum. No free intraperitoneal air or pneumoperitoneum is noted. The liver, spleen, and \nkidneys appear unremarkable. \nImpression: Findings are suggestive of a partial small bowel obstruction. Further imaging studies and clinical correlation are \nrecommended.  Radiology Report Generation \nEvaluation from Professionals: \nCorrect study identification, correct diagnosis Sec. 9.3 Medical \nPrompt: \nWrite a radiology report as an expert radiologist for the following case. \nGPT-4V: \nReport: \nMRI of the right knee demonstrates a large joint effusion, which is most pronounced in the suprapatellar region. There is also evidence \nof a tear in the medial meniscus, as well as mild degenerative changes in the articular cartilage. The anterior cruciate ligament and \nposterior cruciate ligament are intact. The extensor mechanism is also intact. No other significant abnormalities are seen. \nImpression: Large joint effusion and medial meniscal tear in the right knee, with mild degenerative changes in the articular cartilage. \nEvaluation from Professionals: \nCorrect study identification, correct diagnosis (effusion and meniscal tear) \nFigure 75: Application Highlights on Radiology Report Generation. The generated report is reviewed\nby a medical professional to evaluate its correctness. Green highlights that a medical professional\nhas confirmed the described part of the report is correct. Check Section 9.3 for detailed discussions.\nThe medical images are from the internet.\n100 GPT-4 \nPrompt: \nWrite a radiology report as an expert radiologist for the following case: \nCase: \nA 32-year-old woman presented with a complaint of pain in her right wrist. The patient reported that she fell on her outstretched hand \nwhile playing basketball. An X-ray of the right wrist was performed. \nGPT-4V: \nRadiology Report: \nDate: 01/01/2021 \nExam: X-ray of the right wrist \nIndication: Right wrist pain after falling on outstretched hand while playing basketball \nFindings: There is a transverse fracture of the distal radius with mild dorsal angulation. No other fractures or dislocations are seen. The \ncarpal bones are in normal alignment. No significant soft tissue swelling is noted. \nImpression: Transverse fracture of the distal radius with mild dorsal angulation. No other fractures or dislocations are seen. \nRecommendation: Orthopedic consultation for management of the fracture. Radiology Report Generation \nEvaluation from Professionals: \nCorrect diagnosis and management Sec. 9.3 Medical \nPrompt: \nWrite a radiology report for this image as an expert radiologist. \nGPT-4V: \nReport: \nDate: 01/01/2021 \nExam: X-ray of the right hand and wrist \nTechnique: AP and lateral views were obtained \nFindings: The bones of the right hand and wrist appear intact and in proper alignment. There is no evidence of fracture or dislocation. \nThe joint spaces are preserved and there is no evidence of degenerative changes. No soft tissue abnormalities are seen. \nImpression: Normal X-ray of the right hand and wrist. \nEvaluation from Professionals: \nMissed the obvious distal radial fracture \nCorrect identification of the type of study \nFigure 76: Application Highlights on Radiology Report Generation. The generated report is reviewed\nby a medical professional to evaluate its correctness. Green (Red ) highlights that a medical\nprofessional has confirmed the described part of the report is correct (incorrect). Check Section 9.3\nfor detailed discussions. The medical images are from the Internet.\n101 Prompt: \nWrite a radiology report for this image as an expert radiologist.. \nGPT-4V: \nReport: \nDate: 6/25/2021 \nStudy Type: Chest CT \nSubject: 42-year-old male \nFindings: The chest CT images demonstrate a nodular opacity in the left upper lobe, measuring approximately 1.3 cm in diameter. The \nopacity appears to have spiculated margins and is located adjacent to the pleura. There is no evidence of mediastinal or hilar \nlymphadenopathy. \nImpression: The findings are concerning for a primary lung malignancy. Further evaluation with a biopsy is recommended to confirm the \ndiagnosis. Radiology Report Generation \nEvaluation from Professionals: \nCorrect study type and identification \nMensions nodule but in the left rather than in the right. \nGives measurements (hallucination) Sec. 9.3 Medical \nPrompt: \nWrite a radiology report as an expert radiologist for the following case. \nGPT-4V: \nReport: \nDate: 01/01/2021 \nModality: Magnetic Resonance Imaging (MRI) \nBody Part: Brain \nTechnique: Axial T1-weighted \nFindings: The MRI of the brain demonstrates a large intra-axial mass in the left parietal lobe with surrounding edema. The mass appears \nto be heterogeneous in signal intensity with areas of high and low signal intensity. There is a significant mass effect with midline shift to \nthe right. The ventricles appear to be compressed and there is evidence of hydrocephalus. No other significant abnormalities are noted. \nImpression: Large intra-axial mass in the left parietal lobe with surrounding edema and significant mass effect. Likely high-grade glioma. \nFurther imaging and clinical correlation are recommended. \nEvaluation from Professionals: \nCorrect study identification, correct mass description and location, correct diagnosis \nFigure 77: Application Highlights on Radiology Report Generation. The generated report is reviewed\nby a medical professional to evaluate its correctness. Green (Red ) highlights that a medical\nprofessional has confirmed the described part of the report is correct (incorrect). Yellow indicates\nthat the model is hallucinating. Check Section 9.3 for detailed discussions. The medical images are\nfrom the internet.\n102 Prompt: \nThe first image is a chest x-ray scan the patient took last time. \nThe previous diagnoses mention pleural effusion. \nThe second image is a chest x-ray scan the patient took this \ntime. \nWrite a radiology report as an expert radiologist for the \nsymptoms and changes. \nGPT-4V: \nRadiology Report \nDate: 01/01/2021 \nPatient: John Doe \nExam: Chest X-ray \nFindings: The chest x-ray scan taken this time shows a \nsignificant improvement in the patient's condition compared to  \nthe previous scan . The previous scan showed a pleural effusion, \nwhich is a buildup of fluid in the space between the lungs and \nchest wall. However, the current scan shows that the pleural \neffusion has resolved. There is no evidence of any new \nabnormalities in the lungs or chest wall. The lungs appear to be \nwell-expanded and the heart size is normal. The mediastinum, \nwhich is the space between the lungs, is also within normal \nlimits. \nConclusion: The patient's condition has improved since the last \nchest x-ray scan, with resolution of the previously noted pleural \neffusion. No new abnormalities are seen. Radiology Report Generation with Diagnosis History \nAnnotations from Professionals: \nImproving pleural effusion \nSec. 9.3 Medical \nPrompt: \nThe first image is a chest x-ray scan the patient took last time. \nThe previous diagnoses mention pleural effusion. \nThe second image is a chest x-ray scan the patient took this \ntime. \nWrite a radiology report as an expert radiologist for the \nsymptoms and changes. \nGPT-4V: \nRadiology Report \nDate: 12/12/2021 \nPatient: Jane Doe \nExam: Chest X-ray \nFindings: The chest x-ray scan taken this time shows a \nsignificant worsening of the patient's pleural effusion . The fluid \naccumulation in the pleural space is more prominent than in the \nprevious scan. The lung fields are also more hazy and \ncongested, indicating a possible infection or inflammation. The \nheart and mediastinal contours are obscured by the fluid, making \nit difficult to assess their size and shape. \nImpression: The patient's pleural effusion has worsened, and \nthere is a possibility of an underlying infection or inflammation. \nFurther evaluation and management are recommended. \nAnnotations from Professionals: \nWorsening pleural effusion \nFigure 78: Application Highlights on Radiology Report Generation with Diagnosis History. Check\nSection 9.3 for detailed discussions. The medical images are from MIMIC dataset [60].\n103 9.4 Auto Insurance\nIn this section, we explore another practical application of GPT-4V in the field of auto insurance,\nfocusing specifically on car accident reporting. Within this context, we can further delineate two\ndistinct sub-categories: ( i) Damage Evaluation and ( ii) Insurance Reporting. The former involves the\ncrucial task of accurately identifying and assessing the extent of damages sustained by vehicles, while\nthe latter encompasses not only damage identification but also the recognition of vehicle-specific\ninformation depicted in images, such as the make, model, license plate, and other relevant details. By\naddressing both aspects, we aim to demonstrate the comprehensive capabilities of GPT-4V within the\nauto insurance domain.\nDamage evaluation. We present an image depicting car damage to GPT-4V and prompt it with\n\u201cImagine that you are an expert in evaluating the car damage from car accident for auto insurance\nreporting. Please evaluate the damage seen in the image below.\u201d in Figure 79. GPT-4V has\ndemonstrated remarkable proficiency in accurately identifying and precisely localizing the damages\ndepicted in all four images. Furthermore, it impresses with its ability to provide detailed descriptions\nof each specific damage instance. In some instances, GPT-4V even endeavors to estimate the potential\ncost of repair.\nInsurance reporting. Building on the success in damage evaluation, we modify our prompt to\nask GPT-4V to identify the make, model, and license plate of the vehicle depicted in the image, and\nreturn the obtained information in JSON format. The examples depicted in Figure 80 showcase this\ncapability. In both instances, GPT-4V attempts to extract all the requested details from the image.\nHowever, it should be noted that certain information may be unavailable, such as the estimated cost\nof repair, or challenging to discern due to occlusion, as observed with the license plate in the second\nimage. It is important to note that real-life insurance reporting typically involves multiple images\ncapturing the car from various angles, a scenario that is usually not publicly accessible on the Internet.\nNevertheless, the examples in Figures 79-80 vividly illustrate the potential of GPT-4V in automating\nthe insurance reporting process for car accidents.\n104 Damage Evaluation GPT-4 \nPrompt: \nImagine that you are an expert in evaluating the \ncar damage from car accident for auto insurance \nreporting. Please evaluate the damage seen in the \nimage below. \nGPT-4V: \nBased on the image, it appears that the car has \nsustained damage to the front passenger side \nbumper. The damage appears to be in the form of \nscrapes and scratches, with some areas of the \npaint chipped away. The damage appears to be \ncosmetic and does not appear to have affected \nthe structural integrity of the bumper or the car. \nHowever, it is important to note that the image \nonly shows one angle of the car and does not \nprovide a full view of the damage. A more \nthorough inspection of the car would be needed \nto fully assess the extent of the damage and \ndetermine if there are any underlying issues that \nare not visible in the image. Prompt: \nImagine that you are an expert in evaluating the car \ndamage from car accident for auto insurance \nreporting. Please evaluate the damage seen in the \nimage below. \nGPT-4V: \nBased on the image, it appears that the vehicle has \nsustained significant damage to the front end. The \nhood is crumpled and the front bumper is partially \ndetached. The headlights also appear to be damaged. \nIt is likely that the vehicle was involved in a front-end \ncollision. The damage appears to be primarily \ncosmetic, but it is possible that there may be \nunderlying structural damage to the frame or engine. \nFurther inspection would be required to assess the full \nextent of the damage. In terms of repair costs, the \nhood and bumper would likely need to be replaced, \nand the headlights may also need to be replaced or \nrepaired. The cost of these repairs would depend on \nthe make and model of the vehicle, but it is likely that \nthe cost would be several thousand dollars at \nminimum. \nPrompt: \nImagine that you are an expert in evaluating the car \ndamage from car accident for auto insurance \nreporting. Please evaluate the damage seen in the \nimage below. \nGPT-4V: \nBased on the image, it appears that the vehicle has \nsustained significant damage to the front end. The \nhood is crumpled and the front bumper is completely \ndetached from the vehicle. The headlights and grille \nare also damaged and the engine appears to be \nexposed. Additionally, there is damage to the driver's \nside fender and door. Given the extent of the damage, \nit is likely that the vehicle was involved in a \nhigh-speed collision or a head-on collision. The cost \nof repairs for this vehicle will likely be significant and it \nmay be deemed a total loss by the insurance \ncompany. Prompt: \nImagine that you are an expert in evaluating the \ncar damage from car accident for auto insurance \nreporting. Please evaluate the damage seen in the \nimage below. \nGPT-4V: \nBased on the image, it appears that the car has \nsustained damage to the rear bumper and quarter \npanel on the driver's side. The bumper appears to \nbe dented and scratched, and the quarter panel \nhas a large dent and some scuff marks. It is likely \nthat the car was hit from the side, causing the \ndamage to the quarter panel and bumper. In \nterms of repair costs, the bumper may be able to \nbe repaired with some minor body work and \nrepainting. However, the quarter panel may \nrequire more extensive repairs or even \nreplacement, which could be more costly. Overall, \nI would estimate the damage to be in the \nmoderate to severe range, with repair costs likely \nin the thousands of dollars. Sec. 9.4 Auto Insurance \nFigure 79: Application Highlights on Auto Damage Evaluation. Check Section 9.4 for detailed\ndiscussions.\n105 GPT-4 \nPrompt: \nImagine that you are an expert in evaluating the car damage from car accident for auto insurance reporting. \nPlease fill the incident report for the accident shown in image below, with the following format in JSON (note \nxxx is placeholder, if you cannot determine that based on the image, put \"N/A\" instead). \n{\"make\": xxx, \"model\": xxx, \"license plate\": xxx, \"damage description\": xxx, \"estimated cost of repair\": xxx} \nGPT-4V: \n{\"make\": \"Maruti Suzuki\", \"model\": \"Celerio\", \"license plate\": \"KL 52 N 5305\", \"damage description\": \"Front \nbumper, hood, and windshield damaged. Airbags deployed.\", \"estimated cost of repair\": \"N/A\"} \nPrompt: \nImagine that you are an expert in evaluating the car damage from car accident for auto insurance reporting. \nPlease evaluate the damage seen in the image below. For filing the incident report, please follow the following \nformat in JSON (note xxx is placeholder, if the information is not available in the image, put \"N/A\" instead). \n{\"make\": xxx, \"model\": xxx, \"license plate\": xxx, \"damage description\": xxx, \"estimated cost of repair\": xxx} \nGPT-4V: \n{\"make\": \"Toyota\", \"model\": \"Corolla\", \"license plate\": \"N/A\" , \"damage description\": \"Front bumper and hood \ndamage, driver side fender damage, driver side headlight damage\", \"estimated cost of repair\": \n\"$2,500-$3,500\"} Sec. 9.4 Auto Insurance \nInsurance Report Generation \nFigure 80: Application Highlights on Insurance Reporting. For the highlighted text in red, GPT-4V\nfails to read the license plate, potentially due to occlusion. Check Section 9.4 for detailed discussions.\n106 9.5 Customized Captioner\nPhoto organization. In this scenario, let\u2019s picture that we have a family photo album. We demon-\nstrate how GPT-4V can enhance the album by generating captions that explicitly mention the name\nof each family member shown in the photo. This personalized approach facilitates more precise\nand tailored photo organization, as illustrated in Figures 81-82. By providing GPT-4V with visual\nprompts for each family member, along with their respective names, GPT-4V can precisely identify\nthe family members (including person, cat, and dog) to generate detailed and customized captions.\nStoring such captions for all the images in the family album holds the potential to enable highly\npersonalized image search. For instance, a user could search for \u201ca family photo of Linda, Cotton,\nMax, Sam, and Emma\u201d and easily locate the corresponding family photo shown in Figure 81, or\n\u201cMax lying between Sam and Emma\u201d and locate the family photo in Figure 82.\nDense captioning w/ segmentation. We demonstrate the enhanced performance of GPT-4V in\ndense captioning by harnessing powerful segmentation models [ 65,159,160]. Figure 83 illustrates\nthe dense captioning results by incorporating object cut-outs generated by SAM [ 65] into the prompt\nto extract more detailed captions for each object of interest. In addition, we provide GPT-4V with the\noriginal image as the global context and ask it to describe four object cut-outs as detailed as possible,\nand incorporating references to the context image.\nThe results show GPT-4V can generate highly intricate dense captions for each object, some of which\nare accompanied by relevant references to the context image. For instance, when describing object 3\n(a frog), the dense caption makes mention of a close-up shot of a frog with a snail perched on its head,\ndespite the absence of the snail in the corresponding cut-out for object 3. Similarly, when referring\nto object 4 (a turtle), GPT-4V recognizes from the context image that the turtle is floating in water,\nthereby further enriching the generated caption.\n107 Prompt: \nHere are the photos of my family members. \nLinda \nGPT-4V: \nThis image is a family portrait of Linda and her children, Emma and Sam, along with their pets, Max the dog and  \nCotton the cat.  The family is sitting in their living room, with Linda on the couch and Emma and Sam on the floor. \nMax is sitting in between Linda and Emma, with his tongue sticking out, while Cotton is perched on Linda's \nshoulder. All family members are smiling and looking at the camera. The room is bright and airy, with a vase of \nyellow flowers on the coffee table and a white bookshelf in the background. \nCotton \nMax\nSam\nEmma \nPlease describe the image with as many details as possible, using my family members\u2019 names. \nPhoto Organization Sec. 9.5 Customized Captioner Figure 81: Customized Captioner for photo organization (the reference images are cropped from the\nquery image). Blue highlights the mention of family names. Check Sections 9.5, 10.5 for detailed\ndiscussions.\n108 GPT-4 \nGPT-4V: \nThis image shows three members of my family - my brother Sam, our dog Max, and myself, Emma.  We are lying \non a grassy field with a clear blue sky in the background. Sam and I are resting our heads on our hands and \nsmiling at the camera, while Max is lying between us with his tongue out. We are all enjoying the beautiful \nweather and spending time together as a family. \nPrompt: \nHere are the photos of my family members. \nLinda \nCotton \nMax\nSam\nEmma \nPlease describe the image with as many details as possible, using my family members\u2019 names. Photo Organization Sec. 9.5 Customized Captioner Figure 82: Customized Captioner for photo organization (the reference images are cropped from\na different image than the query image) Blue highlights the mention of family names. Check\nSection 9.5 for detailed discussions.\n109 Dense Captioning w/ Segmentation \nPrompt: \nDescribe the objects below with as many details as possible, Reference to the content shown in the context \nimage but focus on describing each object below. \nObject 1 \nGPT-4V: \nObject 1 is a close-up of a snail shell. The shell is a light beige color with a spiral pattern on the top. The shell \nappears to be slightly weathered, with some small chips and cracks visible on the edges. The background is \nwhite, making the shell the focal point of the image. The shell is positioned at a slight angle, with the spiral \npattern on the top facing towards the right side of the image. \nObject 2 is a close-up of a snail. The snail has a light orange-colored shell with a spiral pattern. The body of the \nsnail is a darker orange color with small bumps on its skin. The snail's head is extended forward and its antennae \nare visible. The background of the image is white, making the snail the focal point of the image. \nObject 3 is a close-up of a frog with a snail on its head . The frog is light green in color with a pinkish hue on its \nunderside. It has large, bulging eyes that are black and white. The snail is orange and is perched on top of the \nfrog's head. The frog is sitting on a rock or other flat surface with its legs tucked underneath its body. The \nbackground is white, making the frog and snail stand out. \nObject 4 shows a close-up of a turtle's head and neck, with its body partially visible. The turtle appears to be  \nfloating in water,  with its head and neck extended upwards. The turtle's skin is green and its shell is grey with \nblack stripes. The shell appears to be damaged, with a large chunk missing from the top. The turtle's eyes are \nclosed and its mouth is slightly open. Object 2 \nObject 3 \nObject 4 \nContext image \nSec. 9.5 Customized Captioner Figure 83: Dense captioning w/ segmentation cut-outs from SAM [ 65]Blue highlights the references\nto the context image. Check Section 9.5 for detailed discussions.\n110 9.6 Image Generation\nIn this section, we make connections with another prominent area of multimodal research: visual\nsynthesis. By delving into the realm of image generation, we explore how GPT-4V can contribute to\nthis field through various avenues, including evaluation and prompting.\nEvaluation of generated images. Figure 66 in Section 8.2 demonstrates the capability of GPT-4V in\nassessing the aesthetics of images. Here, we show how we employ GPT-4V to evaluate the generated\nimages based on their alignment with the given prompts for text-to-image generation, inspired by\nRL-Diffusion [ 18]. RL-Diffusion leverages a VL model LLA V A [ 79] to describe the generated\nimage, followed by text similarity computation between the prompt and the image description using\nBERT [ 38]. The resulting text similarity score serves as the feedback signal to guide the training\nof the diffusion model through reinforcement learning (RL). Notably, Figures 84-85 exhibit how\nGPT-4V, as a single model, can effectively rate the similarity between the generated image and the\nprompt. Moreover, GPT-4V provides explanations for the deduction in similarity score, which can\npotentially be used as a feedback to improve the image generation.\nIn Figure 84, we present the evaluation of image similarity using the prompt, \u201cWhat is happening\nin the image? From a scale of 1 to 10, rate the similarity between the image and the text prompt \u2019a\nparrot driving a car\u2019.\u201d GPT-4V assigns a score of 1 to the most irrelevant image (a dolphin jumping\nover the water), while rating the most relevant image at the bottom with a score of 9. Notably, the last\nthree images in Figure 84 are shown in RL-Diffusion as gradually improved generation results for the\ntext prompt \u201ca parrot driving a car.\u201d The ratings assigned by GPT-4V to these three images (4 \u21928\n\u21929) align with the refinement process.\nFigure 85 showcases the evaluation of image generation results that involve text rendering on a cake.\nLeveraging its robust optical character recognition (OCR) capabilities, GPT-4V accurately recognizes\nthe rendered texts in the generated images, such as \u201cAzuze Research,\u201d \u201cARAUIE,\u201d and \u201cAzure Azure,\u201d\nand compares them to the text prompt requirement, which is \u201cAzure Research.\u201d\nPrompt generation for image editing. In addition to its remarkable ability to evaluate generated\nimages, GPT-4V offers a valuable feature that can greatly enhance image editing. By generating or\nrewriting the text prompt used for editing, GPT-4V can refine the edited image, resulting in a more\nvisually appealing outcome. Figure 86 provides a demonstration of how we can harness the power of\nGPT-4V to generate a text prompt specifically tailored for image editing. By providing the original\nimage and text requirements that describe the desired edits, GPT-4V produces an optimized prompt\nfor the task at hand. This optimized prompt takes into account the unique characteristics of the image,\nensuring that the subsequent editing process is well-informed and effective.\nMoreover, Figure 87 showcases another use case of GPT-4V to improve image editing by rewriting\nthe editing prompt. By considering the original image, the initial prompt, and the edited image,\nGPT-4V can generate an improved version of the prompt that incorporates the changes made during\nthe previous editing process. One can alternate the processes depicted in Figures 86-87, allowing\nusers to refine their edits repeatedly until they achieve a satisfying outcome. Consequently, this\niterative process has the potential to significantly enhance the overall quality of the edited image,\nproviding users with more control and creative freedom in their image editing endeavors.\n111 Evaluation of Generated Images (Prompt-Image Alignment) GPT-4 \nPrompt: \nWhat is happening in the image? From scale 1 to 10, \ndecide how similar the image is to the text prompt \"a \nparrot driving a car\"? \nGPT-4V: \nIn the image, there is a dolphin jumping out of the \nwater. On a scale of 1 to 10, I would rate this image a  \n1 in similarity to the text prompt \"a parrot driving a \ncar\" as there is no parrot or car present in the image. Sec. 9.6 Image Generation \nPrompt: \nWhat is happening in the image? From scale 1 to 10, \ndecide how similar the image is to the text prompt \"a \nparrot driving a car\"? \nGPT-4V: \nIn the image, there is a parrot flying in the sky with \ntrees and clouds in the background. I would rate this  \nimage a 2  in similarity to the text prompt \"a parrot \ndriving a car\" as there is a parrot present in the \nimage, but it is not driving a car. \nPrompt: \nWhat is happening in the image? From scale 1 to 10, \ndecide how similar the image is to the text prompt \"a \nparrot driving a car\"? \nGPT-4V: \nIn the image, there is a parrot perched on the side of \na car. It is not driving the car, but it is on the car. I \nwould rate this image a 4  in terms of similarity to the \ntext prompt \"a parrot driving a car\" because there is a \nparrot and a car present, but the parrot is not driving \nthe car. Prompt: \nWhat is happening in the image? From scale 1 to 10, \ndecide how similar the image is to the text prompt \"a \nparrot driving a car\"? \nGPT-4V: \nIn the image, there is a parrot's head superimposed \nonto a car driving on a road. The parrot's head is in \nthe driver's seat and the car appears to be in motion. \nOn a scale of 1 to 10, I would rate this image an 8  in \nterms of similarity to the text prompt \"a parrot driving \na car\". The image clearly shows a parrot in the \ndriver's seat of a car, but the parrot's body is not \nvisible and the car is not clearly a \"car\" but more of a \nblurred image. \nPrompt: \nWhat is happening in the image? From scale 1 to 10, decide how similar the image is to the text prompt \"a \nparrot driving a car\"? \nGPT-4V: \nIn the image, there are two parrots, one red and one yellow, on top of a car. The yellow parrot is holding the \nsteering wheel and appears to be driving the car. On a scale of 1 to 10, I would rate this image a 9  in terms of \nsimilarity to the text prompt \"a parrot driving a car\". The only reason it is not a perfect 10 is because there are two \nparrots in the image, rather than just one as the prompt suggests. Figure 84: Prompt GPT-4V to give a score from 1 to 10 on how similar the generated image is to\nthe prompt. Blue highlights the rating given by GPT-4V. The last three images are generated from\nRL-Diffusion [18]. Check Section 9.6 for detailed discussions.\n112 GPT-4 \nPrompt: \nWhat is happening in the image? From scale 1 to \n10, decide how similar the image is to the text \nprompt \u201ca cake on the table with word \u201cAzure \nResearch\u201d written on it\"? \nGPT-4V: \nIn the image, there is a cake on a table with the \nwords \"Azuze Research\" written on it. I would rate \nthis image an 8 out of 10  in similarity to the text \nprompt \"a cake on the table with word 'Azure \nResearch' written on it\" because the only \ndifference is the misspelling of \"Azure\" as \n\"Azuze.\" \nPrompt: \nWhat is happening in the image? From scale 1 to \n10, decide how similar the image is to the text \nprompt \u201ca cake on the table with word \u201cAzure \nResearch\u201d written on it\"? \nGPT-4V: \nIn the image, there is a blue cake on a table with \nthe word \"ARAUIE\"  written on it. On a scale of 1 to \n10, I would rate this image a 6  in terms of similarity \nto the text prompt \"a cake on the table with word \n'Azure Research' written on it\". \nThe image does show a cake on a table, but the \nword written on the cake is not an exact match to \nthe text prompt. The color of the cake is also blue, \nwhich could be seen as similar to the word \"Azure\" \nin the prompt.  \nPrompt: \nWhat is happening in the image? From scale 1 to \n10, decide how similar the image is to the text \nprompt \u201ca cake on the table with word \u201cAzure \nResearch\u201d written on it\"? \nGPT-4V: \nIn the image, there is a cake on a table with the \nwords \"Azure Aurre\"  written on it. I would rate this  \nimage an 8 out of 10 in similarity to the text \nprompt \"a cake on the table with word 'Azure \nResearch' written on it\" because the image does \nshow a cake on a table with the word \"Azure\" \nwritten on it, but the word \"Research\" is not \npresent.  Prompt: \nWhat is happening in the image? From scale 1 to \n10, decide how similar the image is to the text \nprompt \u201ca cake on the table with word \u201cAzure \nResearch\u201d written on it\"? \nGPT-4V: \nIn the image, there is a two-tiered cake on a table \nwith the words \"Azure Research\"  written on it in \nblue icing. I would rate this image a 9 out of 10  in \nterms of similarity to the text prompt, as it very \nclosely matches the description given. The only \nreason I would not give it a perfect 10 is because \nthe text prompt does not mention the two tiers or \nthe additional decorations on the cake. \nSec. 9.6 Image Generation \nEvaluation of Generated Images (Prompt-Image Alignment) Figure 85: Prompt GPT-4V to give a score from 1 to 10 on how similar the generated image is to\nthe prompt. Blue highlights the rating given by GPT-4V. Red (Green ) indicate wrong (correct)\nrendered text. Generated images are from DeepFloyd IF [ 2], Midjourney V5.1 [ 4], SDXL [ 110], and\nReCo [143]. Check Section 9.6 for detailed discussions.\n113 Prompt Generation for Image Editing GPT-4 \nPrompt: \nThis is the original image: \nI want to make this image look like graphic novel cover. The prompt should be a single sentence in less \nthan 15 words with descriptive words. For example: \u201chave it look like a graphic novel\u201d. \nBased on the above prompt examples and the original image, help me to write a better prompt to an \nimage editing model. \nGPT-4V: \nHow about: \u201cTransform this image of a statue into a dramatic graphic novel cover with a dark tone.\u201d  \nInstruct Pix2Pix: \nWith original image and prompt \u201cTransform this image of a statue into a \ndramatic graphic novel cover with a dark tone\u201d as the input \nSec. 9.6 Image Generation Figure 86: Improving the text prompt for image editing, given the original image and textual\nrequirement. Blue highlights the suggested editing prompt by GPT-4V. Original image/exemplary\nediting prompt are from Instruct Pix2Pix [22]. Check Section 9.6 for detailed discussions.\n114 GPT-4 \nPrompt: \nThis is the original image: \nThe prompt I used to edit this image is \u201chave it look like a graphic novel\u201d. \nThe edited image I got is: \nGPT-4V: \nInstead of using the prompt \u201chave it look like a graphic novel\u201d, you could try being more specific with the \nprompt to guide the AI in the direction you want the image to go. For example, you could use prompts \nsuch as \u201cmake the image look like a black and white comic book illustration\u201d  or \u201cadd bold lines and high  \ncontrast to the image to make it look like a graphic novel cover\u201d. This could help the AI better understand \nthe aesthetic you are going for and produce a more refined edited image. \nInstruct Pix2Pix: \nWith original image and prompt \u201cmake \nthe image look like a black and \nwhite comic book illustration\u201d as \nthe input \nTell me how I should change my prompt to make the edited image better. \nWith original image and prompt \u201cadd \nbold lines and high contrast to the \nimage to make it look like a graphic \nnovel cover\u201d as the input \nPrompt Generation for Image Editing Sec. 9.6 Image Generation Figure 87: Improving the editing prompt, given the original image, the editing prompt, and the\nedited image. Blue highlights the suggested editing prompt by GPT-4V. Original image/editing\nprompt/edited image are from Instruct Pix2Pix [22]. Check Section 9.6 for detailed discussions.\n115 9.7 Embodied Agent\nIn this section, we delve into the exciting applications and implications of GPT-4V for embodied AI,\nexploring how it is poised to bridge the gap between multimodal understanding on static inputs and\nphysical interaction with dynamic environments. To provide a concrete illustration, let us consider\nthe scenario of GPT-4V assuming the role of a home robot. Within this context, we witness how it\ncan read the menu to operate household appliances ( e.g., coffee machine), and perform task-oriented\nnavigation through the house.\nOperating machine. Imagine you\u2019ve just acquired a brand-new coffee machine, and to your delight,\nyour trusty home robot, GPT-4V, learns how to operate it on your behalf. In our experiment, we\nprovide GPT-4V with a single image (Figure 88) featuring an operating menu with both illustrations\nand texts. Our task for GPT-4V is to identify the button that corresponds to the \u201c8 OZ coffee\u201d option\nwithin the coffee machine\u2019s operating panel. Surprisingly, GPT-4V not only accurately locates the\n\u201c8 OZ coffee\u201d button but also successfully recognizes the button for \u201c10 OZ coffee.\u201d However, it\nmistakenly identifies the power button as the \u201c6 OZ coffee\u201d button, potentially due to the visual\nconfusion caused by the positioning of the \u201c6 OZ coffee\u201d option on both the menu and the coffee\nmachine itself. To address this specific failure case, we devise a solution by isolating the operating\nmenu for each button and presenting them all to GPT-4V in a single prompt (Figure 89). With this\nrevised approach, GPT-4V now can recognize the precise position of the \u201c6 OZ coffee\u201d button.\nNavigation. In order to explore navigation capabilities, we utilized Redfin virtual house tour as a\nmeans to replicate interactive environments for embodied agents. The objective was to assess the\nperformance of GPT-4V in a task-oriented scenario. To illustrate this, we present an example depicted\nin Figures 90-91. Initially, we provided GPT-4V with the entry image of a virtual house tour, offering\na view from one corner into the living room. The task assigned to GPT-4V was to \u201cgo to the kitchen\nand retrieve an item from the fridge.\u201d Our aim was to prompt GPT-4V to predict the subsequent\nactions.\nIn the first step, as shown in the first half of Figure 90, GPT-4V anticipated the initial action by\nsuggesting to \u201cturn right and move forward towards the hallway.\u201d This prediction was based on\nGPT-4V\u2019s hypothesis that the kitchen would likely be located in that direction. We then manually\nexecuted this action using the visual house touring portal, capturing the resulting view after the action\nwas taken. This view was then used to prompt GPT-4V for the next action, as displayed in the second\nhalf of Figure 90. It\u2019s important to note that throughout the process, we maintained a record of the\nimmediate previous turn to provide context for GPT-4V\u2019s subsequent actions.\nAs the navigation process unfolded, we successfully reached the fridge within the third turn, as\nindicated by the query image in the second half of Figure 91. The final action predicted by GPT-4V\nwas to \u201cmove forward and slightly to the left in order to align myself with the fridge door. Then, use\nmy robotic arm to open the fridge door and retrieve the requested item.\u201d This decisive action marked\nthe accomplishment of GPT-4V in this task-oriented navigation scenario.\n116 Operating Machine GPT-4 \nPrompt: \nFirst, read the operating menu below for coffee machine buttons. \nMenu: \nSec. 9.7 Embodied Agent \nNow, let's look at the operating panel of the coffee machine in the image below, which button should I use for \na 8 OZ coffee: left, right, top or bottom button? \nGPT-4V: \nYou should use the bottom button for an 8 OZ coffee. According to the menu, the bottom button corresponds  \nto a 8 OZ coffee,  while the top button is for a 6 OZ coffee  and the right button is for a 10 OZ coffee. \nFigure 88: Reading a full menu of coffee machine buttons, GPT-4V recognizes which button to\nchoose for 8 OZ coffee. Green (Red ) highlights the correct (wrong) answer. Check Section 9.7 for\ndetailed discussions.\n117 Operating Machine GPT-4 \nPrompt: \nFirst, read the operating menus below for coffee machine buttons. \nMenu 1: \nNow, let's look at the operating panel of the coffee machine in the image below, which button should I use for \na 6 OZ coffee: left, right, top or bottom button? \nGPT-4V: \nYou should use the left button for a 6 OZ coffee. \nMenu 2: \nMenu 3: \nSec. 9.7 Embodied Agent Figure 89: Converting the full menu of coffee machine buttons to interleaved image-text instructions,\nGPT-4V can recognizes which button to choose for 6 OZ coffee, which GPT-4V failed to do so\nwith full menu instruction. Green highlights the correct answer. Check Section 9.7 for detailed\ndiscussions.\n118 Prompt: \nImagine that you are a home robot, and is asked to go to the kitchen to fetch something from the fridge. \nThe image below shows your current position. Please plan your next action. \nGPT -4V : \nMy next action would be turn right and move forward towards the hallway , as I can see that the kitchen is \nlikely located in that direction. I would then continue to navigate through the hallway until I reach the kitchen \nand locate the fridge. \nPrompt: \nImagine that you are a home robot, and is asked to go to the kitchen to fetch something from the fridge. \nAt the previous turn, you were at the position shown in the below image: \nGPT -4V : \nFrom the current position, I can see the kitchen on the right side, My next action would be to  turn right and  \nmove forward towards the kitchen .\nNote: Same image from the previous turn, resized \nto save space \nThe action taken in previous turn is: turn right and move forward towards the hallway. \nNow you are at the hallway, this is what you can see from there. Can you see the kitchen from here? What is \nyour next action? \nNavigation Sec. 9.7 Embodied Agent Figure 90: Acting as an embodied agent to navigate through a house to fetch something from the\nfridge (the 1st and 2nd turn). Blue highlights the predicted actions. Check Section 9.7 for detailed\ndiscussions.\n119 Navigation GPT-4 \nPrompt: \nImagine that you are a home robot, and is asked to go to the kitchen to fetch something from the fridge. \nAt the previous turn, you were at the position shown in the below image: \nGPT -4V : \nFrom this position, I can see the fridge on the right side of the kitchen. My next action would be to move  \nforward and turn slightly to the right to approach the fridge.  Note: Same image from the previous turn, resized \nto save space \nThe action taken in previous turn is: turn right and move forward towards the kitchen. \nNow you are at the kitchen, this is what you can see from there. Can you see the fridge from here? What is \nyour next action? \nPrompt: \nImagine that you are a home robot, and is asked to go to the kitchen to fetch something from the fridge. \nAt the previous turn, you were at the position shown in the below image: \nGPT -4V : \nMy next action would be to  move forward and slightly to the left to align myself with the fridge door. Then, I  \nwould use my robotic arm to open the fridge door and retrieve the requested item.  Note: Same image from the previous turn, resized \nto save space \nThe action taken in previous turn is: move forward and turn slightly to the right to approach the fridge. \nNow this is what you can see from there. What is your next action? \nSec. 9.7 Embodied Agent Figure 91: Acting as an embodied agent to navigate through a house to fetch something from the\nfridge (the 3rd and 4th turn). Blue highlights the predicted actions. Check Section 9.7 for detailed\ndiscussions.\n120 9.8 GUI Navigation\nBeyond navigating the physical world, this section showcases the capability of GPT-4V to interact\nwith and navigate through the Graphical User Interface (GUI) of a computer or smartphone. We\nexplore the potential for GPT-4V to complete complex tasks, such as web browsing, online shopping,\nandetc.\nWeb browsing. We assess the performance of GPT-4V on computer GUI navigation under a\ntask-oriented setting. The model was provided with the screenshot of current computer screen, the\nend goal of the navigation ( e.g., finding a cooking recipe or reading today\u2019s news), the list of possible\nactions ( e.g., move the mouse, click an icon with the mouse, or type some texts with the keyboard).\nThe model is then instructed to predict the subsequent actions (refer to Figure 92 for a complete\nprompt example). Upon the model\u2019s prediction, we manually execute the predicted action and capture\na screenshot, which served as the input for GPT-4V for the next turn. When the predicted action\nis to move the mouse, GPT-4V is specifically instructed to detail the mouse\u2019s position. Hence, the\npredicted actions are grounded, showing the potential of automating the whole process without human\nin the loop.\nIn Figures 92-96, GPT-4V predicts reasonable actions to operate a computer GUI, and finally\naccomplish the end goal of finding a recipe of Mapo Tofu and print out a copy of the recipe in\nFigure 95. We then provide GPT-4V a screenshot of the printed recipe and ask it to describe the\nprintout as detailed as possible. As shown in Figure 96, GPT-4V is able to recognize the details\npresented in the printout, including the cooking time, the list of ingredients, the author of the recipe,\nthe link to the original recipe and etc. Figures 97-102 present how GPT-4V can navigate through\nGUI to browse the web to \u201cread today\u2019s news\u201d. Despite the minor errors in Figure 100 when it tries\nto return to the previous search result page to continue browsing for more news articles, GPT-4V can\nperform the navigation and read two news articles reasonably well.\nOnline shopping. Figures 103-111 illustrates how GPT-4V can navigate a smartphone GUI for\nonline shopping. Similarly, we provide GPT-4V with the screenshot of the current phone screen, the\nlist of possible actions ( e.g., move your finger to an icon, click an icon with your finger, scroll down a\nscreen, or type some texts with the keyboard) and ask it to predict the subsequent actions to shop for\nan ergonomic keyboard with a budget between $50 and $100. GPT-4V predicts to open the Amazon\napp (Figure 103), search ergonomic keyboard (Figure 104), open the filter options (Figure 105), set\nthe price range filter between $50 and $100 (Figure 106), show filtered results (Figure 107), select\nthe top search result (Figure 108), view product details (Figure 109), add product to the shopping cart\n(Figure 110) and finally proceed to checkout (Figure 111).\nNotification understanding. Notifications are integral to modern human-computer interactions.\nGPT-4V has demonstrated its capacity to interpret notification content and respond accordingly. As\nshown in Figure 112, the model can read and respond to a notification, such as suggesting to open the\nMaps app in response to a meeting proposal in Seattle. It also handles call (Figure 113) and message\n(Figure 114) notifications on a computer screen effectively.\nWatching videos. Alongside web browsing, videos are a key source of online information. GPT-4V\nhas shown its capability to describe video content based on a series of screenshots from popular\nshort-form videos. Regardless of whether the video has subtitle overlay (Figure 115 and 116) or\nnot (Figure 117, 118, 119), GPT-4V can generate insightful descriptions about the video content,\ndemonstrating its potential in automatic transcript generation for user-generated video content.\n121 Prompt: \nImagine that you are a robot operating a computer. Like how humans operate the computer, you can move the \nmouse, click an icon with the mouse, or type some texts with the keyboard. \nYou are asked to find a detailed recipe of Mapo Tofu. \nBelow is what you see on the computer screen, predict your next move. If the action involves moving the mouse, \ndescribe the location as detailed as possible. \nGPT-4V: \nNext move: I would move the mouse to the Google Chrome icon  located on the left side of the screen, \napproximately 1/3 of the way down from the top. I would then click on the icon to open the web browser . \nPrompt: \nImagine that you are a robot operating a computer. Like how humans operate the computer, you can move the \nmouse, click an icon with the mouse, or type some texts with the keyboard. \nYou are asked to find a detailed recipe of Mapo Tofu. \nPrevious action: \"I would move the mouse to the Google Chrome icon located on the left side of the screen, \napproximately 1/3 of the way down from the top. I would then click on the icon to open the web browser. \" \nYou have now opened the web browser. Below is what you see on the computer screen. What will you do next? \nIf the action involves moving the mouse, describe the location as detailed as possible. \nGPT-4V: \nNext, I would move the mouse to the search bar located in the center of the screen , approximately 1/2 of the \nway down from the top. I would then  click on the search bar and type in \"Mapo Tofu recipe\" using the keyboard . \nAfter typing in the search query, I would press the \"Enter\" key on the keyboard to initiate the search.  \nWeb Browsing (Mapo Tofu Recipe) \nSec. 9.8 GUI Navigation Figure 92: GPT-4V navigates through GUI to browse the web to search for the recipe of Mapo Tofu.\nBlue highlights the predicted actions. Check Section 9.8 for detailed discussions.\n122 Prompt: \nImagine that you are a robot operating a computer. Like how humans operate the computer, you can move the \nmouse, click an icon with the mouse, or type some texts with the keyboard. \nYou are asked to find a detailed recipe of Mapo Tofu. \nPrevious action: \" I would move the mouse to the search bar located in the center of the screen, approximately \n1/2 of the way down from the top. I would then click on the search bar and type in \"Mapo Tofu recipe\" using the \nkeyboard. After typing in the search query, I would press the \"Enter\" key on the keyboard to initiate the search.\" \nYou have now completed the previous action as described above. Below is what you see on the computer \nscreen. What will you do next? If the action involves moving the mouse, describe the location as detailed as \npossible. \nGPT-4V: \nNext action: \"I would move the mouse to the first search result under the \"Recipes\" section , which is titled \"Mapo  \nTofu Recipe - The Woks of Life\" . I would then click on the link to open the webpage  and view the detailed \nrecipe.\" Web Browsing (Mapo Tofu Recipe) \nSec. 9.8 GUI Navigation Figure 93: GPT-4V navigates through GUI to browse the web to search for the recipe of Mapo Tofu.\nBlue highlights the predicted actions. Check Section 9.8 for detailed discussions.\n123 Prompt: \nImagine that you are a robot operating a computer. Like how humans operate the computer, you can move \nthe mouse, click an icon with the mouse, or type some texts with the keyboard. \nYou are asked to find a detailed recipe of Mapo Tofu. \nPrevious action: \u201cI would move the mouse to the first search result under the \"Recipes\" section, which is \ntitled \"Mapo Tofu Recipe - The Woks of Life\". I would then click on the link to open the webpage and view the \ndetailed recipe.\" \nYou have now completed the previous action as described above. Below is what you see on the computer \nscreen. Based on the image below, what will you do next? If the action involves moving the mouse, describe \nthe location as detailed as possible. \nGPT-4V: \nNext action: \"I would move the mouse to the \"Jump to Recipe\" button  located at the top right corner of the \nwebpage. I would then click on the button to jump to the detailed recipe section of the page. \"Web Browsing (Mapo Tofu Recipe) \nSec. 9.8 GUI Navigation Figure 94: GPT-4V navigates through GUI to browse the web to search for the recipe of Mapo Tofu.\nBlue highlights the predicted actions. Check Section 9.8 for detailed discussions.\n124 Prompt: \nImagine that you are a robot operating a computer. Like how humans operate the computer, you can move \nthe mouse, click an icon with the mouse, or type some texts with the keyboard. \nYou are asked to find a detailed recipe of Mapo Tofu. \nPrevious action: \u201cI would move the mouse to the \"Jump to Recipe\" button located at the top right corner of \nthe webpage. I would then click on the button to jump to the detailed recipe section of the page.\" \nYou have now completed the previous action as described above. Below is what you see on the computer \nscreen. Based on the image below, what will you do next? If the action involves moving the mouse, describe \nthe location as detailed as possible. \nGPT-4V: \nNext action: \"I would move the mouse to the \"Print Recipe\" button  located at the top right corner of the recipe \nsection, next to the \"Rate This Recipe\" button. I would then click on the button to print out a copy of the detailed  \nrecipe for Mapo Tofu.\u201d  Web Browsing (Mapo Tofu Recipe) \nSec. 9.8 GUI Navigation Figure 95: GPT-4V navigates through GUI to browse the web to search for the recipe of Mapo Tofu.\nBlue highlights the predicted actions. Check Section 9.8 for detailed discussions.\n125 Prompt: \nDescribe the image below as detailed as possible. \nGPT-4V: \nThe image is a recipe for Mapo Tofu, a Chinese dish. The recipe is presented in a structured format with a title, \ndescription, prep time, cook time, total time, course, cuisine, servings, and rating. \nThe title of the recipe is \"Mapo Tofu\" and it is written in bold black letters. Below the title, there is a description of \nthe dish which states that it is the true blue, authentic real deal - the spicy, tongue-numbing, \nrice-is-absolutely-not-optional Mapo Tofu that you get in the restaurants. \nThe prep time for the dish is 10 minutes, the cook time is 25 minutes, and the total time is 35 minutes. The \ncourse is listed as \"Main Course\" and the cuisine is \"Chinese\". The recipe serves 6 people and has a rating of \n4.69 stars out of 5 .\nBelow this information, there is a list of ingredients required for the dish, including tofu, ground pork, Sichuan \npeppercorn, garlic, ginger, soy sauce, and cornstarch. The ingredients are listed in a bullet point format. \nAt the bottom of the image, there is nutritional information for the dish, including calories, carbohydrates, protein, \nfat, saturated fat, cholesterol, sodium, and calcium. The nutritional information is presented in a table format. \nThe image also includes a credit to the author of the recipe, Kaitlin, and a link to the original recipe on the \nwebsite \"The Woks of Life\". Web Browsing (Mapo Tofu Recipe) \nSec. 9.8 GUI Navigation Figure 96: GPT-4V navigates through GUI to browse the web to search for the recipe of Mapo Tofu.\nAs GPT-4V predicts to print out the recipe in the previous turn, we prompt it to read the screenshot\nof the printed recipe and summarize it. Red highlights the inaccurate description about the image.\nCheck Section 9.8 for detailed discussions.\n126 Prompt: \nImagine that you are a robot operating a computer. Like how humans operate the computer, you can move the \nmouse, click an icon with the mouse, or type some texts with the keyboard. \nYou are asked to find what\u2019s in the news today. \nBelow is what you see on the computer screen, predict your next move. If the action involves moving the mouse, \ndescribe the location as detailed as possible. \nGPT-4V: \nMy next move would be to move the mouse to the Google Chrome icon  located on the left side of the screen, \napproximately 1/3 of the way down from the top. I would then click on the icon to open the web browser and  \nsearch for news websites to find out what's in the news today.  \nPrompt: \nImagine that you are a robot operating a computer. Like how humans operate the computer, you can move the \nmouse, click an icon with the mouse, or type some texts with the keyboard. \nYou are asked to find what\u2019s in the news today. \nPrevious action: \"My next move would be to move the mouse to the Google Chrome icon located on the left side \nof the screen, approximately 1/3 of the way down from the top. I would then click on the icon to open the web \nbrowser and search for news websites to find out what's in the news today. \" \nYou have now opened the web browser. Below is what you see on the computer screen, what is your immediate \nnext step? If the action involves moving the mouse, describe the location as detailed as possible. \nGPT-4V: \nMy immediate next step would be to move the mouse to the search bar located in the center of the screen , \napproximately 1/2 of the way down from the top. I would then click on the search bar and type in \"news\" and  \npress the enter key on the keyboard to search for news websites.  Web Browsing (Today\u2019s News) \nSec. 9.8 GUI Navigation Figure 97: GPT-4V navigates through GUI to browse the web to read today\u2019s news. Blue highlights\nthe predicted actions. Check Section 9.8 for detailed discussions.\n127 Prompt: \nImagine that you are a robot operating a computer. Like how humans operate the computer, you can move the \nmouse, click an icon with the mouse, or type some texts with the keyboard. \nYou are asked to find what\u2019s in the news today. \nPrevious action: \"My immediate next step would be to move the mouse to the search bar located in the center of \nthe screen, approximately 1/2 of the way down from the top. I would then click on the search bar and type in \n\"news\" and press the enter key on the keyboard to search for news websites. \" \nYou have now completed the previous action. Below is what you see on the computer screen. Based on the \nimage below, what is your immediate next step? If the action involves moving the mouse, describe the location \nas detailed as possible. \nGPT-4V: \nMy immediate next step would be to move the mouse to the first search result, which is \"State Department  \ndoubles down on Zelensky slams\"  located approximately 1/4 of the way down from the top of the screen and 1/8 \nof the way from the left side of the screen. I would then click on the link to open the news article and read about  \nwhat's in the news today.  Web Browsing (Today\u2019s News) \nSec. 9.8 GUI Navigation Figure 98: GPT-4V navigates through GUI to browse the web to read today\u2019s news. Blue highlights\nthe predicted actions. Check Section 9.8 for detailed discussions.\n128 Prompt: \nRead the article shown in the images below and summarize it. \nGPT-4V: \nSummary: \nThe article discusses the statement made by Ukrainian President Volodymyr Zelensky on Tuesday, where he \nexpressed his country's desire to join NATO. Zelensky stated that Ukraine cannot afford to go to war with Russia \nand that joining NATO would provide the necessary security and protection. \nHowever, Matthew Miller, a spokesperson for NATO, stated that while the alliance is ready to extend an \ninvitation to Ukraine, there are certain conditions that need to be met. These conditions include democratic and \nsecurity-sector reforms. Miller also emphasized that Ukraine's entry into NATO is not a guarantee and that the \ndecision ultimately lies with the members of the alliance. \nThe article also mentions that during a press conference on Tuesday, Zelensky reiterated his commitment to \nstrengthening democracy in Ukraine, which is a necessary condition for NATO membership. Web Browsing (Today\u2019s News) \nSec. 9.8 GUI Navigation Figure 99: GPT-4V navigates through GUI to browse the web to read today\u2019s news. We prompt\nGPT-4V to read the screenshots of the first news article and summarize it. Check Section 9.8 for\ndetailed discussions.\n129 Prompt: \nImagine that you are a robot operating a computer. Like how humans operate the computer, you can move the \nmouse, click an icon with the mouse, or type some texts with the keyboard. \nYou are asked to find what\u2019s in the news today. \nPrevious action: \"My immediate next step would be to move the mouse to the first search result, which is \"State \nDepartment doubles down on Zelensky slams\" located approximately 1/4 of the way down from the top of the \nscreen and 1/8 of the way from the left side of the screen. I would then click on the link to open the news article \nand read about what's in the news today.\" \nYou have now completed reading \u201cState Department doubles down on Zelensky slams\u201d. What is your immediate \nnext step? If the action involves moving the mouse, describe the location as detailed as possible. \nGPT-4V: \nMy immediate next step would be to move the mouse to the top right corner of the screen, where I can see the  \n\"X\" icon to close the current tab . I would then click on the \"X\" icon to close the tab  and return to the previous  \npage to continue browsing for more news articles.  Web Browsing (Today\u2019s News) \nSec. 9.8 GUI Navigation Figure 100: GPT-4V navigates through GUI to browse the web to read today\u2019s news. Upon finishing\nreading the first news article, GPT-4V predicts to close the tab and return to previous page to continue\nbrowsing more news articles (highlighted in blue ).Red highlights the inaccurate action prediction.\nCheck Section 9.8 for detailed discussions.\n130 Prompt: \nImagine that you are a robot operating a computer. Like how humans operate the computer, you can move the \nmouse, click an icon with the mouse, or type some texts with the keyboard. \nYou are asked to find what\u2019s in the news today. \nPrevious action: \"My immediate next step would be to move the mouse to the top right corner of the screen, \nwhere I can see the \"X\" icon to close the current tab. I would then click on the \"X\" icon to close the tab and \nreturn to the previous page to continue browsing for more news articles. \" \nYou have now returned to the previous page after reading the first article \u201cState Department doubles down on \nZelensky slams\u201d. Below is what you see on the computer screen. Based on the image below, what is your \nimmediate next step? If the action involves moving the mouse, describe the location as detailed as possible. \nGPT-4V: \nMy immediate next step would be to move the mouse to the second article on the page, \"Catastrophic flooding  \nswamps Florida's panhandle as intense storms force evacuations\" . I would move the mouse to the left side of \nthe article title, approximately 1/3 of the way down the page, and click on the article title to open it and read the  \nnews.  Browsing the Web (Today\u2019s News) \nSec. 9.8 GUI Navigation Figure 101: GPT-4V navigates through GUI to browse the web to read today\u2019s news. Blue highlights\nthe predicted actions. Check Section 9.8 for detailed discussions.\n131 Prompt: \nRead the article shown in the images below and summarize it. \nGPT-4V: \nThe article discusses the catastrophic flooding that occurred in Vermont's capital, Montpelier, on Tuesday, July \n11, 2023. The flooding was caused by intense rainstorms that inundated the Northeast, leading to evacuations \nand closures in the area. \nThe floods were described as \"unprecedented\" and \"historic\" by officials, with some areas experiencing \nrecord-breaking rainfall. The National Weather Service issued a travel advisory, warning residents to avoid \nnon-essential travel. \nVermont Governor Phil Scott declared a state of emergency, and New Hampshire sent rescue crews and Black \nHawk helicopters to assist with the response. The Vermont Fire Department and other emergency services were \nalso dispatched to the affected areas. \nThe floods caused extensive damage to homes and businesses, with some residents being forced to evacuate. \nA boil water notice was issued for Montpelier, and officials began clearing debris from roadways on Wednesday. \nIn Oklahoma City, two children were rescued from a reservoir after being swept away by floodwaters. The \nchildren, aged 10 and 11, were among a group of four that were fishing in the area. \nIn New York, Hurricane Irene caused 8 inches of rain within a 24-hour period, leading to major infrastructure \ndamage. A state of emergency was declared for Long Lake Town Supervisor Clay Arsenault, and residents were \nurged to evacuate. \nThe article concludes with a quote from Dan Hancock, a resident of Chester, Vermont, who watched floodwater \nenter his basement. He expresses his frustration and helplessness, stating \"Once the water goes down, we go \nback in, clean it up and move on. What can we do?\"                                            Web Browsing (Today\u2019s News) \nSec. 9.8 GUI Navigation Figure 102: GPT-4V navigates through GUI to browse the web to read today\u2019s news. We prompt\nGPT-4V to read the screenshots of the second news article and summarize it. Check Section 9.8 for\ndetailed discussions.\n132 Prompt: \nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger \nto an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard. \nYou are asked to shop for an ergonomic keyboard, your budget is between $50 and $100. \nBelow is what you see on the phone  screen, predict your next move. If the action involves moving your finger to \nan icon, describe the location of that icon as detailed as possible. \nGPT-4V: \nMy next move would be to move my finger to the Amazon icon,  which is located in the third row and third column  \nof the screen. I would then click on the icon to open the app and search for an ergonomic keyboard within my  \nbudget between $50 and $100 . Online Shopping (Ergonomic Keyboard) Sec. 9.8 GUI Navigation \nFigure 103: GPT-4V navigates through GUI to shop for an ergonomic keyboard online. Blue\nhighlights the predicted actions. Red highlights the inaccurate location of the Amazon icon. Check\nSection 9.8 for detailed discussions.\n133 Prompt: \nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger \nto an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard. \nYou are asked to shop for an ergonomic keyboard, your budget is between $50 and $100. \nPrevious action: \"My next move would be to move my finger to the Amazon icon, which is located in the third row \nand third column of the screen. I would then click on the icon to open the app and search for an ergonomic \nkeyboard within my budget of $50. \" \nYou have now opened the Amazon app. Below is what you see on the phone screen. What will you do next? If \nthe action involves moving your finger to an icon, describe the location of that icon as detailed as possible. \nGPT-4V: \nMy next move would be to  move my finger to the search bar , which is located at the top of the screen. I would \nthen click on the search bar and type in \"ergonomic keyboard\" using the keyboard.  After typing in the search \nterm, I would click on the \"Search\" button, which is located on the bottom right corner of the keyboard, to initiate  \nthe search.  Online Shopping (Ergonomic Keyboard) Sec. 9.8 GUI Navigation \nFigure 104: GPT-4V navigates through GUI to shop for an ergonomic keyboard online. Blue\nhighlights the predicted actions. Check Section 9.8 for detailed discussions.\n134 Prompt: \nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger \nto an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard. \nYou are asked to shop for an ergonomic keyboard, your budget is between $50 and $100. \nPrevious action: \"My next move would be to move my finger to the search bar, which is located at the top of the \nscreen. I would then click on the search bar and type in \"ergonomic keyboard\" using the keyboard. After typing \nin the search term, I would click on the \"Search\" button, which is located on the bottom right corner of the \nkeyboard, to initiate the search. \" \nYou have now completed the previous action as described above. Below is what you see on the phone screen. \nPlease predict your immediate next move. If the action involves moving your finger to an icon, describe the \nlocation of that icon as detailed as possible. \nGPT-4V: \nMy immediate next move would be to move my finger to the \"Filters\" button , which is located at the top right \ncorner of the screen, next to the \"Windows\" button. I would then click on the \"Filters\" button to open the filter  \noptions  and narrow down my search results to only show ergonomic keyboards within my budget range of $50 to \n$100. Online Shopping (Ergonomic Keyboard) Sec. 9.8 GUI Navigation \nFigure 105: GPT-4V navigates through GUI to shop for an ergonomic keyboard online. Blue\nhighlights the predicted actions. Check Section 9.8 for detailed discussions.\n135 Prompt: \nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger \nto an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard. \nYou are asked to shop for an ergonomic keyboard, your budget is between $50 and $100. \nPrevious action: \"My immediate next move would be to move my finger to the \"Filters\" button, which is located at \nthe top right corner of the screen, next to the \"Windows\" button. I would then click on the \"Filters\" button to open \nthe filter options and narrow down my search results to only show ergonomic keyboards within my budget range \nof $50 to $100. \" \nYou have now completed the previous action as described above. Below is what you see on the phone screen. \nPlease predict your immediate next move. If the action involves moving your finger to an icon, describe the \nlocation of that icon as detailed as possible. \nGPT-4V: \nMy immediate next move would be to move my finger to the \"Price & Deals\" filter option , which is located under \nthe \"Colors\" filter option and above the \"Sort by\" filter option. I would then click on the \"Price & Deals\" filter  \noption to open the price range filter and set my minimum price to $50 and my maximum price to $100.  This \nwould further narrow down my search results to only show ergonomic keyboards within my budget range. Online Shopping (Ergonomic Keyboard) Sec. 9.8 GUI Navigation \nFigure 106: GPT-4V navigates through GUI to shop for an ergonomic keyboard online. Blue\nhighlights the predicted actions. Check Section 9.8 for detailed discussions.\n136 Prompt: \nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger \nto an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard. \nYou are asked to shop for an ergonomic keyboard, your budget is between $50 and $100. \nPrevious action: \" My immediate next move would be to move my finger to the \"Price & Deals\" filter option, which \nis located under the \"Colors\" filter option and above the \"Sort by\" filter option. I would then click on the \"Price & \nDeals\" filter option to open the price range filter and set my minimum price to $50 and my maximum price to \n$100. This would further narrow down my search results to only show ergonomic keyboards within my budget \nrange. \" \nYou have now completed the previous action as described above. Below is what you see on the phone screen. \nWhat will you do next? If the action involves moving your finger to an icon, describe the location of that icon as \ndetailed as possible. \nGPT-4V: \nMy next move would be to move my finger to the \"Show 1,987 results\" button , which is located at the bottom of \nthe screen, below the \"Clear Filters\" button and above the navigation bar. I would then click on the \"Show 1,987  \nresults\" button to view the search results for ergonomic keyboards within my budget range.  Online Shopping (Ergonomic Keyboard) Sec. 9.8 GUI Navigation \nFigure 107: GPT-4V navigates through GUI to shop for an ergonomic keyboard online. Blue\nhighlights the predicted actions. Check Section 9.8 for detailed discussions.\n137 Prompt: \nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger \nto an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard. \nYou are asked to shop for an ergonomic keyboard, your budget is between $50 and $100. \nPrevious action: \" My next move would be to move my finger to the \"Show 1,987 results\" button, which is located \nat the bottom of the screen, below the \"Clear Filters\" button and above the navigation bar. I would then click on \nthe \"Show 1,987 results\" button to view the search results for ergonomic keyboards within my budget range. \" \nYou have now completed the previous action as described above. Below is what you see on the phone screen. \nWhat will you do next? If the action involves moving your finger to an icon, describe the location of that icon as \ndetailed as possible. \nGPT-4V: \nMy next move would be to scroll down the screen to view the different options for ergonomic keyboards within \nmy budget range. I would then move my finger to the \"Kensington Pro Fit Ergonomic Wireless Keyboard - Black\"  \noption , which is located below the \"Explore Related Brands\" section and above the \"VIVO\" option . I would then \nclick on the \"Kensington Pro Fit Ergonomic Wireless Keyboard - Black\" option to view more details  about the \nproduct and potentially make a purchase. Online Shopping (Ergonomic Keyboard) Sec. 9.8 GUI Navigation \nFigure 108: GPT-4V navigates through GUI to shop for an ergonomic keyboard online. Blue\nhighlights the predicted actions. Red highlights the inaccurate location of the product option to be\nselected. Check Section 9.8 for detailed discussions.\n138 Prompt: \nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger \nto an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard. \nYou are asked to shop for an ergonomic keyboard, your budget is between $50 and $100. \nPrevious action: \" My next move would be to scroll down the screen to view the different options for ergonomic \nkeyboards within my budget range. I would then move my finger to the \"Kensington Pro Fit Ergonomic Wireless \nKeyboard - Black\" option, which is located below the \"Explore Related Brands\" section and above the \"VIVO\" \noption. I would then click on the \"Kensington Pro Fit Ergonomic Wireless Keyboard - Black\" option to view more \ndetails about the product and potentially make a purchase. \" \nYou have now completed the previous action as described above. Below is what you see on the phone screen. \nWhat will you do next? If the action involves moving your finger to an icon, describe the location of that icon as \ndetailed as possible. \nGPT-4V: \nMy next move would be to scroll down the screen to view more details about the \"Kensington Pro Fit Ergonomic  \nWireless Keyboard - Black\" product . I would then move my finger to the \"Buy New\" button,  which is located \nbelow the product image and above the \"Get a $8 bonus when you reload $100 or more to your gift card \nbalance\" section. I would then click on the \"Buy New\" button  to proceed with the purchase. Online Shopping (Ergonomic Keyboard) Sec. 9.8 GUI Navigation \nFigure 109: GPT-4V navigates through GUI to shop for an ergonomic keyboard online. Blue\nhighlights the predicted actions. Red highlights the inaccurate action prediction (\u201cBuy New\u201d is not\na clickable button). Check Section 9.8 for detailed discussions.\n139 Prompt: \nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger \nto an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard. \nYou are asked to shop for an ergonomic keyboard, your budget is between $50 and $100. \nPrevious action: \" My next move would be to scroll down the screen to view more details about the \"Kensington \nPro Fit Ergonomic Wireless Keyboard - Black\" product. I would then move my finger to the \"Buy New\" button, \nwhich is located below the product image and above the \"Get a $8 bonus when you reload $100 or more to your \ngift card balance\" section. I would then click on the \"Buy New\" button to proceed with the purchase. \" \nYou have now completed the previous action as described above. Below is what you see on the phone screen. \nWhat will you do next? If the action involves moving your finger to an icon, describe the location of that icon as \ndetailed as possible. \nGPT-4V: \nMy next move would be to move my finger to the \"Add to Cart\" button , which is located below the \"Buy New\" \nbutton and above the \"Ships from Amazon\" section. I would then click on the \"Add to Cart\" button to add the  item \nto my shopping cart.  Online Shopping (Ergonomic Keyboard) Sec. 9.8 GUI Navigation \nFigure 110: GPT-4V navigates through GUI to shop for an ergonomic keyboard online. Blue\nhighlights the predicted actions. Check Section 9.8 for detailed discussions.\n140 Prompt: \nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger \nto an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard. \nYou are asked to shop for an ergonomic keyboard, your budget is between $50 and $100. \nPrevious action: \" My next move would be to move my finger to the \"Add to Cart\" button, which is located below \nthe \"Buy New\" button and above the \"Ships from Amazon\" section. I would then click on the \"Add to Cart\" button \nto add the item to my shopping cart. \" \nYou have now completed the previous action as described above. Below is what you see on the phone screen. \nWhat will you do next? If the action involves moving your finger to an icon, describe the location of that icon as \ndetailed as possible. \nGPT-4V: \nMy next move would be to move my finger to the \"Proceed to Checkout\" button , which is located below the  \n\"Compare with similar items\" section and above the \"30-day returns\" section.  I would then click on the \"Proceed  \nto Checkout\" button to proceed with the purchase.  Online Shopping (Ergonomic Keyboard) Sec. 9.8 GUI Navigation \nFigure 111: GPT-4V navigates through GUI to shop for an ergonomic keyboard online. Blue\nhighlights the predicted actions. Red highlights the inaccurate location of the \u201cProceed to checkout\u201d\nbuttion. Check Section 9.8 for detailed discussions.\n141 Prompt: \nImagine that you are a robot operating a phone. Like how humans operate the phone, you can move your finger \nto an icon, click an icon with your finger, scroll down a screen, or type some texts with the keyboard. \nBelow is what you see on the phone screen. Describe the image and predict your next move. If the action \ninvolves moving your finger to an icon, describe the location of that icon as detailed as possible. \nGPT-4V: \nThe image on the phone screen is the home screen of an iPhone. There are multiple app icons arranged in a \ngrid-like pattern, including Calendar, Settings, Twitter, Amazon, Instagram, etc. At the top of the screen, there is  \na notification from a user named \"yykhahah\" that reads \"I see you are in Seattle, let's meet up.\" At the bottom of \nthe screen, there is a search bar. \nBased on the notification, my next move would be to move my finger to the Maps app icon , which is located in \nthe second row  and the first column of the app grid. This will allow me to search for a location in Seattle and plan  \na meet up with the user.  Notification Understanding Sec. 9.8 GUI Navigation \nFigure 112: Prompting GPT-4V to predict the action upon receiving a notification. GPT-4V can\naccurately recognize the notification and the corresponding content (highlighted in green ).Blue\nhighlights the predicted actions. Red highlights the inaccurate location of the Maps app icon. Check\nSection 9.8 for detailed discussions.\n142 Prompt: \nImagine that you are a robot operating a computer. Like how humans operate the computer, you can move the \nmouse, click an icon with the mouse, or type some texts with the keyboard. \nBelow is what you see on the computer screen. Describe the image and predict your next move. If the action \ninvolves moving the mouse, describe the location and the icon as detailed as possible. \nGPT-4V: \nThe image on the computer screen is a desktop background of a bridge over a body of water. There are several \nicons on the desktop, including a YouTube icon, a Recycle Bin icon, and a Terminal icon. At the bottom of the \nscreen, there is a taskbar with several application icons, such as Microsoft Edge and File Explorer. On the \nbottom right corner of the screen, there is a notification that says \"Kevin Lin is calling you.\" \nBased on the notification, my next move would be to move the mouse to the bottom right corner of the screen  \nand click on the notification. This would open the call application and allow me to answer Kevin Lin's call.  Notification Understanding Sec. 9.8 GUI Navigation \nFigure 113: Prompting GPT-4V to predict the action upon receiving a notification. GPT-4V can\naccurately recognize the notification and the corresponding content (highlighted in green ).Blue\nhighlights the predicted actions. Red highlights the inaccurate location of the Maps app icon. Check\nSection 9.8 for detailed discussions.\n143 Prompt: \nImagine that you are a robot operating a computer. Like how humans operate the computer, you can move the \nmouse, click an icon with the mouse, or type some texts with the keyboard. \nBelow is what you see on the computer screen. Describe the image and predict your next move. If the action \ninvolves moving the mouse, describe the location as detailed as possible. \nGPT-4V: \nThe image on the computer screen is a desktop background of a beautiful aerial view of a bridge over the \nocean. There are several icons on the left side of the screen, including the Recycle Bin, Terminal, and a few \nothers. On the bottom right corner, there is a notification from a messaging app from a user named Kevin Lin  \nasking if I am free for a chat this afternoon. \n My next move would be to move the mouse to the bottom right corner of the screen and click on the notification  \nfrom Kevin Lin to open the messaging app and respond to his message . I would move the mouse to the exact \nlocation of the notification, which is approximately 2 inches from the right edge of the screen and 1 inch from the \nbottom edge of the screen. Notification Understanding Sec. 9.8 GUI Navigation \nFigure 114: Prompting GPT-4V to predict the action upon receiving a notification. GPT-4V can\naccurately recognize the notification and the corresponding content (highlighted in green ).Blue\nhighlights the predicted actions. Check Section 9.8 for detailed discussions.\n144 Prompt: \nExplain the story shown in the images below. \nGPT-4V: \nThe images show a reel on Instagram, where a user is sharing a humorous post about how much they have \nspent on their dog. The first image shows a man holding his dog with the caption \"When someone asks me how \nmuch I have spent on my dog\". The next three images show a clip of Joe Biden saying \"700 billion\" followed by \n\"700 billion and a trillion 300 million\" and \"700 billion and a trillion 300 million dollars\". The user is jokingly \nexaggerating the amount they have spent on their dog by using the clip of Joe Biden saying a large sum of \nmoney.  \nPrompt: \nTell me about the video content, given the frames below. \nGPT-4V: \nThe video content is a compilation of things that Asian people do for no reason. The video is posted on TikTok  by \nuser laparasian and has 1.3M views and 13.1K likes. The video is set to original audio by sophrelabelous and \nhas the hashtag #asian and #relatable. \nThe video is broken down into 5 different scenes: \n1. The first scene shows a girl grabbing products from the back of the shelf in a supermarket. \n2. The second scene shows a girl rinsing a perfectly clean cup before using it. \n3. The third scene shows two girls asking \"have you eaten yet?\" as a form of greeting. \n4. The fourth scene shows two girls fighting aggressively for the bill in a restaurant. \n5. The fifth scene shows a girl wrapping pants around her neck to see if it fits. \nOverall, the video is a humorous take on some of the quirks and habits that are commonly seen in the Asian \ncommunity. Watching Videos \nSec. 9.8 GUI Navigation Figure 115: Prompting GPT-4V to watch web videos. We present GPT-4V the screenshot of the\nvideo frames following their temporal order in the original video. To save space, we illustrate the\nframes in a row, where the leftmost one is the first frame. Check Section 9.8 for detailed discussions.\nRed highlights the inaccurate descriptions about the video.\n145 Prompt: \nTranscribe the video content, given the frames below. \nGPT-4V: \nHi everyone, it's natyexplora here and today I'm going to be sharing 7 places you need to visit in Washington \nState. \nFirst up, we have North Cascades National Park. This park is filled with beautiful mountain views and hiking \ntrails. It's a must-visit for any nature lover. \nNext, we have the Olympic Peninsula. This area is home to the famous Hoh Rainforest and beautiful beaches. \nIt's a great place to relax and enjoy the natural beauty of Washington. \nThird on the list is Mt Baker Wilderness. This area is known for its stunning mountain views and hiking trails. It's \na great place to escape the city and enjoy the great outdoors. \nNext up, we have Mt Rainier National Park. This park is home to the highest mountain in Washington and is a \ngreat place for hiking and sightseeing. \nFifth on the list is Alpine Lakes Wilderness. This area is filled with beautiful alpine lakes and hiking trails. It's a \ngreat place to spend a day exploring and enjoying the natural beauty of Washington. \nNext, we have Mt Pilchuck State Park. This park is known for its beautiful views and hiking trails. It's a great \nplace to spend a day exploring and enjoying the natural beauty of Washington. \nLast but not least, we have Gifford Pinchot National Forest. This forest is home to beautiful mountain views and \nhiking trails. It's a great place to escape the city and enjoy the great outdoors. \nAnd that concludes my list of 7 places you need to visit in Washington State. I hope you enjoyed it and are \ninspired to explore more of this beautiful state. \nPrompt: \nTell me about the video content, given the frames below. \nGPT-4V: \nThe video content is about a corgi dog named Tofu who is working from home. The first frame shows Tofu \nsleeping on a bed with the caption \"WFH Employees: 8:59 AM\". The second frame shows Tofu sitting in front of \na computer with multiple screens, with the caption \"WFH Employees: 9:00 AM\". The video is likely a humorous \ntake on the work from home experience, highlighting how Tofu is just like any other employee who struggles to \nget out of bed in the morning but is ready to work once they're in front of their computer. Watching Videos \nSec. 9.8 GUI Navigation Figure 116: Watching web videos. We present GPT-4V the screenshot of the video frames following\ntheir temporal order in the original video. To save space, we illustrate the frames in a row, where the\nleftmost one is the first frame. Check Section 9.8 for detailed discussions.\n146 Prompt: \nTell me about the video content, given the frames below. Please include as many details as possible. \nGPT-4V: \nThe video content is a tutorial on how to make Raspberry Bliss Balls, a healthy breakfast option for toddlers and \nbig kids. The video is posted by the Instagram account \"mealtime with mummy\" and has over 13k likes. \nThe video starts with a bowl of fresh raspberries and a fork, which is then mashed into a puree. The next frame \nshows a larger mixing bowl with oats, chia seeds, and peanut butter being added to the raspberry puree. The \nmixture is then stirred together until well combined. \nThe following frame shows a plate with shredded coconut on it, and the raspberry mixture being rolled into small \nballs and coated in the coconut. The final frames show the finished Raspberry Bliss Balls being placed in a bowl \nand ready to eat. \nThroughout the video, there are text overlays with instructions and tips, making it easy for viewers to follow \nalong and make the recipe themselves. Watching Videos \nSec. 9.8 GUI Navigation Figure 117: Watching web videos. We present GPT-4V the screenshot of the video frames following\ntheir temporal order in the original video. To save space, we illustrate frames 1-5 in the first row, and\nframes 6-9 in the second row. Check Section 9.8 for detailed discussions.\n147 Prompt: \nTell me about the video content, given the frames below. Please include as many details as possible. \nGPT-4V: \nThe video content is a recipe tutorial for apple and banana pancakes. The video is posted on Instagram Reels \nby the user \"mealtime with mummy\" and has 13.2K views. \nIn the first frame, we see a glass bowl with a mashed banana and an apple being added to it. The second frame \nshows the banana and apple being mixed together. The third frame shows an egg being added to the mixture. \nThe fourth frame shows the mixture being stirred with a fork. \nThe fifth frame shows a bowl of oats being added to the mixture. The sixth frame shows the oats being mixed in \nwith a spoon. The seventh frame shows the mixture being spooned into a frying pan. The eighth frame shows \nthe pancakes being cooked in the frying pan. \nThe final frame shows the finished pancakes on a plate, served with sliced apples, bananas, and yogurt. The \ncaption on the video reads \"SAVE for easy breakfast inspo\" and the video is set to the song \"Stayin' Alive\" by the \nBee Gees. Watching Videos \nSec. 9.8 GUI Navigation Figure 118: Watching web videos. We present GPT-4V the screenshot of the video frames following\ntheir temporal order in the original video. To save space, we illustrate frames 1-5 in the first row, and\nframes 6-9 in the second row. Check Section 9.8 for detailed discussions.\n148 Prompt: \nTell me about the video content, given the 9 frames below. Please include as many details as possible. \nGPT-4V: \nThe video content is a recipe tutorial for strawberry stuffed French toast. The video is posted on Instagram Reels \nby the user \"mealtime with mummy\". The video has 13.2K likes and 190 comments. \nIn the first frame, we see a white plate with a dollop of red sauce and a dollop of white cream. The second frame \nshows the plate with the red sauce and white cream mixed together. The third frame shows a slice of bread with  \na knife spreading a mixture onto it.  \nThe fourth frame shows a plate with a circular piece of bread on it. The fifth frame shows a hand using a knife to \nspread a pink mixture onto the bread. The sixth frame shows the bread with the pink mixture covered with \nanother piece of bread, making a sandwich. \nThe seventh frame shows a hand dipping the sandwich into a bowl of beaten eggs. The eighth frame shows the \nsandwich being fried in a pan. The final frame shows the finished strawberry stuffed French toast on a plate. \nThe video is set to the song \"Stayin' Alive\" by the Bee Gees. The caption reads \"SAVE for easy breakfast inspo \". Watching Videos \nSec. 9.8 GUI Navigation Figure 119: Watching web videos. We present GPT-4V the screenshot of the video frames following\ntheir temporal order in the original video. To save space, we illustrate frames 1-5 in the first row, and\nframes 6-9 in the second row. Red highlights the inaccurate descriptions about the video. Check\nSection 9.8 for detailed discussions.\n149 10 LMM Powered Agents\nIn this section, we discuss possible future research directions that may further amplify GPT-4V\u2019s\ncapabilities. The discussion focuses on how the intriguing usages in LLMs may extend to the\nmultimodal scenario and its enabled new abilities, e.g., multimodal plugins, multimodal chains,\nself-reflection, self-consistency, and retrieval-augmented LMMs, etc. In the following sub-sections,\nwe use human-generated examples to illustrate potential ways to enhance GPT-4V-based systems.\nGPT-4V: \nSorry, I cannot answer this question accurately as no context or information is provided with the photo. \nHowever, based on the destruction and the presence of emergency responders, it appears to be taken in  a \nwar-torn or disaster-stricken area. Sec. 10.1 Multimodal Plugins \nPrompt: \nWhere is this photo taken? \nPrompt: \nWhere is this photo taken? \n \nGPT-4V: \n-I do not have that information. \n-Enabling Bing Image Search Plugin. Note: Same query image as shown above, resized to \nsave space \nCalling Bing Image Search\u2026 \n-This image is taken in izmir, Turkey \n-The image is related to an earthquake event \n-To summarize, this image was taken in  \u0130zmir, Turkey, during a magnitude 6.6 earthquake in Turkey\u2019s  \nAegean region . The scene shows the aftermath of the disaster with people standing in a pile of rubble, a \ncrane lifting a truck, and a red truck with a black front. \nFigure 120: Illustration of using the Bing Image Search [ 94] plugin to enable GPT-4V with time-\nsensitive knowledge (bottom, highlighted in green ). Note that the earthquake happened on February\n6, 2023, which is after GPT-4V\u2019s training, thereby GPT-4V fails to identify the exact location without\nplugin (top). Check Section 10.1 for detailed discussions.\n150 Thought 1 Sec. 10.2 Multimodal Chains \nPrompt: \nPlease determine whether each person in the image wears a helmet or not. And summarize how many people \nare wearing helmets. \nThis question requires detecting people in the image. \nAction 1 Assistant, detect the people shown in this image <ImagePath> \nObservation 1 Person [4, 126, 72, 175] \nPerson [63, 121, 88, 181] \nPerson [30, 107, 56, 88] \nPerson [223, 123, 43, 78] \nPerson [178, 122, 52, 180] \nPerson [322, 118, 41, 68] \nPerson [268, 109, 55, 178] \nPerson [357, 127, 88, 173] \nThought 2 \nAction 2 \nObservation 2 There are 8 people detected in this image. \nAssistant, crop the regions in this image specified by the bounding boxes <ImagePath> \nImage 1 Image 2 Image 3 Image 4 Image 5 Image 6 Image 7 Image 8 \nGPT-4V: \nImage 1: Not wearing a helmet \nImage 2: Not wearing a helmet \nImage 3: Wearing a helmet \nImage 4: Wearing a helmet \nImage 5: Wearing a helmet \nImage 6: Wearing a helmet \nImage 7: Wearing a helmet \nImage 8: Not wearing a helmet \nSummary: Out of 8 people, 5 are wearing helmets and 3 are not wearing helmets.  Figure 121: Extending GPT-4V to multimodal chains with ReAct [ 145,142] for PPE Counting\nscenario. Check Section 10.2 for detailed discussions.\n151 10.1 Multimodal Plugins\nIn the context of LLMs, plugins [ 98,56,6,112,87,103] play a crucial role in assisting LLMs for\nvarious tasks such as accessing the latest information, performing computations, or utilizing third-\nparty services. These plugins are primarily designed to process inputs in natural language or inputs\nthat can be interpreted as language, such as code and math equations. To illustrate the significance of\nmultimodal plugins, such as Bing Image Search [ 94], especially in the context of LMMs, we present\nFigure 120. By incorporating the Bing Image Search plugin, we empower GPT-4V to acquire time-\nsensitive knowledge related to the input image. In the upper part of the figure, we demonstrate the\nlimitations of GPT-4V without Bing Image Search plugin. It fails to accurately answer the question,\n\"Where was this photo taken?\" due to the fact that the photo captures the aftermath of a massive\nearthquake that occurred on February 6, 2023, at the border of Turkey and Syria\u2014a situation that took\nplace after GPT-4V\u2019s training. Since constantly retraining the model with current information can be\ncomputationally intensive and expensive, plugins like search engines prove to be invaluable resources\nfor the model to access up-to-date information. In the lower part of Figure 120, we showcase the\ncapabilities of GPT-4V when equipped with the Bing Image Search plugin. It effectively leverages the\nretrieved information from the plugin, enabling accurate identification of the location \u02d9Izmir, Turkey.\n10.2 Multimodal Chains\nChaining with LLMs has been explored extensively in recent research [ 145,47,124,107]. This\napproach goes beyond using a single plugin and instead establishes a system paradigm that integrates\nLLMs with a pool of plugins, enabling more advanced reasoning and interactions. By replacing\nlanguage-only plugins with vision/multimodal experts such as image captioners, object detectors, or\nwell-trained models for text-to-image generation and audio-to-text conversion, it becomes possible to\nconstruct a powerful multimodal chain with LLMs [137, 142, 121, 114, 75, 86].\nHowever, the interactions within these chains between LLMs and the plugins typically take place in\ntext format. Although the plugins may accept multimodal inputs, they return results in text to enhance\nthe knowledge of LLMs. There is a notable exception in the case of image synthesis/editing [ 137],\nwhere the plugins can generate images, but these images are not fed back into LLMs for further\nanalysis or knowledge augmentation, as LLMs can only process language-based inputs.\nIn Figure 121, we present an illustration of how GPT-4V, can be extended to support multimodal\nchains with ReAct [ 145,142]. This extension enables the plugins in the chain to provide multimodal\ninformation, which can then be collectively processed by GPT-4V to achieve advanced reasoning in\nscenarios such as PPE counting. The entire chaining process shown in Figure 121 is divided into\ntwo rounds of thought, action, and observation, with each round involving the activation of a specific\nplugin. In the first round, GPT-4V deduces that person detection is necessary to count the number of\npeople wearing helmets (Thought 1). Consequently, it calls the person detector tool (Action 1) and\nreceives the coordinates of bounding boxes for each detected person in the image (Observation 1).\nMoving to the second round, based on the obtained bounding box information, GPT-4V infers that\nthere are a total of 8 people in the image (Thought 2). It then utilizes the image cropping tool to crop\nout individual images of each person according to their corresponding bounding box coordinates\n(Action 2). The resulting outputs (Observation 2) consist of 8 labeled images, numbered from image\n1 to image 8. GPT-4V subsequently determines whether each person in these images is wearing a\nhelmet or not, and summarizes the total count of people wearing helmets.\nOverall, this integration of LMMs with a pool of multimodal plugins opens up new possibilities for\nenhanced reasoning and interaction, leveraging the strengths of both language and vision capabilities.\nThe flexibility of multimodal chains allows for a more comprehensive understanding and analysis of\nmultimodal data, and can potentially lead to improved performance in various applications.\n10.3 Self-Reflection\nFigure 122 demonstrates the application of self-reflection [ 116,88,63] to improve the results shown\nin Figure 47. As we can see, the self-reflected result is better aligned with the reference image. For\nexample, on the left side, the number of data points is corrected from 4 to 3, while on the right side,\nthe percentage is added back above the bar. Although the result is still not exactly identical, it is\nevident that self-reflection can facilitate manual polishing. Figure 123 shows another example of\nself-reflection in improving the prompt generation for text-to-image models [106].\n152 Sec. 10.3 Self-Reflection \nPrompt: \nGenerate python code to draw similar curves. \nGPT-4V: \nPrompt: \nCan you double-check the code and align it better \nwith the reference image? \nNote: Code from the above GPT-4V \nresponse is omitted to save space \nGPT-4V: \nPrompt: \nWrite python codes to generate similar figures. \nGPT-4V: \nPrompt: \nCan you double-check the code and align it better \nwith the reference image? \nNote: Code from the above GPT-4V \nresponse is omitted to save space \nGPT-4V: Self-Reflection to Improve Coding Figure 122: Illustration of using self-reflection to improve the code for figure drawing. Left: after\nreflection, the number of points in the curve aligns with the reference image. Right: the percentage is\nadded to align with the reference image. Check Section 10.3 for detailed discussions.\n153 Sec. 10.3 Self-Reflection \nPrompt: \nInstruction: Given a user imagined IDEA of the scene, \nconverting the IDEA into a self-contained sentence \nprompt that will be used to generate an image. \nIDEA: photo of a dog looks like the one in the given \nimage running on the beach. \nBased on the above information, I wrote a detailed \nprompt exactly about the IDEA follow the rules. \nGPT-4V: \nA blue dog running on the beach with a happy \nexpression on its face. \nSDXL: \nPrompt: \nI am iteratively improving the sentence prompt by \nlooking at the images generated by an AI art \ngeneration model and find out what is different from \nthe given IDEA. The IDEA is: photo of a dog \u2026 beach . \nGenerated sentence prompt for current round is: A \nblue dog running on the beach with a happy \nexpression on its face. Corresponding image \ngenerated by the AI art generation model: \nSelf-Reflection to Improve Text-to-Image Generation \nThe generated image is not perfect. Write key reasons \non how to further improve the image to better follow \nthe user imagined IDEA of the scene. \nGPT-4V: \nThe dog in the generated image does not look like the \none in the given image. The dog in the given image is \na pug, while the dog in the generated image is a \ndifferent breed. To address this issue, we can modify \nthe prompt to specifically mention that the dog should \nbe a pug. \nPrompt: \nInstruction: Given a user imagined IDEA of the scene, \nconverting the IDEA into a sentence prompt that will \nbe used to generate an image. I am iteratively \nimproving the sentence prompt by looking at the \nimages generated by an AI art generation model and \nfind out what is different from the given IDEA. The \nIDEA is: photo of a dog \u2026 beach . Generated sentence \nprompt for current round is: A blue \u2026 its face . \nCorresponding image generated by the AI art \ngeneration model: \nThe image can be improved because: The dog \u2026  \nshould be a pug . Based on the above information, to \nimprove the image, I wrote a detailed prompt exactly \nabout the IDEA follow the rules. \nGPT-4V: \nA blue pug running on the beach with a happy \nexpression on its face, with a sunset and palm trees in \nthe background. \nSDXL: \nPrompt Continued \nFigure 123: Illustration of using self-reflection to improve the generated text prompts for a text-to-\nimage model SDXL [ 106]. GPT-4V reflects the error in the initial prompt that it does not mention the\ndog\u2019s breed, and makes the correct revision. Check Section 10.3 for detailed discussions.\n154 Sec. 10.4 Self-Consistency \nSample 1 \nPrompt: \nCount the number of boats in the image. \nGPT-4V: \nThere are 6 boats in the image. \nSelf-Consistency for Counting \nSample 3 \nPrompt: \nCount the number of boats in the image. Describe \neach of their locations. \nGPT-4V: \nThere are 4 boats in the image.  1) The first boat is on \nthe left side of the image, docked at the pier. It is a \nlarger boat with a tower on top. 2) The second boat is \nin the center of the image, also docked at the pier. It is \na smaller boat with a white canopy on top. 3) The third \nboat is on the right side of the image, docked at the \npier. It is a smaller boat with a white canopy on top, \nsimilar to the second boat. 4) The fourth boat is in the \nbackground, on the far right side of the image. It is \npartially obscured by the third boat, but appears to be \na larger boat with a tower on top. \nSample 2 \nPrompt: \nCount the number of boats in the image. Describe \neach of their locations. \nGPT-4V: \nThere are a total of 4 boats in the image . 1) The first \nboat is on the left side of the image, docked at the \npier. It is a larger boat with a tower on top. 2) The \nsecond boat is in the center of the image, also docked \nat the pier. It is a smaller boat with a white canopy on \ntop. 3) The third boat is on the right side of the image, \ndocked at the pier. It is a smaller boat with a white \ncanopy on top, similar to the second boat. 4) The \nfourth boat is in the background, on the far right side of \nthe image. It is a sailboat, and it appears to be sailing \non the water. \nSample 4 \nPrompt: \nCount the number of boats in the image. Describe \neach of their locations. \nGPT-4V: \nThere are 6 boats in the image.  - 1 boat is on the left \nside of the image, docked at the pier with a tower on \ntop - 2 boats are in the center of the image, docked at \nthe pier side by side - 1 boat is on the right side of the \nimage, docked at the pier with a cover on top - 2 boats \nare in the background on the right side of the image, \ndocked at a different pier. \nSelf Consistency: \nSample 2:  There are a total of 4 boats in the image. 1) The first boat is on the left side of the image \u2026\u2026 \nSample 3:  There are 4 boats in the image. 1) The first boat is on the left side of the image, docked  \u2026\u2026 \nSample 4:  There are 6 boats in the image. - 1 boat is on the left side of the image, docked at the pier \u2026\u2026 \n[Marginalize out reasoning paths to aggregate final answers, e.g., majority vote ] =>\nThere are 4 boats in the image. \nNote: Same query image \nas shown in Sample 1, \nresized to save space Note: Same query image \nas shown in Sample 1, \nresized to save space \nNote: Same query image \nas shown in Sample 1, \nresized to save space Figure 124: Improve the counting reliability with self-consistency [ 130], which aggregates multiple\ncounting results repeated on the same image. Check Section 10.4 for detailed discussions.\n10.4 Self-Consistency\nSelf-consistency [ 130] is a decoding strategy that aggregates multiple sampled outputs to produce\nthe final answer, such as with the majority vote. Extended from marginalizing to aggregating final\nanswers, Tree-of-Thoughts [ 144] shows that the self-consistency idea can be applied to intermediate\nthoughts to improve the LLM reasoning performance. Figure 124 illustrates the use of self-consistency\non GPT-4V for counting problems. We sample multiple counting results by asking GPT-4V to count\n155 the same image multiple times, either conducting multiple runs (Samples 2-4) or rephrasing the input\ntext instruction (Samples 1,2). The example then uses the simple majority vote to aggregate the final\nanswer of \u201c4 boats.\u201d We leave the comprehensive explorations of self-consistency LMMs to future\nworks.\n10.5 Retrieval-Augmented LMMs\nRetrieval-Augmented LMMs [ 93,68,50,19,115,104] enhances text generation by retrieving and\nintegrating relevant information into prompts. The technique is particularly effective when specialized\ntask-relevant information is needed, such as expert knowledge in a highly-specialized expert domain,\nthe most recent information that may differ from LLMs\u2019 memory, and the customizable information\nthat varies from user to user. We imagine retrieval augmentation continues to play an essential role in\nLMMs. Figure 74 shows an example of retrieval-augmented LMMs helping grocery checkout. Since\nthe produces\u2019 image-text-price triplets are different in each store, it would be beneficial to retrieve\nthem from the store\u2019s database and yield the correct checkout information. Similarly, in Figure 81\u2019s\nthe customized captioning scenario, we imagine the system may automatically retrieve the family\nmembers\u2019 photos from the album and achieve the customized captioning.\n11 Conclusions\n11.1 Summary and Conclusions\nIn this report, our primary focus is on probing GPT-4V across various application scenarios. The\nfindings reveal its remarkable capabilities, some of which have not been investigated or demonstrated\nin existing approaches. While we strive to uncover as many of these capabilities as possible, we\nacknowledge that our presentation may not be exhaustive. Nevertheless, this report can serve\nas a reference for future research aimed at exploring additional uses of GPT-4V, deepening the\nunderstanding of LMMs, and building even more powerful LMMs.\n11.2 Towards Future LMMs\nThe weaknesses and limitations of GPT models have been extensively discussed in related reports [ 99,\n100,24]. In this section, we briefly focus on presenting our perspective on future research directions.\nModels like GPT-1, GPT-2, and GPT-3 function primarily as text-in-text-out systems, capable of\nprocessing natural language only. GPT-4 (no vision) demonstrates unparalleled competence in text\nunderstanding and generation, while GPT-4V exhibits a strong ability to comprehend the image\ndomain as well.\nAs a natural progression, LMMs should be able to generate interleaved image-text content, such\nas producing vivid tutorials containing both text and images, to enable comprehensive multimodal\ncontent understanding and generation. Additionally, it would be beneficial to incorporate other\nmodalities, such as video, audio, and other sensor data, to expand the capabilities of LMMs.\nRegarding the learning process, current approaches predominantly rely on well-organized data, such\nas image-tag or image-text datasets. However, a more versatile model may be able to learn from\nvarious sources, including online web content and even real-world physical environments, to facilitate\ncontinuous self-evolution.\nAcknowledgment\nWe express our gratitude to all contributors from OpenAI for their technical efforts on the GPT-4V\nproject [ 99\u2013101,1], and we are profoundly thankful to OpenAI for granting early access to their\nremarkable tool. Our sincere appreciation goes to Misha Bilenko for his invaluable guidance and\nsupport. We also extend heartfelt thanks to our Microsoft colleagues for their insights, with special\nacknowledgment to John Montgomery, Marco Casalaina, Gregory Buehrer, Nguyen Bach, Gopi\nKumar, Luis Vargas, Kun Wu, Meenaz Merchant, Jianfeng Gao, Matt Lungren, Sheela Agarwal,\nYumao Lu, Thomas Soemo, Fisayo Okikiolu, Ce Liu, Michael Zeng, Faisal Ahmed, Ehsan Azarnasab,\nand Lin Liang for their constructive feedback. We also thank Yingkai Yu for helping to create\nscreenshots on GUI Navigation.\n156 References\n[1]Chatgpt can now see, hear, and speak. https://openai.com/blog/\nchatgpt-can-now-see-hear-and-speak , 2023.\n[2] Deepfloyd if. https://github.com/deep-floyd/IF , 2023.\n[3] Guidance. https://github.com/microsoft/guidance/ , 2023.\n[4] Midjourney. https://www.midjourney.com/ , 2023.\n[5]Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven M Seitz,\nand Richard Szeliski. Building rome in a day. Communications of the ACM , 54(10):105\u2013112,\n2011.\n[6]Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David,\nChelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i can, not\nas i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691 , 2022.\n[7]Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,\nKarel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual\nlanguage model for few-shot learning. Advances in Neural Information Processing Systems ,\n35:23716\u201323736, 2022.\n[8]Chris Alberti, Jeffrey Ling, Michael Collins, and David Reitter. Fusion of detected objects in\ntext for visual question answering. In EMNLP , 2019.\n[9]Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould,\nand Lei Zhang. Bottom-up and top-down attention for image captioning and visual question\nanswering. In CVPR , 2018.\n[10] Karpathy Andrej. State of gpt. https://karpathy.ai/stateofgpt.pdf , 2023.\n[11] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report.\narXiv preprint arXiv:2305.10403 , 2023.\n[12] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence\nZitnick, and Devi Parikh. VQA: Visual Question Answering. In ICCV , 2015.\n[13] Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani\nMarathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel\nIlharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo, March 2023.\n[14] Hessam Bagherinezhad, Hannaneh Hajishirzi, Yejin Choi, and Ali Farhadi. Are elephants big-\nger than butterflies? reasoning about sizes of objects. In Proceedings of the AAAI Conference\non Artificial Intelligence , volume 30, 2016.\n[15] Shruthi Bannur, Stephanie Hyland, Qianchu Liu, Fernando Perez-Garcia, Maximilian Ilse,\nDaniel C Castro, Benedikt Boecking, Harshita Sharma, Kenza Bouzid, Anja Thieme, et al.\nLearning to exploit temporal structure for biomedical vision-language processing. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n15016\u201315027, 2023.\n[16] David Barrett, Felix Hill, Adam Santoro, Ari Morcos, and Timothy Lillicrap. Measuring\nabstract reasoning in neural networks. In International conference on machine learning , pages\n511\u2013520. PMLR, 2018.\n[17] Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Mar\u00e7al Rusinol, Ernest Valveny,\nCV Jawahar, and Dimosthenis Karatzas. Scene text visual question answering. In ICCV , 2019.\n[18] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion\nmodels with reinforcement learning, 2023.\n157 [19] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie\nMillican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark,\net al. Improving language models by retrieving from trillions of tokens. In International\nconference on machine learning , pages 2206\u20132240. PMLR, 2022.\n[20] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2013mining discriminative\ncomponents with random forests. In Computer Vision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13 , pages 446\u2013461. Springer,\n2014.\n[21] Marc A Brackett and Peter Salovey. Measuring emotional intelligence with the mayer-salovery-\ncaruso emotional intelligence test (msceit). Psicothema , 18:34\u201341, 2006.\n[22] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow\nimage editing instructions. In CVPR , 2023.\n[23] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language\nmodels are few-shot learners. In NeurIPS , 2020.\n[24] S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece\nKamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general\nintelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712 , 2023.\n[25] Ting Chen, Saurabh Saxena, Lala Li, David J Fleet, and Geoffrey Hinton. Pix2seq: A language\nmodeling framework for object detection. In ICLR , 2022.\n[26] Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J Fleet, and Geoffrey E Hinton.\nA unified sequence interface for vision tasks. Advances in Neural Information Processing\nSystems , 35:31333\u201331346, 2022.\n[27] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r,\nand C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv\npreprint arXiv:1504.00325 , 2015.\n[28] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng,\nand Jingjing Liu. Uniter: Learning universal image-text representations. In ECCV , 2020.\n[29] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language tasks via text\ngeneration. In ICML , 2021.\n[30] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.\n[31] Herbert H Clark and Deanna Wilkes-Gibbs. Referring as a collaborative process. Cognition ,\n22(1):1\u201339, 1986.\n[32] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo\nBenenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic\nurban scene understanding. In Proceedings of the IEEE conference on computer vision and\npattern recognition , pages 3213\u20133223, 2016.\n[33] Tom Cornsweet. Visual perception . Academic press, 2012.\n[34] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. Why can gpt learn\nin-context? language models secretly perform gradient descent as meta optimizers. arXiv\npreprint arXiv:2212.10559 , 2022.\n[35] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng\nWang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose\nvision-language models with instruction tuning. arXiv preprint arXiv:2305.06500 , 2023.\n158 [36] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jos\u00e9 MF Moura, Devi\nParikh, and Dhruv Batra. Visual dialog. In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 326\u2013335, 2017.\n[37] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In CVPR , 2009.\n[38] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. In NAACL-HLT , 2019.\n[39] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing\nXu, and Zhifang Sui. A survey for in-context learning. arXiv preprint arXiv:2301.00234 ,\n2022.\n[40] Zi-Yi Dou, Aishwarya Kamath, Zhe Gan, Pengchuan Zhang, Jianfeng Wang, Linjie Li, Zicheng\nLiu, Ce Liu, Yann LeCun, Nanyun Peng, et al. Coarse-to-fine vision-language pre-training\nwith fusion in the backbone. In Advances in Neural Information Processing Systems .\n[41] Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang\nZhu, Pengchuan Zhang, Lu Yuan, Nanyun Peng, et al. An empirical study of training end-\nto-end vision-and-language transformers. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 18166\u201318176, 2022.\n[42] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian\nIchter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen\nChebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol\nHausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e:\nAn embodied multimodal language model. In arXiv preprint arXiv:2303.03378 , 2023.\n[43] Alicia Fasquel, Ang\u00e8le Brunelli\u00e8re, and Dominique Knutsen. A modified procedure for\nnaming 332 pictures and collecting norms: Using tangram pictures in psycholinguistic studies.\nBehavior Research Methods , pages 1\u201323, 2022.\n[44] Samir Yitzhak Gadre, Kiana Ehsani, and Shuran Song. Act the part: Learning interaction\nstrategies for articulated object part discovery. In Proceedings of the IEEE/CVF International\nConference on Computer Vision , pages 15752\u201315761, 2021.\n[45] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. Large-scale\nadversarial training for vision-and-language representation learning. In NeurIPS , 2020.\n[46] Zhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu, Jianfeng Gao, et al. Vision-\nlanguage pre-training: Basics, recent advances, and future trends. Foundations and Trends \u00ae\nin Computer Graphics and Vision , 14(3\u20134):163\u2013352, 2022.\n[47] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan,\nand Graham Neubig. Pal: Program-aided language models. In International Conference on\nMachine Learning , pages 10764\u201310799. PMLR, 2023.\n[48] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu,\nWenwei Zhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vision and language model for\ndialogue with humans, 2023.\n[49] Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and Jianfeng Gao. Ms-celeb-1m: A\ndataset and benchmark for large-scale face recognition. In Computer Vision\u2013ECCV 2016: 14th\nEuropean Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part\nIII 14 , pages 87\u2013102. Springer, 2016.\n[50] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval\naugmented language model pre-training. In International conference on machine learning ,\npages 3929\u20133938. PMLR, 2020.\n[51] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings\nof the IEEE international conference on computer vision , pages 2961\u20132969, 2017.\n159 [52] Jack *Hessel, Jena D *Hwang, Jae Sung Park, Rowan Zellers, Chandra Bhagavatula, Anna\nRohrbach, Kate Saenko, and Yejin Choi. The Abduction of Sherlock Holmes: A Dataset for\nVisual Abductive Reasoning. In ECCV , 2022.\n[53] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.\nTraining compute-optimal large language models. arXiv preprint arXiv:2203.15556 , 2022.\n[54] Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah A Smith, and Jiebo Luo. Promptcap:\nPrompt-guided task-aware image captioning. In Proceedings of International Conference on\nComputer Vision (ICCV) , 2023.\n[55] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao\nLv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning\nperception with language models. arXiv preprint arXiv:2302.14045 , 2023.\n[56] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as\nzero-shot planners: Extracting actionable knowledge for embodied agents. In International\nConference on Machine Learning , pages 9118\u20139147. PMLR, 2022.\n[57] Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. Pixel-bert: Aligning\nimage pixels with text by deep multi-modal transformers. arXiv preprint arXiv:2004.00849 ,\n2020.\n[58] Fabian Hutmacher. Why is there so much more research on vision than on any other sensory\nmodality? Frontiers in psychology , 10:2246, 2019.\n[59] Anya Ji, Noriyuki Kojima, Noah Rush, Alane Suhr, Wai Keen V ong, Robert Hawkins, and\nYoav Artzi. Abstract visual reasoning with tangram shapes. In Proceedings of the 2022\nConference on Empirical Methods in Natural Language Processing , pages 582\u2013601, 2022.\n[60] Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P\nLungren, Chih-ying Deng, Roger G Mark, and Steven Horng. Mimic-cxr, a de-identified\npublicly available database of chest radiographs with free-text reports. Scientific data , 6(1):317,\n2019.\n[61] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick,\nand Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary\nvisual reasoning. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition , pages 2901\u20132910, 2017.\n[62] Justin Johnson, Andrej Karpathy, and Li Fei-Fei. Densecap: Fully convolutional localization\nnetworks for dense captioning. In Proceedings of the IEEE conference on computer vision and\npattern recognition , pages 4565\u20134574, 2016.\n[63] Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer\ntasks. arXiv preprint arXiv:2303.17491 , 2023.\n[64] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without\nconvolution or region supervision. In ICML , 2021.\n[65] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson,\nTete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In\nProceedings of International Conference on Computer Vision (ICCV) , 2023.\n[66] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.\nLarge language models are zero-shot reasoners. Advances in neural information processing\nsystems , 35:22199\u201322213, 2022.\n[67] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie\nChen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting\nlanguage and vision using crowdsourced dense image annotations. IJCV , 2017.\n160 [68] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Na-\nman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-\naugmented generation for knowledge-intensive nlp tasks. Advances in Neural Information\nProcessing Systems , 33:9459\u20139474, 2020.\n[69] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and Jianfeng\nGao. Multimodal foundation models: From specialists to general-purpose assistants. arXiv\npreprint arXiv:2309.10020 , 2023.\n[70] Gen Li, Nan Duan, Yuejian Fang, Ming Gong, Daxin Jiang, and Ming Zhou. Unicoder-vl: A\nuniversal encoder for vision and language by cross-modal pre-training. In AAAI , 2020.\n[71] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-\nimage pre-training with frozen image encoders and large language models. arXiv preprint\narXiv:2301.12597 , 2023.\n[72] Junnan Li, Ramprasaath R Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty, Caiming Xiong,\nand Steven Hoi. Align before fuse: Vision and language representation learning with momen-\ntum distillation. In NeurIPS , 2021.\n[73] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A\nsimple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557 ,\n2019.\n[74] Xiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu, Pengchuan Zhang, Lei Zhang, Lijuan Wang,\nHoudong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for\nvision-language tasks. In ECCV , 2020.\n[75] Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei\nJi, Shaoguang Mao, et al. Taskmatrix. ai: Completing tasks by connecting foundation models\nwith millions of apis. arXiv preprint arXiv:2303.16434 , 2023.\n[76] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,\nPiotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV ,\n2014.\n[77] Fangyu Liu, Emanuele Bugliarello, Edoardo Maria Ponti, Siva Reddy, Nigel Collier, and\nDesmond Elliott. Visually grounded reasoning across languages and cultures. In Proceed-\nings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages\n10467\u201310485, Online and Punta Cana, Dominican Republic, November 2021. Association for\nComputational Linguistics.\n[78] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning\nlarge multi-modal model with robust instruction tuning. arXiv preprint arXiv:2306.14565 ,\n2023.\n[79] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv\npreprint arXiv:2304.08485 , 2023.\n[80] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the\nwild. In Proceedings of International Conference on Computer Vision (ICCV) , December\n2015.\n[81] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic\nvisiolinguistic representations for vision-and-language tasks. In NeurIPS , 2019.\n[82] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi.\nUnified-io: A unified model for vision, language, and multi-modal tasks. In The Eleventh\nInternational Conference on Learning Representations , 2022.\n[83] Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee. 12-in-1:\nMulti-task vision and language representation learning. In CVPR , 2020.\n[84] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Neural baby talk. In CVPR , 2018.\n161 [85] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind\nTafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought\nchains for science question answering. Advances in Neural Information Processing Systems ,\n35:2507\u20132521, 2022.\n[86] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-\nChun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large\nlanguage models. arXiv preprint arXiv:2304.09842 , 2023.\n[87] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter\nClark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured\nmathematical reasoning. arXiv preprint arXiv:2209.14610 , 2022.\n[88] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe,\nUri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative\nrefinement with self-feedback. arXiv preprint arXiv:2303.17651 , 2023.\n[89] Bertram F Malle, Louis J Moses, and Dare A Baldwin. Intentions and intentionality: Founda-\ntions of social cognition . MIT press, 2001.\n[90] Arjun Mani, Nobline Yoo, Will Hinthorn, and Olga Russakovsky. Point and ask: Incorporating\npointing into visual question answering. arXiv preprint arXiv:2011.13681 , 2020.\n[91] John D Mayer. Msceit: Mayer-salovey-caruso emotional intelligence test. Toronto, Canada:\nMulti-Health Systems , 2002.\n[92] John D Mayer, Richard D Roberts, and Sigal G Barsade. Human abilities: Emotional intelli-\ngence. Annu. Rev. Psychol. , 59:507\u2013536, 2008.\n[93] Gr\u00e9goire Mialon, Roberto Dess\u00ec, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru,\nRoberta Raileanu, Baptiste Rozi\u00e8re, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, et al.\nAugmented language models: a survey. arXiv preprint arXiv:2302.07842 , 2023.\n[94] Microsoft. Bing image search api. https://www.microsoft.com/en-us/bing/\napis/bing-image-search-api , 2023.\n[95] Weiqing Min, Zhiling Wang, Yuxin Liu, Mengjiang Luo, Liping Kang, Xiaoming Wei, Xiaolin\nWei, and Shuqiang Jiang. Large scale visual food recognition. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence , 2023.\n[96] Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task general-\nization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages\n3470\u20133487, 2022.\n[97] Tara Murfitt and Jan McAllister. The effect of production variables in monolog and dialog on\ncomprehension by novel listeners. Language and Speech , 44(3):325\u2013350, 2001.\n[98] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christo-\npher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted\nquestion-answering with human feedback. arXiv preprint arXiv:2112.09332 , 2021.\n[99] OpenAI. Gpt-4 technical report, 2023.\n[100] OpenAI. Gpt-4v(ision) system card. 2023.\n[101] OpenAI. Gpt-4v(ision) technical work and authors. 2023.\n[102] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models\nto follow instructions with human feedback. Advances in Neural Information Processing\nSystems , 35:27730\u201327744, 2022.\n162 [103] Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer,\nand Marco Tulio Ribeiro. Art: Automatic multi-step reasoning and tool-use for large language\nmodels. arXiv preprint arXiv:2303.09014 , 2023.\n[104] Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang,\nLars Liden, Zhou Yu, Weizhu Chen, et al. Check your facts and try again: Improving\nlarge language models with external knowledge and automated feedback. arXiv preprint\narXiv:2302.12813 , 2023.\n[105] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu\nWei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint\narXiv:2306.14824 , 2023.\n[106] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller,\nJoe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution\nimage synthesis. arXiv preprint arXiv:2307.01952 , 2023.\n[107] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng,\nYufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng\nQian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining\nYe, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi\nLu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang,\nCheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. Tool learning with\nfoundation models, 2023.\n[108] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. The Journal of Machine Learning Research , 21(1):5485\u20135551, 2020.\n[109] John C Raven and JH Court. Raven\u2019s progressive matrices . Western Psychological Services\nLos Angeles, 1938.\n[110] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.\nHigh-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , pages 10684\u201310695, 2022.\n[111] Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai,\nAntoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, et al. Multitask prompted training en-\nables zero-shot task generalization. In International Conference on Learning Representations ,\n2021.\n[112] Timo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Luke Zettle-\nmoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach\nthemselves to use tools. arXiv preprint arXiv:2302.04761 , 2023.\n[113] Zhenwei Shao, Zhou Yu, Meng Wang, and Jun Yu. Prompting large language models with\nanswer heuristics for knowledge-based visual question answering. In CVPR , pages 14974\u2013\n14983, 2023.\n[114] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang.\nHugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint\narXiv:2303.17580 , 2023.\n[115] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke\nZettlemoyer, and Wen-tau Yih. Replug: Retrieval-augmented black-box language models.\narXiv preprint arXiv:2301.12652 , 2023.\n[116] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with\ndynamic memory and self-reflection. arXiv preprint arXiv:2303.11366 , 2023.\n[117] Aleksandar Shtedritski, Christian Rupprecht, and Andrea Vedaldi. What does clip know about\na red circle? visual prompt engineering for vlms. arXiv preprint arXiv:2304.06712 , 2023.\n163 [118] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset\nfor image captioning with reading comprehension. In ECCV , pages 742\u2013758, 2020.\n[119] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi\nParikh, and Marcus Rohrbach. Towards vqa models that can read. In CVPR , 2019.\n[120] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert:\nPre-training of generic visual-linguistic representations. In ICLR , 2019.\n[121] D\u00eddac Sur\u00eds, Sachit Menon, and Carl V ondrick. Vipergpt: Visual inference via python execution\nfor reasoning. arXiv preprint arXiv:2303.08128 , 2023.\n[122] Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from\ntransformers. In EMNLP , 2019.\n[123] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.\n[124] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving\nretrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. arXiv\npreprint arXiv:2212.10509 , 2022.\n[125] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill.\nMultimodal few-shot learning with frozen language models. Advances in Neural Information\nProcessing Systems , 34:200\u2013212, 2021.\n[126] Carven V on Bearnensquash. Paper gestalt. Secret Proceedings of Computer Vision and Pattern\nRecognition (CVPR) , 2010.\n[127] Hong Wang, Xuan Luo, Weizhi Wang, and Xifeng Yan. Bot or human? detecting chatgpt\nimposters with a single question. arXiv preprint arXiv:2305.06424 , 2023.\n[128] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu,\nCe Liu, and Lijuan Wang. Git: A generative image-to-text transformer for vision and language.\nTransactions on Machine Learning Research , 2022.\n[129] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou,\nJingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a\nsimple sequence-to-sequence learning framework. In International Conference on Machine\nLearning , pages 23318\u201323340. PMLR, 2022.\n[130] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in\nlanguage models. In The Eleventh International Conference on Learning Representations ,\n2022.\n[131] Zhenhailong Wang, Manling Li, Ruochen Xu, Luowei Zhou, Jie Lei, Xudong Lin, Shuohang\nWang, Ziyi Yang, Chenguang Zhu, Derek Hoiem, et al. Language models with image\ndescriptors are strong few-shot video-language learners. In Advances in Neural Information\nProcessing Systems .\n[132] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm:\nSimple visual language model pretraining with weak supervision. In ICLR , 2022.\n[133] David Wechsler. Wais-r: Manual: Wechsler adult intelligence scale-revised. (No Title) , 1981.\n[134] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan\nDu, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In\nICLR , 2022.\n[135] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani\nYogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large\nlanguage models. arXiv preprint arXiv:2206.07682 , 2022.\n164 [136] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,\nDenny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.\nAdvances in Neural Information Processing Systems , 35:24824\u201324837, 2022.\n[137] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan.\nVisual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint\narXiv:2303.04671 , 2023.\n[138] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and\nLijuan Wang. Grit: A generative region-to-text transformer for object understanding. arXiv\npreprint arXiv:2212.00280 , 2022.\n[139] Zhenjia Xu, Zhijian Liu, Chen Sun, Kevin Murphy, William T Freeman, Joshua B Tenenbaum,\nand Jiajun Wu. Unsupervised discovery of parts, structure, and dynamics. In International\nConference on Learning Representations , 2018.\n[140] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu, Yumao\nLu, and Lijuan Wang. Unitab: Unifying text and box outputs for grounded vision-language\nmodeling. In European Conference on Computer Vision , pages 521\u2013539. Springer, 2022.\n[141] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan\nWang. An empirical study of gpt-3 for few-shot knowledge-based vqa. In Proceedings of the\nAAAI Conference on Artificial Intelligence , volume 36, pages 3081\u20133089, 2022.\n[142] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed,\nZicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for\nmultimodal reasoning and action. arXiv preprint arXiv:2303.11381 , 2023.\n[143] Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin Lin, Chenfei Wu, Nan Duan,\nZicheng Liu, Ce Liu, Michael Zeng, et al. Reco: Region-controlled text-to-image generation.\nInProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,\npages 14246\u201314255, 2023.\n[144] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik\nNarasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv\npreprint arXiv:2305.10601 , 2023.\n[145] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and\nYuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh\nInternational Conference on Learning Representations , 2022.\n[146] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang,\nAnwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large\nlanguage models with multimodality. arXiv preprint arXiv:2304.14178 , 2023.\n[147] Da Yin, Liunian Harold Li, Ziniu Hu, Nanyun Peng, and Kai-Wei Chang. Broaden the vision:\nGeo-diverse visual commonsense reasoning. arXiv preprint arXiv:2109.06860 , 2021.\n[148] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition:\nVisual commonsense reasoning. In CVPR , pages 6720\u20136731, 2019.\n[149] Andy Zeng, Maria Attarian, Krzysztof Marcin Choromanski, Adrian Wong, Stefan Welker,\nFederico Tombari, Aveek Purohit, Michael S Ryoo, Vikas Sindhwani, Johnny Lee, et al.\nSocratic models: Composing zero-shot multimodal reasoning with language. In The Eleventh\nInternational Conference on Learning Representations , 2022.\n[150] Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu. Raven: A dataset for\nrelational and analogical visual reasoning. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition , pages 5317\u20135327, 2019.\n[151] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Mul-\ntimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923 ,\n2023.\n165 [152] Yan-Tao Zheng, Ming Zhao, Yang Song, Hartwig Adam, Ulrich Buddemeier, Alessandro\nBissacco, Fernando Brucher, Tat-Seng Chua, and Hartmut Neven. Tour the world: building a\nweb-scale landmark recognition engine. In 2009 IEEE Conference on Computer Vision and\nPattern Recognition , pages 1085\u20131092. IEEE, 2009.\n[153] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning\ndeep features for discriminative localization. In CVPR , 2016.\n[154] Bolei Zhou, Agata Lapedriza, Antonio Torralba, and Aude Oliva. Places: An image database\nfor deep scene understanding. Journal of Vision , 17(10):296\u2013296, 2017.\n[155] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J Corso, and Jianfeng Gao.\nUnified vision-language pre-training for image captioning and vqa. In AAAI , 2020.\n[156] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan,\nand Jimmy Ba. Large language models are human-level prompt engineers. In The Eleventh\nInternational Conference on Learning Representations , 2022.\n[157] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4:\nEnhancing vision-language understanding with advanced large language models. arXiv\npreprint arXiv:2304.10592 , 2023.\n[158] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat\nBehl, Jianfeng Wang, Lu Yuan, et al. Generalized decoding for pixel, image, and language. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n15116\u201315127, 2023.\n[159] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Jianfeng\nWang, Lu Yuan, Nanyun Peng, Lijuan Wang, Yong Jae Lee, and Jianfeng Gao. Generalized\ndecoding for pixel, image and language. 2022.\n[160] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, and Yong Jae Lee.\nSegment everything everywhere all at once. arXiv preprint arXiv:2304.06718 , 2023.\n166 "}