{"url": "http://arxiv.org/pdf/2310.16534v1", "title": "An Early Evaluation of GPT-4V(ision)", "text": "An Early Evaluation of GPT-4V(ision)\nYang Wu Shilong Wang Hao Yang Tian Zheng\nHongbo Zhang Yanyan Zhao\u2217Bing Qin\nResearch Center for Social Computing and Information Retrieval\nHarbin Institute of Technology\n{ywu, shilongwang, hyang, tzheng, hbzhang, yyzhao, qinb}@ir.hit.edu.cn\nAbstract\nIn this paper, we evaluate different abilities of\nGPT-4V including visual understanding, lan-\nguage understanding, visual puzzle solving,\nand understanding of other modalities such as\ndepth, thermal, video, and audio. To estimate\nGPT-4V\u2019s performance, we manually construct\n656 test instances and carefully evaluate the\nresults of GPT-4V . The highlights of our find-\nings are as follows: (1) GPT-4V exhibits im-\npressive performance on English visual-centric\nbenchmarks but fails to recognize simple Chi-\nnese texts in the images; (2) GPT-4V shows\ninconsistent refusal behavior when answering\nquestions related to sensitive traits such as gen-\nder, race, and age; (3) GPT-4V obtains worse\nresults than GPT-4 (API) on language under-\nstanding tasks including general language un-\nderstanding benchmarks and visual common-\nsense knowledge evaluation benchmarks; (4)\nFew-shot prompting can improve GPT-4V\u2019s\nperformance on both visual understanding and\nlanguage understanding; (5) GPT-4V struggles\nto find the nuances between two similar images\nand solve the easy math picture puzzles; (6)\nGPT-4V shows non-trivial performance on the\ntasks of similar modalities to image, such as\nvideo and thermal. Our experimental results re-\nveal the ability and limitations of GPT-4V and\nwe hope our paper can provide some insights\ninto the application and research of GPT-4V1.\n1 Introduction\nGPT-4V has shown remarkable capabilities on a\nwide of tasks (Yang et al., 2023). However, the per-\nformance of GPT-4V has not been quantitatively\nstudied. In this paper, we manually construct 656\ntest examples to quantitatively assess GPT-4V\u2019s\nabilities and seek answers to the following intrigu-\ning questions.\n\u2217Corresponding Author\n1Our data are available at https://github.com/albertwy/GPT-\n4V-Evaluation1.What is the performance of GPT-4V on visual-\ncentric benchmarks such as image captioning\nand visual question answering? Can GPT-4V\nsurpass the current SOTA multimodal LLMs\nsuch as Qwen-VL-Chat (Bai et al., 2023) on\nthese benchmarks? ( Visual Understanding )\n2.After being equipped with visual percep-\ntion, can GPT-4V maintain its language un-\nderstanding performance and better capture\nvisual commonsense knowledge and world\nknowledge (specifically physics knowledge)?\n(Language Understanding )\n3.Can GPT-4V benefit from exemplars? ( Visual\nUnderstanding ,Language Understanding )\n4.How to evaluate multimodal LLMs given\nthe observation that multimodal LLMs have\nachieved really high performance on the cur-\nrent benchmarks? ( Visual Puzzle Solving )\n5.Can GPT-4V perceive other modalities\nsuch as depth, thermal, video, and audio?\n(Understanding of Other Modalities )\nWe conduct extensive evaluation of GPT-4V and\nthe results not only reveal GPT-4V\u2019s strong abil-\nities, but also point to the following issues that\nshould be addressed in future studies.\n1.GPT-4V tends to generate verbose responses,\neven when provided with exemplars that have\nshort answers, which makes it hard to ac-\ncurately assess GPT-4V\u2019s performance us-\ning current automatic metrics. For example,\nthe CIDEr scores on Nocaps (Agrawal et al.,\n2019) and Flickr30K (Young et al., 2014)2\nobtained by GPT-4V are close to 0.\n2.GPT-4V shows inconsistent refusal behavior\nwhen answering questions related to sensitive\n2We utilize the released code by Bai et al. (2023) to esti-\nmate the results.arXiv:2310.16534v1  [cs.CL]  25 Oct 2023 traits such as gender, race, and age. This is-\nsue causes an obvious performance drop on\nGQA. Future research should address this is-\nsue carefully when comparing GPT-4V with\nother multimodal LLMs.\n3.GPT-4V performs very well with English text\nrecognition, yet it cannot recognize Chinese\ntexts in images.\n4.GPT-4V struggles to solve the easy math pic-\nture puzzle (grades five level) although it ex-\nhibits strong performance on much harder tex-\ntual math datasets such as SAT math (OpenAI,\n2023).\n5.The current version of GPT-4V does not sup-\nport interleaved images and texts and can only\naccept a maximum of four images. These con-\nstraints limit the design space of prompts.\nFigure 1: An example image from GQA.\n2 Visual Understanding\nWe evaluate GPT-4V on various visual-centric\nbenchmarks such as image captioning and visual\nquestion answering to assess its visual understand-\ning ability. Following Qwen-VL-Chat (Bai et al.,\n2023), we choose Nocaps (Agrawal et al., 2019)\nand Flickr30K (Young et al., 2014) as the eval-\nuation datasets for image captioning. As for vi-\nsual question answering, we evaluate GPT-4V on\nVQAv2 (Goyal et al., 2016), OKVQA (Marino\net al., 2019), GQA (Hudson and Manning, 2019),\nScienceQA (Lu et al., 2022), and Vizwiz VQA (Gu-\nrari et al., 2018).\nMetric GPT-4V always tends to generate ver-\nbose responses, which makes it hard to accurately\nevaluate GPT-4V\u2019s performance using current au-\ntomatic metrics. For example, given the image\nshown in Figure 1, we ask GPT-4V to find out\nwhich kind of watercraft is underneath the airplane\nand GPT-4V answers correctly with \u201cthe watercraftTable 1: Human evaluation for GPT-4V and Qwen-VL-\nChat (Zero-shot).\nTask Dataset GPT-4V Qwen-VL-Chat\nImage\nCaptioningNocaps 17/20 15/20\nFlickr30K 19/20 17/20\nVisual\nQuestion\nAnsweringVQAv2 16/20 15/20\nOKVQA 18/20 16/20\nGQA 11/20 15/20\nScienceQA 17/20 14/20\nVizWiz 17/20 14/20\nOCR-VQA 20/20 19/20\nTable 2: Automatic evaluation for GPT-4V and Qwen-\nVL-Chat (Zero-shot). We do not carefully adjust the\nprompts and we acknowledge that using task-specific\nprompts to control the output formats could be helpful.\nTask Dataset Metric GPT-4V Qwen-VL-Chat\nImage\nCaptioningNocaps SPICE 15.9 16.2\nFlickr30K SPICE 15.2 17.3\nVisual\nQuestion\nAnsweringVQAv2 VQA Score 0.0 85.0\nOKVQA VQA Score 5.0 38.0\nGQA EM Accuracy 10.0 40.0\nScienceQA Accuracy 85.0 70.0\nVizWiz VQA Score 23.0 36.0\nOCR-VQA EM Accuracy 0.0 70.0\nunderneath the airplane are sailboats\u201d. However,\nif we utilize EM Accuracy as the metric, which is\nadopted by Qwen-VL-Chat for GQA, the answer of\nGPT-4V will be considered as incorrect given the\nground truth is \u201csailboat\u201d. To address this problem,\nwe manually evaluate the results of GPT-4V and\nQwen-VL-Chat. The automatic evaluation results\nare also reported to reveal the limitation of current\nautomatic metrics. Besides, we utilize SPICE (An-\nderson et al., 2016) instead of CIDEr (Vedantam\net al., 2015) as the metric for image captioning,\nbecause we find that the current implementation\nof CIDEr adopted by Qwen-VL-Chat gives a large\npenalty to the difference between candidate and\nreference sentence lengths. In our experiments, the\nCIDEr scores obtained by GPT-4V are close to 0\non Nocaps and Flickr30K.\nZero-shot Results. We randomly sample 20 test\ninstances for each dataset and manually evaluate\nGPT-4V\u2019s performance. We list the human and\nautomatic evaluation results in Table 1 and Table 2.\nThere are mainly two findings. (1) GPT-4V per-\nforms really well on various tasks. GPT-4V can\ngenerate detailed and coherent descriptions for the\ngiven images and answer the questions based on\nthe images. GPT-4V is able to accurately recognize\nthe English characters in the images, achieving\n100% accuracy on OCR-VQA. We also observe Image Captioning\nPrompt:\nDescribe this imageGPT-4V:\nTheimagedepicts abrideinherwedding\nattire.Sheiswearing awhitewedding gown\nwithintricate beading anddetailing onthe\nbodice.Thebrideisalsoadorned withaveil\nthatcascades downherback,andsheholdsa\nbouquet ofwhiterosesinonehand.Herhair\nisstyledinanupdowithaslightbouffant\natthecrown.Besideher,partially visible,\nareafewpeople, including anelderly woman\ninapinkdressandamaninasuit.Inthe\nbackground, there's astone walland\nblossoming trees,suggesting thephotograph\nmighthavebeentakeninagarden ora\ncourtyard setting.\nQwen -VL-Chat:\nA bride holding a bouquet of flowers in her \nhand.\nGround Truth:\nA bride in her wedding gown holding a bouquet.Figure 2: The caption generated by GPT-4V is more detailed and specific than the ground truth. GPT-4V can capture\nthe details in the whole image (marked with red color ) and also attempts to infer the location where the photo was\ntaken (marked with blue color ).\nthat GPT-4V exhibits superior performance than\nQwen-VL-Chat on all datasets except on GQA. It is\nbecause GPT-4V refuses to answer some questions\nof GQA. We will analyze this later. (2) Current\nautomatic metrics are not suitable for evaluat-\ning the responses generated by GPT-4V . The au-\ntomatic evaluation results show that GPT-4V per-\nforms worse than Qwen-VL-Chat. However, when\nwe manually evaluate the outputs, we find that GPT-\n4V\u2019s performance is better than Qwen-VL-Chat.\nWe have elaborated on the reasons and show an\nexample of visual question answering in the Met-\nricsubsection. We further present an example of\nimage captioning in Figure 2. As shown in this\nfigure, the generated descriptions by GPT-4V are\nmore detailed and specific than the ground truths,\nwhich makes the generated captions less similar to\nthe ground truths leading to lower metric scores.\nFew-shot Results. In-context learning (ICL) as\na training-free learning framework enables us to\ninject human knowledge into models by design-\ning the demonstration and templates (Dong et al.,\n2022). Given the observation that GPT-4V can dis-\ncern the order of uploaded images, we devise theTable 3: Human evaluation for GPT-4V (Zero-shot and\nFew-shot).\nTask Dataset Zero-shot Few-shot\nImage Captioning Flickr30K 19/20 19/20\nVisual Question\nAnsweringVQAv2 16/20 17/20\nGQA 11/20 16/20\nfew-shot prompts to guide GPT-4V to complete the\ntasks. Considering the maximum number of the\nuploaded images to GPT-4V is 4, we prompt GPT-\n4V with 3-shot exemplars. We show an example in\nFigure 3. The human evaluation results of few-shot\nprompting are presented in Table 3. GPT-4V with\nfew-shot prompting achieves better performance\non the VQA tasks, which indicates GPT-4V has\nin-context learning ability. Although the exemplars\nof the image caption task do not help GPT-4V ob-\ntain better human evaluation results, they make\nGPT-4V generate shorter responses leading to an\nimprovement in SPICE from 15.2 to 17.5.\nGPT-4V\u2019s inconsistent refusal behavior. GPT-\n4V is instructed to refuse requests for identity, sen-\nsitive traits (e.g. age, race), and ungrounded infer- Visual Question Answering (Few -shot)\nPrompt:\nImage: The first image\nQuestion: Which type of clothing in this scene is gray?\nAnswer: Coat\nImage: The second image\nQuestion: Which kind of animal is it?\nAnswer: Dog\nImage: The third image\nQuestion: Do you see any people in the living room?\nAnswer: Yes\nImage: The fourth image\nQuestion: Is the tape green?\nAnswer:\nGPT-4V(Few -shot):\nYes,thetapeisgreen.Figure 3: Few-shot prompting for VQA. We specify the corresponding image to each question in the prompt.\nences. GPT-4V refuses to answer some questions\nof GQA resulting in low performance on GQA.\nSpecifically, we evaluate 20 test instances sampled\nfrom GQA and ask GPT-4V to answer the questions\nwith the zero-shot prompt. 4 out of 20 requests are\nrefused by GPT-4V . These four questions are as\nfollows.\n1.Is the player next to the other player female\nor male?\n2.The woman to the right of the camera is watch-\ning who?\n3. Who wears the shorts?\n4. What do you think is the old lady wearing?\nThe first one is asking for sensitive traits. The sec-\nond and third questions, to some extent, are asking\nfor information related to identity. The fourth one\nmay be seen as impolite, particularly due to the\ndescriptor \"old lady\".\nTo study GPT-4V\u2019s refusal behavior, we select\nsome words related to the sensitive traits such as\ngender, race, and age. We filter out the questions\ncontaining these words from the test dataset ofGQA and obtain 20, 11, and 25 test instances re-\nlated to gender, race, and age respectively. We con-\nduct experiments using these instances. And we\nobserve that 10 out of 20 gender-related requests\nare refused. 8 out of 10 refused questions are di-\nrectly asking for gender. Here is an example. \u201cIs\nthe young person female or male?\u201d. It is in line\nwith our expectation as GPT-4V is trained to refuse\nsuch requests.\nBut the following examples make us confused,\nwhich reveal the inconsistent refusal behavior of\nGPT-4V .\n1.Refused Request: What is the person that is\nnot male standing on?\n2.Refused Request: Which side of the photo is\nthe male person on?\n3.Approved Request: On which side of the\nphoto is the female pedestrian?\n4.Approved Request: Does the male person\nseem to be sitting?\nIt seems hard to find out the reasons why does\nGPT-4V refuse the first and second questions while\napprove the third and fourth ones. Chinese Text Recognition \uff081\uff09\nPrompt:\n\u56fe\u4e2d\u662f\u4ec0\u4e48\u6c49\u5b57\uff1f\nGPT-4V:\n\u56fe\u4e2d\u7684\u6c49\u5b57\u662f \u201c\u611b\u201d\uff0c\u8fd9\u662f\u7e41\u4f53\u4e2d\u6587\u7684 \u201c\u7231\u201d\uff0c\u610f\n\u601d\u662f\u201clove\u201d\u3002Translation \uff1aWhat is the Chinese \nword in the picture?\nTranslation \uff1aThe Chinese character in the picture is \u201c \u611b\u201d, \nwhich is the traditional Chinese character for \u201c \u7231\u201d, \nmeaning \u201clove\u201dFigure 4: GPT-4V fails to recognize the Chinese word in the given image.\nChinese Text Recognition \uff082\uff09\nPrompt:\n\u8bf7\u590d\u8ff0\u9898\u5e72\u4e0e\u9009\u9879\nGPT-4V:\n\u5f53\u7136\u53ef\u4ee5 \u3002\u9898\u5e72\u4e0e\u9009\u9879\u5982\u4e0b\uff1a\u9898\u5e72\uff1a 3.\u5f53\u6d89\u53ca\u5230\u7269\u4f53\u7684\u957f\u5ea6\u6216\n\u8005\u5927\u5c0f\u65f6 \uff0c\u4f1a\u53d1\u751f\u7684\u5355\u4f4d\u8f6c\u6362\u662f\u9009\u9879\uff1a A.\u4ecd\u65e7\u6d89\u53ca\u957f\u5ea6\u5355\u4f4d B.\n\u628a\u5229\u6da6\u6362\u7b97\u4e3a\u8d27\u5e01\u5355\u4f4d C.\u4e0b\u5348\u6d89\u53ca\u957f\u5ea6\u5355\u4f4d\u65f6\u9700\u8981\u8003\u8651\u8d27\u5e01\n\u5355\u4f4dD.\u518d\u5c0f\u91cf\u5355\u4f4d\u6362\u7b97\u4e3a\u66f4\u5927\u7684\u91cd\u91cf\u5355\u4f4dTranslation \uff1aPlease extract the \nquestion stem and options.\nTotally Wrong\nFigure 5: GPT-4V fails to extract the question and choices from the given image.\nAs for the questions related to race, 4 out of 11\nrequests are refused.\n1.Refused Request: Is the white person next to\nthe windows wearing shorts?\n2.Approved Request: Does the white person\nnear the plants seem to be standing?\nAs for the questions related to age, 4 out of 25\nrequests are refused.\n1.Refused Request: Does the old man appear to\nbe waiting?2.Approved Request: Are there any old women\nor men?\nThe inconsistent refusal behavior is also ob-\nserved in the early version of GPT-4. They find\nthat GPT-4 tends to become overly cautious in cer-\ntain ways such as refusing innocuous requests. We\nconsider that this issue is vital for future research\nand should be systematically studied.\nGPT-4V fails to recognize the Chinese text in\nimages. Impressed by the strong English OCR\nperformance of GPT-4V , we wonder whether GPT-\n4V can recognize the Chinese text in images. We Table 4: Results on MMLU, HellaSwag, and Wino-\nGrande (Zero-shot).\nDataset GPT-4V GPT-4 API\nMMLU 16/20 17/20\nHellaSwag 14/20 18/20\nWinoGrande 15/20 19/20\nTable 5: Results on MMLU, HellaSwag, and Wino-\nGrande (Few-shot).\nDataset GPT-4V GPT-4 API\nMMLU (5-shot) 17/20 18/20\nHellaSwag (5-shot) 16/20 16/20\nWinoGrande (5-shot) 15/20 17/20\ndevise the following two tasks: (1) Given an image\nwith only one Chinese word, identify this word; (2)\nGiven an image, extract the question and choices\nfrom it. The first task is much easier than the sec-\nond one. However, GPT-4V fails to complete ei-\nther the first task or the second task. Specifically,\nwe create 10 instances for each task and show the\nexamples in Figure 4 and Figure 5. We evaluate\nGPT-4V on these instances, and it achieves 0% ac-\ncuracy on both tasks, revealing that GPT-4V could\nnot recognize the Chinese text in images.\n3 Language Understanding\nWe evaluate GPT-4V on a wide range of bench-\nmarks to answer two intriguing questions. After\nbeing equipped with visual perception, can GPT-\n4V (1) maintain its language understanding perfor-\nmance and (2) better capture visual commonsense\nknowledge, world knowledge (specifically physics\nknowledge)?\nAs for the first question, we conduct the\nexperiments on MMLU (challenging subjects:\nabstract_algebra, anatomy, astronomy, bisi-\nness_ethics), HellaSwag, and WinoGrande to\nevaluate the language understanding ability of\nGPT-4V . Specifically, 20 test instances are sampled\nfor each dataset. Considering that OpenAI may\nutilize different models to process text-only inputs\nand text-image inputs, we upload a white image\nalong with the text input. We acknowledge that\nit is possible that GPT-4V could be affected by\nthe input white image if GPT-4V is not robust\nenough. We manually obtain and evaluate the\nresults. The results of GPT-4V and GPT-4 (API)\nare shown in Table 4 and Table 5. We observe that\nGPT-4V obtains worse results than GPT-4 (API).Table 6: Results on ViComTe (Zero-shot).\nType GPT-4V GPT-4 API\nColor 10/10 10/10\nShape 9/10 10/10\nMaterial 10/10 10/10\nSize 10/10 10/10\nVisual co-occurrence 10/10 10/10\nTable 7: Results on UTOPIA (Zero-shot).\nScene GPT-4V GPT-4 API\nCollision 6/10 9/10\nFree fall 3/10 2/10\nFriction 10/10 10/10\nIncline 10/10 10/10\nMotion 10/10 10/10\nProjection 7/10 7/10\nBut the few-shot results indicate that GPT-4V\u2019s\nperformance could be further boosted by more\nadvanced prompting methods.\nLet us turn to the second question. We choose\nViComTe (Zhang et al., 2022) as our benchmark\nto find out whether GPT-4V can capture a broad\nrange of visually salient attributes. ViComTe cov-\ners 5 property types (color, shape, material, size,\nand visual co-occurrence) and we sample 10 test\ninstances for each property type to construct our\nevaluation dataset. We also upload a white im-\nage along with the test question. The results are\nlisted in Table 6. The results show that both GPT-\n4V and GPT-4 (API) perform well on this task.\nTo evaluate GPT-4V\u2019s ability to capture physics\nknowledge, we utilize UTOPIA (Liu et al., 2023a)\nas our benchmark. This task requires the models\nto understand and reason over some basic laws of\nphysics. UTOPIA covers six common scenes in-\ncluding collision, free fall, friction, incline, motion,\nand projection. We sample 10 test instances for\neach type and evaluate GPT-4V on them. We also\nupload a white image along with the input question.\nThe results are presented in Table 7. GPT-4V does\nnot show superiority over GPT-4 (API).\n4 Visual Puzzle Solving\nGPT-4V has achieved very high performance on\nthe standard visual-centric benchmarks, which in-\ndicates these benchmarks could be insufficient to\nmeasure its capabilities and limitations. To address\nthis problem, in contrast to constructing more com- Spot the Differences (1)\nPrompt:\nSpot ten differences between them\nGPT-4V:\nHerearethetendifferences between thetwoimages:\n1.Thefirstimagehastwobirdsflyinginthesky,whilethesecondimagehasonlyonebird.\n2.Inthefirstimage,thebirdontheleftisdark-colored, whereas inthesecondimage,the\nbirdislighter incolor.\n3.Theshadowofthetreeontherightsideofthefirstimageislongerthaninthesecond\nimage.\n4.Theelephant's tailinthefirstimageispointing downwards, whileinthesecondimage,it's\npointing upwards.\n5.There's asmallplantneartheelephant's footinthesecondimage,whichisnotpresent in\nthefirstimage.\n6.Theshapeandcurveofthetreeontherightinthesecondimageareslightly different from\nthefirstimage.\n7.Inthefirstimage,thehorizon hasamoredefined mountain rangecompared tothesecond\nimage.\n8.Thecloudformations inthetwoimagesaredifferent .\n9.Thetreetrunkontheleftsideofthefirstimageisstraight, whereas inthesecondimage,\nithasacurve.\n10.Thetreebranches ontheleftinthefirstimageareslightly morespreadoutcompared tothe\nsecondimage.\nFigure 6: GPT-4V finds three differences (marked with red color ). GPT-4V hallucinates that the elephant\u2019s tail in\nthe second image is pointing upwards.\nFigure 7: Solution to the level-1 puzzle.\nprehensive evaluation benchmarks like MME (Fu\net al., 2023) and MMBench (Liu et al., 2023b),\nwe ask GPT-4V to solve interesting but challeng-\ning puzzles including Spot the Difference, Draw &\nGuess, and Math Picture Puzzle.\nSpot the Difference. Given two similar images,\nthe goal is to find 10 differences between them.\nThis challenging task requires the model to capture\nthe details of the input images and find out the\ndifferences by comparing two similar images. Wemanually obtain 10 different level puzzles (from\n1 to 10) from CrazyGames3. To evaluate GPT-\n4V\u2019s performance on these puzzles, we devise two\ntypes of prompts. As for the first one, we simply\nposition two images within a single image: one\non the left and the other on the right.4Then we\nupload this obtained image to GPT-4V and ask\nit to spot ten differences. We show the level-1\n3https://www.crazygames.com/game/find-the-difference\n4We also tried to upload two images separately but did not\nfind any significant difference. Spot the Differences (2)\nPrompt:\nSpot ten differences between them and provide the corresponding block numbers\nGPT-4V:\nSure,herearetendifferences between thetwoimages, alongwiththeircorresponding block\nnumbers:\n1.A1:Intheleftimage,therearetwobirds.Intherightimage,there's onlyonebird.\n2.A3:Intheleftimage,thetopbranches ofthetreehavetwoclusters ofleaves.Intheright\nimage,therearethreeclusters ofleaves.\n3.A4:Intheleftimage,there's onebird.Intherightimage,there's nobird.\n4.B1:Intheleftimage,therearethreeclouds.Intherightimage,therearetwoclouds.\n5.C1:Intheleftimage,there's asmalltreeinthebackground .Intherightimage,there's no\nsmalltree.\n6.C4:Intheleftimage,theelephant's tailispointing downward .Intherightimage,thetail\nispointing up.\n7.D2:Intheleftimage,there's asmallgreenplantnearthebottomedge.Intherightimage,\ntheplantismissing.\n8.D3:Intheleftimage,theshadowoftheelephant's frontlegislonger.Intherightimage,\ntheshadowisshorter.\n9.D4:Intheleftimage,there's nosmallplantneartheelephant's backleg.Intherightimage,\nthere's asmallgreenplant.\n10.B2andB3:Intheleftimage,thetopofthetreewhereitsplitshasamorepronounced gap\nbetween thebranches .Intherightimage,thegapisnarrower .\nFigure 8: GPT-4V should not only spot the differences, but also provide the corresponding block numbers. Only\none difference is found (marked with red color ).\nDraw & Guess (1)\nPrompt :\nLet's play Draw & Guess. You are given a drawing. Please guess a \nword (Hints: Profession) based on the drawing. \nGPT-4V:\nMagician!\nFigure 9: GPT-4V guesses the word \u201cmagician\u201d correctly.\npuzzle to GPT-4V and the result is presented in\nFigure 6. We also present the solution to this puzzle\nin Figure 7 for reference. As shown in Figure 6,\nGPT-4V finds three differences but the answersare rough. Therefore, we design another prompt.\nWe label different areas of the picture with block\nnumbers and ask GPT-4V to spot the differences\nand provide the corresponding block numbers. In Draw & Guess (2)\nPrompt :\nLet's play Draw & Guess. You are given a drawing. Please guess a \nword (Hints: Activity) based on the drawing. \nGPT-4V:\nBasedonthedrawing,\nmyguessis\"golf.\"\nFigure 10: GPT-4V guesses the word \u201cgolf\u201d correctly.\nDraw & Guess (3)\nPrompt :\nLet's play Draw & Guess. You are given a drawing. Please guess a \nword (Hints: Object, Machine) based on the drawing. \nGPT-4V:\nBasedonthedrawing,\nmyguessis\"Washing\nMachine\" .\nFigure 11: GPT-4V wrongly recognizes the claw machine as the washing machine.\nthis way, we can evaluate GPT-4V\u2019s results more\naccurately. We show an example in Figure 8. GPT-\n4V only correctly finds one difference. We test 10\npuzzles and manually check the correctness of the\nanswers. GPT-4V finds 14 differences in total with\nthe first prompting method and 8 differences with\nthe second one. The evaluation results show that\nGPT-4V struggles to capture the small differences\nbetween two similar images, which could limit its\napplication. For example, it may be unsuitable\nthat utilizing GPT-4V as a strict discriminator to\nevaluate the predictions of shadow removal models.Draw & Guess. Draw & Guess is a casual draw-\ning game. One player selects one word and draws\nit. The other players should guess the word based\non the drawing and some hints such as describing\nan object. We collect 10 drawings and the corre-\nsponding hints. We want to know whether GPT-4V\ncan understand the meaning of each drawing and\nfurther guess the corresponding word successfully.\nWe show two success examples in Figure 9 and\nFigure 10. GPT-4V can capture the visual concepts\nand guess the words successfully. But GPT-4V\nalso makes mistakes. The only one failure case is\npresented in Figure 11. GPT-4V fails to find the Math Picture Puzzle (1)\nPrompt:\nSlove this question\nGPT-4V:\nLet'ssolvethepuzzlestepbystep:\nFromthefirstequation :\n\ud83c\udf83+\ud83c\udf83+\ud83c\udf83+\ud83c\udf40=32\n3\ud83c\udf83 +\ud83c\udf40 =32\n=>3\ud83c\udf83 =32-\ud83c\udf40       ...(i)\nFromthesecondequation :\n\ud83c\udf83 +\ud83d\udc22 =17=>\ud83c\udf83 =17-\ud83d\udc22        ...(ii)\nFromthethirdequation :\n\ud83c\udf83 +\ud83c\udf40 =10\nUsingequation (ii),weget:\n17-\ud83d\udc22 +\ud83c\udf40 =10=>\ud83c\udf40 -\ud83d\udc22 =-7=>\ud83d\udc22 -\ud83c\udf40 =7 ...(iii)\nSubstituting equation (ii)inequation (i):\n3(17-\ud83d\udc22)+\ud83c\udf40 =32\n=>51-3\ud83d\udc22 +\ud83c\udf40 =32=>\ud83c\udf40 -3\ud83d\udc22 =-19=>3\ud83d\udc22 -\ud83c\udf40 =19 ...(iv)\nNow,subtracting equation (iii)fromequation (iv):\n3\ud83d\udc22 -\ud83d\udc22 =19\u20137=>2\ud83d\udc22 =12=>\ud83d\udc22 =6\nUsingequation (ii):\n\ud83c\udf83 =17\u20136=>\ud83c\udf83 =11\nUsingequation (iii):\n\ud83c\udf40 =6-7\n\ud83c\udf40 =-1(Thisdoesn't seemlogical sincewecan'thaveanegative valueforasymbolinsuch\npuzzles.)\nFromthefourthequation :\n\ud83d\udc22 +\ud83c\udf83 \u00d7\ud83c\udf40 =?\n6+11\u00d7(-1)=6-11=-5\nHowever, thevaluefor\ud83c\udf40 as-1doesn't makeintuitive senseforthistypeofpuzzle.There\nmightbeanerrorintheequations provided orthesymbols mightrepresent unconventional values.\nFigure 12: GPT-4V does not recognize the first math equation correctly and fails to solve the puzzle.\nclues such as the claw in the image and recognize\nit as the washing machine. Draw & Guess requires\nthe model to understand the visual concepts (rec-\nognizing the hat and the rabbit), recall the related\ncommonsense knowledge (magic), and conduct rea-\nsoning to guess the words (magician). It could be\npossible to construct a visual reasoning benchmark\nby collecting more challenging instances.\nMath Picture Puzzle. Previous work has shown\nthat GPT-4 is good at solving math problems. In-\nspired by it, we are curious about GPT-4V\u2019s per-\nformance on the math picture puzzles. Surpris-\ningly, the performance of GPT-4V is really low\neven though the math picture puzzles are much eas-\nier than the problems of the widely used datasets\nsuch as GSM-8K (Cobbe et al., 2021). We show\nan example in Figure 12. GPT-4V wrongly rec-\nognizes the first math equation leading to cascad-\ning errors and fails to solve the puzzle. We also\nask Bard to solve the same puzzle (shown in Fig-\nure 13) and Bard fails to recognize all equations.\nWe collect 10 math picture puzzles and test GPT-\n4V on them. GPT-4V only solves one problem\ncorrectly, which is out of our expectations consid-ering the strong visual understanding ability and\nmath problem-solving ability. We consider the rea-\nson could be that GPT-4V does not generalize well\nto this domain or GPT-4V utilizes some additional\nmodules such as the OCR module to help itself but\nthe additional modules do not work well in this\ndomain.\n5 Understanding of Other Modalities\nThe research question that we investigate in this sec-\ntion is whether can GPT-4V perceive other modali-\nties including depth, thermal, video, and audio. We\nmanually construct test instances for each modality\nand evaluate GPT-4V\u2019s performance.\nDepth. Based on the SUN-D dataset (Song et al.,\n2015), we create 20 test questions of asking for\nGPT-4V to select the scene label from the five\nchoices for the given depth image. An example\nis shown in Figure 14. In this case, GPT-4V rec-\nognizes the chair through the depth image and cor-\nrectly predicts the scene label as \u201cliving room\".\nWe prompt GPT-4V with 20 questions and 4 out\nof 20 questions are answered correctly, which re-\nveals that GPT-4V struggles to understand the given Math Picture Puzzle (2)\nPrompt:\nSlove this question\nBard:\nLet'ssolvethepuzzlestepbystep:\nTosolvethepuzzle, weneedtofindthemissing numbers intheequations .\nEquation 1:32+?=17\nEquation 2:?+3=10\nEquation 3:?+?=15\nWecanstartbysolving Equation 2.Weknowthat10-3=7,sothemissing numberinEquation 2\nis7.\nNowwecanusethisinformation tosolveEquation 1.Weknowthat17-7=10,sothemissing\nnumberinEquation 1is10.\nFinally, wecanusetheinformation fromEquations 1and2tosolveEquation 3.Weknowthat15-\n10-7=2,sothemissing numbers inEquation 3are10and2.\nTherefore, thecompleted puzzlelookslikethis:\n=+32+10=17+7+3=10+10+2=15\nFigure 13: Bard can not recognize the math equations correctly and fails to solve the puzzle.\ndepth image and further training could be necessary\nbefore the application.\nThermal. We sample 20 test instances from\nLLVIP (Jia et al., 2021) and ask GPT-4V to find\nthe number of pedestrians and their location in the\nthermal infrared image. We present an example in\nFigure 15. GPT-4V successfully detects all pedes-\ntrians in the image. We test GPT-4V on the sampled\n20 instances and 9 out of 20 instances are solved.\nAs for the remaining instances, GPT-4V also can\ndetect correctly several pedestrians from the given\nimages.\nVideo. It is hard to evaluate accurately and fairly\nGPT-4V\u2019s performance on video understanding\nconsidering the maximum number of the uploaded\nimages to GPT-4V is 4 and too much information\nof the input video is lost. Nevertheless, we are still\ncurious about the GPT-4V\u2019s performance on video\nunderstanding. We sample 20 test instances from\nthe MSR-VTT dataset (Xu et al., 2016). We select\nfour frames from each video and ask GPT-4V to\ngenerate the corresponding video description based\non the four frames. We present an example in Fig-\nure 16. We find that GPT-4V tends to describethe images separately and struggles to generate the\ncaption for the whole video. We attribute it to the\nlow sampling rate of the video and increasing the\nnumber of the uploaded images could be helpful.\nWe conduct the experiment with 20 instances and\n6 out of 20 instances are described correctly.\nAudio. It is a common way to treat mel spectro-\ngrams as images and adopt pre-trained visual en-\ncoders to process the mel spectrograms (Wu et al.,\n2022). Hence, we attempt to evaluate GPT-4V\u2019s\nability to perceive audio signals by converting the\ninput audio waveforms into the mel spectrograms.\nBased on the ESC dataset (Piczak, 2015), we devise\ntwo tasks to assess GPT-4V\u2019s ability: (1) Given a\nmel spectrum diagram and four choices, the goal\nis to select an audio label from the four choices;\n(2) Given four mel spectrograms and four audio\nlabels, the goal is to match the four spectrograms\nto the four audio labels. We show one example for\neach task in Figure 17 and Figure 18. As shown in\nFigure 17, GPT-4V knows the specific patterns of\ncommon animal sounds. It could be easy to elimi-\nnate \u201cPig\u201d and \u201cCow\u201d by the audio duration, but it\nis harder to decide which one of the other choices\nis the correct answer. The ground truth is \u201cDog\u201d Scene Understanding\nPrompt:\nThis image is the depth image of scene. Please determine the category of this \nscene.\nA. Office  B. Kitchen  C. Bedroom  D. Bathroom  E. Living room\nGPT-4V:\nBasedonthedepthimageprovided, itischallenging todefinitively determine the\ncategory ofthescene.However, theimageappears toshowachair-likestructure\nwhichcouldbeassociated withmultiple roomcategories .\nIfIhadtoguessbasedontheprovided options, Iwouldsuggest:\nE.livingroom\nHowever, thisisaspeculative answerastheimagedoesnotprovide cleardetails\nabouttheentirescene.\nFigure 14: GPT-4V can recognize the chair in the depth image and predict the scene label correctly.\nPedestrian Detection\nPrompt:\nThis is an infrared image. Please describe the number of pedestrians and their location in the \nimage for me.\nGPT-4V:\nIntheinfrared image,Iseetwopedestrians .\n1.Onepedestrian isneartheleftsideoftheimage,walking onthestreetbesidea\ntree.\n2.Another pedestrian isontherightsideoftheimage, closer toafenceor\nbarrier andappears tobeonamarkedcrosswalk orpavement .\nTheimageisingrayscale andcaptures anight-timeorlow-lightscenario using\ninfrared technology .\nFigure 15: GPT-4V can detect the pedestrians in the given infrared image correctly. Video Captioning\nPrompt:\nHere are some pictures from a video. Could you explain \nwhat the video is describing?\nGPT-4V:\nTheseimagesdepictfantastical cityscapes, likelyfrom\namovieoraseries.Theintricate architecture, the\nflowing waterfalls, andthetowering structures giveit\namajestic andotherworldly vibe.Thesilhouette ofa\npersoninthesecondimageaddstothesenseofscale\nandwonder.\nFigure 16: GPT-4V describes the image content well but struggles to generate the caption for the whole video.\nIncreasing the number of the sampled frames could be helpful.\nAudio Classification (1)\nPrompt:\nThis image is the melspectrum diagram of a piece of audio. Please determine the source of \nthis audio.\nA. Dog    B. Rooster    C. Pig    D. Cow.\nGPT-4V:\nTheimageisaMelspectrogram, whichrepresents thedistribution offrequencies in\nanaudiosignalovertime.Bylooking attheimagealone,Icannotdefinitively\nidentify thesourceoftheaudio.However, basedoncommonaudiopatterns :Arooster\ntypically hasasharpanddistinctive callthatmightshowapronounced and\nrelatively shortpattern inaspectrogram .Dogscanhaveavariety ofbarks,howls,\nandwhines, withdifferent frequency distributions andlengths.Pigsoftenproduce\ngruntsandsqueals.Cowstypically moo,whichcanbelongerinduration andmight\nshowamorespread-outpattern inthespectrogram .Without listening totheactual\naudio,Ican'tprovide adefinitive answer.Ifyou'relooking foraguessbasedon\nthespectrogram's visual representation andthechoices provided, Iwould lean\ntowards B.Rooster due tothe sharp and pronounced pattern inthe spectrogram .\nHowever, thisisjustaneducated guess.\nFigure 17: The ground truth is \u201cDog\u201d while GPT-4V wrongly recognize it as \u201cRooster\u201d.\nwhile GPT-4V wrongly recognizes it as \u201cRooster\u201d.\nWe construct 20 test instances covering 5 majorcategories including Animals, Natural soundscapes\n& water sounds, Human(non-speech sounds), In- Audio Classification (2)\nPrompt:\nThese four images are melspectrograms of four audio clips. Match the four sounds to the four \noptions below\na: Pig  b. Cow  c. Dog  d. Rooster\nGPT-4V:\nMatching audio clips totheir visual\nrepresentations asmelspectrograms canbe\nchallenging, especially without hearing theactual\nsound.However, basedontypical patterns, I'll\nmakeaneducated guess:1.Thefirstspectrogram\nshowsmultiple harmonic lines,whichmightindicate\nsomesortofvocalization witharepetitive pattern.\nThiscouldpotentially represent thecrowing of**d.\nRooster** .2.Thesecond spectrogram ismore\nconsistent withfewervariations, whichmight\nindicate aconstant soundorbackground noise.This\nmightrepresent thegrunting orambient soundsof\n**a.Pig**.3.Thethirdspectrogram hasamore\nirregular pattern, possibly indicating barking or\notherintermittent sounds.Thismightcorrespond to\n**c.Dog**.4.Thefourthspectrogram haspatterns\nthatsuggest amixture ofconsistent sounds with\nsomeharmonic overtones .Thiscouldbeindicative\nofthemooingof**b.Cow**.Pleasenotethatthese\narespeculative matches basedonthevisual\nrepresentation andmightnotbeaccurate .Listening\ntotheactual audioclipswouldprovide amore\naccurate match.\nFigure 18: GPT-4V fails to match the given four audios to their labels. The ground truth is \u201c1. Dog 2. Rooster 3.\nPig 4. Cow\u201d\nterior/domestic sounds, and Exterior/urban noises\nbased on ESC. The result is that GPT-4V success-\nfully recognizes 5 out of 20 instances, which is\nthe same as the random selecting method. As for\nthe second task, GPT-4V successfully matches 2\nout of audios to their labels. We show an example\nfor the second task in Figure 18. GPT-4V fails to\nmatch the given four audios to their labels, which\nindicates that although GPT-4V knows some com-\nmon patterns of sounds, it is still challenging for\nGPT-4V to recognize the audio labels directly from\nthe mel spectrograms.\n6 Conclusion\nIn this paper, we quantitatively study GPT-4V\u2019s per-\nformance on various tasks. According to the results,\nwe find that although GPT-4V achieves high per-\nformance on standard English visual-centric bench-\nmarks, it still can not perform Chinese text recog-\nnition. This observation suggests further in-depth\nevaluation on Chinese benchmarks is necessary for\nmeasure GPT-4V\u2019s capability. We also observe\nthat GPT-4V fails to solve easy math picture puz-zles even though it has strong visual understanding\nability and math problem solving ability. The rea-\nson could be that GPT-4V does not generalize well\nto this domain. Another problem is that GPT-4V\nexhibits inconsistent refusal behavior when answer-\ning questions related to identity and sensitive traits\nsuch as gender, race, and age. This issue could lead\nto an obvious performance drop of GPT-4V and\nshould be dealt with carefully in future studies.\nAs for the limitations, we acknowledge that GPT-\n4V\u2019s performance could be different by adopting\ndifferent prompting methods. For example, more\nspecific instructions and better exemplars will im-\nprove its performance. We would like to explore\nutilizing other advanced prompts such as chain-of-\nthought prompting (Wei et al., 2022) in future work.\nWe also acknowledge that more test instances for\neach task can make the estimated results more ac-\ncurate, but we only sample a part of instances due\nto the high labor cost.\nNevertheless, it is the first attempt to quanti-\ntatively study GPT-4V\u2019s performance on a wide\nrange of tasks. In our study, we reveal the strengths and limitations of GPT-4V . We hope our study can\nprovide insights into future research and applica-\ntion.\nReferences\nHarsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen,\nRishabh Jain, Mark Johnson, Dhruv Batra, Devi\nParikh, Stefan Lee, and Peter Anderson. 2019. No-\ncaps: Novel object captioning at scale. In Proceed-\nings of the IEEE/CVF international conference on\ncomputer vision , pages 8948\u20138957.\nPeter Anderson, Basura Fernando, Mark Johnson, and\nStephen Gould. 2016. Spice: Semantic proposi-\ntional image caption evaluation. In Computer Vision\u2013\nECCV 2016: 14th European Conference, Amsterdam,\nThe Netherlands, October 11-14, 2016, Proceedings,\nPart V 14 , pages 382\u2013398. Springer.\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,\nSinan Tan, Peng Wang, Junyang Lin, Chang Zhou,\nand Jingren Zhou. 2023. Qwen-vl: A frontier large\nvision-language model with versatile abilities. arXiv\npreprint arXiv:2308.12966 .\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training verifiers to solve math\nword problems. arXiv preprint arXiv:2110.14168 .\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiy-\nong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and\nZhifang Sui. 2022. A survey for in-context learning.\narXiv preprint arXiv:2301.00234 .\nChaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,\nMengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jin-\nrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Ron-\ngrong Ji. 2023. Mme: A comprehensive evaluation\nbenchmark for multimodal large language models.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. 2016. Making the v in vqa\nmatter: Elevating the role of image understanding in\nvisual question answering. International Journal of\nComputer Vision , 127:398 \u2013 414.\nDanna Gurari, Qing Li, Abigale J Stangl, Anhong Guo,\nChi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P\nBigham. 2018. Vizwiz grand challenge: Answering\nvisual questions from blind people. In Proceedings of\nthe IEEE conference on computer vision and pattern\nrecognition , pages 3608\u20133617.\nDrew A Hudson and Christopher D Manning. 2019.\nGqa: A new dataset for real-world visual reasoning\nand compositional question answering. In Proceed-\nings of the IEEE/CVF conference on computer vision\nand pattern recognition , pages 6700\u20136709.\nXinyu Jia, Chuang Zhu, Minzhen Li, Wenqi Tang, and\nWenli Zhou. 2021. Llvip: A visible-infrared paireddataset for low-light vision. In Proceedings of the\nIEEE/CVF international conference on computer vi-\nsion, pages 3496\u20133504.\nRuibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu,\nSoroush V osoughi, Claire Cui, Denny Zhou, and An-\ndrew M. Dai. 2023a. Mind\u2019s eye: Grounded language\nmodel reasoning through simulation. In The Eleventh\nInternational Conference on Learning Representa-\ntions .\nYuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li,\nSongyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi\nWang, Conghui He, Ziwei Liu, Kai Chen, and Dahua\nLin. 2023b. Mmbench: Is your multi-modal model\nan all-around player?\nPan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-\nWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\nClark, and Ashwin Kalyan. 2022. Learn to explain:\nMultimodal reasoning via thought chains for science\nquestion answering. Advances in Neural Information\nProcessing Systems , 35:2507\u20132521.\nKenneth Marino, Mohammad Rastegari, Ali Farhadi,\nand Roozbeh Mottaghi. 2019. Ok-vqa: A visual ques-\ntion answering benchmark requiring external knowl-\nedge. In Proceedings of the IEEE/cvf conference\non computer vision and pattern recognition , pages\n3195\u20133204.\nOpenAI. 2023. Gpt-4 technical report.\nKarol J. Piczak. 2015. Esc: Dataset for environmental\nsound classification. In Proceedings of the 23rd ACM\nInternational Conference on Multimedia , MM \u201915,\npage 1015\u20131018, New York, NY , USA. Association\nfor Computing Machinery.\nShuran Song, Samuel P Lichtenberg, and Jianxiong\nXiao. 2015. Sun rgb-d: A rgb-d scene understanding\nbenchmark suite. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition ,\npages 567\u2013576.\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi\nParikh. 2015. Cider: Consensus-based image de-\nscription evaluation. In Proceedings of the IEEE\nconference on computer vision and pattern recogni-\ntion, pages 4566\u20134575.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems , 35:24824\u201324837.\nYang Wu, Zhenyu Zhang, Pai Peng, Yanyan Zhao, and\nBing Qin. 2022. Leveraging multi-modal interac-\ntions among the intermediate representations of deep\ntransformers for emotion recognition. In Proceed-\nings of the 3rd International on Multimodal Senti-\nment Analysis Workshop and Challenge , MuSe\u2019 22,\npage 101\u2013109, New York, NY , USA. Association for\nComputing Machinery. Jun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. Msr-\nvtt: A large video description dataset for bridging\nvideo and language. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition ,\npages 5288\u20135296.\nZhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang,\nChung-Ching Lin, Zicheng Liu, and Lijuan Wang.\n2023. The dawn of lmms: Preliminary explorations\nwith gpt-4v(ision).\nPeter Young, Alice Lai, Micah Hodosh, and Julia Hock-\nenmaier. 2014. From image descriptions to visual\ndenotations: New similarity metrics for semantic in-\nference over event descriptions. Transactions of the\nAssociation for Computational Linguistics , 2:67\u201378.\nChenyu Zhang, Benjamin Van Durme, Zhuowan Li,\nand Elias Stengel-Eskin. 2022. Visual commonsense\nin pretrained unimodal and multimodal models. In\nProceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 5321\u20135335. "}