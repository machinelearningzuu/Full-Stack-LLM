{"url": "http://arxiv.org/pdf/2305.02499v1", "title": "AutoML-GPT: Automatic Machine Learning with GPT", "text": "AutoML-GPT: Automatic Machine Learning with GPT\nShujian Zhang Chengyue Gong Lemeng Wu Xingchao Liu\nMingyuan Zhou\nThe University of Texas at Austin\n{szhang19, mzhou}@utexas.edu\nAbstract\nAI tasks encompass a wide range of domains and \ufb01elds. While numerous AI models have been designed\nfor speci\ufb01c tasks and applications, they often require considerable human efforts in \ufb01nding the right model\narchitecture, optimization algorithm, and hyperparameters. Recent advances in large language models\n(LLMs) like ChatGPT show remarkable capabilities in various aspects of reasoning, comprehension,\nand interaction. Consequently, we propose developing task-oriented prompts and automatically utilizing\nLLMs to automate the training pipeline. To implement this concept, we present the AutoML-GPT,\nwhich employs GPT as the bridge to diverse AI models and dynamically trains models with optimized\nhyperparameters. AutoML-GPT dynamically takes user requests from the model and data cards and\ncomposes the corresponding prompt paragraph. Ultimately, with this prompt paragraph, AutoML-GPT\nwill automatically conduct the experiments from data processing to model architecture, hyperparameter\ntuning, and predicted training log. By leveraging AutoML-GPT\u2019s robust language capabilities and the\navailable AI models, AutoML-GPT can tackle numerous intricate AI tasks across various tasks and datasets.\nThis approach achieves remarkable results in computer vision, natural language processing, and other\nchallenging areas. Extensive experiments and ablation studies demonstrate that our method can be general,\neffective, and bene\ufb01cial for many AI tasks.\n1 Introduction\nArti\ufb01cial intelligence (AI) has experienced signi\ufb01cant advancements recently. Among these developments,\nChatGPT [OpenAI, 2023] has particularly stood out due to its ability to reason, comprehend, and interact\n[Wu et al., 2023]. The ability to execute new tasks based on instructions is a crucial step towards achieving\narti\ufb01cial general intelligence, and the remarkable capabilities of large language models (LLMs) have spurred\nnumerous emerging research topics, such as in-context learning [Ram et al., 2023; Xie et al., 2021], chain-of-\nthought prompting [Pilault et al., 2023; Wei et al., 2022b], retrieve and read [Izacard and Grave, 2020; Zhang\net al., 2021, 2022], and GPT-based intelligent systems [Zheng et al., 2023]. These areas aim to explore the\nvast potential of LLMs and present boundless opportunities for constructing sophisticated AI systems.\nLLMs, such as GPT-4 [Brown et al., 2020; OpenAI, 2023], LLaMA [Touvron et al., 2023], Flan-T5\n[Chung et al., 2022], and PaLM [Chowdhery et al., 2022], have demonstrated a deep comprehension of\nnatural language and the capacity to produce coherent, contextually appropriate responses. This progress has\nopened up new potential applications for challenging tasks involving different domain data, such as image\nand text processing, as well as the incorporation of domain-speci\ufb01c knowledge. In this context, LLMs play a\ncrucial role, as their capacity to comprehend and produce natural language allows AI to better understand\nand tackle a wide range of challenges.\nIn this paper, we aim to develop an Automatic Machine Learning (AutoML) system called AutoML-GPT,\nwhich utilizes LLMs to automatically train the models on datasets with user inputs and descriptions. The\nLLMs are employed as an automatic training system to establish connections with versatile models and\nprocess the inputs. We suggest using language as a universal interface and prompt for LLMs to interact\n1arXiv:2305.02499v1  [cs.CL]  4 May 2023 with users. By incorporating both data and model descriptions into prompts, LLMs can manage AI models\nfor data processing, model architecture design, and hyperparameter tuning. They can invoke these models\nas needed to tackle AI tasks and return the predicted training log. However, incorporating multiple AI\nmodels into LLMs demands a substantial number of high-quality model descriptions. To overcome this\nchallenge, we recommend tapping into both model card [Mitchell et al., 2019] that provides well-de\ufb01ned\nmodel descriptions and data card [Gebru et al., 2021] for speci\ufb01c AI tasks. This approach would enable us\nto connect diverse models through a language-based interface, thus facilitating the solution of complex AI\ntasks. It can also enhance the transferability among models and datasets by capturing their similarity.\nAutoML-GPT connects versatile machine learning models, training pipelines, and datasets to solve\nnumerous complex AI tasks. More speci\ufb01cally, for each AI task we aim to solve, using its corresponding\ndescription (such as model card and data card ), we fuse the paragraph as the prompt into a pretrained LLMs\n(such as ChatGPT) to establish the AutoML pipeline. Afterward, in our system, LLMs perform the automatic\ntraining to return the predicted training logs for the input questions of users. Based on these training logs, we\ncan further interact with the LLM to solve requests (such as hyperparameter tuning) shown in Figure 1. Thus,\nthe whole process of AutoML-GPT can be divided into four stages: 1) data processing, 2) model architecture\ndesign, 3) hyper-parameter tuning with the predicted training log, 4) human feedback on experimental data.\nBene\ufb01ting from such a design, AutoML-GPT in Figure 1 is able to use external models and thus can\nhandle multiple tasks on well-known benchmarks, and transfer the knowledge to unknown private dataset\nwhen only given metadata (data card). Furthermore, this pipeline also allows AutoML-GPT to continue\nabsorbing the powers from task-speci\ufb01c experts, enabling growable and scalable AI capabilities. In summary,\nour contributions are as follows:\n\u2022To complement the advantages of large language models and expert models, we propose AutoML-GPT,\nwhich acts as the system for data processing and model architecture design and automatically conducts\nthe experiments for each speci\ufb01c task.\n\u2022By integrating the model card with model descriptions and the data card with data descriptions, we\nprovide a \ufb01xed-format prompt paragraph and build a training pipeline to tackle general AI tasks.\n\u2022Extensive evaluations on multiple AI tasks across language, vision, and continual learning demonstrate\nthe capability of AutoML-GPT in auto training. It further demonstrates the effectiveness of providing\nthe hyperparameter tuning for an unseen or new dataset.\nData Processing Model ArchitectureHyperparameter \nTuning Predicted Training \nLogData Card Model CardEval Metric & \nAddInput \nParagraph\nFigure 1: Overview of AutoML-GPT. Some notations are labeled along with corresponding components.\n\u2018Eval Metrics & Add\u2019 refers to the evaluation metrics and additional requests.\n2 AutoML-GPT\nAutoML-GPT is a collaborative system that relies on the data and model information to format the prompt\ninput paragraph. The LLM serves as the controller, while numerous expert models as collaborative executors.\n2 The work\ufb02ow of AutoML-GPT consists of four stages: data processing, model architecture design, hyper-\nparameter tuning, and training log generation. Speci\ufb01cally, we suggest a general recipe for AutoML-GPT:\n1) generate a \ufb01xed-format prompt paragraph with both the model card and data card, 2) build the training\npipeline and process the user request on the selected dataset and model architectures, 3) generate the\nperformance training log and tune the hyperparameters, and 4) tune the model with the auto-suggested\nhyperparameters.\n2.1 Input Decomposition\nIn the \ufb01rst stage of AutoML-GPT, an LLM takes the input from the users. To boost the performance of the\nLLM and generate an effective prompt, we employ speci\ufb01c instructions for the input prompt. The instructions\ncontain three parts described below.\nData Card To clarify the intended use cases of datasets and minimize their usage in contexts for which\nthey are not well suited, we utilize the data card that provides comprehensive documentation for this dataset.\nAs shown in Figure 2, the key components of the data card are comprised of the dataset name, input dataset\ntype ( e.g., image data or text data), label space ( e.g., the class types or resolution), and default evaluation\nmetrics.\nFigure 2: The Data Card includes the data name, input data type, label space, and evaluation metric. Within\nthe data card, the same color denotes information originating from a single dataset.\nModel Card The model cards in Figure 3, complementary to the \u201cData Card\u201d discussed earlier, serve\nas one of the proposed paradigms that report details of the model used to train and test the datasets. The\nmodel card consists of the model name, model structure ( e.g., Swin transformer [Liu et al., 2021] with\na UperNet [Xiao et al., 2018] head), model descriptions, and architecture hyperparameter. By providing\nthis information, model cards inform the LLM about the machine learning systems used and the degree\nof \ufb02exibility the user would like to have on the model architecture. It would further create more inclusive\noutcomes with the LLM.\nFigure 3: The Model Card comprises model name, model structure, model descriptions, and architecture\nhyperparameters. In the model card, the same color represents information from a single model card.\n3 Evaluation Metrics and Additional Requests In addition to the model cards and data cards, users can\nhave the option to request more evaluation benchmarks, metrics, or any constraints. Except for the default\nevaluation metrics, we can add speci\ufb01c metrics or constraints according to the user\u2019s request when selecting\nthe model architecture. For example, given a constraint \u201cthe inference time smaller than 10 FPS,\u201d we then\nprocess the user requests under the evaluation metrics and constraints. Bene\ufb01ting from this instruction and\nhuman feedback of these evaluation metrics and additional requests, the LLM can follow instructions better.\nAutoML-GPT provides these task speci\ufb01cations to the LLM as high-level instructions for analyzing the\nuser\u2019s requests accordingly.\n2.2 Data Processing\nData processing is an integral step in machine learning as the quality of data and the derived useful information\ndirectly affect the ability of our model to learn. It is thus crucial that we process the data before feeding\nit into our model. For example, in computer vision, data processing refers to the set of techniques and\nmethods used to prepare raw image data for analysis or machine learning algorithms. This can include\nimage resizing, normalization, augmentation, and \ufb01ltering. Similarly, in Natural Language Processing (NLP)\nprojects, data processing refers to transforming raw text data into a structured and clean format that machine\nlearning algorithms can easily understand and process. Techniques such as tokenization, stopword removal,\nlowercasing, and removal of special characters and numbers are commonly used. Based on the provided data\ncard and data descriptions, AutoML-GPT provides speci\ufb01c process techniques depending on the project\u2019s\nrequirements and the data\u2019s nature.\n2.3 Model Architecture\nUpon processing the list of tasks, AutoML-GPT needs to match each task with a corresponding model,\nessentially selecting the suitable model for every task in the list. To achieve this, we \ufb01rst acquire model\ncards and descriptions of the models from the user inputs. Following that, we dynamically assign models to\ntasks using the in-context task-model assignment mechanism. This approach enables incremental model\naccess and offers greater openness and \ufb02exibility by combining the providing model descriptions and a better\nunderstanding of the user requests.\nModel architectures refer to detailed explanations of a machine learning model\u2019s design, structure, and\ncomponents. These descriptions typically include the following elements: input and output layers, hidden\nlayers, activation functions, loss functions, and model-speci\ufb01c components (such as attention mechanisms,\nconvolutional layers, or recurrent layers).\n2.4 Hyperparameter Tuning with Predicted Training Log\nTo \ufb01nd the optimal set of hyperparameters that yield the best performance for a given model on a speci\ufb01c\ndataset, hyperparameter tuning is a crucial step in machine learning. Hyperparameters are con\ufb01guration\nsettings that are not learned during the training process but are prede\ufb01ned and control various aspects of\nthe model\u2019s learning behavior. Examples of common hyperparameters include the learning rate, batch size,\nnumber of hidden layers, and number of neurons per layer.\nIn order to tune hyper-parameters without training on real machines, we predict the performance by\ngenerating a training log for a given hyper-parameter setting for the provided data card and model card.\nAutoML-GPT will automatically conduct the training and return the training log. The training log of model\nperformance on a dataset records various metrics and information collected during the training process. It\nhelps in understanding the model\u2019s progress, identifying potential issues, and evaluating the effectiveness of\nthe chosen architecture, hyperparameters, and optimization techniques. A typical training log includes the\nepoch numbers with training and validation metrics. By examining the training log, we can form a basic\nunderstanding of the model performance according to the user feedback.\n4 Label SpaceNew dataset: object dataset \u2026 ImageInput Data Type Data Name\nShifted window with a 2 -layer MLP \u2026 Window size: 7 \u2026Model Des Architecture Hyperparameter\nObject Categories: 10 classes \u2026 Box APEvalSwin Transformer Multi -head self attention with \u2026Model Name Model StructureData Card Model Card\nAutoML -GPT Prompt Paragraph \nAssume we have { theset of Data Cards with similarity : New dataset with 5 image classes \u2026}, we adopt the { the corresponding set of Model \nCards with model parameters : slided window swin transformer with \u2026} as the model. \nHere is antraining log for a ViT model trained on New dataset using the suggested hyperparameters:\nEpoch: [0][ 0/25]       Time  1.015 ( 1.015)    Data  0.353 ( 0.353)    Loss 0.4065e+00  Acc@1  97.75 ( 97.75)   Acc@5 100.00 (100.00)\nEpoch: [0][10/25]       Time  0.511 ( 0.583)    Data  0.000 ( 0.032)    Loss 0.3827e+00 Acc@1  93.75 ( 98.86)   Acc@5 100.00 (100.00) \n...Predicted Training LogSimilarityModel Parameters\nAuto\nML \nGPTText EncoderFigure 4: Overview of AutoML-GPT for the unseen dataset: the top block showcases data card and model\ninformation. We \ufb01rst log the training information for several datasets. The data cards for these datasets are\nprocessed through a text encoder to obtain similarity scores, which are then combined with model parameters\nof corresponding trained models to form the AutoML-GPT prompt paragraph. The bottom block presents\nthe predicted training log based on the recommended hyperparameter settings for the unseen dataset.\nUnseen Datasets The hyperparameter tuning for unseen private datasets could be even more challenging.\nGiven the metadata of an unseen dataset, AutoML-GPT can recommend a hyperparameter con\ufb01guration that\nis likely to be effective for that dataset. We rely on the data card to leverage the necessary text descriptions\nand identify the correlation between the unseen dataset and the existing ones. Based on the correlation, we\ntransfer the hyper-parameter settings from the existing datasets to the new unseen dataset.\nTo calculate the correlation, we use a text encoder to encode the data card. Speci\ufb01cally, in the data\ncard, it contains information such as class type, resolution, image size, and other relevant metadata. We\ntake the dataset scale, task description, label space, and input/output data type as the input to a text encoder\n(e.g., CLIP [Radford et al., 2021]) and describe the correlation between this unseen dataset and the existing\ndatasets using the similarity score of the encoded latent representation.\n3 Experiments\nWe assess the performance of our AutoML-GPT and implement it using ChatGPT (OpenAI\u2019s \u201cGPT-4\u201d\nversion)1. Various case studies are carried out to showcase the ef\ufb01cacy of our approach from multiple angles.\n3.1 Unseen Dataset\nIn Figure 4, we present the results of training on an unseen dataset using AutoML-GPT. To verify the\nperformance in real cases, we construct a set of performance and hyper-parameters on already trained\ndatasets, and some coming untrained datasets. We will predict hyperparameter con\ufb01gurations for these\nuntrained datasets. We make our test environment based on the classi\ufb01cation setting described in Vinyals\net al. [2016]. We also follow the MiniImageNet [Vinyals et al., 2016] to subsample and split the training\ndataset [Deng et al., 2009] into 80% and 20% portions. From the 80% data, we construct the data cards and\ncorresponding model cards (containing model best hyperparameters). We randomly select \ufb01fteen classes to\ncreate various subset datasets ( e.g., dataset A, B, etc.), grid search the hyper-parameters, \ufb01netune the ViT\nbase model [Dosovitskiy et al., 2020] and log the best performance on these subset datasets. We then create\na new dataset called \u201cNew\u201d with ten image classes from the remaining 20% data.\n1https://platform.openai.com/\n5 Label SpaceCOCO Data: large scale object \u2026 ImageInput Data Type Data Name\nShifted window with a 2 -layer MLP \u2026 Window size: 7 \u2026Model Des Architecture Hyperparameter\nObject Categories: person \u2026 Box APEvalSwin Transformer Multi -head self attention with \u2026Model Name Model StructureData Card Model Card\nAutoML -GPT Prompt Paragraph \nAssume we have a { Data Card : COCO dataset with 328K images and evaluation metric \u2026}, we adopt the { Model Card : slided window swin transformer \u2026} as \nthe model. We use { A: Data Processing } and { B: Model Architecture } script for processing COCO dataset. We use { C: Hyperparameter } for the hyper -\nparameters on { Resource: 4 Nvidia a5000 GPU }. Then we would have the training log { D: Predicted Training Log }. \nHere isthepython script weusefordata preprocessing forswin transformer -based object\ndection onCOCO :Forobject detection onCOCO dataset using aslided window approach with transformer -based\nmodels, DETR hasatransformer encoder anddecoder, anditisdesigned todirectly output aset\nofobject queries andtheir corresponding bounding box predictions inasingle forward pass.\nThis eliminates theneed foranchor -based oranchor -freedetection methods andachieves state-\nof-the-artperformance onCOCO object detection \u2026\nWe use your suggested model card and data card. The hyperparameter of DETR model on \nCOCO dataset for object detection is\n\u2022 Learning rate: 0.0001\n\u2022 Weight decay: 1e -4\n\u2022 Optimizer: AdamW\n\u2022 Scheduler: MultiStepLR with warmup \u2026Here isantraining logforaDETR model with Swin Transformer Base backbone trained on\nCOCO dataset using thesuggested hyperparameters :import torch\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import CocoDetection\nfrom torchvision import transforms\nfrom models.detr import DETR\n# set up data loaders\ntrain_transforms = transforms.Compose([\ntransforms.Resize ((800, 800)),\ntransforms.RandomHorizontalFlip (),\ntransforms.ToTensor (),\ntransforms.Normalize ([0.485 , 0.456 , 0.406 ], [0.229 , 0.224 , 0.225 ]) \u2026 \nEpoch: [1]  [  0/1000]  eta: 1 day, 6:54:48  lr: 0.000100  loss: 4.8577 (4.8577)  loss_ce: 3.9479 (3.9479)  loss_bbox: 0.0363 (0.0363)  loss_giou: \n0.8735 (0.8735)  loss_cls: 0.0000 (0.0000)  acc: 0.0000 (0.0000)  loss_obj: 0.0000 (0.0000)  loss_rpn_box: 0.0000 (0.0000)  t ime: 25.7651  data: \n3.7903  max mem: 1801\nEpoch: [1]  [ 10/1000]  eta: 4:51:08  lr: 0.000100  loss: 4.4567 (4.5028)  loss_ce: 3.3551 (3.5238)  loss_bbox: 0.0429 (0.039 8) loss_giou: \n0.9994 (0.9398)  loss_cls: 0.0000 (0.0000)  acc: 0.0000 (0.0000)  loss_obj: 0.0000 (0.0000)  loss_rpn_box: 0.0000 (0.0000)  t ime: 14.7129  data: \n0.5279  max mem: 2147\n\u2026AutoML -\nGPT\nHyperparameter TuningModel Architecture Data Processing\nPredicted Training Log\nFigure 5: Overview of AutoML-GPT for object detection: The top block displays the data card and model\ncard. The middle block showcases the AutoML-GPT prompt paragraph, derived from the data card and\nmodel card. The bottom block outlines the four steps: data processing, model architecture, hyperparameter\ntuning, and predicted training log. We use the predicted training log to tune the hyperparameters before\nfeedbacking the hyperparameters to the users.\nTo demonstrate the capabilities of our approach on unseen datasets, we utilize AutoML-GPT to recom-\nmend the best training con\ufb01guration for the \u201cNew\u201d dataset based on the provided data card and model card.\nIn our data card, we log the label space, i:e:, text descriptions for each class. In practice, we incorporate a\nsimilarity score between two data cards by passing the text in the data card through a text encoder, e.g., the\nCLIP text encoder, and calculating the similarity. Speci\ufb01cally, in Figure 4, we state that the \u201cNew\u201d dataset has\na 60% label space similarity to dataset A and a 40% label space similarity to dataset B. Using this information\nand the hyper-parameter settings in the data cards for dataset A and B, AutoML-GPT can recommend the\nappropriate hyperparameter settings for training on the \u201cNew\u201d dataset. In our experiments, we achieve\n98% accuracy for the Top 1 prediction, compared to 80% Top 1 accuracy with average random-selected\nhyperparameters. Moreover, we also initialize the model using the suggested hyperparameter settings from\nAutoML-GPT without giving any additional datasets With this con\ufb01guration, we achieve 82% Top 1 accuracy,\nwhich is better than the average randomly-selected hyperparameters but not as good as our recommended\nsetting. It also suggests that ChatGPT can give good hyperparameter settings for a speci\ufb01c task ( e:g:, image\nclassi\ufb01cation). This demonstrates the effectiveness of our proposed auto-training approach in addressing\nmachine learning problems, even with unseen or new datasets. These \ufb01ndings highlight the potential of our\nauto-training method to enhance machine learning by providing accurate hyperparameter recommendations.\n3.2 Object Detection\nFigure 5 presents our results on the COCO dataset [Lin et al., 2014] for object detection. \u00c0The top block\ndisplays the data card for the COCO dataset and the model card for ImageNet, based on user input. The\nmiddle block demonstrates the AutoML-GPT Prompt Paragraph derived from the input decomposition. The\ninformation from the data card and model card is automatically incorporated into our prompt format. We\nreport the results for data processing, model architecture design, hyperparameter tuning, and training log\ngeneration. \u00c1In data processing, AutoML-GPT generates a script for handling the input dataset. We also\nprovide a Python script example in Figure 5. For model architecture design, our pipeline generates a model\ncomposition for subsequent training. Once both the data and model are prepared, the detailed con\ufb01gurations\nare provided in the hyperparameter-tuning stage ( e:g:, learning rate: 10\u00004, weight decay: 10\u00004) and are\nfurther tuned with predicted training logs. \u00c2These results further validate that our method can serve as\n6 Label SpaceNatural Questions: open domain \u2026 TextInput Data Type Data Name\nDense retriever is bi -encoder \u2026. Encoder sequence length: 350 \u2026Model Des Architecture Hyperparameter\nWikipedia that may or may not \u2026 Exact match\u2026EvalDPR Question encoder \u2026Model Name Model StructureData Card Model Card\nAutoML -GPT Prompt Paragraph \nAssume we have a { Data Card : natural question is open \u2026}, we adopt the { Model Card : DPR with the dense retriever which is biencoder\u2026} as the model. We \nuse { A: Data Processing } and { B: Model Architecture } script for processing natural questions answering dataset. We use { C: Hyperparameter } for the hyper -\nparameters on { Resource: 8 Nvidia v100 GPU }. Then we would have the training log { D: Predicted Training Log }. We also have an { Additional Request: the \nfaster inference time for DPR retriever }.\nHere's aPython script fordata preprocessing ofDPR onNQdataset foropen domain question\nanswering :The model architecture forDPR (Dense Passage Retrieval) ontheNatural Questions (NQ)\ndataset foropen domain question answering involves twocomponents :Retriever \u2026\nWe use your suggested model card and data card. The hyperparameter of DPR model on the \nNatural Questions (NQ) dataset is:\nRetriever Training Hyperparameters:\n\u2022batch_size: 128\n\u2022learning_rate: 1e -5\n\u2022max_epochs: 40\n\u2022warmup_steps: 1000 \n\u2022drop_out: 0.1 \u2026Here isantraining logforaDPR model with theretriever trained onNQdataset using the\nsuggested hyperparameters :import json\nimport random\ndef load_data(file_path):\nwith open (file_path, 'r') as f:\ndata = json.load(f)\nreturn data\ndef prepare_data(data):\nprocessed_data = [] \u2026Training Log:\nEpoch 1:\nIteration 100-Loss :6.7532\nIteration 200-Loss :4.4215\nIteration 300-Loss :3.6890 \u2026AutoML -\nGPTHyperparameter TuningModel Architecture Data Processing\nPredicted Training Log\nToachieve faster inference time without sacrificing toomuch performance, youcanconsider\nadjusting certain \u2026Here's asuggested configuration fortraining themodel :\nbatch_size :128\nlearning_rate :1e-5\nmax_epochs :40\n#Additional hyperparameters forfaster inference\nmodel_dimension :256 #Reduce model dimensionality (default is768forBERT -based DPR)\nmax_sequence_length :128 #Limit theinput sequence length (default is512)\u2026Additional Requests ( Yes): \nfaster inference time for DPR retriever \nFigure 6: Overview of AutoML-GPT for question answering: The top block presents data card and model\ninformation, while the middle block highlights the AutoML-GPT prompt paragraph, derived from both\ndata card and model card. The bottom block details the four steps: data processing, model architecture,\nhyperparameter tuning, and predicted training log.\nan effective pipeline for \ufb02exibly adapting LLMs to downstream tasks. Our approach, which employs data\nand model cards to derive the AutoML-GPT prompt paragraph, can also be considered as a complementary\nmodule for works focused on enhancing LLM prompt components.\n3.3 Question Answering\nWe present the experimental results on the Natural Questions Open dataset [Kwiatkowski et al., 2019] in\nFigure 6. We utilize Dense Passage Retrieval (DPR) [Karpukhin et al., 2020]. \u00acFor the data card, users\ninput the data name, input data type, label space, and evaluation metrics. \u00adFor the model card, it includes\nmodel name, model structure, model descriptions, and architecture hyperparameters. \u00aeWith the generated\nAutoML-GPT prompt paragraph, AutoML-GPT carries out data processing, model architecture creation,\nhyperparameter tuning, and generates a predicted training log. As seen in the \u201cHyperparameter Tuning,\u201d the\nhyperparameters generated by AutoML-GPT and those provided by DPR align closely, e:g:, the learning\nrate is 10\u00005and max epochs is 40. \u00afOnce the predicted training log is available, we showcase a scenario\nwhere the user can ask AutoML-GPT for different evaluation metrics or model architectures based on their\nrequirements, as illustrated in Figure 6 \u201cAdditional requests: fast inference time for DPR retriever.\u201d As\nseen in the returned response in Figure 6, AutoML-GPT also offers hints such as \u201cwithout sacri\ufb01cing too\nmuch performance.\u201d AutoML-GPT further tunes the hyper-parameters based on these requests and predicted\nlogs. Our method demonstrates the powerful ability to automatically conduct experiments and perform\ninteractive hyperparameter tuning. It further con\ufb01rms that our approach works well for various datasets and\ncan generalize across different input types and domains.\n3.4 Classi\ufb01cation\nWe also evaluate AutoML-GPT on the UCI Adult dataset [Dua and Graff, 2017] using XGBoost. As before,\nwe supply the data card and model card to generate the input prompt paragraph. The same training pipeline is\napplied here, as shown in Figure 7. We also adhere to the hyperparameter settings suggested by AutoML-GPT\nand train the XGBoost model. This training results in a \ufb01nal validation loss of 0.277 with 85.92% accuracy.\nDespite the different inputs and tasks, our proposed AutoML-GPT consistently delivers strong performance\nin classi\ufb01cation. This further demonstrates that AutoML-GPT can be employed for a wide range of machine\n7 Label SpaceUCI Adult income dataset \u2026 Categorical, Int \u2026 Input Data Type Data Name\nAn optimized distributed \u2026 Booster: tree -based methods \u2026Model Des Architecture Hyperparameter\nClassification AUC\u2026EvalXGBoost It is an ensemble learning \u2026Model Name Model StructureData Card Model Card\nAutoML -GPT Prompt Paragraph \nAssume we have a { Data Card : UCI adut income dataset is\u2026}, we adopt the { Model Card : XGBoost is an ensemble learning\u2026} as the model. We use { A: Data \nProcessing } and { B: Model Architecture } script for processing UCI ault income dataset. We use { C: Hyperparameter } for the hyper -parameters on { Resource: \nAMD EPYC 7702P 64 -Core Processor }. Then we would have the training log { D: Predicted Training Log }.  \nHere's aPython script fordata preprocessing ofXGBoost onUCI adult dataset :XGBoost (eXtreme Gradient Boosting) isanensemble method that utilizes decision trees as\nbase learners .The model architecture ofXGBoost consists ofmultiple decision trees, where\neach treeisadded iteratively tocorrect theerrors oftheprevious trees intheensemble .The\nfinal prediction ismade based onthesum ofthepredictions ofalltrees intheensemble \u2026\nWe use your suggested model card and data card. The hyperparameter of XGBoost model on the UCI adult dataset is:\nxgb_params = {\n'objective': 'binary:logistic',  # Binary classification problem\n'n_estimators': 100,             # Number of trees in the ensemble\n'max_depth': 6,                  # Maximum depth of each tree\n'learning_rate': 0.3,            # Contribution of each tree to the final \u2026Here isantraining log for aXGBoost onUCI adult dataset using the suggested\nhyperparameters :import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\n# Load the dataset\nurl = \"https://archive.ics.uci.edu/ml/machine -learning -databases/adult/adult.data\"\ncolumn_names = [ 'age' , 'workclass' , 'fnlwgt' , 'education' , 'education -num' , 'marital -status' ,\n'occupation' , 'relationship' , 'race' , 'sex', 'capital -gain' , 'capital -loss',\n'hours -per-week' , 'native -country' , 'income\u2019 ]\u2026\n[0]validation_ 0-logloss :0.55936\n[1]validation_ 0-logloss :0.49738\n[2]validation_ 0-logloss :0.45553\n[3]validation_ 0-logloss :0.42562 \u2026\nHyperparameter TuningModel Architecture Data Processing\nPredicted Training LogAutoML -\nGPT\nFinal Val \nLoss \n0.277Figure 7: Overview of AutoML-GPT for classi\ufb01cation: The top block displays data card and model\ninformation, and the middle block showcases the AutoML-GPT prompt paragraph, derived from both\ndata card and model card. The bottom block outlines the four steps: data processing, model architecture,\nhyperparameter tuning, and predicted training log. Additionally, we include the \ufb01nal validation results,\nfollowing the hyperparameter recommendations from AutoML-GPT and training the model.\nlearning problems across various tasks.\n4 Related Work\nAdvanced Large Language Model LLMs have exhibited robustness and generalizability through zero-\nshot and few-shot learning by having parameter sizes exceeding one hundred billion. Notable examples of\nLLMs include Megatron-turing NLG [Smith et al., 2022] with 530 billion parameters, Gopher [Rae et al.,\n2021] with 280 billion parameters, and PaLM [Chowdhery et al., 2022] with 540 billion parameters. The\nscaling of LLM has unlocked new emergent abilities previously unobserved under smaller models [Wei et al.,\n2022a]. These LLMs have demonstrated the superiority of LLMs for zero-shot learning. Among existing\nLLMs, ChatGPT has unique characteristics. It has the ability to interact with users in a conversation-like\nmanner, while retaining its accumulated knowledge and generalization ability gained from pre-training.\nGoing a step further, we explore the zero-shot learning capability of ChatGPT on different tasks beyond\ndialogue in this work.\nChain of Thought Chain-of-thought (CoT) prompting induces LLMs to generate intermediate reasoning\nsteps before answering [Wei et al., 2022b]. There are two lines of research focusing on the current CoT\nprompting. One line is exploring the manually designed CoT. In the manually designed CoT, LLMs adapt\nthe manually designed features and demonstration for the reasoning process [Wei et al., 2022b]. Wang et al.\n[2022] proposes a new decoding strategy, self-consistency, to replace the naive greedy decoding used in\nchain-of-thought prompting. Recently, Interactive-Chain-Prompting [Pilault et al., 2023] is introduced to\nresolve the ambiguity for crosslingual conditional generation. Another line is conducting research on the\nzero-shot setting, where STaR [Zelikman et al., 2022] is introduced for the self-generation and helps the\nmodel to self-improve, and Automatic Reasoning and Tool-use (ART) [Paranjape et al., 2023] is a framework\nthat uses frozen LLMs to automatically generate intermediate reasoning steps as a program.\nGPT-based Systems GPT [Brown et al., 2020] has shown promising performance improvements. A recent\nline of research has focused on integrating the GPT model into AI systems. HuggingGPT [Shen et al.,\n8 2023] is built with the HuggingFace transformers library and utilizes the GPT as the interaction agent.\nVisualGPT [Wu et al., 2023] incorporates different Visual Foundation Models to enable the user to interact\nwith ChatGPT. OpenAGI [Ge et al., 2023], an open-source AGI research platform, is designed to offer\ncomplex, multi-step tasks and accompany by task-speci\ufb01c datasets. Similarly, we also integrate the GPT\ninto our AutoML pipeline. There is also another GPT based system that can incorporate extra information\nfrom search engines, e:g:, AutoGPT2. AutoML-GPT rethinks the impact of ChatGPT from the auto training\nperspective. We focus on building the training pipeline and establishing an AutoML system from the start to\nend.\n5 Conclusion\nOur work demonstrates the bene\ufb01ts of building AutoML systems upon GPT. The proposed method can\nautomatically conduct machine learning experiments. This automatic learning dramatically improves training\nef\ufb01ciency and enhances the model\u2019s performance. We demonstrate use cases across computer vision,\nnatural questions answering, and classi\ufb01cation benchmarks. We further conduct a detailed use case with\nthe unseen datasets and additional interactions between the user and AutoML-GPT. To summarize, the\nproposed AutoML-GPT is effective and general, with the potential to create a natural language interface for\ntuning machine learning models for various tasks. In the future, we will 1) automatically generate the model\nand data cards for well-known benchmarks and make them a part of our system, and 2) extract task-aware\nsub-networks from large pretrained models with the help of ChatGPT.\nReferences\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing systems , 33:1877\u20131901.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language\nmodeling with pathways. arXiv preprint arXiv:2204.02311 .\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-\ufb01netuned language models. arXiv\npreprint arXiv:2210.11416 .\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale\nhierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition , pages\n248\u2013255. Ieee.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-\nterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is\nworth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 .\nDheeru Dua and Casey Graff. 2017. UCI machine learning repository.\nYingqiang Ge, Wenyue Hua, Jianchao Ji, Juntao Tan, Shuyuan Xu, and Yongfeng Zhang. 2023. Openagi:\nWhen llm meets domain experts. arXiv preprint arXiv:2304.04370 .\nTimnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach,\nHal Daum\u00e9 Iii, and Kate Crawford. 2021. Datasheets for datasets. Communications of the ACM ,\n64(12):86\u201392.\n2https://github.com/Significant-Gravitas/Auto-GPT\n9 Gautier Izacard and Edouard Grave. 2020. Leveraging passage retrieval with generative models for open\ndomain question answering. arXiv preprint arXiv:2007.01282 .\nVladimir Karpukhin, Barlas O \u02d8guz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau\nYih. 2020. Dense passage retrieval for open-domain question answering. Empirical Methods in Natural\nLanguage Processing (EMNLP) .\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red\ufb01eld, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova,\nLlion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural\nQuestions: a benchmark for question answering research. TACL .\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r,\nand C Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV\n2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13 ,\npages 740\u2013755. Springer.\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. 2021. Swin\ntransformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF\ninternational conference on computer vision , pages 10012\u201310022.\nMargaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena\nSpitzer, Inioluwa Deborah Raji, and Timnit Gebru. 2019. Model cards for model reporting. In Proceedings\nof the conference on fairness, accountability, and transparency , pages 220\u2013229.\nOpenAI. 2023. Gpt-4 technical report. arXiv .\nBhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and Marco Tulio\nRibeiro. 2023. Art: Automatic multi-step reasoning and tool-use for large language models. arXiv preprint\narXiv:2303.09014 .\nJonathan Pilault, Xavier Garcia, Arthur Bra\u017einskas, and Orhan Firat. 2023. Interactive-chain-prompting:\nAmbiguity resolution for crosslingual conditional generation with interaction. arXiv preprint\narXiv:2301.10309 .\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models\nfrom natural language supervision. In International conference on machine learning , pages 8748\u20138763.\nPMLR.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446 .\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav\nShoham. 2023. In-context retrieval-augmented language models. arXiv preprint arXiv:2302.00083 .\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023. Hugginggpt:\nSolving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580 .\nShaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper,\nZhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. 2022. Using deepspeed and\nmegatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint\narXiv:2201.11990 .\n10 Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and ef\ufb01cient\nfoundation language models. arXiv preprint arXiv:2302.13971 .\nOriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. 2016. Matching networks for one\nshot learning. Advances in neural information processing systems , 29.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022. Self-consistency\nimproves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171 .\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al. 2022a. Emergent abilities of large language models.\narXiv preprint arXiv:2206.07682 .\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b.\nChain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903 .\nChenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. 2023. Visual\nchatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671 .\nTete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. 2018. Uni\ufb01ed perceptual parsing for\nscene understanding. In Proceedings of the European conference on computer vision (ECCV) , pages\n418\u2013434.\nSang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2021. An explanation of in-context\nlearning as implicit bayesian inference. arXiv preprint arXiv:2111.02080 .\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. Star: Bootstrapping reasoning with\nreasoning. Advances in Neural Information Processing Systems , 35:15476\u201315488.\nShujian Zhang, Chengyue Gong, and Eunsol Choi. 2021. Knowing more about questions can help: Improving\ncalibration in question answering. arXiv preprint arXiv:2106.01494 .\nShujian Zhang, Chengyue Gong, and Xingchao Liu. 2022. Passage-mask: A learnable regularization strategy\nfor retriever-reader models. arXiv preprint arXiv:2211.00915 .\nMingkai Zheng, Xiu Su, Shan You, Fei Wang, Chen Qian, Chang Xu, and Samuel Albanie. 2023. Can gpt-4\nperform neural architecture search? arXiv preprint arXiv:2304.10970 .\n11 "}